(lp0
(icmyPackage
FText
p1
(dp2
S'CEText'
p3
(lp4
VAutomated analysis of such aspects of meaning is important for natural language processing tasks which need to consider the truth value of statements, such as for example text mining [ 9 ] or sentiment analysis [ 5 ]
p5
aVOwing to its immediate utility in the curation of scholarly results, the analysis of negation and so-called hedges in bio-medical research literature has been the focus of several workshops, as well as the Shared Task at the 2011 Conference on Computational Language Learning (CoNLL
p6
aV2 2 Resolving negation scope is a more difficult sub-problem at least in part because (unlike cue and event identification) it is concerned with much larger, non-local and often discontinuous parts of each utterance
p7
aVThis intuition is confirmed by \u005cciteA Rea:Vel:Ovr:12, who report results for each sub-problem using gold-standard inputs; in this setup, scope resolution showed by far the lowest performance levels
p8
aVWhere \u005cciteA Mor:Dae:12 characterize negation as an u'\u005cu2018' u'\u005cu2018' extra-propositional aspect of meaning u'\u005cu2019' u'\u005cu2019' (p.1563), we in fact see it as a core piece of compositionally constructed logical-form representations
p9
aVOur system implements these findings through a notion of functor-argument u'\u005cu2018' crawling u'\u005cu2019' , using as our starting point the underspecified logical-form meaning representations provided by a general-purpose deep parser
p10
aVThey investigated two approaches for scope resolution, both of which were based on syntactic constituents
p11
aVSecondly they trained an SVM ranker over candidate constituents, generated by following the path from a cue to the root of the tree and describing each candidate in terms of syntactic properties along the path and various surface features
p12
aVBoth approaches attempted to handle discontinuous instances by applying two heuristics to the predicted scope a) removing preceding conjuncts from the scope when the cue is in a conjoined phrase and (b) removing sentential adverbs from the scope
p13
aV\u005cciteA Bas:Bos:Eva:12 describe some amount of tailoring of the Boxer lexicon to include more of the Shared Task scope cues among those that produce the negation operator in the DRSs, but otherwise the system appears to directly take the notion of scope of negation from the DRS and project it out to the string, with one caveat
p14
aVAs with the logical-forms representations we use, the DRS logical forms do not include function words as predicates in the semantics
p15
aVSince the Shared Task gold standard annotations included such arguably semantically vacuous (see \u005cciteNP [p.107]Bender:13) words in the scope, further heuristics are needed to repair the string-based annotations coming from the DRS-based system
p16
aV\u005cciteauthor Bas:Bos:Eva:12 resort to counting any words between in-scope tokens which are not themselves cues as in-scope
p17
aVThis simple heuristic raises their F 1 for full scopes from 20.1 to 53.3 on system-predicted cues
p18
aVWe used the grammar together with one of its pre-packaged conditional Maximum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures
p19
aVThus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an u'\u005cu2018' off the shelf u'\u005cu2019' setting
p20
aVWe combine our system with the outputs from the best-performing 2012 submission, the system of \u005cciteA Rea:Vel:Ovr:12, firstly by relying on the latter for system negation cue detection, 4 4 \u005cciteA Rea:Vel:Ovr:12 predicted cues using a closed vocabulary assumption with a supervised classifier to disambiguate instances of cues and secondly as a fall-back in system combination as described in § 3.4 below
p21
aVScopal information in MRS analyses delivered by the ERG fixes the scope of operators u'\u005cu2014' such as negation, modals, scopal adverbs (including subordinating conjunctions like while ), and clause-embedding verbs (e.g., believe ) u'\u005cu2014' based on their position in the constituent structure, while leaving the scope of quantifiers (e.g., a or every , but also other determiners) free
p22
aVIf an EP shares its label with the negation cue, or is a quantifier whose restriction ( \u005csrl RSTR) is \u005csqeq equated with the label of the negation cue, it cannot be in-scope unless its \u005csrl ARG0 is an argument of the negation cue, or the \u005csrl ARG0 of the negation cue is one of its own arguments
p23
aV\u005cSTATE Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctions reached by ARG1 whether , when , because , to , with , although , unless , until , or as
p24
aVThe arguments of one EP are linked to the arguments of others either directly (sharing the same variable as their value), or indirectly (through so-called u'\u005cu2018' handle constraints u'\u005cu2019' , where \u005csqeq in Fig
p25
aVFor the purposes of the present task, we take a negation cue as our entry point into the MRS graph (as our initial active EP), and then move through the graph according to the following simple operations to add EPs to the active set
p26
aVThis leads us to \u005cspred _no_q as our entry point into the graph
p27
aVOur algorithm states that for this type of cue (a quantifier) the first step is functor crawling (see § 3.3 below), which brings \u005cspred _know_v_1 into the scope
p28
aVWe proceed with argument crawling and label crawling , which pick up \u005cspred _the_q \u005cslnkc 03 and \u005cspred _german_n_1 as the \u005csrl ARG1
p29
aVFurther, as the \u005csrl ARG2 of \u005cspred _know_v_1, we reach \u005cspred thing and through recursive invocation we activate \u005cspred _of_p and, in yet another level of recursion, \u005cspred _the_q \u005cslnkc 5760 and \u005cspred _matter_n_of
p30
aVAt this point, crawling has no more links to follow
p31
aVThus, the MRS crawling operations u'\u005cu2018' paint u'\u005cu2019' a subset of the MRS graph as in-scope for a given negation cue
p32
aVOur crawling rules operate on semantic representations, but the annotations are with reference to the surface string
p33
aVAccordingly, we need projection rules to map from the u'\u005cu2018' painted u'\u005cu2019' MRS to the string
p34
aVHowever, the string-based annotations also include words which the ERG treats as semantically vacuous
p35
aVThus in order to match the gold annotations, we define a set of heuristics for when to count vacuous words as in scope
p36
aVIn ( 1 ), there are no semantically empty words in-scope, so we illustrate these heuristics with another example
p37
aVA semantically empty word is determined to be in-scope if there is an in-scope syntax tree node in the right position relative to it, as governed by a short list of templates organized by the type of the semantically empty word (particles, complementizers, non-referential pronouns, relative pronouns, and auxiliary verbs
p38
aVAs an example, the rule for auxiliary verbs like have in our example ( 3.2 ) is that they are in scope when their verb phrase complement is in scope
p39
aVSince overlooked is marked as in-scope by the crawler, the semantically empty have becomes in-scope as well
p40
aVFor example, the main rule for relative pronouns is that they are in-scope when they fill a gap in an in-scope constituent; which fills a gap in the constituent have overlooked , but since have is the (syntactic) lexical head of that constituent, the verb phrase is not considered in-scope the first time the rules are tried
p41
aVOur MRS crawling algorithm was defined by looking at the annotated data rather than the annotation guidelines for the Shared Task [ 7 ]
p42
aVNonetheless, our algorithm can be seen as a first pass formalization of the guidelines
p43
aVIn this section, we briefly sketch how our algorithm corresponds to different aspects of the guidelines
p44
aVFor negated verbs, the guidelines state that u'\u005cu2018' u'\u005cu2018' If the negated verb is the main verb in the sentence, the entire sentence is in scope u'\u005cu2019' u'\u005cu2019' [ 7 , 17]
p45
aVSince these structures are analogous in the semantic representations, the same operations that handle negated verbs also handle negated predicative adjectives correctly
p46
aVThe negation cue for a negated nominal argument will appear as a quantifier EP in the MRS, triggering line 3 of our algorithm
p47
aVIn contrast to subjects and objects, negation of a clausal argument is not treated as negation of the verb (ibid., p.18
p48
aVSince in this case, the negation cue will not be a quantifier in the MRS, there will be no functor crawling to the verb u'\u005cu2019' s EP
p49
aVFor negated modifiers, the situation is somewhat more complex, and this is a case where our crawling algorithm, developed on the basis of the annotated data, does not align directly with the guidelines as given
p50
aVThe guidelines state that negated attributive adjectives have scope over the entire NP (including the determiner) (ibid., p.20) and analogously negated adverbs have scope over the entire clause (ibid., p.21
p51
aVHowever, the annotations are not consistent, especially with respect to the treatment of negated adjectives while the head noun and determiner (if present) are typically annotated as in scope, other co-modifiers, especially long, post-nominal modifiers (including relative clauses) are not necessarily included
p52
aVFurthermore, the guidelines treat relative clauses as subordinate clauses and thus negation inside a relative clause is treated as bound to that clause only, and includes neither the head noun of the relative clause nor any of its other dependents in its scope
p53
aVHowever, from the perspective of MRS, a negated relative clause is indistinguishable from any other negated modifier of a noun
p54
aVThis treatment of relative clauses (as well as the inconsistencies in other forms of co-modification) is the reason for the exception noted at line 7 of Fig
p55
aVBy disallowing the addition of EPs to the scope if they share the label of the negation cue but are not one of its arguments, we block the head noun u'\u005cu2019' s EP (and any EPs only reachable from it) in cases of relative clauses where the head verb inside the relative clause is negated
p56
aVIt also blocks co-modifiers like great , own , and the phrases headed by ready and about in ( 3.3 ) u'\u005cu2013' ( 3.3
p57
aVAs illustrated in these examples, this is correct some but not all of the time
p58
aVHaving been unable to find a generalization capturing when comodifiers are annotated as in scope, we stuck with this approximation
p59
aVThe ERG treats all subordinating conjunctions as two-place predicates taking two scopal arguments
p60
aVThus, as with clausal complements of clause-embedding verbs, the embedding subordinating conjunction and any other arguments it might have are inaccessible, since functor crawling is restricted to a handful of specific configurations
p61
aVAs is usually the case with exercises in formalization, our crawling algorithm generalizes beyond what is given explicitly in the annotation guidelines
p62
aVFor example, all arguments that are treated as semantically nominal (including PP arguments where the preposition is semantically null) are treated in the same way as subjects and objects; similarly, all arguments which are semantically clausal (including certain PP arguments) are handled the same way as clausal complements
p63
aVThis is possible because we take advantage of the high degree of normalization that the ERG accomplishes in mapping to the MRS representation
p64
aVFinally, we note that even carefully worked out annotation guidelines such as these are never followed perfectly consistently by the human annotators who apply them
p65
aVBecause our crawling algorithm so closely models the guidelines, this puts our system in an interesting position to provide feedback to the Shared Task organizers
p66
aVHowever, the analysis engine does not always provide the desired analysis, largely because of idiosyncrasies of the genre (e.g., vocatives appearing mid-sentence) that are either not handled by the grammar or not well modeled in the parse selection component
p67
aVIn addition, as noted above, there are a handful of negation cues we do not yet handle
p68
aVThus, we also tested fall-back configurations which use scope predictions based on MRS in some cases, and scope predictions from the system of \u005cciteA Rea:Vel:Ovr:12 in others
p69
aVSince we do not attempt to perform cue detection, we report performance using gold cues and also using the system cues predicted by \u005cciteA Rea:Vel:Ovr:12
p70
aVThe training set contains 848 negated sentences, the development set 144, and the evaluation set 235
p71
aVAs there can be multiple usages of negation in one sentence, this corresponds to 984, 173, and 264 instances, respectively
p72
aVBeing rule-based, our system does not require any training data per se
p73
aVThe Oracle results are interesting because they show that there is much more to be gained in combining our semantics-based system with the \u005cciteA Rea:Vel:Ovr:12 syntactically-focused system
p74
aVTo shed more light on specific strengths and weaknesses of our approach, we performed a manual error analysis of scope predictions by Crawler, starting from gold cues so as to focus in-depth analysis on properties specific to scope resolution over MRSs
p75
aVIn this work, we have treated the ERG as an off-the-shelf system, but coverage could certainly be straightforwardly improved by adding analyses for phenomena particular to turn-of-the-20th-century British English
p76
aVAnother 33% of our false scope predictions are Crawler-external, viz. owing to erroneous input MRSs due to imperfect disambiguation by the parser or other inadequacies in the parser output
p77
aVHere, we could anticipate improvements by training the parse ranker on in-domain data or otherwise adapting it to this task
p78
aVThis first type of genuine crawling failure often relates to cues expressed as affixation ( 4.3 ), as well as to rare usages of cue expressions that predominantly occur with different categories, e.g., neither as a generalized quantifier ( 4.3
p79
aV{exe} \u005cex Please arrange your thoughts and let me know, in their due sequence, exactly what those events are { which have sent you out } u'\u005cu27e8' un u'\u005cu27e9' { brushed } and unkempt, with dress boots and waistcoat buttoned awry, in search of advice and assistance
p80
aV\u005cex You saw yourself { how } u'\u005cu27e8' neither u'\u005cu27e9' { of the inspectors dreamed of questioning his statement } , extraordinary as it was
p81
aVThe main points of difference are in the robustness of the system and in the degree of tailoring of both the rules for determining scope on the logical form level and the rules for handling semantically vacuous elements
p82
aVOur system, on the other hand, models the annotation guidelines more closely in the definition of the MRS crawling rules, and has more elaborated rules for handling semantically empty words
p83
aVSince full-scope recall depends on token-level precision, the Crawler does better across the board at the full-scope level
p84
aVUnlike the rather complex top-performing systems from the original 2012 competition, our MRS Crawler is defined by a small set of general rules that operate over general-purpose, explicit meaning representations
p85
aVThus, our approach scores high on transparency, adaptability, and replicability
p86
aVThis work grew out of a discussion with colleagues of the Language Technology Group at the University of Oslo, notably Elisabeth Lien and Jan Tore Lønning, to whom we are indebted for stimulating cooperation
p87
asS'CE_B_Text'
p88
(lp89
sS'CE_A_Text'
p90
(lp91
sS'ConcText'
p92
(lp93
VOur motivation in this work was to take the design of the 2012 * SEM Shared Task on negation analysis at face value as an overtly semantic problem that takes a central role in our long-term pursuit of language understanding.
p94
aVThrough both theoretical and practical reflection on the nature of representations at play in this task, we believe we have demonstrated that explicit semantic structure will be a key driver of further progress in the analysis of negation.
p95
aVWe were able to closely align two independently developed semantic analyses the negation-specific annotations of \u005cciteA Mor:Sch:Dae:11, on the one hand, and the broad-coverage, MRS meaning representations of the ERG, on the other hand.
p96
aVIn our view, the conceptual correlation between these two semantic views on negation analysis reinforces their credibility.
p97
aVUnlike the rather complex top-performing systems from the original 2012 competition, our MRS Crawler is defined by a small set of general rules that operate over general-purpose, explicit meaning representations.
p98
aVThus, our approach scores high on transparency, adaptability, and replicability.
p99
aVIn isolation, the Crawler provides premium precision but comparatively low recall.
p100
aVIts limitations, we conjecture, reflect primarily on ERG parsing challenges and inconsistencies in the target data.
p101
aVIn a sense, our approach pushes a larger proportion of the task into the parser, meaning (a) there should be good opportunities for parser adaptation to this somewhat idiosyncratic text type; (b) our results can serve to offer feedback on ERG semantic analyses and parse ranking; and (c) there is a much smaller proportion of very task-specific engineering.
p102
aVWhen embedded in a confidence-thresholded cascading architecture, our system advances the state of the art on this task, and oracle combination scores suggest there is much remaining room to better exploit the complementarity of approaches in our study.
p103
aVIn future work, we will seek to better understand the division of labor between the systems involved through contrastive error analysis and possibly another oracle experiment, constructing gold-standard MRSs for part of the data.
p104
aVIt would also be interesting to try a task-specific adaptation of the ERG parse ranking model, for example re-training on the pre-existing treebanks but giving preference to analyses that lead to correct Crawler results downstream.
p105
aV
p106
asS'Ftag'
p107
S'P14-1007'
p108
sS'AbsText'
p109
(lp110
VIn this work, we revisit Shared Task 1 from the 2012 * SEM Conference: the automated analysis of negation.
p111
aVUnlike the vast majority of participating systems in 2012, our approach works over explicit and formal representations of propositional semantics, i.e., derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations.
p112
aVWe relate the task-specific interpretation of (negation) scope to the concept of (quantifier and operator) scope in mainstream underspecified semantics.
p113
aVWith reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation decisions made for the 2012 * SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system.
p114
aVIn a system combination setting, our approach improves over the best published results on this task to date.
p115
aV4.5cm.
p116
aV1em \u005cbibleftmargin \u005cbibitemsep 0.75.
p117
ag106
asba(icmyPackage
FText
p118
(dp119
g3
(lp120
VIn this paper, we equip the DCS framework with logical inference, by defining abstract denotations as an abstraction of the computing process of denotations in original DCS
p121
aVAn inference engine is built to achieve inference on abstract denotations
p122
aVFurthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation
p123
aVDependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees []
p124
aVBecause, answers returned by a query depend on the specific database, but implication is independent of any databases
p125
aVFor example, answers to the question u'\u005cu201c' What books are read by students u'\u005cu201d' , should always be a subset of answers to u'\u005cu201c' What books are ever read by anyone u'\u005cu201d' , no matter how we store the data of students and how many records of books are there in our database
p126
aVThus, our first step is to fix a notation which abstracts the calculation process of DCS trees, so as to clarify its meaning without the aid of any existing database
p127
aVMoreover, to compensate the lack of background knowledge in practical inference, we combine our framework with the idea of tree transformation [] , to propose a way of generating knowledge in logical representation from entailment rules [] , which are by now typically considered as syntactic rewriting rules
p128
aVWe test our system on FraCaS [] and PASCAL RTE datasets []
p129
aVThe DCS tree in Figure 1 is interpreted as a command for querying these tables, obtaining u'\u005cu201c' reading u'\u005cu201d' entries whose u'\u005cu201c' SUBJ u'\u005cu201d' field is student and whose u'\u005cu201c' OBJ u'\u005cu201d' field is book
p130
aVThe result is a set { John reads Ulysses , u'\u005cu2026' } , which is called a denotation
p131
aVFigure 2 shows an example with a quantifier u'\u005cu201c' every u'\u005cu201d' , which is marked as u'\u005cu201c' u'\u005cu2282' u'\u005cu201d' on the edge ( u'\u005cud835' u'\u005cudc25' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc2f' u'\u005cud835' u'\u005cudc1e' ) u'\u005cu2062' OBJ-ARG u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1d' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc20' ) and interpreted as a division operator q u'\u005cu2282' u'\u005cud835' u'\u005cude7e' u'\u005cud835' u'\u005cude71' u'\u005cud835' u'\u005cude79' (§ 2.2
p132
aVThis is not trivial, however, because DCS works under the assumption that databases are explicitly available
p133
aVObviously this is unrealistic for logical inference on unrestricted texts, because we cannot prepare a database for everything in the world
p134
aVOur solution is to redefine DCS trees without the aid of any databases, by considering each node of a DCS tree as a content word in a sentence (but may no longer be a table in a specific database), while each edge represents semantic relations between two words
p135
aVThe labels on both ends of an edge, such as SUBJ (subject) and OBJ (object), are considered as semantic roles of the corresponding words 1 1 The semantic role ARG is specifically defined for denoting nominal predicate
p136
aVTo formulate the database querying process defined by a DCS tree, we provide formal semantics to DCS trees by employing relational algebra [] for representing the query
p137
aVAs described below, we represent meanings of sentences with abstract denotations , and logical relations among sentences are computed as relations among their abstract denotations
p138
aVwhere read , student and book denote sets represented by these words respectively, and w r represents the set w considered as the domain of the semantic role r (e.g., u'\u005cud835' u'\u005cudc1b' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc24' u'\u005cud835' u'\u005cude7e' u'\u005cud835' u'\u005cude71' u'\u005cud835' u'\u005cude79' is the set of books considered as objects
p139
aVq u'\u005cu2282' r the division operator, where q u'\u005cu2282' r u'\u005cu2062' ( A , B ) is defined as the largest set X which satisfies B r × X u'\u005cu2282' A
p140
aV2 2 If A and B has the same dimension, q u'\u005cu2282' u'\u005cu2062' ( A , B ) is either u'\u005cu2205' or { * } ( 0 -dimension point set), depending on if A u'\u005cu2282' B
p141
aVAn abstract denotation is then defined as finite applications of functions on either constants or other abstract denotations
p142
aVAs the semantics of DCS trees is formulated by abstract denotations, the meanings of declarative sentences are represented by statements on abstract denotations
p143
aV3 3 Using division operator, subsumption can be represented by non-emptiness, since for sets A , B of the same dimension, q u'\u005cu2282' u'\u005cu2062' ( A , B ) u'\u005cu2260' u'\u005cu2205' u'\u005cu21d4' A u'\u005cu2282' B
p144
aVAbstract denotations and statements are convenient for representing semantics of various types of expressions and linguistic knowledge
p145
aVBased on abstract denotations, we briefly describe our process to apply DCS to textual inference
p146
aVA DCS tree u'\u005cud835' u'\u005cudcaf' = ( u'\u005cud835' u'\u005cudca9' , u'\u005cu2130' ) is defined as a rooted tree, where each node u'\u005cu03a3' u'\u005cu2208' u'\u005cud835' u'\u005cudca9' is labeled with a content word w u'\u005cu2062' ( u'\u005cu03a3' ) and each edge ( u'\u005cu03a3' , u'\u005cu03a3' u'\u005cu2032' ) u'\u005cu2208' u'\u005cu2130' u'\u005cu2282' u'\u005cud835' u'\u005cudca9' × u'\u005cud835' u'\u005cudca9' is labeled with a pair of semantic roles ( r , r u'\u005cu2032' ) 7 7 The definition differs slightly from the original , mainly for the sake of simplicity and clarity
p147
aVIf ( u'\u005cu03a3' , u'\u005cu03a4' i ) is assigned by a quantification marker u'\u005cu201c' u'\u005cu2282' u'\u005cu201d' 8 8 Multiple quantifiers can be processed similarly then the abstract denotation is 9 9 The result of [[ u'\u005cud835' u'\u005cudcaf' ]] depends on the order of the children u'\u005cu03a4' 1 , u'\u005cu2026' , u'\u005cu03a4' n
p148
aVwhere u'\u005cud835' u'\u005cudcaf' u'\u005cu2032' is the same tree as u'\u005cud835' u'\u005cudcaf' except that the edge ( u'\u005cu03a3' , u'\u005cu03a4' i ) is removed
p149
aVSince meanings of sentences are represented by statements on abstract denotations, logical inference among sentences is reduced to deriving new relations among abstract denotations
p150
aVThis is done by applying axioms to known statements, and approximately 30 axioms are implemented (Table 3
p151
aVWe built an inference engine to perform logical inference on abstract denotations as above
p152
aVIn this logical system, we treat abstract denotations as terms and statements as atomic sentences , which are far more easier to handle than first order predicate logic (FOL) formulas
p153
aVFurthermore, all implemented axioms are horn clauses, hence we can employ forward-chaining, which is very efficient
p154
aVUsing disjointness we implemented two types of negations i) atomic negation, for each content word w we allow negation w ¯ of that word, characterized by the property w u'\u005cu2225' w ¯ ; and (ii) root negation, for a DCS tree u'\u005cud835' u'\u005cudcaf' and its denotation [[ u'\u005cud835' u'\u005cudcaf' ]] , the negation of u'\u005cud835' u'\u005cudcaf' is represented by u'\u005cud835' u'\u005cudcaf' u'\u005cu2225' u'\u005cud835' u'\u005cudcaf' , meaning that u'\u005cud835' u'\u005cudcaf' = u'\u005cu2205' in its effect
p155
aVSelection operators are implemented as markers assigned to abstract denotations, with specially designed axioms
p156
aVWe use Stanford CoreNLP to resolve coreferences [] , whereas coreference is implemented as a special type of selection
p157
aVIf a node u'\u005cu03a3' in a DCS tree u'\u005cud835' u'\u005cudcaf' belongs to a mention cluster m , we take the abstract denotation [[ u'\u005cud835' u'\u005cudcaf' u'\u005cu03a3' ]] and make a selection s m u'\u005cu2062' ([[ u'\u005cud835' u'\u005cudcaf' u'\u005cu03a3' ]]) , which is regarded as the abstract denotation of that mention
p158
aVHowever, this method does not work for real-world datasets such as PASCAL RTE [] , because of the knowledge bottleneck it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs u'\u005cu201c' no entailment u'\u005cu201d' for almost all pairs []
p159
aVThe transparent syntax-to-semantics interface of DCS enables us to back off to NLP techniques during inference for catching up the lack of knowledge
p160
aVOn-the-fly knowledge is generated by aligning paths in DCS trees
p161
aVA path is considered as joining two germs in a DCS tree, where a germ is defined as a specific semantic role of a node
p162
aVFor example, Figure 5 shows DCS trees of the following sentences (a simplified pair from RTE2-dev
p163
aVTwo paths are aligned if the joined germs are aligned, and we impose constraints on aligned germs to inhibit meaningless alignments, as described below
p164
aVThe abstract denotation of a germ is defined in a top-down manner for the root node u'\u005cu03a1' of a DCS tree u'\u005cud835' u'\u005cudcaf' , we define its denotation [[ u'\u005cu03a1' ]] u'\u005cud835' u'\u005cudcaf' as the denotation of the entire tree [[ u'\u005cud835' u'\u005cudcaf' ]] ; for a non-root node u'\u005cu03a4' and its parent node u'\u005cu03a3' , let the edge ( u'\u005cu03a3' , u'\u005cu03a4' ) be labeled by semantic roles ( r , r u'\u005cu2032' ) , then define
p165
aVNow for a germ r u'\u005cu2062' ( u'\u005cu03a3' ) , the denotation is defined as the projection of the denotation of node u'\u005cu03a3' onto the specific semantic role r
p166
aV[[ r u'\u005cu2062' ( u'\u005cu03a3' ) ]] u'\u005cud835' u'\u005cudcaf' = u'\u005cu03a0' r u'\u005cu2062' ( [[ u'\u005cu03a3' ]] u'\u005cud835' u'\u005cudcaf' )
p167
aVSimilarly, denotation of germ u'\u005cud835' u'\u005cude7e' u'\u005cud835' u'\u005cude71' u'\u005cud835' u'\u005cude79' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1b' u'\u005cud835' u'\u005cudc25' u'\u005cud835' u'\u005cudc1a' u'\u005cud835' u'\u005cudc26' u'\u005cud835' u'\u005cudc1e' ) in T of Figure 5 indicates the object of u'\u005cu201c' blame u'\u005cu201d' as in the sentence u'\u005cu201c' Tropical storm Debby is blamed for death u'\u005cu201d' , which is a tropical storm , is Debby , etc
p168
aVTechnically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values [] of that variable
p169
aVThe logical clue to align germs is if there exists an abstract denotation, other than W , that is a superset of both abstract denotations of two germs, then the two germs can be aligned
p170
aVAccepted aligned paths are converted into statements, which are used as new knowledge
p171
aVThe conversion is done by first performing a DCS tree transformation according to the aligned paths, and then declare a subsumption relation between the denotations of aligned germs
p172
aVFurthermore, since the on-the-fly knowledge is generated by transformed pairs of DCS trees, all contexts are preserved in Figure 6 , though the tree transformation can be seen as generated from the entailment rule u'\u005cu201c' X is blamed for death u'\u005cu2192' X causes loss of life u'\u005cu201d' , the generated on-the-fly knowledge, as shown above the trees, only fires with the additional condition that X is a tropical storm and is Debby
p173
aVHence, the process can also be used to generate knowledge from context sensitive rules [] , which are known to have higher quality []
p174
aVFor example, named entities are singletons, so we add axioms such as u'\u005cu2200' x ; ( x u'\u005cu2282' u'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc26' x u'\u005cu2260' u'\u005cu2205' ) u'\u005cu2192' u'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc26' u'\u005cu2282' x
p175
aVFor example, the similarity score of the path alignment u'\u005cu201c' OBJ ( blame ) IOBJ - ARG ( death ) u'\u005cu2248' SUBJ ( cause ) OBJ - ARG ( loss ) MOD - ARG ( life ) u'\u005cu201d' is calculated as the cosine similarity of vectors blame + death and cause + loss + life
p176
aVThe threshold for accepted path alignments is set to 0.4 , based on pre-experiments on RTE development sets
p177
aVMost of the problems do not require lexical knowledge, so we use our primary textual inference system without on-the-fly knowledge nor WordNet, to test the performance of the DCS framework as formal semantics
p178
aVTo obtain the three-valued output (i.e., yes , no , and unknown ), we output u'\u005cu201c' yes u'\u005cu201d' if H is proven, or try to prove the negation of H if H is not proven
p179
aVTo negate H , we use the root negation as described in § 2.5
p180
aVIf the negation of H is proven, we output u'\u005cu201c' no u'\u005cu201d' , otherwise we output u'\u005cu201c' unknown u'\u005cu201d'
p181
aVSince our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences
p182
aVCompared to and , our system does not need a pre-trained alignment model, and it improves by making multi-sentence inferences
p183
aVOn PASCAL RTE datasets, strict logical inference is known to have very low recall [] , so on-the-fly knowledge is crucial in this setting
p184
aVWhen only primary knowledge is used in inference (the first row), recalls are actually very low; After we activate the on-the-fly knowledge, recalls jump to over 50%, with a moderate fall of precision
p185
aVAs a result, accuracies significantly increase
p186
aVStern11 [] and Stern12 [] extend this framework to utilize entailment rules as tree transformations
p187
aVThese are more tailored systems using machine learning with many handcrafted features
p188
aVSumming up test data from RTE2 to RTE5, Figure 7 shows the proportion of all proven pairs and their precision
p189
aVFinally, to see if we u'\u005cu201c' get lucky u'\u005cu201d' on RTE5 data in the choice of word vectors and thresholds, we change the thresholds from 0.1 to 0.7 and draw the precision-recall curve, using two types of word vectors, Mikolov13 and Turian10
p190
aVAs shown in Figure 8 , though the precision drops for Turian10 , both curves show the pattern that our system keeps gaining recall while maintaining precision to a certain level
p191
aVWe have presented a method of deriving abstract denotation from DCS trees, which enables logical inference on DCS, and we developed a textual inference system based on the framework
p192
aVDescription logic, being less expressive than FOL but featuring more efficient reasoning, is used as a theory base for Semantic Web []
p193
aVIdeas similar to our framework, including the use of sets in a representation that benefits efficient reasoning, are also found in description logic and knowledge representation community []
p194
aVAmong these, the ( u'\u005cu039b' -)DCS [] framework defines algorithms that transparently map a labeled tree to a database querying procedure
p195
aVEssentially, this is because DCS trees restrict the querying process to a very limited subset of possible operations
p196
aVOur main contribution, the abstract denotation of DCS trees, can thus be considered as an attempt to characterize a fragment of FOL that is suited for both natural language inference and transparent syntax-semantics mapping, through the choice of operations and relations on sets
p197
aVThese previous works include various techniques for acquiring and incorporating different kinds of linguistic and world knowledge, and further fight against the knowledge bottleneck problem, e.g., by back-off to shallower representations
p198
aVFor example, since abstract denotations are readily suited for data querying, they can be used to verify newly generated assumptions by fact search in a database
p199
asg88
(lp200
sg90
(lp201
sg92
(lp202
VWe have presented a method of deriving abstract denotation from DCS trees, which enables logical inference on DCS, and we developed a textual inference system based on the framework.
p203
aVExperimental results have shown the power of the representation that allows both strict inference as on FraCaS data and robust reasoning as on RTE data.
p204
aVExploration of an appropriate meaning representation for querying and reasoning on knowledge bases has a long history.
p205
aVDescription logic, being less expressive than FOL but featuring more efficient reasoning, is used as a theory base for Semantic Web [].
p206
aVIdeas similar to our framework, including the use of sets in a representation that benefits efficient reasoning, are also found in description logic and knowledge representation community [].
p207
aVTo our knowledge, however, their applications to logical inference beyond the use for database querying have not been much explored in the context of NLP.
p208
aVThe pursue of a logic more suitable for natural language inference is not new.
p209
aVFor instance, has implemented a model of natural logic [].
p210
aVWhile being computationally efficient, various inference patterns are out of the scope of their system.
p211
aVMuch work has been done in mapping natural language into database queries [].
p212
aVAmong these, the ( -)DCS [] framework defines algorithms that transparently map a labeled tree to a database querying procedure.
p213
aVEssentially, this is because DCS trees restrict the querying process to a very limited subset of possible operations.
p214
aVOur main contribution, the abstract denotation of DCS trees, can thus be considered as an attempt to characterize a fragment of FOL that is suited for both natural language inference and transparent syntax-semantics mapping, through the choice of operations and relations on sets.
p215
aVWe have demonstrated the utility of logical inference on DCS through the RTE task.
p216
aVA wide variety of strategies tackling the RTE task have been investigated [] , including the comparison of surface strings [] , syntactic and semantic structures [] , semantic vectors [] and logical representations [].
p217
aVAcquisition of basic knowledge for RTE is also a huge stream of research [].
p218
aVThese previous works include various techniques for acquiring and incorporating different kinds of linguistic and world knowledge, and further fight against the knowledge bottleneck problem, e.g., by back-off to shallower representations.
p219
aVLogic-based RTE systems employ various approaches to bridge knowledge gaps proposes features from a model builder; proposes an abduction process; shows handcrafted rules could drastically improve the performance of a logic-based RTE system.
p220
aVAs such, our current RTE system is at a proof-of-concept stage, in that many of the above techniques are yet to be implemented.
p221
aVNonetheless, we would like to emphasize that it already shows performance competitive to state-of-the-art systems on one data set (RTE5.
p222
aVOther directions of our future work include further exploitation of the new semantic representation.
p223
aVFor example, since abstract denotations are readily suited for data querying, they can be used to verify newly generated assumptions by fact search in a database.
p224
aVThis may open a way towards a hybrid approach to RTE wherein logical inference is intermingled with large scale database querying.
p225
aVThis research was supported by the Todai Robot Project at National Institute of Informatics.
p226
ag106
asg107
S'P14-1008'
p227
sg109
(lp228
VDependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics.
p229
aVIn this paper, we equip the DCS framework with logical inference, by defining abstract denotations as an abstraction of the computing process of denotations in original DCS.
p230
aVAn inference engine is built to achieve inference on abstract denotations.
p231
aVFurthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation.
p232
aVExperiments on FraCaS and PASCAL RTE datasets show promising results.
p233
aVAbstract denotations are formulas constructed from a minimal set of relational algebra [] operators, which is already able to formulate the database queries defined by DCS trees.
p234
aVFor example, the semantics of students read books is given by the abstract denotation.
p235
aVwhere read , student and book denote sets represented by these words respectively, and w r represents the set w considered as the domain of the semantic role r (e.g., is the set of books considered as objects.
p236
aVThe operators and × represent intersection and Cartesian product respectively, both borrowed from relational algebra.
p237
aVIt is not hard to see the abstract denotation denotes the intersection of the reading set (as illustrated by the read table in Table 1 ) with the product of student set and book set, which results in the same denotation as computed by the DCS tree in Figure 1 , i.e., { John reads Ulysses , }.
p238
aVHowever, the point is that F 1 itself is an algebraic formula that does not depend on any concrete databases.
p239
aVFormally, we introduce the following constants.
p240
aVW a universal set containing all entities.
p241
aVContent words a content word (e.g., read ) defines a set representing the word (e.g., = { ( x , y r e a d ( x , y ) }.
p242
aVIn addition we introduce following functions.
p243
aV× the Cartesian product of two sets.
p244
aV the intersection of two sets.
p245
aV r projection onto domain of semantic role r (e.g., ( ) = { y x ; r e a d ( x , y ) }.
p246
aVGenerally we admit projections onto multiple semantics roles, denoted by R where R is a set of semantic roles.
p247
aV r relabeling (e.g., ( ) = .
p248
aVq r the division operator, where q r ( A , B ) is defined as the largest set X which satisfies B r × X A.
p249
aV2 2 If A and B has the same dimension, q ( A , B ) is either or { * } ( 0 -dimension point set), depending on if A B.
p250
aVThis is used to formulate universal quantifiers, such as Mary loves every dog and books read by all students .
p251
aVAn abstract denotation is then defined as finite applications of functions on either constants or other abstract denotations.
p252
ag106
asba(icmyPackage
FText
p253
(dp254
g3
(lp255
VIf distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors
p256
aVFor instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in composition pandas eat bamboo is identical to bamboo eats pandas
p257
aV2010 ) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model
p258
aVA related approach [ 24 ] assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister
p259
aVDespite positive empirical evaluation, this approach is hardly practical for general-purpose semantic language processing, since it requires computationally expensive approximate parameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks
p260
aVThe lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ]
p261
aVIn lf, arguments are vectors and functions taking arguments (e.g.,, adjectives that combine with nouns) are tensors, with the number of arguments (n) determining the order of tensor (n+1
p262
aVTensor by vector multiplication formalizes function application and serves as the general composition method
p263
aVBaroni and Zamparelli ( 2010 ) propose a practical and empirically effective way to estimate matrices representing adjectival modifiers of nouns by linear regression from corpus-extracted examples of noun and adjective-noun vectors
p264
aV[ 23 , 24 ] , the Baroni and Zamparelli method does not require manually labeled data nor costly iterative estimation procedures, as it relies on automatically extracted phrase vectors and on the analytical solution of the least-squares-error problem
p265
aVWith all the advantages of lf, scaling it up to arbitrary sentences, however, leads to several issues
p266
aVFor example, if noun meanings are encoded in vectors of 300 dimensions, adjectives become matrices of 300 2 cells, and transitive verbs are represented as tensors with 300 3 = 27 , 000 , 000 dimensions
p267
aVSince predicate arity is encoded in the order of the corresponding tensor, eat and the like have to be assigned different representations (matrix or tensor) depending on the context
p268
aVPrepositions are the hardest, as the syntactic positions in which they occur are most diverse ( park in the dark vs play in the dark vs be in the dark vs a light glowing in the dark
p269
aVSince each of these tensors must be learned from examples individually, their obvious relation is missed
p270
aVThis issue is unavoidable since we don u'\u005cu2019' t expect to find all words in all possible constructions even in the largest corpus
p271
aVAfter applying the matrices to the corresponding argument vectors, a single representation is obtained by summing across all resulting vectors
p272
aVFor instance, we treat adjectives that modify nouns (0-ary) as unary functions, encoded in a vector-matrix pair
p273
aVAdverbs have different semantic types depending on their syntactic role
p274
aVSentential adverbs are unary, while adverbs that modify adjectives ( very ) or verb phrases ( quickly ) are encoded as binary functions, represented by a vector and two matrices
p275
aVThe form of semantic representations we are using is shown in Table 1
p276
aVOur system incorporates semantic composition via two composition rules, one for combining structures of different arity and the other for symmetric composition of structures with the same arity
p277
aVThese rules incorporate insights of two empirically successful models, lexical function and the simple additive approach, used as the default structure merging strategy
p278
aVThe first rule is function application , illustrated in Figure 1
p279
aVFor ternary predicates such as give in a ditransitive construction, the first step in the derivation absorbs the innermost argument by multiplying its vector by the third give matrix, and then composition proceeds like for transitives
p280
aVOur current plf implementation treats most grammatical words, including conjunctions, as u'\u005cu201c' empty u'\u005cu201d' elements, that do not project into semantics
p281
aVThis choice leads to some interesting u'\u005cu201c' serendipitous u'\u005cu201d' treatments of various constructions
p282
aVFor example, since the copula is empty, a sentence with a predicative adjective ( cars are red ) is treated in the same way as a phrase with the same adjective in attributive position ( red cars ) u'\u005cu2013' although the latter, being a phrase and not a full sentence, will later be embedded as argument in a larger construction
p283
aVAn n-ary predicate is no longer encoded as an n+1-way tensor; instead we have a sequence of n matrices
p284
aVThe representation size grows linearly, not exponentially, for higher semantic types, allowing for simpler and more efficient parameter estimation, storage, and computation
p285
aVAs a consequence of our architecture, we no longer need to perform the complicated step-by-step estimation for elements of higher arity
p286
aVThe semantic representations we propose include a semantic vector for constituents of any semantic type, thus enabling semantic comparison for words of different parts of speech (the case of demolition vs demolish
p287
aVFinally, the fact that we represent the predicate interaction with each of its arguments in a separate matrix allows for a natural and intuitive treatment of argument alternations
p288
aVFor instance, as shown in Table 4 , one can distinguish the transitive and intransitive usages of the verb to eat by the presence of the object-oriented matrix of the verb while keeping the rest of the representation intact
p289
aVTo model passive usages, we insert the object matrix of the verb only, which will be multiplied by the syntactic subject vector, capturing the similarity between eat meat and meat is eaten
p290
aVSo keeping the verb u'\u005cu2019' s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types, but also provides a sensible built-in strategy for handling a word u'\u005cu2019' s occurrence in multiple constructions
p291
aVIndeed, if we encounter a verb used intransitively which was only attested as transitive in the training corpus, we can simply omit the object matrix to obtain a type-appropriate representation
p292
aVOn the other hand, if the verb occurs with more arguments than usual in testing materials, we can add a default diagonal identity matrix to its representation, signaling agnosticism about how the verb relates to the unexpected argument
p293
aVLast but not least, our implementation is suitable for realistic language processing since it allows to produce vectors for sentences of arbitrary size, including those containing novel syntactic configurations
p294
aVEvaluation is carried out by computing the Spearman correlation between the annotator similarity ratings for the sentence pairs and the cosines of the vectors produced by the various systems for the same sentence pairs
p295
aVThe foils have high lexical overlap with the targets but very different meanings, due to different determiners and/or word order
p296
aVThe two remaining data sets are larger and more u'\u005cu2018' natural u'\u005cu2019' , as they were not constructed by linguists under controlled conditions to focus on specific phenomena
p297
aVFollowing standard practice in paraphrase detection studies (e.g.,, Blacoe and Lapata ( 2012 ) ), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues word overlap between the two sentences and difference in sentence length
p298
aVWe obtain a final similarity score by weighted addition of the 3 cues, with the optimal weights determined by linear regression on separate msrvid train data that were also provided by the SemEval task organizers (before combining, we checked that the collinearity between cues was low
p299
aVOur main interest in this set stems from the fact that glosses are rarely well-formed full sentences (consider, e.g.,, cause something to pass or lead somewhere ; coerce by violence, fill with terror
p300
aVFor this reason, they are very challenging for standard parsers
p301
aVSince plf needs syntactic information to construct sentence vectors compositionally, we test it on onwn to make sure that it is not overly sensitive to parser noise
p302
aV3 3 We did not evaluate on other STS benchmarks since they have characteristics, such as high density of named entities, that would require embedding our compositional models into more complex systems, obfuscating their impact on the overall performance
p303
aVWe collected a 30K-by-30K matrix by counting co-occurrence of the 30K most frequent content lemmas (nouns, adjectives and verbs) within a 3-word window
p304
aVThis setup was picked without tuning, as we found it effective in previous, unrelated experiments
p305
aV4 4 With the multiplicative composition model we also tried Nonnegative Matrix Factorization instead of Singular Value Decomposition, because the negative values produced by SVD are potentially problematic for mult
p306
aVThe overall pattern of results did not change significantly, and thus for consistency we report all models u'\u005cu2019' performance only for the SVD-reduced space
p307
aVThe add (additive) model produces the vector of a sentence by summing the vectors of all content words in it
p308
aVThese are trained using Ridge regression with generalized cross-validation from corpus-extracted vectors of nouns, as input, and phrases including those nouns as output (e.g.,, the matrix for red is trained from corpus-extracted u'\u005cu27e8' noun , red-noun u'\u005cu27e9' vector pairs
p309
aVWe did not attempt to train a lf model for the larger and more varied msrvid and onwn data sets, as this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above
p310
aVTraining plf (practical lexical function) proceeds similarly, but we also build preposition matrices (from u'\u005cu27e8' noun , preposition-noun u'\u005cu27e9' vector pairs), and for verbs we prepare separate subject and object matrices
p311
aVSince syntax guides lf and plf composition, we supplied all test sentences with categorial grammar parses
p312
aVEvery sentence in the anvan1 and anvan2 datasets has the form (subject) Adjective + Noun + Transitive Verb + (object) Adjective + Noun, so parsing them is trivial
p313
aVFor anvan1, plf is just below the state of the art, which is based on disambiguating the verb vector in context [ 18 ] , and lf outperforms the baseline, which consists in using the verb vector only as a proxy to sentence similarity
p314
aV2013 ) , since only the former used a source corpus that is comparable to ours
p315
aVIn the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only
p316
aVOur plf approach is able to handle determiners and word order correctly, as demonstrated by a highly significant ( p 0.01 ) difference between paraphrase and foil similarity (average difference in cosine .017, standard deviation .077
p317
aVIn this case, however, the traditional lf model (average difference .044, standard deviation .092) outperforms plf
p318
aVSince determiners are handled identically under the two approaches, the culprit must be word order
p319
aVWe conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences
p320
aVIndeed, if we limit evaluation to those foils characterized by word order changes only, lf discriminates between paraphrases and foils even more clearly, whereas the plf difference, while still significant, decreases slightly
p321
aVIf we don u'\u005cu2019' t normalize, we do get larger differences for our models as well, but consistently lower performance in all other tasks
p322
aVThis is a very positive result, in the light of the fact that the parser has very low performance on the onwn glosses, thus suggesting that plf can produce sensible semantic vectors from noisy syntactic representations
p323
aVHere the overlap+length baseline does not perform so well, and again the best STS 2013 system [ 16 ] uses considerably richer knowledge sources and algorithms than ours
p324
aVWe introduced an approach to compositional distributional semantics based on a linguistically-motivated syntax-to-semantics type mapping, but simple and flexible enough that it can produce representations of English sentences of arbitrary size and structure
p325
asg88
(lp326
sg90
(lp327
sg92
(lp328
VWe introduced an approach to compositional distributional semantics based on a linguistically-motivated syntax-to-semantics type mapping, but simple and flexible enough that it can produce representations of English sentences of arbitrary size and structure.
p329
aVWe showed that our approach is competitive against the more complex lexical function model when evaluated on the simple constructions the latter can be applied to, and it outperforms the additive and multiplicative compositionality models when tested on more realistic benchmarks (where the full-fledged lexical function approach is difficult or impossible to use), even in presence of strong noise in its syntactic input.
p330
aVWhile our results are encouraging, no current benchmark combines large-scale, real-life data with the syntactic variety on which a syntax-driven approach to semantics such as ours could truly prove its worth.
p331
aVThe recently announced SemEval 2014 Task 1 7 7 http://alt.qcri.org/semeval2014/task1/ is filling exactly this gap, and we look forward to apply our method to this new benchmark, as soon as it becomes available.
p332
aVOne of the strengths of our framework is that it allows for incremental improvement focused on specific constructions.
p333
aVFor example, one could add representations for different conjunctions ( and vs or ), train matrices for verb arguments other than subject and direct object, or include new types of modifiers into the model, etc.
p334
aVWhile there is potential for local improvements, our framework, which extends and improves on existing compositional semantic vector models, has demonstrated its ability to account for full sentences in a principled and elegant way.
p335
aVOur implementation of the model relies on simple and efficient training, works fast, and shows good empirical results.
p336
ag106
asg107
S'P14-1009'
p337
sg109
(lp338
VDistributional semantic methods to approximate word meaning with context vectors have been very successful empirically, and the last years have seen a surge of interest in their compositional extension to phrases and sentences.
p339
aVWe present here a new model that, like those of Coecke et al.
p340
aV2010 ) and Baroni and Zamparelli ( 2010 ) , closely mimics the standard Montagovian semantic treatment of composition in distributional terms.
p341
aVHowever, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences.
p342
aVWe test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals.
p343
ag106
asba(icmyPackage
FText
p344
(dp345
g3
(lp346
VIn this paper, we expand our translation options by desegmenting n -best lists or lattices
p347
aVMorphological complexity leads to much higher type to token ratios than English, which can create sparsity problems during translation model estimation
p348
aVMorphological segmentation addresses this issue by splitting surface forms into meaningful morphemes, while also performing orthographic transformations to further reduce sparsity
p349
aVFor example, the Arabic noun ¡laldwl¿ lldwl u'\u005cu201c' to the countries u'\u005cu201d' is segmented as l+ u'\u005cu201c' to u'\u005cu201d' Aldwl u'\u005cu201c' the countries u'\u005cu201d'
p350
aVWhen translating from Arabic, this segmentation process is performed as input preprocessing and is otherwise transparent to the translation system
p351
aVDesegmentation is typically performed as a post-processing step that is independent from the decoding process
p352
aVWhile this division of labor is useful, the pipeline approach may prevent the desegmenter from recovering from errors made by the decoder
p353
aVWe demonstrate that significant improvements in translation quality can be achieved by training a linear model to re-rank this transformed translation space
p354
aVMost techniques approach the problem by transforming the target language in some manner before training the translation model
p355
aVOther approaches train an SMT system to predict lemmas instead of surface forms, and then inflect the SMT output as a post-processing step [ 21 , 7 , 11 , 10 ]
p356
aVAlternatively, one can reparameterize existing phrase tables as exponential models, so that translation probabilities account for source context and morphological features [ 16 , 30 ]
p357
aVInstead of producing an abstract feature layer, morphological segmentation transforms the target sentence by segmenting relevant morphemes, which are then handled as regular tokens during alignment and translation
p358
aVSuch a segmentation can be produced as a byproduct of analysis [ 24 , 2 , 9 ] , or may be produced using an unsupervised morphological segmenter such as Morfessor [ 20 , 7 ]
p359
aVWork on target language morphological segmentation for SMT can be divided into three subproblems segmentation, desegmentation and integration
p360
aVIf a morphological feature does not manifest itself as a separate token in the source, then it may be best to leave its corresponding segment attached to the stem
p361
aVSince our focus here is on integrating segmentation into the decoding process, we simply adopt the segmentation strategies recommended by previous work the Penn Arabic Treebank scheme for English-Arabic [ 9 ] , and an unsupervised scheme for English-Finnish [ 7 ]
p362
aVWork on integration attempts to improve SMT performance for morphologically complex target languages by going beyond simple pre- and post-processing
p363
aVWe approach this problem by augmenting an SMT system built over target segments with features that reflect the desegmented target words
p364
aVIn this section, we describe our various strategies for desegmenting the SMT system u'\u005cu2019' s output space, along with the features that we add to take advantage of this desegmented view
p365
aVThe one-best approach can be extended easily by desegmenting n -best lists of segmented decoder output
p366
aVDoing so enables the inclusion of an unsegmented target language model, and with a small amount of bookkeeping, it also allows the inclusion of features related to the operations performed during desegmentation (see Section 3.4
p367
aVOnce n -best lists have been desegmented, we can tune on unsegmented references as a side-benefit
p368
aVThis could improve translation quality, as it brings our training scenario closer to our test scenario (test BLEU is always measured on unsegmented references
p369
aVThe search graph of a phrase-based decoder can be interpreted as a lattice, which can be interpreted as a finite state acceptor over target strings
p370
aVThis is sometimes referred to as a word graph [ 32 ] , although in our case the segmented phrase table also produces tokens that correspond to morphemes
p371
aVOur goal is to desegment the decoder u'\u005cu2019' s output lattice, and in doing so, gain access to a compact, desegmented view of a large portion of the translation search space
p372
aVThis can be accomplished by composing the lattice with a desegmenting transducer that consumes morphemes and outputs desegmented words
p373
aV3 3 Throughout this paper, we use u'\u005cu201c' + u'\u005cu201d' to mark morphemes as prefixes or suffixes, as in w+ or +h
p374
aVIn Equation 1 only, we overload u'\u005cu201c' + u'\u005cu201d' as the Kleene cross
p375
aVX + = = X X *
p376
aVEquation 1 may need to be modified for other languages or segmentation schemes, but our techniques generalize to any definition that can be written as a regular expression
p377
aVA desegmenting transducer can be constructed by first encoding our desegmenter as a table that maps morpheme sequences to words
p378
aVRegardless of whether the original desegmenter was based on concatenation, rules or table-lookup, it can be encoded as a lattice-specific table by applying it to an enumeration of all words found in the lattice
p379
aVFinally, we take the closure of this transducer, so that the resulting machine can transduce any sequence of words
p380
aVThe lattice (Figure 1 a) can then be desegmented by composing it with the transducer ( 1 b), producing a desegmented lattice ( 1 c
p381
aVIf this property does not hold, then nodes must be split until it does
p382
aVFortunately, LM annotation as well as any necessary lattice modifications can be performed simultaneously by composing the desegmented lattice with a finite state acceptor encoding the LM [ 26 ]
p383
aVIn summary, we are given a segmented lattice, which encodes the decoder u'\u005cu2019' s translation space as an acceptor over morphemes
p384
aVWe compose this acceptor with a desegmenting transducer, and then with an unsegmented LM acceptor, producing a fully annotated, desegmented lattice
p385
aVA chain is valid if it emits the beginning of a word as defined by the regular expression in Equation 1
p386
aVA valid chain is complete if its edges form an entire word, and if it is part of a path through the lattice that consists only of words
p387
aV5 5 Sentence-initial suffix morphemes and sentence-final prefix morphemes represent a special case that we omit for the sake of brevity
p388
aVLacking stems, they are left segmented
p389
aVFor example, if we removed the edge 2 u'\u005cu2192' 3 ( AlTfl ) from Figure 1 a, then [ 0 u'\u005cu2192' 2 ] ([ b+ lEbp ]) would cease to be a complete chain, but it would still be a valid chain
p390
aVThe search recognizes the valid chain c to be complete by finding an edge e such that c + e forms a chain, but not a valid one
p391
aVThe chains found by this search are desegmented and then added to the output lattice as edges
p392
aVThe nodes at end points of these chains are added to the work list, as they lie at word boundaries by definition
p393
aVWe use a table-based desegmentation method for Arabic, which is based on segmenting an Arabic training corpus and memorizing the observed transformations to reverse them later
p394
aVFinnish does not require a table, as all words can be desegmented with simple concatenation
p395
aVFor the sake of symmetry with the unambiguous Finnish case, we augment Arabic n -best lists or lattices with only the most frequent desegmentation Y
p396
aVWe provide the desegmentation score log p ( Y
p397
aVX ) = log u'\u005cu2061' ( count of X u'\u005cu2192' Y count of X ) as a feature, to indicate the entry u'\u005cu2019' s ambiguity in the training data
p398
aV7 7 We also experimented on log p ( X
p399
aVY ) as an additional feature, but observed no improvement in translation quality
p400
aVIn order to maintain some control over this powerful capability, we create three binary features that indicate the contiguity of a desegmentation
p401
aVThe first feature indicates that the desegmented morphemes were translated from contiguous source words
p402
aVThe second indicates that the source words contained a single discontiguity, as in a word-by-word translation of the u'\u005cu201c' with his blue car u'\u005cu201d' example from Section 2.2
p403
aVThe third indicates two or more discontiguities
p404
aVWe tune on the NIST 2004 evaluation set (1353 sentences) and evaluate on NIST 2005 (1056 sentences
p405
aVAs these evaluation sets are intended for Arabic-to-English translation, we select the first English reference to use as our source text
p406
aVFor both segmented and unsegmented Arabic, we further normalize the script by converting different forms of Alif ¡a u'\u005cu2019' A u'\u005cu2019' a u'\u005cu2019' i ¿ and Ya ¡y _A¿ to bare Alif ¡a¿ and dotless Ya ¡_A¿
p407
aVTo improve coverage, words are further segmented according to their longest matching suffix from the list
p408
aVAs Morfessor does not perform any orthographic normalizations, it can be desegmented with simple concatenation
p409
aVMIRA was selected over MERT because we have an in-house implementation that can tune on lattices very quickly
p410
aV8 8 Development experiments on a small-data English-to-Arabic scenario indicated that the Desegmentation Score was not particularly useful, so we exclude it from the main comparison, but include it in the ablation experiments
p411
aV2011 ) , we report average scores over five random tuning replications to account for optimizer instability
p412
aVFor the baselines, this means 5 runs of decoder tuning
p413
aVFor the desegmenting re-rankers, this means 5 runs of re-ranker tuning, each working on n -best lists or lattices produced by the same (representative) decoder weights
p414
aVAs we attempted to replicate the approach of Clifton and Sarkar ( 2011 ) exactly by working with their segmented data, this difference is likely due to changes in Moses since the publication of their result
p415
aVNonetheless, the 1000-best and lattice desegmenters both produce significant improvements over the 1-best desegmentation baseline, with Lattice Deseg achieving a 1-point improvement in TER
p416
aVWe also tried a similar Morfessor-based segmentation for Arabic, which has an unsegmented test set BLEU of 32.7
p417
aVAs in Finnish, the 1-best desegmentation using Morfessor did not surpass the unsegmented baseline, producing a test BLEU of only 31.4 (not shown in Table 1
p418
aVWe consider a phrase to be correct only if it can be found in the reference
p419
aVTable 4 breaks down per-phrase accuracy according to four manually-assigned categories
p420
aV1) clitical u'\u005cu2013' the two systems agree on a stem, but at least one clitic, often a prefix denoting a preposition or determiner, was dropped, added or replaced; (2) lexical u'\u005cu2013' a word was changed to a morphologically unrelated word with a similar meaning; (3) inflectional u'\u005cu2013' the words have the same stem, but different inflection due to a change in gender, number or verb tense; (4) part-of-speech u'\u005cu2013' the two systems agree on the lemma, but have selected different parts of speech
p421
aVThis category is challenging for the decoder because English prepositions tend to correspond to multiple possible forms when translated into Arabic
p422
asg88
(lp423
sg90
(lp424
sg92
(lp425
VWe have explored deeper integration of morphological desegmentation into the statistical machine translation pipeline.
p426
aVWe have presented a novel, finite-state-inspired approach to lattice desegmentation, which allows the system to account for a desegmented view of many possible translations, without any modification to the decoder or any restrictions on phrase extraction.
p427
aVWhen applied to English-to-Arabic translation, lattice desegmentation results in a 1.0 BLEU point improvement over one-best desegmentation, and a 1.7 BLEU point improvement over unsegmented translation.
p428
aVWe have also applied our approach to English-to-Finnish translation, and although segmentation in general does not currently help, we are able to show significant improvements over a 1-best desegmentation baseline.
p429
aVIn the future, we plan to explore introducing multiple segmentation options into the lattice, and the application of our method to a full morphological analysis (as opposed to segmentation) of the target language.
p430
aVEventually, we would like to replace the functionality of factored translation models [ 18 ] with lattice transformation and augmentation.
p431
ag106
asg107
S'P14-1010'
p432
sg109
(lp433
VMorphological segmentation is an effective sparsity reduction strategy for statistical machine translation (SMT) involving morphologically complex languages.
p434
aVWhen translating into a segmented language, an extra step is required to desegment the output; previous studies have desegmented the 1-best output from the decoder.
p435
aVIn this paper, we expand our translation options by desegmenting n -best lists or lattices.
p436
aVOur novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs, which allows for inclusion of features related to the desegmentation process, as well as an unsegmented language model (LM.
p437
aVWe investigate this technique in the context of English-to-Arabic and English-to-Finnish translation, showing significant improvements in translation quality over desegmentation of 1-best decoder outputs.
p438
ag106
asba(icmyPackage
FText
p439
(dp440
g3
(lp441
VDue to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing [ 13 , 15 , 6 ]
p442
aVMost of these works attempt to improve some components in SMT based on word embedding , which converts a word into a dense, low dimensional, real-valued vector representation [ 2 , 3 , 5 , 20 ]
p443
aVHowever, in the conventional (phrase-based) SMT, phrases are the basic translation units
p444
aVThe models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules
p445
aVTherefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work
p446
aVObviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g., reordering pattern), or impose strong assumptions (e.g., bag-of-words or indivisible n -gram
p447
aVTherefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase
p448
aVInstead, we focus on learning phrase embeddings from the view of semantic meaning, so that our phrase embedding can fully represent the phrase and best fit the phrase-based SMT
p449
aVAssuming the phrase is a meaningful composition of its internal words, we propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings
p450
aVThe core idea behind is that a phrase and its correct translation should share the same semantic meaning
p451
aVThus, they can supervise each other to learn their semantic phrase embeddings
p452
aVIn our method, the standard recursive auto-encoder (RAE) pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error [ 22 ] , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs
p453
aVIf we learn the embedding of the Chinese phrase correctly, we can regard it as the gold representation for the English phrase and use it to guide the process of learning English phrase embedding
p454
aVThis procedure can be performed with an co-training style algorithm so as to minimize the semantic distance between the translation equivalents 1 1 For simplicity, we do not show non-translation pairs here
p455
aVWith the learned model, we can accurately measure the semantic similarity between a source phrase and a translation candidate
p456
aVAccordingly, we evaluate the BRAE model on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to check whether a translation candidate and the source phrase are in the same meaning
p457
aVIn decoding with phrasal semantic similarities, we apply the semantic similarities of the phrase pairs as new features during decoding to guide translation candidate selection
p458
aVThe experiments show that up to 72% of the phrase table can be discarded without significant decrease on the translation quality, and in decoding with phrasal semantic similarities up to 1.7 BLEU score improvement over the state-of-the-art baseline can be achieved
p459
aVInstead, our bilingually-constrained recursive auto-encoders not only learn the composition mechanism of generating phrases from words, but also fine tune the word embeddings during the model training stage, so that we can induce the full information of the phrases and internal words
p460
aVHowever, this kind of phrase embedding is hard to capture full semantics since the context of a phrase is limited
p461
aVFurthermore, this method can only account for a very small part of phrases, since most of the phrases are compositional
p462
aVThe third method views any phrase as the meaningful composition of its internal words
p463
aVThe recursive auto-encoder is typically adopted to learn the way of composition [ 22 , 23 , 21 , 24 , 16 ]
p464
aVAnd then, they fine-tune the RAE according to the label of the phrase, such as the syntactic category in parsing (Socher et al., 2013a), the polarity in sentiment analysis [ 23 , 24 ] , and the reordering pattern in SMT [ 16 ]
p465
aVSecond, even though we have no correct semantic phrase representation as the gold label, the phrases sharing the same meaning provide an indirect but feasible way
p466
aVAssuming we are given a phrase w 1 u'\u005cu2062' w 2 u'\u005cu2062' u'\u005cu22ef' u'\u005cu2062' w m , it is first projected into a list of vectors ( x 1 , x 2 , u'\u005cu22ef' , x m ) using Eq
p467
aVIn order to apply this auto-encoder to each pair of children, the representation of the parent p should have the same dimensionality as the c i u'\u005cu2019' s
p468
aVTo assess how well the parent u'\u005cu2019' s vector represents its children, the standard auto-encoder reconstructs the children in a reconstruction layer
p469
aV2 again to compute y 2 by setting the children to be [ c 1 ; c 2 ] = [ y 1 ; x 3 ]
p470
aVSeveral researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis [ 23 ] , syntactic category in parsing [ 21 ] and phrase reordering pattern in SMT [ 16 ]
p471
aVBy optimizing the above objective, the phrases in the vector embedding space will be grouped according to the labels
p472
aVWe know from the semi-supervised phrase embedding that the learned vector representation can be well adapted to the given label
p473
aVTherefore, we can imagine that learning semantic phrase embedding is reasonable if we are given gold vector representations of the phrases
p474
aVFortunately, we know the fact that the two phrases should share the same semantic representation if they express the same meaning
p475
aVWe can make inference from this fact that if a model can learn the same embedding for any phrase pair sharing the same meaning, the learned embedding must encode the semantics of the phrases and the corresponding model is our desire
p476
aVAs translation equivalents share the same semantic meaning, we employ high-quality phrase translation pairs as training corpus in this work
p477
aVAccordingly, we propose the Bilingually-constrained Recursive Auto-encoders (BRAE), whose basic goal is to minimize the semantic distance between the phrases and their translations
p478
aVSince word embeddings for two languages are learned separately and locate in different vector space, we do not enforce the phrase embeddings in two languages to be in the same semantic vector space
p479
aVWe suppose there is a transformation between the two semantic embedding spaces
p480
aVThus, the semantic distance is bidirectional the distance between p t and the transformation of p s , and that between p s and the transformation of p t
p481
aVAs a result, the overall semantic error becomes
p482
aVHowever, the current model cannot guarantee this since the above semantic error E s u'\u005cu2062' e u'\u005cu2062' m ( s t , u'\u005cu0398' ) only accounts for positive ones
p483
aVWe thus enhance the semantic error with both positive and negative examples, and the corresponding max-semantic-margin error becomes
p484
aVUsing the above error function, we need to construct a negative example for each positive example
p485
aVSuppose we are given a positive example ( s , t ) , the correct translation t can be converted into a bad translation t u'\u005cu2032' by replacing the words in t with randomly chosen target language words
p486
aVAssuming the target phrase representation p t is available, the optimization of the source-side parameters is similar to that of semi-supervised RAE
p487
aVIn parameter initialization, u'\u005cu0398' r u'\u005cu2062' e u'\u005cu2062' c and u'\u005cu0398' s u'\u005cu2062' e u'\u005cu2062' m for the source language is randomly set according to a normal distribution
p488
aVWe prefer to the second one since this kind of word embedding has already encoded some semantics of the words
p489
aVTermination Check if the joint error reaches a local minima or the iterations reach the pre-defined number (25 is used in our experiments), we terminate the training procedure, otherwise we set p s = p s u'\u005cu2032' , p t = p t u'\u005cu2032' , and go to step 2
p490
aVThe SMT evaluation is conducted on Chinese-to-English translation
p491
aVAccordingly, our BRAE model is trained on Chinese and English
p492
aVThe NIST MT03 is used as the development data
p493
aVNIST MT04-06 and MT08 (news data) are used as the test data
p494
aVCase-insensitive BLEU is employed as the evaluation metric
p495
aVThese algorithms are based on corpus statistics including co-occurrence statistics, phrase pair usage and composition information
p496
aVIn experiments, we discard the phrase pair whose similarity in two directions are smaller than a threshold 3 3 To avoid the situation that all the translation candidates for a source phrase are pruned, we always keep the first 10 best according to the semantic similarity
p497
aVWhen the two algorithms using a similar portion of the phrase table 4 4 In the future, we will compare the performance by enforcing the two algorithms to use the same portion of phrase table (35% in BRAE and 36% in Significance), the BRAE-based algorithm outperforms the Significance algorithm on all the test sets except for MT04
p498
aVFurthermore, our model is much more intuitive because it is directly based on the semantic similarity
p499
aVBesides using the semantic similarities to prune the phrase table, we also employ them as two informative features like the phrase translation probability to guide translation hypotheses selection during decoding
p500
aVThe phrase translation probability is based on co-occurrence statistics and the lexical weights consider the phrase as bag-of-words
p501
aVIn contrast, our BRAE model focuses on compositional semantics from words to phrases
p502
aVTherefore, the semantic similarities computed using our BRAE model are complementary to the existing four translation probabilities
p503
aVIn order to investigate the influence of the dimensionality of the embedding space, we have tried three different settings n = 50 , 100 , 200
p504
aVAs shown in Table 2, no matter what n is, the BRAE model can significantly improve the translation quality in the overall test data
p505
aVIn contrast, our BRAE model learns the semantic meaning for each phrase no matter whether it is short or relatively long
p506
aVThis indicates that the proposed BRAE model is effective at learning semantic phrase embeddings
p507
aVAs the semantic phrase embedding can fully represent the phrase, we can go a step further in the phrase-based SMT and feed the semantic phrase embeddings to DNN in order to model the whole translation process (e.g., derivation structure prediction
p508
aVBesides SMT, the semantic phrase embeddings can be used in other cross-lingual tasks, such as cross-lingual question answering, since the semantic similarity between phrases in different languages can be calculated accurately
p509
aVIn fact, the phrases having the same meaning are translation equivalents in different languages, but are paraphrases in one language
p510
aVTherefore, our model can be easily adapted to learn semantic phrase embeddings using paraphrases
p511
aV1) we will try to model the decoding process with DNN based on our semantic embeddings of the basic translation units
p512
aVWe thank Nan Yang for sharing the baseline code and anonymous reviewers for their valuable comments
p513
asg88
(lp514
sg90
(lp515
sg92
(lp516
VThis paper has explored the bilingually-constrained recursive auto-encoders in learning phrase embeddings, which can distinguish phrases with different semantic meanings.
p517
aVWith the objective to minimize the semantic distance between translation equivalents and maximize the semantic distance between non-translation pairs simultaneously, the learned model can semantically embed any phrase in two languages and can transform the semantic space in one language to the other.
p518
aVTwo end-to-end SMT tasks are involved to test the power of the proposed model at learning the semantic phrase embeddings.
p519
aVThe experimental results show that the BRAE model is remarkably effective in phrase table pruning and decoding with phrasal semantic similarities.
p520
aVWe have also discussed many other potential applications and extensions of our BRAE model.
p521
aVIn the future work, we will explore four directions.
p522
aV1) we will try to model the decoding process with DNN based on our semantic embeddings of the basic translation units.
p523
aV2) we are going to learn semantic phrase embeddings with the paraphrase corpus.
p524
aV3) we will apply the BRAE model in other monolingual and cross-lingual tasks.
p525
aV4) we plan to learn semantic sentence embeddings by automatically learning different weight matrices for different nodes in the BRAE model.
p526
ag106
asg107
S'P14-1011'
p527
sg109
(lp528
VWe propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings.
p529
aVThe BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously.
p530
aVAfter training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other.
p531
aVWe evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates.
p532
aVExtensive experiments show that the BRAE is remarkably effective in these two tasks.
p533
ag106
asba(icmyPackage
FText
p534
(dp535
g3
(lp536
VIn this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep auto-encoder (DAE) paradigm for phrase-based translation model
p537
aVUsing the unsupervised pre-trained deep belief net (DBN) to initialize DAE u'\u005cu2019' s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features
p538
aVHowever, most of these features are manually designed on linguistic phenomena that are related to bilingual language pairs, thus they are very difficult to devise and estimate
p539
aVInstead of designing new features based on intuition, linguistic knowledge and domain, for the first time, Maskey and Zhou (2012) explored the possibility of inducing new features in an unsupervised fashion using deep belief net (DBN) [ Hinton et al.2006 ] for hierarchical phrase-based translation model
p540
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p541
aVThese new features are appended as extra features to the phrase table for the translation decoder
p542
aVHowever, the above approach has two major shortcomings
p543
aVSecond, it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines (RBM) [ Hinton2002 ] , does not have a training objective, so its performance relies on the empirical parameters
p544
aVThus, this approach is unstable and the improvement is limited
p545
aVTo address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments
p546
aVTo address the second shortcoming, inspired by the successful use of DAEs for handwritten digits recognition [ Hinton and Salakhutdinov2006 , Hinton et al.2006 ] , information retrieval [ Salakhutdinov and Hinton2009 , Mirowski et al.2010 ] , and speech spectrograms [ Deng et al.2010 ] , we propose new feature learning using semi-supervised DAE for phrase-based translation model
p547
aVBy using the input data as the teacher, the u'\u005cu201c' semi-supervised u'\u005cu201d' fine-tuning process of DAE addresses the problem of u'\u005cu201c' back-propagation without a teacher u'\u005cu201d' [ Rumelhart et al.1986 ] , which makes the DAE learn more powerful and abstract features [ Hinton and Salakhutdinov2006 ]
p548
aVFor our semi-supervised DAE feature learning task, we use the unsupervised pre-trained DBN to initialize DAE u'\u005cu2019' s parameters and use the input original phrase features as the u'\u005cu201c' teacher u'\u005cu201d' for semi-supervised back-propagation
p549
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable
p550
aVIt is encouraging that, non-parametric feature expansion using gaussian mixture model (GMM) [ Nguyen et al.2007 ] , which guarantees invariance to the specific embodiment of the original features, has been proved as a feasible feature generation approach for SMT
p551
aVDeep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM
p552
aVThus, instead of GMM, we use DNN (DBN, DAE and HCDAE) to learn new non-parametric features, which has the similar evolution in speech recognition [ Dahl et al.2012 , Hinton et al.2012 ]
p553
aV2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations
p554
aV2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words
p555
aV2013) presented an ITG reordering classifier based on recursive auto-encoders, and generated vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information
p556
aVHowever, none of these above works have focused on learning new features automatically with input data, and while learning suitable features (representations) is the superiority of DNN since it has been proposed
p557
aVNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p558
aVWe assume that source phrase f = f 1 , u'\u005cu22ef' , f l f and target phrase e = e 1 , u'\u005cu22ef' , e l e include l f and l e words, respectively
p559
aV2004) proposed a way of using term weight based models in a vector space as additional evidences for phrase pair translation quality
p560
aVThis model employ phrase pair similarity to encode the weights of content and non-content words in phrase translation pairs
p561
aVK = k 1 u'\u005cu2062' ( ( 1 - b ) + J / a u'\u005cu2062' v u'\u005cu2062' g u'\u005cu2062' ( l ) ) , and J is the phrase length ( l e or l f ), a u'\u005cu2062' v u'\u005cu2062' g u'\u005cu2062' ( l ) is the average phrase length
p562
aVThus, we have the second type of input features
p563
aVWe adapt and extend bidirectional phrase generative probabilities as the input features, which have been used for domain adaptation [ Foster et al.2010 ]
p564
aVAccording to the background LMs, we estimate the bidirectional (source/target side) forward and backward phrase generative probabilities as
p565
aV2011), which successfully capture both the preceding and succeeding contexts of the current word, and we estimate the backward LM by inverting the order in each sentence in the training data from the original order to the reverse order background 4-gram LMs are trained by the corresponding side of bilingual corpus 2 2 This corpus is used to train the translation model in our experiments, and we will describe it in detail in section 5.1
p566
aVWe consider bidirectional phrase frequency as the input features, and estimate them as
p567
aVWe normalize bidirectional phrase length by the maximum phrase length, and introduce them as the last type of input features
p568
aVIn summary, except for the first type of phrase feature X 1 which is used by [ Maskey and Zhou2012 ] , we introduce another four types of effective phrase features X 2 , X 3 , X 4 and X 5
p569
aVAfter learning the first RBM, we treat the activation probabilities of its hidden units, when they are being driven by data, as the data for training a second RBM
p570
aVSimilarly, a n t u'\u005cu2062' h RBM is built on the output of the n - 1 t u'\u005cu2062' h one and so on until a sufficiently deep architecture is created
p571
aVTo deal with real-valued input features X in our task, we use an RBM with Gaussian visible units (GRBM) [ Dahl et al.2012 ] with a variance of 1 on each dimension
p572
aVHence, P ( v h ) and E u'\u005cu2062' ( v , h ) in the first RBM of DBN need to be modified as
p573
aVAfter the pre-training, for each phrase pair in the phrase table, we generate the DBN features [ Maskey and Zhou2012 ] by passing the original phrase features X through the DBN using forward computation
p574
aVTo learn a semi-supervised DAE, we first u'\u005cu201c' unroll u'\u005cu201d' the above n layer DBN by using its weight matrices to create a deep, 2n-1 layer network whose lower layers use the matrices to u'\u005cu201c' encode u'\u005cu201d' the input and whose upper layers use the matrices in reverse order to u'\u005cu201c' decode u'\u005cu201d' the input [ Hinton and Salakhutdinov2006 , Salakhutdinov and Hinton2009 , Deng et al.2010 ] , as shown in Figure 2
p575
aVThe layer-wise learning of DBN as above must be treated as a pre-training stage that finds a good region of the parameter space, which is used to initialize our DAE u'\u005cu2019' s parameters
p576
aVStarting in this region, the DAE is then fine-tuned using average squared error (between the output and input) back-propagation to minimize reconstruction error, as to make its output as equal as possible to its input
p577
aVFor the fine-tuning of DAE, we use the method of conjugate gradients on larger mini-batches of 1000 cases, with three line searches performed for each mini-batch in each epoch
p578
aVAfter the fine-tuning, for each phrase pair in the phrase table, we estimate our DAE features by passing the original phrase features X through the u'\u005cu201c' encoder u'\u005cu201d' part of the DAE using forward computation
p579
aVThen, we append these features for each phrase pair to the phrase table as extra features
p580
aVAlthough DAE can learn more powerful and abstract feature representation, the learned features usually have smaller dimensionality compared with the dimensionality of the input features, such as the successful use for handwritten digits recognition [ Hinton and Salakhutdinov2006 , Hinton et al.2006 ] , information retrieval [ Salakhutdinov and Hinton2009 , Mirowski et al.2010 ] , and speech spectrograms [ Deng et al.2010 ]
p581
aVMoreover, although we have introduced another four types of phrase features ( X 2 , X 3 , X 4 and X 5 ), the only 16 features in X are a bottleneck for learning large hidden layers feature representation, because it has limited information, the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory
p582
aVTo learn high-dimensional feature representation and to further improve the performance, we introduce a natural horizontal composition for DAEs that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs [ Baldi2012 ] , as shown in Figure 3
p583
aVTwo single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture
p584
aVThus, these new m 1 + m 2 -dimensional DAE features are added as extra features to the phrase table
p585
aVIn our task, we introduce differences by using different initializations and different fractions of the phrase table
p586
aVAlthough we have pre-trained the corresponding DBN, this DAE network is so deep, the fine-tuning does not work very well and typically finds poor local minima
p587
aVWe suspect this leads to the decreased performance
p588
aVIn this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we have learned new features using the DAE for the phrase-based translation model
p589
aVUsing the unsupervised pre-trained DBN to initialize DAE u'\u005cu2019' s parameters and using the input original phrase features as the u'\u005cu201c' teacher u'\u005cu201d' for semi-supervised back-propagation, our semi-supervised DAE features are more effective and stable than the unsupervised DBN features [ Maskey and Zhou2012 ]
p590
aVMoreover, to further improve the performance, we introduce some simple but effective features as the input features for feature learning
p591
asg88
(lp592
sg90
(lp593
sg92
(lp594
VIn this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we have learned new features using the DAE for the phrase-based translation model.
p595
aVUsing the unsupervised pre-trained DBN to initialize DAE s parameters and using the input original phrase features as the teacher for semi-supervised back-propagation, our semi-supervised DAE features are more effective and stable than the unsupervised DBN features [ Maskey and Zhou2012 ].
p596
aVMoreover, to further improve the performance, we introduce some simple but effective features as the input features for feature learning.
p597
aVLastly, to learn high dimensional feature representation, we introduce a natural horizontal composition of two DAEs for large hidden layers feature learning.
p598
aVOn two Chinese-English translation tasks, the results demonstrate that our solutions solve the two aforementioned shortcomings successfully.
p599
aVFirstly, our DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the DBN features and the baseline features, respectively.
p600
aVSecondly, compared with the baseline phrase features X 1 , our introduced input original phrase features X significantly improve the performance of not only our DAE features but also the DBN features.
p601
aVThe results also demonstrate that DNN (DAE and HCDAE) features are complementary to the original features for SMT, and adding them together obtain statistically significant improvements of 3.16 (IWSLT) and 2.06 (NIST) BLEU points over the baseline features.
p602
aVCompared with the original features, DNN (DAE and HCDAE) features are learned from the non-linear combination of the original features, they strong capture high-order correlations between the activities of the original features, and we believe this deep learning paradigm induces the original features to further reach their potential for SMT.
p603
ag106
asg107
S'P14-1012'
p604
sg109
(lp605
VIn this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep auto-encoder (DAE) paradigm for phrase-based translation model.
p606
aVUsing the unsupervised pre-trained deep belief net (DBN) to initialize DAE s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features.
p607
aVMoreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning.
p608
aVOn two Chinese-English tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the baseline features, respectively.
p609
ag106
asba(icmyPackage
FText
p610
(dp611
g3
(lp612
VHowever, it is often limited to contexts within sentence boundaries, hence broader topical information cannot be leveraged
p613
aVBy associating each translation rule with the topic representation, topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding
p614
aVIn this case, people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge
p615
aVTherefore, it is important to leverage topic information to learn smarter translation models and achieve better translation performance
p616
aVTopic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents
p617
aVHowever, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries
p618
aVIn addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency
p619
aVThis makes previous approaches inefficient when applied them in real-world commercial SMT systems
p620
aVTherefore, we need to devise a systematical approach to enriching the sentence and inferring its topic more accurately
p621
aVSince the information within the sentence is insufficient for topic modeling, we first enrich sentence contexts via Information Retrieval (IR) methods using content words in the sentence as queries, so that topic-related monolingual documents can be collected
p622
aVNeural network is an effective technique for learning different levels of data representations
p623
aVOur problem fits well into the neural network framework and we expect that it can further improve inferring the topic representations for sentences
p624
aVTo incorporate topic representations as translation knowledge into SMT, our neural network based approach directly optimizes similarities between the source language and target language in a compact topic space
p625
aVAdditionally, our model can be discriminatively trained with a large number of training instances, without expensive sampling methods such as in LDA or HTMM, thus it is more practicable and scalable
p626
aVTopic-related rules are selected according to distributional similarity with the source text, which helps hypotheses generation in SMT decoding
p627
aVThen, in the fine-tuning phase (Section 3.2), our model directly optimizes the similarity of two low-dimensional representations, so that it highly correlates to SMT decoding
p628
aVThis dense representation should preserve the information from the bag-of-words input, meanwhile alleviate data sparse problem
p629
aVTherefore, we use a specially designed mechanism called auto-encoder to solve this problem
p630
aVAssuming that the input is a n -of- V binary vector x representing the bag-of-words ( V is the vocabulary size), an auto-encoder consists of an encoding process g u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' ) and a decoding process h u'\u005cu2062' ( g u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' )
p631
aVThis is done by corrupting the initial input x to get a partially destroyed version u'\u005cud835' u'\u005cudc31' ~
p632
aVWith DAE, the input x is manually corrupted by applying masking noise (randomly mask 1 to 0) and getting u'\u005cud835' u'\u005cudc31' ~
p633
aVDenoising training is considered as u'\u005cu201c' filling in the blanks u'\u005cu201d' [ 33 ] , which means the masking components can be recovered from the non-corrupted components
p634
aVFor example, in IT related texts, if the word driver is masked, it should be predicted through hidden units in neural networks by active signals such as u'\u005cu201c' buffer u'\u005cu201d' , u'\u005cu201c' user response u'\u005cu201d' , etc
p635
aVAssuming that the dimension of the g u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' ~ ) is L , the linear layer forms a L × V matrix W which projects the n -of- V vector to a L -dimensional hidden layer
p636
aVIn this work, we use the rectifier function as our non-linear function due to its efficiency and better performance [ 13 ]
p637
aVThe decoding process consists of a linear layer and a non-linear layer with similar network structures, but different parameters
p638
aVTo minimize reconstruction error with respect to u'\u005cud835' u'\u005cudc31' ~ , we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input
p639
aVMulti-layer neural networks are trained with the standard back-propagation algorithm [ 26 ]
p640
aVThe similarity scores are integrated into the standard log-linear model for making translation decisions
p641
aVSince the vectors from DAE are trained using information from monolingual training data independently, these vectors may be inadequate to measure bilingual topic similarity due to their different topic spaces
p642
aVTherefore, in this stage, parallel sentence pairs are used to help connecting the vectors from different languages because they express the same topic
p643
aVGiven a parallel sentence pair u'\u005cu27e8' f , e u'\u005cu27e9' , the DAE learns representations for f and e respectively, as u'\u005cud835' u'\u005cudc33' f = g u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1f' ) and u'\u005cud835' u'\u005cudc33' e = g u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1e' ) in Figure 1
p644
aVWe then take two vectors as the input to calculate their similarity
p645
aVConsequently, the whole neural network can be fine-tuned towards the supervised criteria with the help of parallel data
p646
aVThe similarity score of the representation pair u'\u005cu27e8' u'\u005cud835' u'\u005cudc33' f , u'\u005cud835' u'\u005cudc33' e u'\u005cu27e9' is defined as the cosine similarity of the two vectors
p647
aVSince a parallel sentence pair should have the same topic, our goal is to maximize the similarity score between the source sentence and target sentence
p648
aVInspired by the contrastive estimation method [ 27 ] , for each parallel sentence pair u'\u005cu27e8' f , e u'\u005cu27e9' as a positive instance, we select another sentence pair u'\u005cu27e8' f u'\u005cu2032' , e u'\u005cu2032' u'\u005cu27e9' from the training data and treat u'\u005cu27e8' f , e u'\u005cu2032' u'\u005cu27e9' as a negative instance
p649
aVSince different sentences may have very similar topic distributions, we select negative instances that are dissimilar with the positive instances based on the following criteria
p650
aVWhen a synchronous rule u'\u005cu27e8' u'\u005cu0391' , u'\u005cu0393' u'\u005cu27e9' is extracted from a sentence pair u'\u005cu27e8' f , e u'\u005cu27e9' , a triple instance u'\u005cu2110' = ( u'\u005cu27e8' u'\u005cu0391' , u'\u005cu0393' u'\u005cu27e9' , u'\u005cu27e8' f , e u'\u005cu27e9' , c ) is collected for inferring the topic representation of u'\u005cu27e8' u'\u005cu0391' , u'\u005cu0393' u'\u005cu27e9' , where c is the count of rule occurrence
p651
aVThe topic representation of u'\u005cu27e8' u'\u005cu0391' , u'\u005cu0393' u'\u005cu27e9' is then calculated as the weighted average
p652
aVwhere u'\u005cud835' u'\u005cudcaf' denotes all instances for the rule u'\u005cu27e8' u'\u005cu0391' , u'\u005cu0393' u'\u005cu27e9' , u'\u005cud835' u'\u005cudc33' u'\u005cu0391' and u'\u005cud835' u'\u005cudc33' u'\u005cu0393' are the source-side and target-side topic vectors respectively
p653
aVBy measuring the similarity between the source texts and bilingual translation rules, the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates
p654
aVTherefore, it helps to train a smarter translation model with the embedded topic information
p655
aVWe also consider the topic sensitivity estimation since general rules have flatter distributions while topic-specific rules have sharper distributions
p656
aVThe vocabulary size for the input layer is 100,000, and we choose different lengths for the hidden layer as L = { 100 , 300 , 600 , 1000 } in the experiments
p657
aVIn the pre-training phase, all parallel data is fed into two neural networks respectively for DAE training, where network parameters W and b are randomly initialized
p658
aVThe CKY decoding algorithm is used and cube pruning is performed with the same default parameter settings as in Chiang ( 2007
p659
aVThe parallel data we use is released by LDC 3 3 LDC2003E14, LDC2002E18, LDC2003E07, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09
p660
aVThe phrase pairs that appear only once in the parallel data are discarded because most of them are noisy
p661
aVSince some of our parallel data does not have document-level information, we rely on the IR method to retrieve the most relevant document and simulate this approach
p662
aVHowever, we find that as N becomes larger in the experiments, e.g., N =50, the translation accuracy drops drastically
p663
aVAs more documents are retrieved, less relevant information is also used to train the neural networks
p664
aVIrrelevant documents bring so many unrelated topic words hence degrade neural network learning performance
p665
aVIn deep learning, this parameter is often empirically tuned with human efforts
p666
aVAs shown in Figure 3 , the translation accuracy is better when L is relatively small
p667
aVHowever, when L equals 1,000, the translation accuracy is inferior to other settings
p668
aVThe main reason is that parameters in the neural networks are too many to be effectively trained
p669
aVAs we know when L =1000, there are a total of 100 , 000 × 1 , 000 parameters between the linear and non-linear layers in the network
p670
aVLimited training data prevents the model from getting close to the global optimum
p671
aVTherefore, the model is likely to fall in local optima and lead to unacceptable representations
p672
aVThe results confirm that topic information is indispensable for SMT since both [ 34 ] and our neural network based method significantly outperforms the baseline system
p673
aVBecause topic-specific rules usually have a larger sensitivity score, they can beat general rules when they obtain the same similarity score against the input sentence
p674
aVThis is not simply coincidence since we can interpret their approach as a special case in our neural network method when a parallel sentence pair has document-level information, that document will be retrieved for training; otherwise, the most relevant document will be retrieved from the monolingual data
p675
aVTherefore, our method can be viewed as a more general framework than previous LDA-based approaches
p676
aVAlthough the translation probability of u'\u005cu201c' send X u'\u005cu201d' is much higher, it is inappropriate in this context since it is usually used in IT texts
p677
aVThe similarity scores indicate that u'\u005cu201c' deliver X u'\u005cu201d' and u'\u005cu201c' distribute X u'\u005cu201d' are more appropriate to translate the sentence
p678
aVTherefore, adding topic-related features is able to keep the topic consistency and substantially improve the translation accuracy
p679
aVThe advantage of our method is that it is applicable to both sentence-level and document-level SMT, since we do not place any restrictions on the input
p680
aVWe proposed a more general approach to leveraging topic information for SMT by using IR methods to get a collection of related documents, regardless of whether or not document boundaries are explicitly given
p681
aVWe directly optimized bilingual topic similarity in the deep learning framework with the help of sentence-level parallel data, so that the learned representation could be easily used in SMT decoding procedure
p682
aVIn SMT decoding, appropriate rules are selected to best match source texts according to their similarity in the topic space
p683
aVSince the translation of the current sentence is usually influenced by the topic of previous sentences, we plan to leverage recurrent neural networks to model this phenomenon, where the history translation information is naturally combined in the model
p684
asg88
(lp685
sg90
(lp686
sg92
(lp687
VIn this paper, we propose a neural network based approach to learning bilingual topic representation for SMT.
p688
aVWe enrich contexts of parallel sentence pairs with topic related monolingual data and obtain a set of documents to represent sentences.
p689
aVThese documents are converted to a bag-of-words input and fed into neural networks.
p690
aVThe learned low-dimensional vector is used to obtain the topic representations of synchronous rules.
p691
aVIn SMT decoding, appropriate rules are selected to best match source texts according to their similarity in the topic space.
p692
aVExperimental results show that our approach is promising for SMT systems to learn a better translation model.
p693
aVIt is a significant improvement over the state-of-the-art Hiero system, as well as a conventional LDA-based method.
p694
aVIn the future research, we will extend our neural network methods to address document-level translation, where topic transition between sentences is a crucial problem to be solved.
p695
aVSince the translation of the current sentence is usually influenced by the topic of previous sentences, we plan to leverage recurrent neural networks to model this phenomenon, where the history translation information is naturally combined in the model.
p696
ag106
asg107
S'P14-1013'
p697
sg109
(lp698
VStatistical Machine Translation (SMT) usually utilizes contextual information to disambiguate translation candidates.
p699
aVHowever, it is often limited to contexts within sentence boundaries, hence broader topical information cannot be leveraged.
p700
aVIn this paper, we propose a novel approach to learning topic representation for parallel data using a neural network architecture, where abundant topical contexts are embedded via topic relevant monolingual data.
p701
aVBy associating each translation rule with the topic representation, topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding.
p702
aVExperimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline.
p703
aV1] LeiCui 2] DongdongZhang 2] ShujieLiu 3] QimingChen 2] MuLi 2] MingZhou 1] MuyunYang \u005caffil [1]SchoolofComputerScienceandTechnology,HarbinInstituteofTechnology,Harbin,P.R.China \u005cauthorcr leicui@hit.edu.cn,ymy@mtlab.hit.edu.cn \u005caffil [2]MicrosoftResearch,Beijing,P.R.China \u005cauthorcr { dozhang,shujliu,muli,mingzhou } @microsoft.com \u005caffil [3]ShanghaiJiaoTongUniversity,Shanghai,P.R.China \u005cauthorcr simoncqm@gmail.com.
p704
aVUTF8gkai.
p705
ag106
asba(icmyPackage
FText
p706
(dp707
g3
(lp708
VAnalysing and extracting useful information from the web has become an increasingly important research direction for the NLP community, where many tasks require part-of-speech (POS) tagging as a fundamental preprocessing step
p709
aVHowever, state-of-the-art POS taggers in the literature [ 5 , 23 ] are mainly optimized on the the Penn Treebank (PTB), and when shifted to web data, tagging accuracies drop significantly [ 18 ]
p710
aVThe problem we face here can be considered as a special case of domain adaptation , where we have access to labelled data on the source domain (PTB) and unlabelled data on the target domain (web data
p711
aVWe integrate the learned encoder with a set of well-established features for POS tagging [ 21 , 5 ] in a single neural network, which is applied as a scorer to an easy-first POS tagger
p712
aVWe choose the easy-first tagging approach since it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger [ 23 , 15 ]
p713
aVThe idea of learning representations from unlabelled data and then fine-tuning a model with such representations according to some supervised criterion has been studied before [ 26 , 6 , 8 ]
p714
aVPrevious work treats the learned representations either as model parameters that are further optimized in supervised fine-tuning [ 6 ] or as fixed features that are kept unchanged [ 26 , 8 ]
p715
aVOur method achieves a 93.27 u'\u005cu2062' % average accuracy across the web-domain, which is the best result reported so far on this data set, higher than those given by ensembled syntactic parsers
p716
aVWhile in our case, each visible variable corresponds to a word, which may take on tens-of-thousands of different values
p717
aVTherefore, the RBM need to be re-factorized to make inference tractable
p718
aVBy adopting greedy layer-wise training [ 10 , 2 ] , DBNs are capable of modelling higher order non-linear relations between the input, and has been demonstrated to improve performance for many computer vision tasks [ 10 , 2 , 13 ]
p719
aVHowever, in this work we do not observe further improvement by employing DBNs
p720
aVThis may partly be due to the fact that unlike computer vision tasks, the input structure of POS tagging or other sequential labelling tasks is relatively simple, and a single non-linear layer is enough to model the interactions within the input [ 27 ]
p721
aVThe main challenge to designing the neural network structure is on the one hand, we hope that the model can take the advantage of information provided by the learned WRRBM, which reflects general properties of web texts, so that the model generalizes well in the web domain; on the other hand, we also hope to improve the model u'\u005cu2019' s discriminative power by utilizing well-established POS tagging features, such as those of Ratnaparkhi ( 1996 )
p722
aVOur approach is to leverage the two sources of information in one neural network by combining them though a shared output layer, as shown in Figure 1
p723
aVThis can be achieved by initializing only the first layer of the web module with the projection matrix u'\u005cud835' u'\u005cudc03' of the learned WRRBM
p724
aVAlternatively, we can choose to use the hidden states of the WRRBM, which can be treated as the representations of the input n-gram
p725
aVThis can be achieved by also initializing the parameters of the second layer of the web-feature module using the position-dependent weight matrix and hidden bias of the learned WRRBM
p726
aVThe input for this module is a vector of boolean values u'\u005cu03a6' u'\u005cu2062' ( x ) = ( f 1 u'\u005cu2062' ( x ) , u'\u005cu2026' , f k u'\u005cu2062' ( x ) ) , where x denotes the partially tagged input sentence and f i u'\u005cu2062' ( x ) denotes a feature function, which returns 1 if the corresponding feature fires and 0 otherwise
p727
aVSuch distribution can be easily obtained by adding a soft-max layer on top of the output layer to perform a local normalization, as done by Collobert et al
p728
aVRather than tagging a sentence from left to right, easy-first tagging is based on a deterministic process, repeatedly selecting the easiest word to tag
p729
aVHere u'\u005cu201c' easiness u'\u005cu201d' is evaluated based on a statistical model
p730
aVAt each step during the processing of a training example, the algorithm calculates a margin loss based on two word-tag pairs ( w ¯ , t ¯ ) and ( w ^ , t ^ ) (line 4 u'\u005cu223c' line 6 w ¯ , t ¯ ) denotes the word-tag pair that has the highest model score among those that are inconsistent with the gold standard, while ( w ^ , t ^ ) denotes the one that has the highest model score among those that are consistent with the gold standard
p731
aVIf the loss is zero, the algorithm continues to process the next untagged word
p732
aVThe standard back-propagation algorithm [ 22 ] cannot be applied directly
p733
aVThis is because the standard loss is calculated based on a unique input vector
p734
aVThis condition does not hold in our case, because w ^ and w ¯ may refer to different words, which means that the margin loss in line 6 of Algorithm 2 is calculated based on two different input vectors, denoted by u'\u005cu27e8' w ^ u'\u005cu27e9' and u'\u005cu27e8' w ¯ u'\u005cu27e9' , respectively
p735
aVWhile previous work [ 23 , 29 , 9 ] apply guided learning to train a linear classifier by using variants of the perceptron algorithm, we are the first to combine guided learning with a neural network, by using a margin loss and a modified back-propagation algorithm
p736
aV[t] Training over one sentence {algorithmic} [1] \u005cREQUIRE ( x , t ) a tagged sentence, neural net n u'\u005cu2062' n \u005cENSURE updated neural net n u'\u005cu2062' n u'\u005cu2032'
p737
aVu'\u005cud835' u'\u005cudc14' u'\u005cu2260' [ ] \u005cSTATE ( w ¯ , t ¯ ) u'\u005cu2190' arg u'\u005cu2062' max ( w , t ) u'\u005cu2208' ( u'\u005cud835' u'\u005cudc14' × u'\u005cud835' u'\u005cudc13' / u'\u005cud835' u'\u005cudc11' ) u'\u005cu2061' n u'\u005cu2062' n u'\u005cu2062' ( w , t ) \u005cSTATE ( w ^ , t ^ ) u'\u005cu2190' arg u'\u005cu2062' max ( w , t ) u'\u005cu2208' u'\u005cud835' u'\u005cudc11' u'\u005cu2061' n u'\u005cu2062' n u'\u005cu2062' ( w , t
p738
aVl u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' s u'\u005cu2190' max u'\u005cu2061' ( 0 , 1 + n u'\u005cu2062' n u'\u005cu2062' ( w ¯ , t ¯ ) - n u'\u005cu2062' n u'\u005cu2062' ( w ^ , t ^ ) ) \u005cIF l u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' s 0 \u005cSTATE e ^ u'\u005cu2190' n u'\u005cu2062' n
p739
aVWhile emails and weblogs are used as the development sets, reviews, news groups and Yahoo!Answers are used as the final test sets
p740
aVThe tagging performance is evaluated according to the official evaluation metrics of SANCL 2012
p741
aVThe tagging accuracy is defined as the percentage of words (punctuations included) that are correctly tagged
p742
aVThe averaged accuracies are calculated across the web domain data
p743
aVThe data sets are generated by first concatenating all the cleaned unlabelled data, then selecting sentences evenly across the concatenated file
p744
aVAll these parameters are selected according to the averaged accuracy on the development set
p745
aVFeature templates are shown in Table 3, which are based on those of Ratnaparkhi (1996) and Shen et al
p746
aVCompared with the performance of the official baseline (row 4 of Table 6), which is evaluated based on the output of BerkeleyParser [ 16 , 17 ] , our baseline tagger achieves comparable accuracies on both the source and target domain data
p747
aVThis is consistent with previous work (Le Roux et al., 2011), which found that for noisy data such as web domain text, data cleaning is a effective and necessary step
p748
aVAs mentioned in Section 3.1, the knowledge learned from the WRRBM can be investigated incrementally, using word representation , which corresponds to initializing only the projection layer of web-feature module with the projection matrix of the learned WRRBM, or ngram-level representation , which corresponds to initializing both the projection and sigmoid layers of the web-feature module by the learned WRRBM
p749
aVHowever, since fine-tuning is conducted with respect to the source domain , adjusting the parameters of the pre-trained representation towards optimizing source domain tagging accuracies would disrupt its ability in modelling the web domain data
p750
aVTherefore, a better idea is to keep the representation unchanged so that we can learn a function that maps the general web-text properties to its syntactic categories
p751
aVThis result illustrates that the ngram-level knowledge captures more complex interactions of the web text, which cannot be recovered by using only word embeddings
p752
aVThe results suggest that using mixed data can achieve almost as good performance as using the target sub-domain data, while using mixed data yields a much more robust tagger across all sub-domains
p753
aVThe best result achieved by using a 4-gram WRRBM, ( w i - 2 , u'\u005cu2026' , w i + 1 ) , with 300 hidden units learned on 1,000k web domain sentences are shown in row 3 of Table 6
p754
aVMoreover, we achieve the highest tagging accuracy reported so far on this data set, surpassing those achieved using parser combinations based on self-training [ 24 , 12 ]
p755
aV2006 ) propose to induce shared representations for domain adaptation, which is based on the alternating structure optimization (ASO) method of Ando and Zhang ( 2005
p756
aVThe new representations are induced based on the auxiliary tasks defined on unlabelled data together with a dimensionality reduction technique
p757
asg88
(lp758
sg90
(lp759
sg92
(lp760
VWe built a web-domain POS tagger using a two-phase approach.
p761
aVWe used a WRRBM to learn the representation of the web text and incorporate the representation in a neural network, which is trained using guided learning for easy-first POS tagging.
p762
aVExperiment showed that our approach achieved significant improvement in tagging the web domain text.
p763
aVIn addition, we found that keeping the learned representations unchanged yields better performance compared with further optimizing them on the source domain data.
p764
aVWe release our tools at https://github.com/majineu/TWeb.
p765
aVFor future work, we would like to investigate the two-phase approach to more challenging tasks, such as web domain syntactic parsing.
p766
aVWe believe that high-accuracy web domain taggers and parsers would benefit a wide range of downstream tasks such as machine translation.
p767
ag106
asg107
S'P14-1014'
p768
sg109
(lp769
VIn this paper 1 1 This work was done while the first author was visiting SUTD we address the problem of web-domain POS tagging using a two-phase approach.
p770
aVThe first phase learns representations that capture regularities underlying web text.
p771
aVThe representation is integrated as features into a neural network that serves as a scorer for an easy-first POS tagger.
p772
aVParameters of the neural network are trained using guided learning in the second phase.
p773
aVExperiment on the SANCL 2012 shared task show that our approach achieves 93.27 % average tagging accuracy, which is the best accuracy reported so far on this data set, higher than those given by ensembled syntactic parsers.
p774
ag106
asba(icmyPackage
FText
p775
(dp776
g3
(lp777
VIdentifying solution posts from discussion forums, hence, is an important research problem
p778
aVOur technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums
p779
aVOur clustering-based iterative solution identification approach based on the EM-formulation performs favorably in an empirical evaluation, beating the only unsupervised solution identification technique from literature by a very large margin
p780
aVWe also show that our unsupervised technique is competitive against methods that require supervision, outperforming one such technique comfortably
p781
aVDiscussion forums have become a popular knowledge source for finding solutions to common problems
p782
aVTypical response posts include solutions or clarification requests, whereas feedback posts form another major category of forum posts
p783
aVAs is the case with any community of humans, discussion forums have their share of inflammatory remarks too
p784
aVSince the first post most usually contains the problem description, identifying its solutions from among the other posts in the thread has been the focus of many recent efforts (e.g.,, [ 8 , 9 ]
p785
aVIn this paper, we address the problem of unsupervised solution post identification 7 7 This problem has been referred to as answer extraction by some papers earlier
p786
aVHowever, we use solution identification to refer to the problem since answer and extraction have other connotations in the Question-Answering and Information Extraction communities respectively from discussion forums
p787
aV[ 3 ] reports a study that illustrates that non-solution posts are, on an average, as similar to the problem as solution posts in technical forums
p788
aVThe second assumption (i.e.,, (b) above) was also not seen to be useful in discussion forums since posts that are highly similar to other posts were seen to be complaints, repetitive content being more pervasive among complaint posts than solutions [ 2 ]
p789
aVHaving exhausted the two obvious textual features for solution identification, subsequent approaches have largely used the presence of lexical cues signifying solution-like narrative (e.g.,, instructive narratives such as u'\u005cu201d' check the router for any connection issues u'\u005cu201d' ) as the primary content-based feature for solution identification
p790
aVAll solution identification approaches since [ 4 ] have used supervised methods that require training data in the form of labeled solution and non-solution posts
p791
aVA variety of high precision assumptions such as solution post typically follows a problem post [ 15 ] , solution posts are likely to be within the first few posts , solution posts are likely to have been acknowledged by the problem post author [ 3 ] , users with high authoritativeness are likely to author solutions [ 9 ] , and so on have been seen to be useful in solution identification
p792
aVBeing supervised methods, the above assumptions are implicitly factored in by including the appropriate feature (e.g.,, post position in thread) in the feature space so that the learner may learn the correlation (e.g.,, solution posts typically are among the first few posts) using the training data
p793
aVIn particular, we show that by using post position as the only non-textual feature, we are able to achieve accuracies comparable to supervision-based approaches that use many structural features [ 2 ]
p794
aVThough most of the answer/solution identification approaches proposed so far in literature are supervised methods that require a labeled training corpus, there are a few that require limited or no supervision
p795
aVThe common observation that most problem-solving discussion threads have a problem description in the first post has been explicitly factored into many techniques; knowing the problem/question is important for solution identification since author relations between problem and other posts provide valuable cues for solution identification
p796
aVOf particular interest to us are approaches that use limited or no supervision, since we focus on unsupervised solution identification in this paper
p797
aVIn addition to various features explored in literature, they use acknowledgement modeling so that posts that have been acknowledged positively may be favored for being labeled as solutions
p798
aVSince we assume, much like many other earlier papers, that the first post is the problem post, the task is to identify which among the remaining t - 1 posts are solutions
p799
aVWe use an IBM Model 1 translation model [ 1 ] in our technique; simplistically, such a model m may be thought of as a 2-d associative array where the value m u'\u005cu2062' [ w 1 ] u'\u005cu2062' [ w 2 ] is directly related to the probability of w 1 occuring in the problem when w 2 occurs in the solution
p800
aVwhere u'\u005cud835' u'\u005cudcaf' S p denotes the multionomial distribution obtained from u'\u005cud835' u'\u005cudcaf' S conditioned over the words in the post p ; this is obtained by assigning each candidate solution word w a weight equal to a u'\u005cu2062' v u'\u005cu2062' g u'\u005cu2062' { u'\u005cud835' u'\u005cudcaf' S u'\u005cu2062' [ w u'\u005cu2032' ] u'\u005cu2062' [ w ] w u'\u005cu2032' u'\u005cu2208' p } , and normalizing such weights across all solution words
p801
aVIn short, each solution word is assumed to be generated from the language model or the translation model (conditioned on the problem words) with a probability of u'\u005cu039b' and 1 - u'\u005cu039b' respectively, thus accounting for the correlation assumption
p802
aVThe generative model above is similar to the proposal in [ 5 ] , adapted suitably for our scenario
p803
aVOf the solution words above, generic words such as try and should could probably be explained by (i.e.,, sampled from) the solution language model, whereas disconnect and rejoin could be correlated well with surf and wifi and hence are more likely to be supported better by the translation model
p804
aVWe propose a clustering based approach so as to cluster each of the ( p , r ) pairs into either the solution cluster or the non-solution cluster
p805
aVF u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' , u'\u005cud835' u'\u005cudcaf' ) indicates the conformance of the ( p , r ) pair (details in Section 4.3.1 ) with the generative model that uses the u'\u005cud835' u'\u005cudcae' and u'\u005cud835' u'\u005cudcaf' models as the language and translation models respectively
p806
aVThe clustering based approach labels each ( p , r ) pair as either solution (i.e.,, S ) or non-solution (i.e.,, N
p807
aVSince we do not know the models or the labelings to start with, we use an iterative approach modeled on the EM meta-algorithm [ 6 ] involving iterations, each comprising of an E-step followed by the M-step
p808
aVFor simplicity and brevity, instead of deriving the EM formulation, we illustrate our approach by making an analogy with the popular K-Means clustering [ 13 ] algorithm that also uses the EM formulation and crisp assignments of data points like we do
p809
aVK-Means is a clustering algorithm that clusters objects represented as multi-dimensional points into k clusters where each cluster is represented by the centroid of all its members
p810
aVEach iteration in K-Means starts off with assigning each data object to its nearest centroid, followed by re-computing the centroid vector based on the assignments made
p811
aVAt each iteration, the post-pairs are labeled as either solution ( S ) or non-solution ( N ) based on which pair of models they better conform to
p812
aVWe describe the various details in separate subsections herein
p813
aVAs outlined in Table 2 , each ( p , r ) pair would be assigned to one of the classes, solution or non-solution, based on whether it conforms better with the solution models (i.e.,, u'\u005cud835' u'\u005cudcae' S u'\u005cud835' u'\u005cudcaf' S ) or non-solution models ( u'\u005cud835' u'\u005cudcae' N u'\u005cud835' u'\u005cudcaf' N ), as determined using the F u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' , u'\u005cud835' u'\u005cudcaf' ) function, i.e
p814
aVwhere u'\u005cud835' u'\u005cudcae' u'\u005cu2062' [ w ] denotes the probability of w from u'\u005cud835' u'\u005cudcae' and u'\u005cud835' u'\u005cudcaf' p u'\u005cu2062' [ w ] denotes the probability of w from the multinomial distribution derived from u'\u005cud835' u'\u005cudcaf' conditioned over the words in p , as in Section 4.2
p815
aVSince the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models
p816
aVThus, we estimate the proportional contribution of each word from the language and translation models too, in the E-step
p817
aVIn our example from Section 4.2 , words such as rejoin are likely to get higher f u'\u005cud835' u'\u005cudcaf' S ( p , r ) scores due to being better correlated with problem words and consequently better supported by the translation model; those such as try may get higher f u'\u005cud835' u'\u005cudcae' S ( p , r ) scores
p818
aVWe use the labels and reply-word source estimates from the E-step to re-learn the language and translation models in this step
p819
aVAs may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N
p820
aVWe let each reply word contribute as much to the respective language and translation models according to the estimates in Section 4.3.2
p821
aVIn our example, if the word disconnect is assigned a source probability of 0.9 and 0.1 for the translation and language models respectively, the virtual document-pair from ( p , r ) that goes into the training of the respective u'\u005cud835' u'\u005cudcaf' model would assume that disconnect occurs in r with a frequency of 0.9 ; similarly, the respective u'\u005cud835' u'\u005cudcae' would account for disconnect with a frequency of 0.1
p822
aVThe language models are learnt only over the r parts of the ( p , r ) pairs since they are meant to characterize reply behavior; on the other hand, translation models learn over both p and r parts to model correlation
p823
aVIn our formulation, the language and translation models may be seen as competing for u'\u005cu201d' ownership u'\u005cu201d' of reply words
p824
aVConsider the post and reply vocabularies to be of sizes A and B respectively; then, the translation model would have A × B variables, whereas the unigram language model has only B variables
p825
aVThe IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words
p826
aVOur regularization method uses a parameter u'\u005cu03a4' to discard the long tail in the alignment vector by resetting entries having a value u'\u005cu2264' u'\u005cu03a4' to 0.0 followed by re-normalizing the alignment vector to add up to 1.0
p827
aVSuch pruning is performed at each iteration in the learning of the translation model, so that the following M-steps learn the probability matrix according to such modified alignment vectors
p828
aVIf we would like to allow alignment vectors to allow a problem word to align with upto two reply words, we would need to set u'\u005cu03a4' to a value close to 0.5 ( = 1 2 ) ; ideally though, to allow for the mass consumed by an almost inevitable long tail of very low values in the alignment vector, we would need to set it to slightly lower than 0.5 , say 0.4
p829
aV\u005culn u'\u005cu2200' ( p , r ) u'\u005cu2208' u'\u005cud835' u'\u005cudc9e' \u005culn l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( ( p , r ) ) = arg u'\u005cu2062' max i F u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' i , u'\u005cud835' u'\u005cudcaf' i ) \u005culn u'\u005cu2200' w u'\u005cu2208' r \u005culn
p830
aVMoreover, an initialization such that the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models favor the solution pairs more than the non-solution pairs is critical so that they may progressively lean towards modeling solution behaviour better across iterations
p831
aV[ 3 ] ), we label the pairs that have the the reply from the second post (note that the first post is assumed to be the problem post) in the thread as a solution post, and all others as non-solution posts
p832
aVSuch an initialization along with uniform reply word source probabilities is used to learn the initial estimates of the u'\u005cud835' u'\u005cudcae' S , u'\u005cud835' u'\u005cudcaf' S , u'\u005cud835' u'\u005cudcae' N and u'\u005cud835' u'\u005cudcaf' N models to be used in the E-step for the first iteration
p833
aVWe will show that we are able to effectively perform solution identification using our approach by exploiting just one structural feature, the post position, as above
p834
aVHowever, we will also show that we can exploit other features as and when available, to deliver higher accuracy clusterings
p835
aVThe models are then re-learnt in the M-Step (Lines 11-12) as outlined in Sec 4.3.3
p836
aVAt the end of the iterations that may run up to 10 times if the labelings do not stabilize earlier, the pairs labeled S are output as identified solutions (Line 13
p837
aVWe will denote the vocabulary size of problem posts as A and that of reply posts as B
p838
aVLearning of the language and translation models in each iteration costs u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n u'\u005cu2062' b + B ) and u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2032' u'\u005cu2062' ( n u'\u005cu2062' a u'\u005cu2062' b + A u'\u005cu2062' B ) ) respectively (assuming the translation model learning runs for k u'\u005cu2032' iterations
p839
aVFor k iterations of our algorithm, this leads to an overall complexity of u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2062' k u'\u005cu2032' u'\u005cu2062' ( n u'\u005cu2062' a u'\u005cu2062' b + A u'\u005cu2062' B ) )
p840
aVOut of these, 300 threads (comprising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of [ 2 ] (who were kind enough to share the data with us) with an inter-annotator agreement 11 11 http://en.wikipedia.org/wiki/Cohen u'\u005cu2019' s_kappa of 0.71
p841
aVWe use the F-measure 12 12 http://en.wikipedia.org/wiki/F1_score for solution identification, as the primary evaluation measure
p842
aVWhile we vary the various parameters separately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set u'\u005cu039b' = 0.5 and u'\u005cu03a4' = 0.4 unless otherwise mentioned
p843
aVSince we have only 300 labeled threads, accuracy measures are reported on those (like in [ 2 ]
p844
aVWe pre-process the post data by stemming words [ 14 ]
p845
aVWe use an independent implementation of the technique using Kullback-Leibler Divergence [ 12 ] as the similarity measure between posts; KL-Divergence was seen to perform best in the experiments reported in [ 4 ]
p846
aVANS-ACK PCT is an enhanced method that requires author-id information and a means of classifying posts as acknowledgements (which is done using additional supervision); a post being acknowledged by the problem author is then used as a signal to enhance the solution-ness of a post
p847
aVOur technique is seen to outperform ANS CT by a respectable margin ( 8.6 F-measure points) while trailing behind the enhanced ANS-ACK PCT method with a reasonably narrow 3.8 F-measure point margin
p848
aVThus, our unsupervised method is seen to be a strong competitor even for techniques using supervision outlined in [ 2 ] , illustrating the effectiveness of LM and TM modeling of reply posts
p849
aVFor scenarios where computation is at a premium, it is useful to know how quickly the quality of solution identification stabilizes, so that the results can be collected after fewer iterations
p850
aVFigure 1 plots the F-measure across iterations for the run with u'\u005cu039b' = 0.5 , u'\u005cu03a4' = 0.4 setting, where the F-measure is seen to stabilize in as few as 4-5 iterations
p851
aVSimilar trends were observed for other runs as well, confirming that the run may be stopped as early as after the fourth iteration without considerable loss in quality
p852
aVVarying u'\u005cu039b' u'\u005cu039b' is the weighting parameter that indicates the fraction of weight assigned to LMs (vis-a-vis TMs
p853
aVAs may be seen from Figure 4 , the quality of the results as measured by the F-measure is seen to peak around the middle (i.e.,, u'\u005cu039b' = 0.5 ), and decline slowly towards either extreme, with a sharp decline at u'\u005cu039b' = 0 (i.e.,, pure-TM setting
p854
aVThis indicates that a uniform mix is favorable; however, if one were to choose only one type of model, usage of LMs is seen to be preferable than TMs
p855
aVVarying u'\u005cu03a4' u'\u005cu03a4' is directly related to the extent of pruning of TMs, in the regularization operation; all values in the alignment vector u'\u005cu2264' u'\u005cu03a4' are pruned
p856
aVThus, each problem word is roughly allowed to be aligned with at most u'\u005cu223c' 1 u'\u005cu03a4' solution words
p857
aVThough more data always tends to be beneficial since statistical models benefit from redundancy, the marginal utility of additional data drops to very small levels beyond a point; we are interested in the amount of data beyond which the quality of solution identification flattens out
p858
aVIn Apple discussion forums, posts by Apple employees that are labeled with the Apple employees tag (approximately u'\u005cu223c' 7 u'\u005cu2062' % of posts in our dataset) tend to be solutions
p859
aVSo are posts that are marked Helpful ( u'\u005cu223c' 3 u'\u005cu2062' % of posts) by other users
p860
aVBeing specific to Apple forums, we did not use them for initialization in experiments so far with the intent of keeping the technique generic
p861
aVHowever, when such posts are initialized as solutions (in addition to first replies as we did earlier), the F-score for solution identification for our technique was seen to improve slightly, to 64.5 u'\u005cu2062' % (from 64 u'\u005cu2062' %
p862
aVThus, our technique is able to exploit any extra solution identifying structural features that are available
p863
aVTowards identifying solutions to the problem posed in the initial post, we proposed the usage of a hitherto unexplored textual feature for the solution identification problem; that of lexical correlations between problems and solutions
p864
aVIn short, our empirical analysis illustrates the superior performance and establishes our method as the method of choice for unsupervised solution identification
p865
aVExploration into the usage of translation models to aid other operations in discussion forums such as proactive word suggestions for solution authoring would be interesting direction for follow-up work
p866
asg88
(lp867
sg90
(lp868
sg92
(lp869
VWe considered the problem of unsupervised solution post identification from discussion forum threads.
p870
aVTowards identifying solutions to the problem posed in the initial post, we proposed the usage of a hitherto unexplored textual feature for the solution identification problem; that of lexical correlations between problems and solutions.
p871
aVWe model and harness lexical correlations using translation models, in the company of unigram language models that are used to characterize reply posts, and formulate a clustering-based EM approach for solution identification.
p872
aVWe show that our technique is able to effectively identify solutions using just one non-content based feature, the post position, whereas previous techniques in literature have depended heavily on structural features (that are not always available in many forums) and supervised information.
p873
aVOur technique is seen to outperform the sole unsupervised solution identification technique in literature, by a large margin; further, our method is even seen to be competitive to recent methods that use supervision, beating one of them comfortably, and trailing another by a narrow margin.
p874
aVIn short, our empirical analysis illustrates the superior performance and establishes our method as the method of choice for unsupervised solution identification.
p875
aVExploration into the usage of translation models to aid other operations in discussion forums such as proactive word suggestions for solution authoring would be interesting direction for follow-up work.
p876
aVDiscovery of problem-solution pairs in cases where the problem post is not known beforehand, would be a challenging problem to address.
p877
ag106
asg107
S'P14-1015'
p878
sg109
(lp879
VDiscussion forums have evolved into a dependable source of knowledge to solve common problems.
p880
aVHowever, only a minority of the posts in discussion forums are solution posts.
p881
aVIdentifying solution posts from discussion forums, hence, is an important research problem.
p882
aVIn this paper, we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature, that of lexical correlations between problems and solutions.
p883
aVWe use translation models and language models to exploit lexical correlations and solution post character respectively.
p884
aVOur technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums.
p885
aVOur clustering-based iterative solution identification approach based on the EM-formulation performs favorably in an empirical evaluation, beating the only unsupervised solution identification technique from literature by a very large margin.
p886
aVWe also show that our unsupervised technique is competitive against methods that require supervision, outperforming one such technique comfortably.
p887
ag106
asba(icmyPackage
FText
p888
(dp889
g3
(lp890
VGiven this sparsity, we i) leverage content from the local neighborhood of a user; (ii) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods; and (iii) estimate the amount of time and tweets required for a dynamic model to predict user preferences
p891
aVWhen updating models over time based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author u'\u005cu2019' s tweeting frequency
p892
aVResources like Twitter 1 1 http://www.demographicspro.com/ or Facebook 2 2 http://www.wolframalpha.com/facebook/ become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population [ 18 , 33 ]
p893
aVThe existing batch models for predicting latent user attributes rely on thousands of tweets per author [ 31 , 7 , 27 , 5 , 42 , 21 ]
p894
aVHowever, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g.,, the median number of tweets produced by a random Twitter user per day is 10
p895
aVIn addition, we propose streaming models for personal analytics that dynamically update user labels based on their stream of communications which has been addressed previously by Van Durme ( 2012b
p896
aVEach edge e i u'\u005cu2062' j u'\u005cu2208' E represents a connection between v i and v j , e i u'\u005cu2062' j = ( v i , v j ) and defines different social circles between Twitter users e.g.,, follower ( f ) , friend ( b ) , user mention ( m ) , hashtag ( h ) , reply ( y ) and retweet ( w
p897
aVThus, E u'\u005cu2208' V ( 2 ) × { f , b , h , m , w , y }
p898
aVWe denote a set of edges of a given type as u'\u005cu03a6' r u'\u005cu2062' ( E ) for r u'\u005cu2208' { f , b , h , m , w , y }
p899
aVWe denote a set of vertices adjacent to v i by social circle type r as N r u'\u005cu2062' ( v i ) which is equivalent to { v j u'\u005cu2223' e i u'\u005cu2062' j u'\u005cu2208' u'\u005cu03a6' r u'\u005cu2062' ( E ) }
p900
aVFollowing Filippova ( 2012 ) we refer to N r u'\u005cu2062' ( v i ) as v i u'\u005cu2019' s social circle, otherwise known as a neighborhood
p901
aVWe labeled users as Democratic if they exclusively follow both Democratic candidates 4 4 As of Oct 12, 2012, the number of followers for Obama, Biden, Romney and Ryan were 2m, 168k, 1.3m and 267k u'\u005cu2013' BarackObama and JoeBiden but do not follow both Republican candidates u'\u005cu2013' MittRomney and RepPaulRyan and vice versa
p902
aVWe collectively refer to D and R as our u'\u005cu201c' users of interest u'\u005cu201d' for which we aim to predict political preference
p903
aVFor each such user we collect recent tweets and randomly sample their immediate k = 10 neighbors from follower, friend, user mention, reply, retweet and hashtag social circles
p904
aVSharing restrictions and rate limits on Twitter data collection only allowed us to recreate a semblance of ZLR data 6 6 This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work u'\u005cu2013' 193 Democratic and 178 Republican users with 1K tweets per user, and 20 neighbors of four types including follower, friends, user mention and retweet with 200 tweets per neighbor for each user of interest
p905
aVBaseline User Model As input we are given a set of vertices representing users of interest v i u'\u005cu2208' V along with feature vectors f u'\u005cu2192' u'\u005cu2062' ( v i ) derived from content authored by the user of interest
p906
aVOur goal is assign to a category each user of interest v i based on f u'\u005cu2192' u'\u005cu2062' ( v i
p907
aVThe proposed baseline model follows the same trends as the existing state-of-the-art approaches for user attribute classification in social media as described in Section 8
p908
aVNext we propose to extend the baseline model by taking advantage of language in user social circles as describe below
p909
aVOur goal is to classify users of interest using evidence (e.g.,, communications) from their local neighborhood u'\u005cu2211' n f u'\u005cu2192' t u'\u005cu2062' [ N r u'\u005cu2062' ( v i ) ] u'\u005cu2261' f u'\u005cu2192' u'\u005cu2062' ( N r ) as Democratic or Republican
p910
aVThe corresponding log-linear model is defined as
p911
aVFollowing the streaming nature of social media, we see the scarce available resource as the number of requests allowed per day to the Twitter API
p912
aVFor instance, 85.3% of all Twitter users post less than one update per day as reported at http://www.sysomos.com/insidetwitter/
p913
aVThus, their communications are scare even if we could get all of them without rate limiting from Twitter API
p914
aVWe rely on straightforward Bayesian rule update to our batch models in order to simulate a real-time streaming prediction scenario as a first step beyond the existing models as shown in Figure 2
p915
aVThe model dynamically updates posterior probability estimates p ( a ( v i ) = R t k ) for a given user v i as an additional evidence t k is acquired, as defined in a general form below for any latent attribute a u'\u005cu2062' ( v i ) u'\u005cu2208' A given the tweets T of user v i
p916
aVFor example, to predict user political preference, we start with a prior P u'\u005cu2062' ( R ) = 0.5 , and sequentially update the posterior p ( R u'\u005cu2223' T ) by accumulating evidence from the likelihood p ( t k
p917
aV11 11 For brevity we omit reporting results for bigram and trigram features, since unigrams showed superior performance
p918
aVWe prefer binary to normalized count-based features to overcome sparsity issues caused by making predictions on each tweet individually
p919
aVFigure 3 demonstrates that more tweets during prediction time lead to higher accuracy by showing that more users with 100 tweets are correctly classified e.g.,, filled green markers in the right upper quadrant are true Republicans and in the left lower quadrant are true Democrats
p920
aVThese results follow naturally from the underlying feature representation having more tweets per user leads to a lower variance estimate of a target multinomial distribution
p921
aVThe more robustly this distribution is estimated (based on having more tweets) the more confident we should be in the classifier output
p922
aVWe show that three of six social circles u'\u005cu2013' friend, retweet and user-mention yield better accuracy compared to the user model for all graphs when t u'\u005cu2265' 250
p923
aVThus, for effectively classifying a given user v i it is better to take 200 tweets each from 10 neighbors rather than 2,000 tweets from the user
p924
aVFinally, similarly to the results for the user model given in Figure 3 , increasing the number of tweets per neighbor from 5 to 200 leads to a significant gain in performance for all neighborhood types
p925
aVOur results demonstrate that even small changes to the neighborhood size n lead to better performance which does not support the claims by Zamal et al
p926
aVWe demonstrate that increasing the size of the neighborhood leads to better performance across six neighborhood types
p927
aVThe average probability estimates p u'\u005cu039c' ( R u'\u005cu2223' T ) are reported for every 5 tweets in a stream T = ( t 1 , u'\u005cu2026' u'\u005cu2062' t k ) as u'\u005cu2211' P n ( R u'\u005cu2223' t k ) n , where n is the total number of users with the same attribute R or D
p928
aVWe represent p u'\u005cu039c' ( R u'\u005cu2223' T ) as a box and whisker plot with the median, lower and upper quantiles to show the variance; the length of whiskers indicate lower and upper extreme values
p929
aVIt means that users in G c u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' d are more politically vocal compared to users in G Z u'\u005cu2062' L u'\u005cu2062' R and G g u'\u005cu2062' e u'\u005cu2062' o
p930
aVAs a result, less active users in G g u'\u005cu2062' e u'\u005cu2062' o just need more than 250 tweets to converge to a true 0 or 1 class
p931
aVThese findings further confirm that differences in performance are caused by various biases present in the data due to distinct sampling and annotation approaches
p932
aVThe amount of time needed can be evaluated for different accuracy levels e.g.,, 0.75 and 0.95
p933
aVThus, with 75% accuracy we classify
p934
aVSuch extreme divergences in the amount of time required for classification across all graphs should be of strong interest to researchers concerned with latent attribute prediction tasks because Twitter users produce messages with extremely different frequencies
p935
aVOther methods characterize Twitter users by applying limited amounts of network structure information in addition to lexical features
p936
aV[ 7 ] rely on identifying strong partisan clusters of Democratic and Republican users in a Twitter network based on retweet and user mention degree of connectivity, and then combine this clustering information with the follower and friend neighborhood size features
p937
aVSimilar to our work, they assume that users from a particular class tend to reply and retweet messages of the users from the same class
p938
aVAdditionally, using social media for mining political opinions [ 23 , 19 ] or understanding socio-political trends and voting outcomes [ 36 , 12 , 15 ] is becoming a common practice
p939
aV[ 15 ] propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data
p940
aVOther works explore political blogs to predict what content will get the most comments [ 41 ] or analyze communications from Capitol Hill 12 12 http://www.tweetcongress.org to predict campaign contributors based on this content [ 40 ]
p941
aVUnsupervised Batch Approaches Bergsma et al
p942
aV[ 30 ] suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity
p943
aVIn this paper, we extensively examined state-of-the-art static approaches and proposed novel models with dynamic Bayesian updates for streaming personal analytics on Twitter
p944
aVBecause our streaming models rely on communications from Twitter users and content from various notions of user-local neighborhood they can be effectively applied to real-time dynamic data streams
p945
aVContent extracted from various notions of a user-local neighborhood can be as effective or more effective for political preference classification than user self-authored content
p946
aVThis may be an effect of u'\u005cu2018' sparseness u'\u005cu2019' of relevant user data, in that users talk about politics very sporadically compared to a random sample of their neighbors
p947
aVQuerying for more neighbors per user is more beneficial than querying for extra content from the existing neighbors e.g.,, 5 tweets from 10 neighbors leads to higher accuracy than 25 tweets from 2 neighbors or 50 tweets from 1 neighbor
p948
aVFriend, user mention and retweet neighborhoods show the best accuracy for predicting political preferences of Twitter users
p949
aVWe think that friend relationships are more effective than e.g.,, follower relationships because it is very likely that users share common interests and preferences with their friends, e.g., Facebook friends can even be used to predict a user u'\u005cu2019' s credit score
p950
aVOur results for both static and dynamic models show that the accuracy indeed depends on the way the data was constructed
p951
aVTherefore, publicly available datasets need to be released for a meaningful comparison of the approaches for personal analytics in social media
p952
asg88
(lp953
sg90
(lp954
sg92
(lp955
VIn this paper, we extensively examined state-of-the-art static approaches and proposed novel models with dynamic Bayesian updates for streaming personal analytics on Twitter.
p956
aVBecause our streaming models rely on communications from Twitter users and content from various notions of user-local neighborhood they can be effectively applied to real-time dynamic data streams.
p957
aVOur results support several key findings listed below.
p958
aVNeighborhood content is useful for personal analytics.
p959
aVContent extracted from various notions of a user-local neighborhood can be as effective or more effective for political preference classification than user self-authored content.
p960
aVThis may be an effect of sparseness of relevant user data, in that users talk about politics very sporadically compared to a random sample of their neighbors.
p961
aVSubstantial signal for political preference prediction is distributed in the neighborhood.
p962
aVQuerying for more neighbors per user is more beneficial than querying for extra content from the existing neighbors e.g.,, 5 tweets from 10 neighbors leads to higher accuracy than 25 tweets from 2 neighbors or 50 tweets from 1 neighbor.
p963
aVThis may be also the effect of data heterogeneity in social media compared to e.g.,, political debate text [ 35 ].
p964
aVThese findings demonstrate that a substantial signal is distributed over the neighborhood content.
p965
aVNeighborhoods constructed from friend, user mention and retweet relationships are most effective.
p966
aVFriend, user mention and retweet neighborhoods show the best accuracy for predicting political preferences of Twitter users.
p967
aVWe think that friend relationships are more effective than e.g.,, follower relationships because it is very likely that users share common interests and preferences with their friends, e.g., Facebook friends can even be used to predict a user s credit score.
p968
aV13 13 http://money.cnn.com/2013/08/26/technology/social/ facebook-credit-score/ User mentions and retweets are two primary ways of interaction on Twitter.
p969
aVThey both allow to share information e.g.,, political news, events with others and to be involved in direct communication e.g.,, live political discussions, political groups.
p970
aVStreaming models are more effective than batch models for personal analytics.
p971
aVThe predictions made using dynamic models with Bayesian updates over user and joint user-neighbor communication streams demonstrate higher performance with lower resources spent compared to the batch models.
p972
aVDepending on user political involvement, expressiveness and activeness, the perfect prediction (approaching 100% accuracy) can be made using only 100 - 500 tweets per user.
p973
aVGeneralization of the classifiers for political preference prediction.
p974
aVThis work raises a very important but under-explored problem of the generalization of classifiers for personal analytics in social media, also recently discussed by Cohen and Ruth [ 6 ].
p975
aVFor instance, the existing models developed for political preference prediction are all trained on Twitter data but report significantly different results even for the same baseline models trained using bag-of-word lexical features as shown in Table 1.
p976
aVIn this work we experiment with three different datasets.
p977
aVOur results for both static and dynamic models show that the accuracy indeed depends on the way the data was constructed.
p978
aVTherefore, publicly available datasets need to be released for a meaningful comparison of the approaches for personal analytics in social media.
p979
aVIn future work, we plan to incorporate iterative model updates from newly classified communications similar to online perceptron-style updates.
p980
aVIn addition, we aim to experiment with neighborhood-specific classifiers applied towards the tweets from neighborhood-specific streams e.g.,, friend classifier used for friend tweets, retweet classifier applied to retweet tweets etc.
p981
ag106
asg107
S'P14-1018'
p982
sg109
(lp983
VExisting models for social media personal analytics assume access to thousands of messages per user, even though most users author content only sporadically over time.
p984
aVGiven this sparsity, we i) leverage content from the local neighborhood of a user; (ii) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods; and (iii) estimate the amount of time and tweets required for a dynamic model to predict user preferences.
p985
aVWe show that even when limited or no self-authored data is available, language from friend, retweet and user mention communications provide sufficient evidence for prediction.
p986
aVWhen updating models over time based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author s tweeting frequency.
p987
ag106
asba(icmyPackage
FText
p988
(dp989
g3
(lp990
VWe introduce two samplers for traversing the space of trees, Gibbs and Metropolis-Hastings with Random Walk
p991
aVDependency parsing is commonly cast as a maximization problem over a parameterized scoring function
p992
aVIn this view, the use of more expressive scoring functions leads to more challenging combinatorial problems of finding the maximizing parse
p993
aVMuch of the recent work on parsing has been focused on improving methods for solving the combinatorial maximization inference problems
p994
aVIndeed, state-of-the-art results have been obtained by adapting powerful tools from optimization [ 16 , 17 , 27 ]
p995
aVOur combination outperforms the state-of-the-art parsers and remains comparable even if we adopt their scoring functions
p996
aVThey first appeared in the context of reranking [ 6 ] , where a simple parser is used to generate a candidate list which is then reranked according to the scoring function
p997
aVBecause the number of alternatives is small, the scoring function could in principle involve arbitrary (global) features of parse trees
p998
aVBecause the steps are small, the complexity of the scoring function has limited impact on the computational cost of the procedure
p999
aVBecause the inference procedure is so simple, it is important that the parameters of the scoring function are chosen in a manner that facilitates how we climb the scoring function in small steps
p1000
aVThis approach was suggested in the SampleRank framework [ 32 ] for training structured prediction models
p1001
aVWhen proposing a small move, i.e.,, sampling a head of the word, we can also jointly sample its POS tag from a set of alternatives provided by the tagger
p1002
aVAs a result, the selected tag is influenced by a broad syntactic context above and beyond the initial tagging model and is directly optimized to improve parsing performance
p1003
aVOur method provides a more effective mechanism for handling global features than reranking, outperforming it by 1.3%
p1004
aVReranking can be combined with an arbitrary scoring function, and thus can easily incorporate global features over the entire parse tree
p1005
aVFor instance, Nakagawa ( 2007 ) deals with tractability issues by using sampling to approximate marginals
p1006
aVIn SampleRank, the parameters are adjusted so as to guide the sequence of candidates closer to the target structure along the search path
p1007
aVIn this paper, we demonstrate how to adapt the method for parsing with rich scoring functions
p1008
aVA training set of size N is given as a set of pairs u'\u005cud835' u'\u005cudc9f' = { ( x ( i ) , y ( i ) ) } i = 1 N where y ( i ) is the ground truth parse for sentence x ( i )
p1009
aVSimilarly, parsers based on dual decomposition [ 17 , 14 ] assume that s u'\u005cu2062' ( x , y ) decomposes into a sum of terms where each term can be maximized over y efficiently
p1010
aVWe find a high scoring tree through sampling, and (later) learn the parameters u'\u005cu0398' so as to further guide this process
p1011
aVOur sampler generates a sequence of dependency structures so as to approximate independent samples from
p1012
aVThe temperature parameter T controls how concentrated the samples are around the maximum of s u'\u005cu2062' ( x , y ) (e.g.,, see Geman and Geman ( 1984 )
p1013
aVThe target distribution p folds into the procedure by defining the probability that we will accept the proposed move
p1014
aVThis is feasible if we restrict the proposed moves to only small changes in the current tree
p1015
aVIn our case, we choose a word j randomly, and then sample its head h j according to p with the constraint that we obtain a valid tree (when projective trees are sought, this constraint is also incorporated
p1016
aVFor this choice of q , the probability of accepting the new tree ( u'\u005cu0391' in Figure 1 ) is identically one
p1017
aVThus new moves are always accepted
p1018
aVWhile in general this is increasingly difficult with more heads, it is indeed tractable if the model corresponds to a first-order parser
p1019
aVwhere y corresponds to a tree with a spcified root and w i u'\u005cu2062' j is the exponential of the first-order score y is always a valid parse tree if we allow multiple children of the root and do not impose projective constraint
p1020
aVThe algorithm in Wilson ( 1996 ) iterates over all the nodes, and for each node performs a random walk according to the weights w i u'\u005cu2062' j until the walk creates a loop or hits a tree
p1021
aVIf the walk hits the current tree, the walk path is added to form a new tree with more nodes
p1022
aVSince our features do not by design correspond to a first-order parser, we cannot use the Wilson algorithm as it is
p1023
aVInstead we use it as the proposal function and sample a subset of the dependencies from the first-order distribution of our model, while fixing the others
p1024
aVNote that blocked Gibbs sampling would be exponential in K , and is thus very slow already at K = 4
p1025
aVMoreover, alternatives that are far from the gold parse should score even lower
p1026
aVAs a result, we require that
p1027
aVWe cannot necessarily assume that s u'\u005cu2062' ( x , y ) is greater than s u'\u005cu2062' ( x , y u'\u005cu2032' ) without additional encouragement
p1028
aVThus, we can complement the constraints in Equation 4 with additional pairwise constraints [ 32 ]
p1029
aVSpecifically, if the current parameters are u'\u005cu0398' t , and we enforce constraint Equation 5 for a particular pair y , y u'\u005cu2032' , then we will find u'\u005cu0398' t + 1 that minimizes
p1030
aVWe repeatedly generate parses based on the current parameters u'\u005cu0398' t for each sentence x ( i ) , and use successive samples to enforce constraints in Equation 4 and Equation 5 one at a time
p1031
aVWe generate the POS candidate list for each word based on the confusion matrix on the training set
p1032
aVFor each word w , we first prune out its POS candidates by using the vocabulary from the training set
p1033
aVWe don u'\u005cu2019' t prune anything if w is unseen
p1034
aVAssuming that the predicted tag for w is t p , we further remove those tags t if their counts are smaller than some threshold c u'\u005cu2062' ( t , t p ) u'\u005cu0391' u'\u005cu22c5' c u'\u005cu2062' ( t p , t p ) 2 2 In our work we choose u'\u005cu0391' = 0.003 , which gives a 98.9% oracle POS tagging accuracy on the CATiB development set
p1035
aVWe also define features based on consecutive sibling, grandparent, arbitrary sibling, head bigram, grand-sibling and tri-siblings, which are also used in the Turbo parser [ 16 ]
p1036
aVFor instance, in cats and dogs , the conjuncts are both short noun phrases
p1037
aVTherefore, we add different features to capture POS tag and span length consistency in a coordinate structure
p1038
aVGenerally, this feature can be defined based on an instance of grandparent structure
p1039
aVNon-projective Arcs A flag indicating if a dependency is projective or not (i.e., if it spans a word that does not descend from its head) [ 17 ]
p1040
aVHowever, we are free to add higher order features because we do not rely on dynamic programming decoding
p1041
aVWe handle long sentences during testing by applying a simple split-merge strategy
p1042
aVWe split the sentence based on the ending punctuation, predict the parse tree for each segment and group the roots of resulting trees into a single node
p1043
aVEvaluation Measures Following standard practice, we use Unlabeled Attachment Score (UAS) as the evaluation metric in all our experiments
p1044
aVWe report UAS excluding punctuation on CoNLL datasets, following Martins et al
p1045
aVThe reranker operates over the top-50 list obtained from the MST parser 4 4 The MST parser is trained in projective mode for reranking because generating top-k list from second-order non-projective model is intractable
p1046
aVWe then train the reranker by running 10 epochs of cost-augmented MIRA
p1047
aVThe reranker uses the same features as our model, along with the tree scores obtained from the MST parser (which is a standard practice in reranking
p1048
aVBecause the CoNLL datasets do not have a standard development set, we randomly select a held out of 200 sentences from the training set
p1049
aVHowever, for the joint parsing and POS correction on the CATiB dataset we do not use the Random Walk method because the first-order features in normal parsing are no longer first-order when POS tags are also variables
p1050
aVTherefore, the first-order distribution is not well-defined and we only employ Gibbs sampling for simplicity
p1051
aVOn the CATiB dataset, we restrict the sample trees to always be projective as described in Section 3.2.1
p1052
aVHowever, we do not impose this constraint for the CoNLL datasets
p1053
aVThis suggests that our learning and inference procedures are as effective as the dual decomposition method in the Turbo parser
p1054
aVNext, we add global features that are not used by the Turbo parser
p1055
aVComparison with Reranking As column 6 of Table 2 shows, our model outperforms the reranker by 1.3% 5 5 Note that the comparison is conservative because we can also add MST scores as features in our model as in reranker
p1056
aV6 6 We ran this experiment on 5 languages with small datasets due to the scalability issues associated with reranking top-500 list
p1057
aVAs column 7 shows, this increase in the list size does not change the relative performance of the reranker and our model
p1058
aVJoint Parsing and POS Correction Table 3 shows the results of joint parsing and POS correction on the CATiB dataset, for our model and state-of-the-art systems
p1059
aVAs the upper part of the table shows, the parser with corrected tags reaches 88.38% compared to the accuracy of 88.46% on the gold tags
p1060
aVImpact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency
p1061
aVSpecifically, we measure the score of the retrieved trees in testing as a function of the decoding speed, measured by the number of tokens per second
p1062
aVWe select these two languages as they correspond to two extremes in sentence length
p1063
aVDecoding Speed Our sampling-based parser is an anytime algorithm, and therefore its running time can be traded for performance
p1064
asg88
(lp1065
sg90
(lp1066
sg92
(lp1067
VThis paper demonstrates the power of combining a simple inference procedure with a highly expressive scoring function.
p1068
aVOur model achieves the best results on the standard dependency parsing benchmark, outperforming parsing methods with elaborate inference procedures.
p1069
aVIn addition, this framework provides simple and effective means for joint parsing and corrective tagging.
p1070
ag106
asg107
S'P14-1019'
p1071
sg109
(lp1072
VMuch of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions.
p1073
aVIn contrast, we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures.
p1074
aVSpecifically, we introduce a sampling-based parser that can easily handle arbitrary global features.
p1075
aVInspired by SampleRank, we learn to take guided stochastic steps towards a high scoring parse.
p1076
aVWe introduce two samplers for traversing the space of trees, Gibbs and Metropolis-Hastings with Random Walk.
p1077
aVThe model outperforms state-of-the-art results when evaluated on 14 languages of non-projective CoNLL datasets.
p1078
aVOur sampling-based approach naturally extends to joint prediction scenarios, such as joint parsing and POS correction.
p1079
aVThe resulting method outperforms the best reported results on the CATiB dataset, approaching performance of parsing with gold tags.
p1080
aV1 1 The source code for the work is available at http://groups.csail.mit.edu/rbg/code/global/acl2014.
p1081
ag106
asba(icmyPackage
FText
p1082
(dp1083
g3
(lp1084
VDue to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points
p1085
aVIn this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU
p1086
aVBecause NLP models typically treat sentences independently, NLP problems have long been seen as u'\u005cu201c' embarrassingly parallel u'\u005cu201d' u'\u005cu2013' large corpora can be processed arbitrarily fast by simply sending different sentences to different machines
p1087
aVFirst, classic single-core processors and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine
p1088
aVSince tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost
p1089
aVTheir system uses a grammar based on the Berkeley parser [ 9 ] (which is particularly amenable to GPU processing), u'\u005cu201c' compiling u'\u005cu201d' the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart
p1090
aVIn this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-to-fine pruning to a GPU setting
p1091
aVSuch extreme speedups over a dense GPU baseline currently seem unlikely because fine-grained sparsity appears to be directly at odds with dense parallelism
p1092
aVWe use a coarse-to-fine approach as in Petrov and Klein ( 2007 ) , but with only one coarse pass
p1093
aVFigure 1 shows an overview of the approach we first parse densely with a coarse grammar and then parse sparsely with the fine grammar, skipping symbols that the coarse pass deemed sufficiently unlikely
p1094
aVUsing this approach, we see gains of nearly 2.5x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second
p1095
aVUnless otherwise noted, all experiments are conducted on sentences of length u'\u005cu2264' 40 words, and we estimate times based on batches of 20K sentences
p1096
aVFor instance, in a latent variable parser, the coarse grammar would have symbols like N u'\u005cu2062' P , V u'\u005cu2062' P , etc., and the fine pass would have refined symbols N u'\u005cu2062' P 0 , N u'\u005cu2062' P 1 , V u'\u005cu2062' P 4 , and so on
p1097
aVIn coarse-to-fine inference, one applies the grammars in sequence, computing inside and outside scores
p1098
aVThis approach works because a low quality coarse grammar can still reliably be used to prune many symbols from the fine chart without loss of accuracy
p1099
aVThus, the vast majority of rules can be skipped, and therefore most computation can be avoided
p1100
aVIt is worth pointing out that although 98% of labeled spans can be skipped due to X-bar pruning, we found that only about 79% of binary rule applications can be skipped, because the unpruned symbols tend to be the ones with a larger grammar footprint
p1101
aVGPUs work by executing thousands of threads at once, but impose the constraint that large blocks of threads must be executing the same instructions in lockstep, differing only in their input data
p1102
aVThus sparsely skipping rules and symbols will not save any work
p1103
aVAll threads in a warp must execute the same instruction at every clock cycle if one thread takes a branch the others do not, then all threads in the warp must follow both code paths
p1104
aVBecause all threads execute all code paths that any thread takes, time can only be saved if an entire warp agrees to skip any particular branch
p1105
aVEach SM can process up to 48 different warps at a time it interleaves the execution of each warp, so that when one warp is stalled another warp can execute
p1106
aVOn the 600 series, maximum occupancy can only be achieved if each thread uses at most 63 registers [ 8 ]
p1107
aV2 2 A thread can use more registers than this, but the full complement of 48 warps cannot execute if too many are used
p1108
aVRegisters are many times faster than variables located in thread-local memory, which is actually the same speed as global memory
p1109
aVThis architecture environment puts very different constraints on parsing algorithms from a CPU environment
p1110
aVIn this section, we describe their dense algorithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow
p1111
aVAt the top level, the CPU and GPU communicate via a work queue of parse items of the form ( s , i , k , j ) , where s is an identifier of a sentence, i is the start of a span, k is the split point, and j is the end point
p1112
aVBecause all rules are applied to all parse items, all threads are executing the same sequence of instructions
p1113
aVThus, there is no concern of warp divergence
p1114
aVBecause registers are so much faster than thread-local memory, it is critical to keep as many variables in registers as possible
p1115
aVOne way to accomplish this is to unroll loops at compilation time
p1116
aVTherefore, they inlined the iteration over the grammar directly into the GPU kernels (i.e., the code itself), which allows the compiler to more effectively use all of its registers
p1117
aVBecause the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills
p1118
aVThe natural implementation would be for each thread to check if each rule is licensed before applying it
p1119
aVHowever, we would only avoid the work of applying the rule if all threads in the warp agreed to skip it
p1120
aVSince each thread in the warp is processing a different span (perhaps even from a different sentence), consensus from all 32 threads on any skip would be unlikely
p1121
aVBecause of the overhead associated with creating pruning masks and the further overhead of GPU communication, we found that this method did not actually produce any time savings at all
p1122
aVThe result is a parsing speed of 185.5 sentences per second, as shown in Table 1 on the row labeled u'\u005cu2018' Reimpl u'\u005cu2019' with u'\u005cu2018' Empty, Coarse u'\u005cu2019' pruning
p1123
aVWe call the set of coarse symbols for a partition (and therefore the corresponding labeled work queue) a signature
p1124
aVDuring parsing, we only enqueue items ( s , i , k , j ) to a labeled queue if two conditions are met
p1125
aVBecause the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence
p1126
aVWe tested our new pruning approach using an X-bar grammar as the coarse pass
p1127
aVThe resulting speed is 187.5 sentences per second, labeled in Table 1 as row labeled u'\u005cu2018' Reimpl u'\u005cu2019' with u'\u005cu2018' Labeled, Coarse u'\u005cu2019' pruning
p1128
aVHow can we best cluster and subcluster the grammar so as to maximize performance
p1129
aVA good clustering will group rules together that use the same symbols, since this means fewer memory accesses to read and write scores for symbols
p1130
aVFinally, when pruning, it is best if symbols that have the same coarse projection are clustered together
p1131
aVThat way, we are more likely to be able to skip a subcluster, since fewer distinct symbols need to be u'\u005cu201c' off u'\u005cu201d' for a parse item to be skipped in a given subcluster
p1132
aVClustering using this method is labeled u'\u005cu2018' Reimplementation u'\u005cu2019' in Table 1
p1133
aVSecond, we are able to skip a parse item for an entire cluster if that item u'\u005cu2019' s pruning mask does not intersect the cluster u'\u005cu2019' s signature
p1134
aVSpreading symbols across clusters may be inefficient if a parse item licenses a given symbol, we will have to enqueue that item to any queue that has the symbol in its signature, no matter how many other symbols are in that cluster
p1135
aVThus, it makes sense to choose a clustering algorithm that exploits the structure introduced by the pruning masks
p1136
aVWhen coarse symbols are extremely unlikely (and therefore have few corresponding rules), we merge their clusters to avoid the overhead of beginning work on clusters where little work has to be done
p1137
aV3 3 Specifically, after clustering based on the coarse parent symbol, we merge all clusters with less than 300 rules in them into one large cluster
p1138
aVIn order to subcluster, we divde up rules among subclusters so that each subcluster has the same number of active parent symbols
p1139
aVClustering using this method is labeled u'\u005cu2018' Parent u'\u005cu2019' in Table 1
p1140
aVThe coarse to fine pruning approach of Petrov and Klein ( 2007 ) employs an X-bar grammar as its first pruning phase, but there is no reason why we cannot begin with a more complex grammar for our initial pass
p1141
aVAs Petrov and Klein ( 2007 ) have shown, intermediate-sized Berkeley grammars prune many more symbols than the X-bar system
p1142
aVHowever, they are slower to parse with in a CPU context, and so they begin with an X-bar grammar
p1143
aVBecause of the overhead associated with transferring work items to GPU, using a very small grammar may not be an efficient use of the GPU u'\u005cu2019' s computational resources
p1144
aVBecause parsing with these grammars is still quite fast, we tried using them as the coarse pass instead
p1145
aVMBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit
p1146
aVFor instance, one might want to maximize the expected number of correct constituents [ 3 ] , or the expected rule counts [ 10 , 9 ]
p1147
aVWe should expect this algorithm to be at least a factor of two slower the outside pass performs at least as much work as the inside pass
p1148
aVMoreover, it typically has worse memory access patterns, leading to slower performance
p1149
aVBecause the grammars are compiled into code, the additional operations are all inlined into the kernels, producing much larger kernels
p1150
aVIndeed, in practice the compiler will often hang if we use the same size grammar clusters as we did for Viterbi
p1151
aVBecause so many labeled spans are pruned, we are able to skip many of the grammar clusters and thus avoid many of the expensive operations
p1152
aVUsing coarse pruning and log domain calculations, our system produces MBR trees at a rate of 130.4 sentences per second, a four-fold increase
p1153
aVThat is, the number is represented as f u'\u005cu2032' = f u'\u005cu22c5' exp u'\u005cu2061' ( s
p1154
aVWhenever f becomes either too big or too small, the number is rescaled back to a less u'\u005cu201c' dangerous u'\u005cu201d' range by shifting mass from the exponent e to the scaling factor s
p1155
aVIn our GPU system, multiple scores in any given span are being updated at the same time, which makes this dynamic rescaling tricky and expensive, especially since inter-warp communication is fairly limited
p1156
aVBecause the grammar used in the coarse pass is a projection of the grammar used in the fine pass, these coarse scores correlate reasonably closely with the probabilities computed in the fine pass
p1157
aVIf a span has a very high or very low score in the coarse pass, it typically has a similar score in the fine pass
p1158
aVThus, we can use the coarse pass u'\u005cu2019' s inside and outside scores as the scaling values for the fine pass u'\u005cu2019' s scores
p1159
aVThen, when applying rules in the fine pass, each fine inside score over a split span ( i , k , j ) is scaled to the appropriate s i , j I by multiplying the score by exp u'\u005cu2061' ( s i , k I + s k , j I - s i , j I ) , where s i , k I , s k , j I , s i , j I are the scaling factors for the left child, right child, and parent, respectively
p1160
aVBecause we are summing instead of maxing scores in the fine pass, the scaling factors computed using max scores are not quite large enough, and so the rescaled inside probabilities grow too large when multiplied together
p1161
aVMost of this difference arises at the leaves, where the lexicon typically has more uncertainty than higher up in the tree
p1162
aVTherefore, in the fine pass, we normalize the inside scores at the leaves to sum to 1.0
p1163
aV4 4 One can instead interpret this approach as changing the scaling factors to s i , j I u'\u005cu2032' = s i , j I u'\u005cu22c5' u'\u005cu220f' i u'\u005cu2264' k j u'\u005cu2211' A inside u'\u005cu2062' ( A , k , k + 1 ) , where inside is the array of scores for the fine pass
p1164
aVUsing scaling, we are able to push our parser to 190.6 sentences/second for MBR extraction, just under half the speed of the Viterbi system
p1165
aVIt is of course important verify the correctness of our system; one easy way to do so is to examine parsing accuracy, as compared to the original Berkeley parser
p1166
aVThese timing numbers are computed using the built-in profiling capabilities of the programming environment
p1167
aVAs usual, profiles exhibit an observer effect, where the act of measuring the system changes the execution
p1168
aVNevertheless, the general trends should more or less be preserved as compared to the unprofiled code
p1169
aVTo begin, we can compute the number of seconds needed to parse 1000 sentences
p1170
aVWe use seconds per sentence rather than sentences per second because the former measure is additive.) The results are in Table 3
p1171
aVIn Table 4 , we break down the time taken by our system into individual components
p1172
aVAs expected, binary rules account for the vast majority of the time in the unpruned Viterbi case, but much less time in the pruned case, with the total time taken for binary rules in the coarse and fine passes taking about 1/5 of the time taken by binaries in the unpruned version
p1173
aVThere is greater overhead in the scaling system, because scaling factors are copied to the CPU between the coarse and fine passes
p1174
aVThroughput increases through parsing 10,000 sentences, and then levels off by the time it reaches 100,000 sentences
p1175
aVOur system is available as open-source at https://www.github.com/dlwh/puck
p1176
aVThis work was partially supported by BBN under DARPA contract HR0011-12-C-0014, by a Google PhD fellowship to the first author, and an NSF fellowship to the second
p1177
asg88
(lp1178
sg90
(lp1179
sg92
(lp1180
VGPUs represent a challenging opportunity for natural language processing.
p1181
aVBy carefully designing within the constraints imposed by the architecture, we have created a parser that can exploit the same kinds of sparsity that have been developed for more traditional architectures.
p1182
aVOne of the key remaining challenges going forward is confronting the kind of lexicalized sparsity common in other NLP models.
p1183
aVThe Berkeley parser s grammars by virtue of being unlexicalized can be applied uniformly to all parse items.
p1184
aVThe bilexical features needed by dependency models and lexicalized constituency models are not directly amenable to acceleration using the techniques we described here.
p1185
aVDetermining how to efficiently implement these kinds of models is a promising area for new research.
p1186
aVOur system is available as open-source at https://www.github.com/dlwh/puck.
p1187
ag106
asg107
S'P14-1020'
p1188
sg109
(lp1189
VDue to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points.
p1190
aVNatural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.
p1191
aVRecently, Canny et al.
p1192
aV2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU.
p1193
aVIn this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU.
p1194
aVThe resulting system is capable of computing over 404 Viterbi parses per second more than a 2x speedup on the same hardware.
p1195
aVMoreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning nearly a 6x speedup.
p1196
ag106
asba(icmyPackage
FText
p1197
(dp1198
g3
(lp1199
VMoving context out of the grammar and onto surface features can greatly simplify the structural component of the parser because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as X-bar grammars
p1200
aVNaïve context-free grammars, such as those embodied by standard treebank annotations, do not parse well because their symbols have too little context to constrain their syntactic behavior
p1201
aVMuch of the last few decades of parsing research has therefore focused on propagating contextual information from the leaves of the tree to internal nodes
p1202
aVThe underlying reason that such propagation is even needed is that PCFG parsers score trees based on local configurations only, and any information that is not threaded through the tree becomes inaccessible to the scoring function
p1203
aVThere have been non-local approaches as well, such as tree-substitution parsers [ 2 , 26 ] , neural net parsers [ 13 ] , and rerankers [ 6 , 4 , 14 ]
p1204
aVThese non-local approaches can actually go even further in enriching the grammar u'\u005cu2019' s structural complexity by coupling larger domains in various ways, though their non-locality generally complicates inference
p1205
aVIn this work, we instead try to minimize the structural complexity of the grammar by moving as much context as possible onto local surface features
p1206
aVWe examine the position that grammars should not propagate any information that is available from surface strings, since a discriminative parser can access that information directly
p1207
aVWe therefore begin with a minimal grammar and iteratively augment it with rich input features that do not enrich the context-free backbone
p1208
aVBy contrast, we investigate the extent to which we need a grammar at all
p1209
aVAs a thought experiment, consider a parser with no grammar, which functions by independently classifying each span ( i , j ) of a sentence as an NP, VP, and so on, or null if that span is a non-constituent
p1210
aVAn independent classification approach is actually very viable for part-of-speech tagging [ 29 ] , but is problematic for parsing u'\u005cu2013' if nothing else, parsing comes with a structural requirement that the output be a well-formed, nested tree
p1211
aVFor example, Socher et al
p1212
aV2013 ) demonstrates that sentiment analysis, which is usually approached as a flat classification task, can be viewed as tree-structured
p1213
aVOur parser can be easily adapted to this task by replacing the X-bar grammar over treebank symbols with a grammar over the sentiment values to encode the output variables and then adding n-gram indicators to our feature set to capture the bulk of the lexical effects
p1214
aVAn anchored rule r is the conjunction of an unanchored grammar rule rule u'\u005cu2062' ( r ) and the start, stop, and split indexes where that rule is anchored, which we refer to as span u'\u005cu2062' ( r
p1215
aVWe start with a simple X-bar grammar whose only symbols are NP, NP-bar, VP, and so on
p1216
aVOur base model has no surface features formally, on each anchored rule r we have only an indicator of the (unanchored) rule identity, rule u'\u005cu2062' ( r
p1217
aVBecause the X-bar grammar is so minimal, this grammar does not parse very accurately, scoring just 73 F1 on the standard English Penn Treebank task
p1218
aVHall and Klein ( 2012 ) attempted to reduce this state space by factoring these annotations into individual components
p1219
aVTheir approach changed the multiplicative penalty of annotation into an additive penalty, but even so their individual grammar projections are much larger than the base X-bar grammar
p1220
aVWe say that an indicator is a surface property if it can be extracted without reference to the parse tree
p1221
aVFor example, the first word in a constituent is a surface property, as is the word directly preceding the constituent
p1222
aVAs illustrated in Figure 1 , the actual features of the model are obtained by conjoining surface properties with various abstractions of the rule identity
p1223
aVThe surface features are somewhat more involved, and so we introduce them incrementally
p1224
aVBecause these u'\u005cu201c' positive u'\u005cu201d' features correspond to observed constituents, they are far less numerous than the set of all possible features extracted from all spans
p1225
aVHowever, negative features u'\u005cu2014' features that are not observed in any tree u'\u005cu2014' are still powerful indicators of (un)grammaticality if we have never seen a PRN that starts with u'\u005cu201c' has, u'\u005cu201d' or a span that begins with a quotation mark and ends with a close bracket, then we would like the model to be able to place negative weights on these features
p1226
aVThus, we use a simple feature hashing scheme where positive features are indexed individually, while negative features are bucketed together
p1227
aVBecause the lexicon is especially sensitive to morphological effects, we also fire features on all prefixes and suffixes of the current word up to length 5, regardless of frequency
p1228
aVSubsequent lines in Table 1 indicate additional surface feature templates computed over the span, which are then conjoined with the rule identity as shown in Figure 1 to give additional features
p1229
aVNote that many of these features have been used before [ 28 , 10 , 23 ] ; our goal here is not to amass as many feature templates as possible, but rather to examine the extent to which a simple set of features can replace a complicated state space
p1230
aVWe start with some of the most obvious properties available to us, namely, the identity of the first and last words of a span
p1231
aVBecause heads of constituents are often at the beginning or the end of a span, these feature templates can (noisily) capture monolexical properties of heads without having to incur the inferential cost of lexicalized annotations
p1232
aVWe try to capture some of this same intuition by introducing a feature on the length of a span
p1233
aVFor instance, VPs embedded in NPs tend to be short, usually as embedded gerund phrases
p1234
aVBecause constituents in the treebank can be quite long, we bin our length features into 8 buckets, of lengths 1, 2, 3, 4, 5, 10, 20, and u'\u005cu2265' 21 words
p1235
aVAdding these simple features (first word, last word, and lengths) as span features of the X-bar grammar already gives us a substantial improvement over our baseline system, improving the parser u'\u005cu2019' s performance from 73.0 F1 to 85.0 F1 (see Table 1
p1236
aVOf course, there is no reason why we should confine ourselves to just the words within the span words outside the span also provide a rich source of context
p1237
aVAs an example, consider disambiguating the POS tag of the word read in Figure 2
p1238
aVA VP is most frequently preceded by a subject NP, whose rightmost word is often its head
p1239
aVTherefore, we fire features that (separately) look at the words immediately preceding and immediately following the span
p1240
aVFigure 3 shows an example of one instance of this feature template impact is a noun that is more likely to take a PP than other nouns, and so we expect this feature to have high weight and encourage the attachment; this feature proves generally useful in resolving such cases of right-attachments to noun phrases, since the last word of the noun phrase is often the head
p1241
aVAs another example, coordination can be represented by an indicator of the conjunction, which comes immediately after the split point
p1242
aVIf it begins with punctuation, we indicate the punctuation mark explicitly
p1243
aVBecause this feature indicates capitalization, it can also capture properties of NP internal structure relevant to named entities, and its sensitivity to capitalization and punctuation makes it useful for recognizing appositive constructions
p1244
aVBy annotating grammar nonterminals with their headwords, the idea is to better model phenomena that depend heavily on the semantics of the words involved, such as coordination and PP attachment
p1245
aVThe u'\u005cu201c' Replaced u'\u005cu201d' system modifies the Berkeley parser by replacing rare words with morphological descriptors of those words computed using language-specific modules, which have been hand-crafted for individual languages or are trained with additional annotation layers in the treebanks that we do not exploit
p1246
aVHowever, even when language-specific unknown word handling is added to the parser, our model still outperforms the Berkeley parser overall, showing that our model generalizes even better across languages than a parser for which this is touted as a strength [ 22 ]
p1247
aVOur span features appear to work well on both head-initial and head-final languages (see Basque and Korean in the table), and the fact that our parser performs well on such morphologically-rich languages as Hungarian indicates that our suffix model is sufficient to capture most of the morphological effects relevant to parsing
p1248
aVThese closely related languages use templatic morphology, for which suffixing is not appropriate; however, using additional surface features based on the output of a morphological analyzer did not lead to increased performance
p1249
aVFinally, because the system is, at its core, a classifier of spans, it can be used equally well for tasks that do not normally use parsing algorithms
p1250
aVFigure 5 shows an example that requires some analysis of sentence structure to correctly understand
p1251
aVThe grammar rule 2 u'\u005cu2192' 4 1 already encodes the notion of the sentiment of the right child being dominant, so when this is conjoined with our span feature on the first word ( While ), we end up with a feature that captures this effect
p1252
aVSyntax is often driven by heads of constituents, which tend to be located at the beginning or the end, whereas sentiment is more likely to depend on modifiers such as adjectives, which are typically present in the middle of spans
p1253
aVTherefore, we augment our existing model with standard sentiment analysis features that look at unigrams and bigrams in the span [ 31 ]
p1254
aVWe exploit this by adding an additional feature template similar to our span shape feature from Section 4.4 which uses the (deterministic) tag for each word as its descriptor
p1255
aVWe evaluated our model on the fine-grained sentiment analysis task presented in Socher et al
p1256
aVThe task is to predict the root sentiment label of each parse tree; however, because the data is annotated with sentiment at each span of each parse tree, we can also evaluate how well our model does at these intermediate computations
p1257
aVFollowing their experimental conditions, we filter the test set so that it only contains trees with non-neutral sentiment labels at the root
p1258
aVTheir model has high capacity to model complex interactions of words through a combinatory tensor, but it appears that our simpler, feature-driven model is just as effective at capturing the key effects of compositionality for sentiment analysis
p1259
aVTo date, the most successful constituency parsers have largely been generative, and operate by refining the grammar either manually or automatically so that relevant information is available locally to each parsing decision
p1260
aVOur main contribution is to show that there is an alternative to such annotation schemes namely, conditioning on the input and firing features based on anchored spans
p1261
aVWe build up a small set of feature templates as part of a discriminative constituency parser and outperform the Berkeley parser on a wide range of languages
p1262
aVOur system is available as open-source at https://www.github.com/dlwh/epic
p1263
aVThis work was partially supported by BBN under DARPA contract HR0011-12-C-0014, by a Google PhD fellowship to the first author, and an NSF fellowship to the second
p1264
asg88
(lp1265
sg90
(lp1266
sg92
(lp1267
VTo date, the most successful constituency parsers have largely been generative, and operate by refining the grammar either manually or automatically so that relevant information is available locally to each parsing decision.
p1268
aVOur main contribution is to show that there is an alternative to such annotation schemes namely, conditioning on the input and firing features based on anchored spans.
p1269
aVWe build up a small set of feature templates as part of a discriminative constituency parser and outperform the Berkeley parser on a wide range of languages.
p1270
aVMoreover, we show that our parser is adaptable to other tree-structured tasks such as sentiment analysis; we outperform the recent system of Socher et al.
p1271
aV2013 ) and obtain state of the art performance on their dataset.
p1272
aVOur system is available as open-source at https://www.github.com/dlwh/epic.
p1273
ag106
asg107
S'P14-1022'
p1274
sg109
(lp1275
VWe present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure.
p1276
aVFor example, instead of creating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP.
p1277
aVMoving context out of the grammar and onto surface features can greatly simplify the structural component of the parser because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as X-bar grammars.
p1278
aVKeeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks.
p1279
aVOn the SPMRL 2013 multilingual constituency parsing shared task [ 25 ] , our system outperforms the top single parser system of Björkelund et al.
p1280
aV2013 ) on a range of languages.
p1281
aVIn addition, despite being designed for syntactic analysis, our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al.
p1282
aV2013.
p1283
aVFinally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features.
p1284
ag106
asba(icmyPackage
FText
p1285
(dp1286
g3
(lp1287
VThe results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts
p1288
aVA long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantically similar words tend to have similar contextual distributions [ 36 ]
p1289
aVIt has been clear for decades now that raw co-occurrence counts don u'\u005cu2019' t work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques
p1290
aVThis vector optimization process is generally unsupervised, and based on independent considerations (for example, context reweighting is often justified by information-theoretic considerations, dimensionality reduction optimizes the amount of preserved variance, etc
p1291
aVSeveral parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning
p1292
aVThe last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus [ 6 , 15 , 14 , 25 , 32 , 44 ]
p1293
aVInstead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear
p1294
aVSince similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words
p1295
aVThis new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models with a single, well-defined supervised learning step
p1296
aV1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents
p1297
aVWe will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training-based alternative as predict(ive) models
p1298
aVPredictive DSMs are also called neural language models, because their supervised context prediction training is performed with neural networks, or, more cryptically, u'\u005cu201c' embeddings u'\u005cu201d'
p1299
aVThis is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based u'\u005cu201c' deep learning u'\u005cu201d' NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interesting side effect
p1300
aVBlacoe and Lapata ( 2012 ) compare count and predict representations as input to composition functions
p1301
aVCount vectors make for better inputs in a phrase similarity task, whereas the two representations are comparable in a paraphrase classification experiment
p1302
aVIn this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks
p1303
aVBoth count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC, 5 5 http://wacky.sslmit.unibo.it the English Wikipedia 6 6 http://en.wikipedia.org and the British National Corpus
p1304
aVThe latter were obtained by applying the Singular Value Decomposition [ 20 ] or Non-negative Matrix Factorization [ 29 ] , Lin ( 2007 ) algorithm, with reduced sizes ranging from 200 to 500 in steps of 100
p1305
aVThe CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window
p1306
aVAs an alternative, negative sampling estimates the probability of an output word by learning to distinguish it from draws from a noise distribution
p1307
aVWe train models without subsampling and with subsampling at t = 1 u'\u005cu2062' e - 5 (the toolkit page suggests 1 u'\u005cu2062' e - 3 - 1 u'\u005cu2062' e - 5 as a useful range based on empirical observations
p1308
aV10 10 http://clic.cimec.unitn.it/dm/ This model, based on the same input corpus we use, exemplifies a u'\u005cu201c' linguistically rich u'\u005cu201d' count-based DSM, that relies on lemmas instead or raw word forms, and has dimensions that encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts
p1309
aVA first set of semantic benchmarks was constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numerical scale
p1310
aVThe DSMs compute cosines of each candidate vector with the target, and pick the candidate with largest cosine as their answer
p1311
aVPerformance is evaluated in terms of correct-answer accuracy
p1312
aVFollowing previous art, we tackle categorization as an unsupervised clustering task
p1313
aVThe vectors produced by a model are clustered into n groups (with n determined by the gold standard partition) using the CLUTO toolkit [ 26 ] , with the repeated bisections with global optimization method and CLUTO u'\u005cu2019' s default settings otherwise (these are standard choices in the literature
p1314
aVIf the gold partition is reproduced perfectly, purity reaches 100%; it approaches 0 as cluster quality deteriorates
p1315
aVState-of-the-art purity was reached by Rothenhäusler and Schütze ( 2009 ) with a count model based on carefully crafted syntactic links
p1316
aVKatrenko and Adriaans ( 2008 ) reached top performance on this set using the full Web as a corpus and manually crafted, linguistically motivated patterns
p1317
aVWe experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb (e.g.,, people received a high average score as subject of to eat , and a low score as object of the same verb
p1318
aVFor each verb, we use the corpus-based tuples they make available to select the 20 nouns that are most strongly associated to the verb as subjects or objects, and we average the vectors of these nouns to obtain a u'\u005cu201c' prototype u'\u005cu201d' vector for the relevant argument slot
p1319
aVMikolov and colleagues tackle the challenge by subtracting the second example term vector from the first, adding the test term, and looking for the nearest neighbour of the resulting vector (what is the nearest neighbour of b u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' t u'\u005cu2062' h u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2192' - s u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2192' + g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' d u'\u005cu2062' s u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2192'
p1320
aV2013a ) reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large
p1321
aVTop accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al
p1322
aVNote however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched
p1323
aVThe latter emerge as clear winners, with a large margin over count vectors in most tasks
p1324
aVIndeed, the predictive models achieve an impressive overall performance, beating the current state of the art in several cases, and approaching it in many more
p1325
aVIt is worth stressing that, as reviewed in Section 3 , the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning
p1326
aVOur predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers
p1327
aVRecall from Section 3 that we tackle selectional preference by creating average vectors representing typical verb arguments
p1328
aVAre our results robust to parameter choices, or are they due to very specific and brittle settings
p1329
aVWe see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem
p1330
aVA more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning
p1331
aVThe fourth block reports performance in what might be the most realistic scenario, namely by tuning the parameters on a development task
p1332
aVSpecifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets
p1333
aVTables 3 and 4 let us take a closer look at the most important count and predict parameters, by reporting the characteristics of the best models (in terms of average performance-based ranking across tasks) from both classes
p1334
aVFor the count models, PMI is clearly the better weighting scheme, and SVD outperforms NMF as a dimensionality reduction technique
p1335
aVHowever, no compression at all (using all 300K original dimensions) works best
p1336
aVWe must leave the investigation of the parameters that make our predict vectors so much better than cw (more varied training corpus window size objective function being used subsampling u'\u005cu2026' ) to further work
p1337
aVAs seasoned distributional semanticists with thorough experience in developing and using count vectors, we set out to conduct this study because we were annoyed by the triumphalist overtones often surrounding predict models, despite the almost complete lack of a proper comparison to count vectors
p1338
aVInstead, we found that the predict models are so good that, while the triumphalist overtones still sound excessive, there are very good reasons to switch to the new architecture
p1339
aVHowever, due to space limitations we have only focused here on quantitative measures
p1340
aVBased on the results reported here and the considerations we just made, we would certainly recommend anybody interested in using DSMs for theoretical or practical applications to go for the predict models, with the important caveat that they are not all created equal (cf. the big difference between word2vec and cw models
p1341
aVFor example, the developers of Latent Semantic Analysis [ 28 ] , Topic Models [ 21 ] and related DSMs have shown that the dimensions of these models can be interpreted as general u'\u005cu201c' latent u'\u005cu201d' semantic domains, which gives the corresponding models some a priori cognitive plausibility while paving the way for interesting applications
p1342
aVDoes all of this even matter, or are we on the cusp of discovering radically new ways to tackle the same problems that have been approached as we just sketched in traditional distributional semantics
p1343
asg88
(lp1344
sg90
(lp1345
sg92
(lp1346
VThis paper has presented the first systematic comparative evaluation of count and predict vectors.
p1347
aVAs seasoned distributional semanticists with thorough experience in developing and using count vectors, we set out to conduct this study because we were annoyed by the triumphalist overtones often surrounding predict models, despite the almost complete lack of a proper comparison to count vectors.
p1348
aV12 12 Here is an example, where word2vec is called the crown jewel of natural language processing http://bit.ly/1ipv72M Our secret wish was to discover that it is all hype, and count vectors are far superior to their predictive counterparts.
p1349
aVA more realistic expectation was that a complex picture would emerge, with predict and count vectors beating each other on different tasks.
p1350
aVInstead, we found that the predict models are so good that, while the triumphalist overtones still sound excessive, there are very good reasons to switch to the new architecture.
p1351
aVHowever, due to space limitations we have only focused here on quantitative measures.
p1352
aVIt remains to be seen whether the two types of models are complementary in the errors they make, in which case combined models could be an interesting avenue for further work.
p1353
aVThe space of possible parameters of count DSMs is very large, and it s entirely possible that some options we did not consider would have improved count vector performance somewhat.
p1354
aVStill, given that the predict vectors also outperformed the syntax-based dm model, and often approximated state-of-the-art performance, a more proficuous way forward might be to focus on parameters and extensions of the predict models instead.
p1355
aVAfter all, we obtained our already excellent results by just trying a few variations of the word2vec defaults.
p1356
aVAdd to this that, beyond the standard lexical semantics challenges we tested here, predict models are currently been successfully applied in cutting-edge domains such as representing phrases [ 34 , 43 ] or fusing language and vision in a common semantic space [ 19 , 42 ].
p1357
aVBased on the results reported here and the considerations we just made, we would certainly recommend anybody interested in using DSMs for theoretical or practical applications to go for the predict models, with the important caveat that they are not all created equal (cf. the big difference between word2vec and cw models.
p1358
aVAt the same time, given the large amount of work that has been carried out on count DSMs, we would like to explore, in the near future, how certain questions and methods that have been considered with respect to traditional DSMs will transfer to predict models.
p1359
aVFor example, the developers of Latent Semantic Analysis [ 28 ] , Topic Models [ 21 ] and related DSMs have shown that the dimensions of these models can be interpreted as general latent semantic domains, which gives the corresponding models some a priori cognitive plausibility while paving the way for interesting applications.
p1360
aVAnother important line of DSM research concerns context engineering .
p1361
aVThere has been for example much work on how to encode syntactic information into context features [ 37 ] , and more recent studies construct and combine feature spaces expressing topical vs. functional information [ 46 ].
p1362
aVTo give just one last example, distributional semanticists have looked at whether certain properties of vectors reflect semantic relations in the expected way e.g.,, whether the vectors of hypernyms distributionally include the vectors of hyponyms in some mathematical precise sense.
p1363
aVDo the dimensions of predict models also encode latent semantic domains.
p1364
aVDo these models afford the same flexibility of count vectors in capturing linguistically rich contexts.
p1365
aVDoes the structure of predict vectors mimic meaningful semantic relations.
p1366
aVDoes all of this even matter, or are we on the cusp of discovering radically new ways to tackle the same problems that have been approached as we just sketched in traditional distributional semantics.
p1367
aVEither way, the results of the present investigation indicate that these are important directions for future research in computational semantics.
p1368
ag106
asg107
S'P14-1023'
p1369
sg109
(lp1370
VContext-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block.
p1371
aVDespite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches.
p1372
aVIn this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings.
p1373
aVThe results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.
p1374
ag106
asba(icmyPackage
FText
p1375
(dp1376
g3
(lp1377
VUsing a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages
p1378
aVWe provide results on three new test sets in Spanish, Farsi, and Russian
p1379
aVThe results support the hypothesis that metaphors are conceptual, rather than lexical, in nature
p1380
aVLakoff and Johnson ( 1980 ) characterize metaphor as reasoning about one thing in terms of another, i.e.,, a metaphor is a type of conceptual mapping , where words or phrases are applied to objects and actions in ways that do not permit a literal interpretation
p1381
aVOn one hand, there is a subjective component humans may disagree whether a particular expression is used metaphorically or not, as there is no clear-cut semantic distinction between figurative and metaphorical language [ 32 ]
p1382
aV1) we develop a new state-of-the-art English metaphor detection system that uses conceptual semantic features, such as a degree of abstractness and semantic supersenses; 2 2 https://github.com/ytsvetko/metaphor (2) we create new metaphor-annotated corpora for Russian and English; 3 3 http://www.cs.cmu.edu/~ytsvetko/metaphor/datasets.zip (3) using a paradigm of model transfer [ 21 , 41 , 15 ] , we provide support for the hypothesis that metaphors are conceptual (rather than lexical) in nature by showing that our English-trained model can detect metaphors in Spanish, Farsi, and Russian
p1383
aVAccording to Wilks [ 42 ] , this metaphor represents a violation of selectional preferences for the verb drink , which is normally associated with animate subjects (the car is inanimate and, hence, cannot drink in the literal sense of the verb
p1384
aV6 6 If word one is represented by features u'\u005cud835' u'\u005cudc2e' u'\u005cu2208' u'\u005cu211d' n and word two by features u'\u005cud835' u'\u005cudc2f' u'\u005cu2208' u'\u005cu211d' m then the conjunction feature vector is the vectorization of the outer product u'\u005cud835' u'\u005cudc2e' u'\u005cud835' u'\u005cudc2f' u'\u005cu22a4'
p1385
aVEnglish adjectives do not, as yet, have a similar high-level semantic partitioning in WordNet, thus we use a 13-class taxonomy of adjective supersenses constructed by Tsvetkov et al
p1386
aVSupersenses are particularly attractive features for metaphor detection coarse sense taxonomies can be viewed as semantic concepts, and since concept mapping is a process in which metaphors are born, we expect different supersense co-occurrences in metaphoric and literal combinations
p1387
aV2013 ) reveal an interesting cross-lingual property of distributed word representations there is a strong similarity between the vector spaces across languages that can be easily captured by linear mapping
p1388
aVThus, vector space models can also be seen as vectors of (latent) semantic concepts, that preserve their u'\u005cu201c' meaning u'\u005cu201d' across languages
p1389
aVRandom forest ensembles are particularly suitable for our resource-scarce scenario rather than overfitting, they produce a limiting value of the generalization error as the number of trees increases, 8 8 See Theorem 1.2 in [ 3 ] for details and no hyperparameter tuning is required
p1390
aVIf this probability is above a threshold, the relation is classified as metaphoric, otherwise it is literal
p1391
aVThey were chosen empirically based on accuracy during cross-validation
p1392
aVA Russian word u'\u005cu0413' u'\u005cu041e' u'\u005cu041b' u'\u005cu041e' u'\u005cu0412' u'\u005cu0410' is translated as head and brain
p1393
aVHence, we select all the synsets of the nouns head and brain
p1394
aVFour of these synsets are associated with the supersense noun.body
p1395
aVTherefore, the value of the feature noun.body is 4 / 38 u'\u005cu2248' 0.11
p1396
aVAfter filtering, there are 953 metaphorical and 656 literal SVO relations which we use as a training set
p1397
aVAt least one additional person carefully examined and culled the collected metaphors, by removing duplicates, weak metaphors, and metaphorical phrases (such as drowning students ) whose interpretation depends on the context
p1398
aVWe collect and annotate metaphoric and literal test sentences in four languages
p1399
aVThus, we compile eight test datasets, four for SVO relations, and four for AN relations
p1400
aVOur test sentence domains are, therefore, diverse economic, political, sports, etc
p1401
aVThen she used the SketchEngine, which provides searching capability for the TenTen Web corpus, 19 19 http://trac.sketchengine.co.uk/wiki/Corpora/enTenTen to extract sentences with words that frequently co-occurred with words from the seed lists
p1402
aVNo English annotators of the test set, and only one Russian annotator out of 6 participated in the selection of the training samples
p1403
aVThus, we trust that annotator judgments were not biased towards the cases that the system is trained to process
p1404
aVThis is done by computing an accuracy in the 10-fold cross validation
p1405
aVYet, combining all features leads to even higher accuracy during cross-validation
p1406
aVAlthough the first experiment shows very high scores, the 10-fold cross-validation cannot fully reflect the generality of the model, because all folds are parts of the same corpus
p1407
aVThey are collected by the same human judges and belong to the same domain
p1408
aVTherefore, experiments on out-of-domain data are crucial
p1409
aVIt is possible to trade precision for recall by choosing a different threshold
p1410
aVThus, in addition to giving a single f -score value for balanced thresholds, we present a Receiver Operator Characteristic (ROC) curve, where we plot a fraction of true positives against the fraction of false positives for 100 threshold values in the range from zero to one
p1411
aVThe area under the ROC curve (AUC) can be interpreted as the probability that a classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen negative example
p1412
aV20 20 Assuming that positive examples are labeled by ones, and negative examples are labeled by zeros
p1413
aVAccording to ROC plots in Figure 3 , all three feature sets are effective, both for SVO and for AN tasks
p1414
aVAccording to Table 3 , we obtain higher performance scores for both Russian and English
p1415
aVAccording to results in Table 4 , almost all of the judge-specific f -scores are slightly higher for our system, as well as the overall average f -score
p1416
aVWe hypothesize that this happens due to a higher-quality translation dictionary (which allows a more accurate model transfer
p1417
aVRelatively lower (yet reasonable) results for Farsi can be explained by a smaller size of the bilingual dictionary (thus, fewer feature projections can be obtained
p1418
aVAlso note that, in our experience, most of Farsi metaphors are adjective-noun constructions
p1419
aVThis is why the AN fa dataset in Table 1 is significantly larger than SVO fa
p1420
aVWe view this result as a strong evidence of language-independent nature of our metaphor detection method
p1421
aVIn particular, this shows that proposed conceptual features can be used to detect selectional preferences violation across languages
p1422
aVFor example, in English we classify as metaphoric dirty word and cloudy future
p1423
aVWord pairs dirty diaper and cloudy weather have same adjectives
p1424
aVYet they are classified as literal
p1425
aVIndeed, diaper is a more concrete term than word and weather is more concrete than future
p1426
aVIf this adjective modifies a noun with the supersense noun.feeling , we conclude that a metaphor is found
p1427
aVBecause they heavily rely on WordNet and availability of imageability scores, their approach may not be applicable to low-resource languages
p1428
aVWe cannot compare directly our model with this work because our classifier is restricted to detection of only SVO and AN metaphors
p1429
aVCurrent work builds on this study, and incorporates new syntactic relations as metaphor candidates, adds several new feature sets and different, more reliable datasets for evaluating results
p1430
aVWe demonstrate results on two new languages, Spanish and Farsi, to emphasize the generality of the method
p1431
aVThe key contribution of our work is that we show how to identify metaphors across languages by building a model in English and applying it u'\u005cu2014' without adaptation u'\u005cu2014' to other languages
p1432
aVFuture work will expand the scope of metaphor identification by including nominal metaphoric relations as well as explore techniques for incorporating contextual features, which can play a key role in identifying certain kinds of metaphors
p1433
asg88
(lp1434
sg90
(lp1435
sg92
(lp1436
VThe key contribution of our work is that we show how to identify metaphors across languages by building a model in English and applying it without adaptation to other languages.
p1437
aVSpanish, Farsi, and Russian.
p1438
aVThis model uses language-independent (rather than lexical or language specific) conceptual features.
p1439
aVNot only do we establish benchmarks for Spanish, Farsi, and Russian, but we also achieve state-of-the-art performance in English.
p1440
aVIn addition, we present a comparison of relative contributions of several types of features.
p1441
aVWe concentrate on metaphors in the context of two kinds of syntactic relations subject-verb-object (SVO) relations and adjective-noun (AN) relations, which account for a majority of all metaphorical phrases.
p1442
aVFuture work will expand the scope of metaphor identification by including nominal metaphoric relations as well as explore techniques for incorporating contextual features, which can play a key role in identifying certain kinds of metaphors.
p1443
aVSecond, cross-lingual model transfer can be improved with more careful cross-lingual feature projection.
p1444
ag106
asg107
S'P14-1024'
p1445
sg109
(lp1446
VWe show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction.
p1447
aVOur model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language.
p1448
aVUsing a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages.
p1449
aVWe provide results on three new test sets in Spanish, Farsi, and Russian.
p1450
aVThe results support the hypothesis that metaphors are conceptual, rather than lexical, in nature.
p1451
ag106
asba(icmyPackage
FText
p1452
(dp1453
g3
(lp1454
VUnsupervised word sense disambiguation ( wsd ) methods are an attractive approach to all-words wsd due to their non-reliance on expensive annotated data
p1455
aVUnsupervised estimates of sense frequency have been shown to be very useful for wsd due to the skewed nature of word sense distributions
p1456
aVWord sense distributions tend to be Zipfian, and as such, a simple but surprisingly high-accuracy back-off heuristic for word sense disambiguation ( wsd ) is to tag each instance of a given word with its predominant sense []
p1457
aVSuch an approach requires knowledge of predominant senses; however, word sense distributions u'\u005cu2014' and predominant senses too u'\u005cu2014' vary from corpus to corpus
p1458
aVTherefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required [ 12 ]
p1459
aVTopic models have been used for wsd in a number of studies [ 1 , 13 , 19 , 3 , 11 ] , but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses
p1460
aVBecause of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus
p1461
aVWe further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses u'\u005cu2014' i.e.,, senses that are in the sense inventory but not attested in a given corpus
p1462
aVIn contrast to , the use of topic models makes this possible, using topics as a proxy for sense []
p1463
aVEarlier work on identifying novel senses focused on individual tokens [ 7 ] , whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense
p1464
aVThere has been a considerable amount of research on representing word senses and disambiguating usages of words in context ( wsd ) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense wsd algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g., in the form of sense definitions [] , semantic relationships [ 17 ] or annotated data [ 20 ]
p1465
aVOne extremely useful piece of information is the word sense prior or expected word sense frequency distribution
p1466
aVThis is important because word sense distributions are typically skewed [] , and systems do far better when they take bias into account []
p1467
aVDue to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically [ 5 , 16 , 6 ]
p1468
aVThe word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over
p1469
aVThe work of Boyd-Graber and Blei ( 2007 ) is highly related in that it extends the method of to provide a generative model which assumes the words in a given document are generated according to the topic distribution appropriate for that document
p1470
aVThey then predict the most likely sense for each word in the document based on the topic distribution and the words in context ( u'\u005cu201c' corroborators u'\u005cu201d' ), each of which, in turn, depends on the document u'\u005cu2019' s topic distribution
p1471
aVUsing this approach, they get comparable results to McCarthy et al. when context is ignored (i.e., using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into account
p1472
aVSince the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context, we will compare our model with that proposed by McCarthy et al
p1473
aVIn contrast to these studies, we propose a model for comparing a corpus with a sense inventory
p1474
aVOur methodology is based on the WSI system described in , 1 1 Based on the implementation available at https://github.com/jhlau/hdp-wsi which has been shown [] to achieve state-of-the-art results over the WSI tasks from SemEval-2007 [] , SemEval-2010 [] and SemEval-2013 []
p1475
aVFollowing , we assign one topic to each usage by selecting the topic that has the highest cumulative probability density, based on the topic allocations of all words in the context window for that usage
p1476
aVDue to the computational overhead associated with these features, and the fact that the empirical impact of the features was found to be marginal, we make no use of parser-based features in this paper
p1477
aVWe design our topic u'\u005cu2013' sense alignment methodology with portability in mind u'\u005cu2014' it should be applicable to any sense inventory
p1478
aVAs such, our alignment methodology assumes only that we have access to a conventional sense gloss or definition for each sense, and does not rely on ontological/structural knowledge (e.g., the \u005csmaller WordNet hierarchy
p1479
aVTo compute the similarity between a sense and a topic, we first convert the words in the gloss/definition into a multinomial distribution over words, based on simple maximum likelihood estimation
p1480
aVWe then calculate the Jensen u'\u005cu2013' Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1
p1481
aVTo learn the predominant sense, we compute the prevalence score of each sense and take the sense with the highest prevalence score as the predominant sense
p1482
aVThe prevalence score for a sense is computed by summing the product of its similarity scores with each topic (i.e., sim u'\u005cu2062' ( s i , t j ) ) and the prior probability of the topic in question (based on maximum likelihood estimation
p1483
aVFor each domain, annotators were asked to sense-annotate a random selection of sentences for each of 40 target nouns, based on \u005csmaller WordNet v1.7
p1484
aVThe predominant sense and distribution across senses for each target lemma was obtained by aggregating over the sense annotations
p1485
aVThe authors evaluated their method in terms of wsd accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus
p1486
aVFor the remainder of the paper, we denote their system as MKWC
p1487
aVTo compare our system (HDP-WSI) with MKWC, we apply it to the three datasets of
p1488
aVFor each dataset, we use HDP to induce topics for each target lemma, compute the similarity between the topics and the \u005csmaller WordNet senses (Equation ( 1 )), and rank the senses based on the prevalence scores (Equation ( 2
p1489
aVIn addition to the wsd accuracy based on the predominant sense inferred from a particular corpus, we additionally compute
p1490
aV1) Acc ub , the upper bound for the first sense-based wsd accuracy (using the gold standard predominant sense for disambiguation); 7 7 The upper bound for a wsd approach which tags all token occurrences of a given word with the same sense, as a first step towards context-sensitive unsupervised wsd and (2) ERR , the error rate reduction between the accuracy for a given system ( Acc ) and the upper bound ( Acc ub ), calculated as follows
p1491
aVLooking at the results in Table 2 , we see little difference in the results for the two methods, with MKWCperforming better over two of the datasets ( BNC and SPORTS ) and HDP-WSIperforming better over the third ( FINANCE ), but all differences are small
p1492
aVBased on the McNemar u'\u005cu2019' s Test with Yates correction for continuity, MKWCis significantly better over BNC and HDP-WSIis significantly better over FINANCE ( p 0.0001 in both cases), but the difference over SPORTS is not statistically significance ( p 0.1
p1493
aVNote that there is still much room for improvement with both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies
p1494
aVGiven that both systems compute a continuous-valued prevalence score for each sense of a target lemma, a distribution of senses can be obtained by normalising the prevalence scores across all senses
p1495
aVIn the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the \u005csmaller Macmillan English Dictionary
p1496
aVIn our second set of experiments, we move to a new dataset [ 9 ] based on text from ukWaC [ 8 ] and Twitter, and annotated using the \u005csmaller Macmillan English Dictionary 10 10 http://www.macmillandictionary.com/ (henceforth u'\u005cu201c' \u005csmaller Macmillan u'\u005cu201d'
p1497
aVIn terms of the original research which gave rise to the sense-tagged dataset, \u005csmaller Macmillan was chosen over \u005csmaller WordNet for reasons including
p1498
aV1) the well-documented difficulties of sense tagging with fine-grained \u005csmaller WordNet senses [] ; (2) the regular update cycle of \u005csmaller Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than \u005csmaller WordNet (and also \u005csmaller OntoNotes
p1499
aV100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3 u'\u005cu2013' Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py [ ] and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 [] for Twitter, and the POS tags provided with the corpus for ukWaC
p1500
aVAmazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to \u005csmaller Macmillan , including allowing the annotators the option to label a usage as u'\u005cu201c' Other u'\u005cu201d' in instances where the usage was not captured by any of the \u005csmaller Macmillan senses
p1501
aVWe refer to these two datasets as ukWaC and Twitter henceforth
p1502
aVTo apply our method to the two datasets, we use HDP-WSIto train a model for each target noun, based on the combined set of usages of that lemma in each of the two background corpora, namely the original Twitter crawl that gave rise to the Twitter dataset, and all of ukWaC
p1503
aVAs in Section 4 , we evaluate in terms of wsd accuracy (Table 4 ) and JS divergence over the gold-standard sense distribution (Table 5
p1504
aVWe also present the results for a) a supervised baseline ( u'\u005cu201c' FS corpus u'\u005cu201d' ), based on the most frequent sense in the corpus; and (b) an unsupervised baseline ( u'\u005cu201c' FS dict u'\u005cu201d' ), based on the first-listed sense in \u005csmaller Macmillan
p1505
aVIn each case, the sense distribution is based on allocating all probability mass for a given word to the single sense identified by the respective method
p1506
aVWe first notice that, despite the coarser-grained senses of \u005csmaller Macmillan as compared to \u005csmaller WordNet , the upper bound wsd accuracy using \u005csmaller Macmillan is comparable to that of the \u005csmaller WordNet -based datasets over the balanced BNC, and quite a bit lower than that of the two domain corpora of
p1507
aVPart of the reason for this improvement is simply that the average polysemy in \u005csmaller Macmillan (5.6 senses per target lemma) is slightly less than in \u005csmaller WordNet (6.7 senses per target lemma), 13 13 Note that the set of lemmas differs between the respective datasets, so this isn u'\u005cu2019' t an accurate reflection of the relative granularity of the two dictionaries making the task slightly easier in the \u005csmaller Macmillan case
p1508
aVIntuitively, an unused sense should have low similarity with the HDP induced topics
p1509
aVAs such, we introduce sense-to-topic affinity, a measure that estimates how likely a sense is not attested in the corpus
p1510
aVWe treat the task of identification of unused senses as a binary classification problem, where the goal is to find a sense-to-topic affinity threshold below which a sense will be considered to be unused
p1511
aVWe pool together all the senses and run 10-fold cross validation to learn the threshold for identifying unused senses, 14 14 We used a fixed step and increment at steps of 0.001, up to the max value of st-affinity when optimising the threshold evaluated using sense-level precision ( P ), recall ( R ) and F-score ( F ) at detecting unattested senses
p1512
aVWe found encouraging results for the task, as detailed in Table 6
p1513
aVFor the threshold, the average value with standard deviation is 0.092 ± 0.044 over ukWaC and 0.125 ± 0.052 over Twitter , indicating relative stability in the value of the threshold both internally within a dataset, and also across datasets
p1514
aVAlso, while we have annotations of u'\u005cu201c' Other u'\u005cu201d' usages in Twitter and ukWaC , there is no real expectation that all such usages will correspond to the same sense in practice, they are attributable to a myriad of effects such as incorporation in a non-compositional multiword expression, and errors in POS tagging (i.e., the usage not being nominal
p1515
aVAs such, we can u'\u005cu2019' t use the u'\u005cu201c' Other u'\u005cu201d' annotations to evaluate novel sense identification
p1516
aVNote that we do not consider high-frequency senses with frequency higher than 0.6, as it is rare for a medium- to high-frequency word to take on a novel sense which is then the predominant sense in a given corpus
p1517
aVNote also that not all target lemmas will have a novel sense through synthesis, as they may have no senses that fall within the indicated bounds of relative occurrence (e.g., if 60 u'\u005cu2062' % of usages are a single sense
p1518
aVInstead, we are seeking to identify clusters of usages which are instances of a novel sense, e.g., for presentation to a lexicographer as part of a dictionary update process []
p1519
aVThat is, for each usage, we want to classify whether it is an instance of a given novel sense
p1520
aVBased on this intuition, we introduce topic-to-sense affinity to estimate the similarity of a topic to the set of senses, as follows
p1521
aVwhere, once again, sim u'\u005cu2062' ( s i , t j ) is defined as in Equation ( 1 ), and T and S represent the number of topics and senses, respectively
p1522
aVUsing topic-to-sense affinity as the sole feature, we pool together all instances and optimise the affinity feature to classify instances that have novel senses
p1523
aVEvaluation is done by computing the mean precision, recall and F-score across 10 separate runs; results are summarised in Table 7
p1524
aVThis is unsurprising given that high-frequency senses have a higher probability of generating related topics (sense-related words are observed more frequently in the corpus), and as such are more easily identifiable
p1525
aVIn other words, we are assuming knowledge of which words have novel sense, and the task is to identify specifically what the novel sense is, as represented by novel usages
p1526
aVResults are presented in Table 8
p1527
aVThat we use the frequency rather than the probability of the topic here is deliberate, as topics with a higher raw number of occurrences (whether as a low-probability topic for a high-frequency word, or a high-probability topic for a low-frequency word) are indicative of a novel word sense
p1528
aVFor each of our three datasets (with low-, medium- and high-frequency novel senses, respectively), we compute the novelty of the target lemmas and the p -value of a one-tailed Wilcoxon rank sum test to test if the two groups of lemmas (i.e., lemmas with a novel sense vs. lemmas without a novel sense) are statistically different
p1529
aVFuture work could pursue a more sophisticated methodology, using non-linear combinations of sim u'\u005cu2062' ( s i , t j ) for computing the affinity measures or multiple features in a supervised context
p1530
aVIn summary, we have proposed a topic modelling-based method for estimating word sense distributions, based on Hierarchical Dirichlet Processes and the earlier work of on word sense induction, in probabilistically mapping the automatically-learned topics to senses in a sense inventory
p1531
aVWe evaluated the ability of the method to learn predominant senses and induce word sense distributions, based on a broad range of datasets and two separate sense inventories
p1532
aVIn doing so, we established that our method is comparable to the approach of at predominant sense learning, and superior at inducing word sense distributions
p1533
aVThis research was supported in part by funding from the Australian Research Council
p1534
asg88
(lp1535
sg90
(lp1536
sg92
(lp1537
S''
p1538
asg107
S'P14-1025'
p1539
sg109
(lp1540
VUnsupervised word sense disambiguation ( wsd ) methods are an attractive approach to all-words wsd due to their non-reliance on expensive annotated data.
p1541
aVUnsupervised estimates of sense frequency have been shown to be very useful for wsd due to the skewed nature of word sense distributions.
p1542
aVThis paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text.
p1543
aVWe demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren t attested in the corpus, and identifying novel senses in the corpus which aren t captured in the sense inventory.
p1544
ag106
asba(icmyPackage
FText
p1545
(dp1546
g3
(lp1547
VWe show that a learner can use Bayesian model selection to determine the location of function words in their language, even though the input to the model only consists of unsegmented sequences of phones
p1548
aVThus our computational models support the hypothesis that function words play a special role in word learning
p1549
aVWe do this by comparing two computational models of word segmentation which differ solely in the way that they model function words
p1550
aVa word segmentation model should segment this as ju wÉnt tu si Ã°É bÊk, which is the IPA representation of u'\u005cu201c' you want to see the book u'\u005cu201d'
p1551
aVThis suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially in human lexical acquisition
p1552
aVAs a reviewer points out, we present no evidence that children use function words in the way that our model does, and we want to emphasise we make no such claim
p1553
aVWhile absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%
p1554
aVAs a reviewer points out, the changes we make to our models to incorporate function words can be viewed as u'\u005cu201c' building in u'\u005cu201d' substantive information about possible human languages
p1555
aVBy comparing the posterior probability of two models u'\u005cu2014' one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases u'\u005cu2014' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words
p1556
aVProperties 1 u'\u005cu2013' 4 suggest that function words might play a special role in language acquisition because they are especially easy to identify, while property 5 suggests that they might be useful for identifying lexical categories
p1557
aVIn addition, it is plausible that function words play a crucial role in children u'\u005cu2019' s acquisition of more complex syntactic phenomena [] , so it is interesting to investigate the roles they might play in computational models of language acquisition
p1558
aVAdaptor grammars are non-parametric, i.e.,, not characterisable by a finite set of parameters, if the set of possible subtrees of the adapted nonterminals is infinite
p1559
aVAdaptor grammars are useful when the goal is to learn a potentially unbounded set of entities that need to satisfy hierarchical constraints
p1560
aVAs section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy
p1561
aVIf X u'\u005cu2208' W (i.e.,, if X is a terminal) then G X is the distribution that puts probability 1 on the single-node tree labelled X
p1562
aVThere are Markov Chain Monte Carlo (MCMC) and Variational Bayes procedures for estimating the posterior distribution over rule probabilities u'\u005cud835' u'\u005cudf3d' and parse trees given data consisting of terminal strings alone []
p1563
aVAs explain, by inserting a Dirichlet Process (DP) or Pitman-Yor Process (PYP) into the generative mechanism ( 1 ) the model u'\u005cu201c' concentrates u'\u005cu201d' mass on a subset of trees []
p1564
aVIn an Adaptor Grammar the unadapted nonterminals N u'\u005cu2216' A expand via ( 1 ), just as in a PCFG, but the distributions of the adapted nonterminals A are u'\u005cu201c' concentrated u'\u005cu201d' by passing them through a DP or PYP
p1565
aVThere are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings describe a MCMC sampler and describe a Variational Bayes procedure
p1566
aVWe use the MCMC procedure here since this has been successfully applied to word segmentation problems in previous work []
p1567
aVPerhaps the simplest word segmentation model is the unigram model , where utterances are modeled as sequences of words, and where each word is a sequence of segments []
p1568
aVA unigram model can be expressed as an Adaptor Grammar with one adapted non-terminal u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' ¯ (we indicate adapted nonterminals by underlining them in grammars here; regular expressions are expanded into right-branching productions
p1569
aVBecause u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' is an adapted nonterminal, the adaptor grammar memoises u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' subtrees, which corresponds to learning the phone sequences for the words of the language
p1570
aVSection 2.3 presents the major novel contribution of this paper by explaining how we modify these adaptor grammars to capture some of the special properties of function words
p1571
aVThe rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u005cu201c' monkey model u'\u005cu201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter
p1572
aVThe model just described assumes that word-internal syllables have the same structure as word-peripheral syllables, but in languages such as English word-peripheral onsets and codas can be more complex than the corresponding word-internal onsets and codas
p1573
aVFor example, the word u'\u005cu201c' string u'\u005cu201d' begins with the onset cluster str, which is relatively rare word-internally showed that word segmentation accuracy improves if the model can learn different consonant sequences for word-inital onsets and word-final codas
p1574
aVIt is easy to express this as an Adaptor Grammar
p1575
aV4 ) is replaced with ( 10 u'\u005cu2013' 11 ) and ( 12 u'\u005cu2013' 17 ) are added to the grammar
p1576
aVIn this grammar the suffix u'\u005cu201c' u'\u005cud835' u'\u005cudda8' u'\u005cu201d' indicates a word-initial element, and u'\u005cu201c' u'\u005cud835' u'\u005cudda5' u'\u005cu201d' indicates a word-final element
p1577
aVInformally, a model that generates words independently is likely to incorrectly segment multi-word expressions such as u'\u005cu201c' the doggie u'\u005cu201d' as single words because the model has no way to capture word-to-word dependencies, e.g.,, that u'\u005cu201c' doggie u'\u005cu201d' is typically preceded by u'\u005cu201c' the u'\u005cu201d'
p1578
aVAdaptor grammar models cannot express bigram dependencies, but they can capture similiar inter-word dependencies using phrase-like units that calls collocations showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations
p1579
aVThis can be achieved by replacing ( 2 ) with ( 18 u'\u005cu2013' 21
p1580
aVWhile not designed to correspond to syntactic phrases, by examining the sample parses induced by the Adaptor Grammar we noticed that the collocations often correspond to noun phrases, prepositional phrases or verb phrases
p1581
aVThe starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure ( 5 - 21 ), as prior work has found that this yields the highest word segmentation token f-score []
p1582
aVWe put u'\u005cu201c' function words u'\u005cu201d' in scare quotes below because our model only approximately captures the linguistic properties of function words
p1583
aVFor comparison purposes we also include results for a mirror-image model that permits u'\u005cu201c' function words u'\u005cu201d' on the right periphery, a model which permits u'\u005cu201c' function words u'\u005cu201d' on both the left and right periphery (achieved by changing rules 22 u'\u005cu2013' 24 ), as well as a model that analyses all words as monosyllabic
p1584
aVSection 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u005cu201c' function word u'\u005cu201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u005cu2013' 24 ) are replaced with the mirror-image rules in which u'\u005cu201c' function words u'\u005cu201d' are attached to the right periphery
p1585
aVWe use the Adaptor Grammar software available from http://web.science.mq.edu.au/~mjohnson/ with the same settings as described in , i.e.,, we perform Bayesian inference with u'\u005cu201c' vague u'\u005cu201d' priors for all hyperparameters (so there are no adjustable parameters in our models), and perform 8 different MCMC runs of each condition with table-label resampling for 2,000 sweeps of the training data
p1586
aVAt every 10th sweep of the last 1,000 sweeps we use the model to segment the entire corpus (even if it is only trained on a subset of it), so we collect 800 sample segmentations of each utterance
p1587
aVWhen the training data is very small the Monosyllabic grammar produces the highest accuracy results, presumably because a large proportion of the words in child-directed speech are monosyllabic
p1588
aVIt u'\u005cu2019' s interesting that after about 1,000 sentences the model that allows u'\u005cu201c' function words u'\u005cu201d' only on the right periphery is considerably less accurate than the baseline model
p1589
aVPresumably this is because it tends to misanalyse multi-syllabic words on the right periphery as sequences of monosyllabic words
p1590
aVa, the, your, little 1 1 The phone u'\u005cu2018' l u'\u005cu2019' is generated by both Consonant and Vowel , so u'\u005cu201c' little u'\u005cu201d' can be (incorrectly) analysed as one syllable in
p1591
aVThus, the present model, initially aimed at segmenting words from continuous speech, shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words [] , it allows learners to acquire at least some of the function words of their language (e.g., [] ); and furthermore, it may also allow them to start grouping together function words according to their category []
p1592
aVThis question is important because knowing the side where function words preferentially occur is related to the question of the direction of syntactic headedness in the language, and an accurate method for identifying the location of function words might be useful for initialising a syntactic learner
p1593
aVExperimental evidence suggests that infants as young as 8 months of age already expect function words on the correct side for their language u'\u005cu2014' left-periphery for Italian infants and right-periphery for Japanese infants [] u'\u005cu2014' so it is interesting to see whether purely distributional learners such as the ones studied here can identify the correct location of function words in phrases
p1594
aVIn this section, we show that learners could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the marginal probability of the data for the left-periphery and the right-periphery models
p1595
aVwhere the marginal likelihood or u'\u005cu201c' evidence u'\u005cu201d' for a model G is obtained by integrating over all of the hidden or latent structure and parameters u'\u005cud835' u'\u005cudf3d'
p1596
aVTextbooks such as describe a number of methods for calculating P ( D u'\u005cu2223' G ) , but most of them assume that the parameter space u'\u005cu0394' is continuous and so cannot be directly applied here
p1597
aVThe Harmonic Mean estimator ( 4 ) for ( 31 ), which we used here, is a popular estimator for ( 31 ) because it only requires the ability to calculate P ( D , u'\u005cud835' u'\u005cudf3d' u'\u005cu2223' G ) for samples from P ( u'\u005cud835' u'\u005cudf3d' u'\u005cu2223' D , G )
p1598
aVFigure 3 depicts how the Bayes factor in favour of left-peripheral attachment of u'\u005cu201c' function words u'\u005cu201d' varies as a function of the number of utterances in the training data D (calculated from the last 1000 sweeps of 8 MCMC runs of the corresponding adaptor grammars
p1599
aVAs that figure shows, once the training data contains more than about 1,000 sentences the evidence for the left-peripheral grammar becomes very strong
p1600
aVUnfortunately, as Murphy and others warn, the Harmonic Mean estimator is extremely unstable (Radford Neal calls it u'\u005cu201c' the worst MCMC method ever u'\u005cu201d' in his blog), so we think it is important to confirm these results using a more stable estimator
p1601
aVThis paper showed that the word segmentation accuracy of a state-of-the-art Adaptor Grammar model is significantly improved by extending it so that it explicitly models some properties of function words
p1602
aVThe models of u'\u005cu201c' function words u'\u005cu201d' we investigated here only capture two of the 7 linguistic properties of function words identified in section 1 (i.e.,, that function words tend to be monosyllabic, and that they tend to appear phrase-peripherally), so it would be interesting to develop and explore models that capture other linguistic properties of function words
p1603
aVIn an Adaptor Grammar the frequency distribution of function words might be modelled by specifying the prior for the Pitman-Yor Process parameters associated with the function words u'\u005cu2019' adapted nonterminals so that it prefers to generate a small number of high-frequency items
p1604
aVIn order to do this it is imperative to develop better methods than the problematic u'\u005cu201c' Harmonic Mean u'\u005cu201d' estimator used here for calculating the evidence (i.e.,, the marginal probability of the data) that can handle the combination of discrete and continuous hidden structure that occur in computational linguistic models
p1605
aVAs well as substantially improving the accuracy of unsupervised word segmentation, this work is interesting because it suggests a connection between unsupervised word segmentation and the induction of syntactic structure
p1606
aVIt is reasonable to expect that hierarchical non-parametric Bayesian models such as Adaptor Grammars may be useful tools for exploring such a connection
p1607
asg88
(lp1608
sg90
(lp1609
sg92
(lp1610
VThis paper showed that the word segmentation accuracy of a state-of-the-art Adaptor Grammar model is significantly improved by extending it so that it explicitly models some properties of function words.
p1611
aVWe also showed how Bayesian model selection can be used to identify that function words appear on the left periphery of phrases in English, even though the input to the model only consists of an unsegmented sequence of phones.
p1612
aVOf course this work only scratches the surface in terms of investigating the role of function words in language acquisition.
p1613
aVIt would clearly be very interesting to examine the performance of these models on other corpora of child-directed English, as well as on corpora of child-directed speech in other languages.
p1614
aVOur evaluation focused on word-segmentation, but we could also evaluate the effect that modelling function words has on other aspects of the model, such as its ability to learn syllable structure.
p1615
aVThe models of function words we investigated here only capture two of the 7 linguistic properties of function words identified in section 1 (i.e.,, that function words tend to be monosyllabic, and that they tend to appear phrase-peripherally), so it would be interesting to develop and explore models that capture other linguistic properties of function words.
p1616
aVFor example, following the suggestion by that human learners use frequency cues to identify function words, it might be interesting to develop computational models that do the same thing.
p1617
aVIn an Adaptor Grammar the frequency distribution of function words might be modelled by specifying the prior for the Pitman-Yor Process parameters associated with the function words adapted nonterminals so that it prefers to generate a small number of high-frequency items.
p1618
aVIt should also be possible to develop models which capture the fact that function words tend not to be topic-specific and show how Adaptor Grammars can model the association between words and non-linguistic topics ; perhaps these models could be extended to capture some of the semantic properties of function words.
p1619
aVIt would also be interesting to further explore the extent to which Bayesian model selection is a useful approach to linguistic parameter setting .
p1620
aVIn order to do this it is imperative to develop better methods than the problematic Harmonic Mean estimator used here for calculating the evidence (i.e.,, the marginal probability of the data) that can handle the combination of discrete and continuous hidden structure that occur in computational linguistic models.
p1621
aVAs well as substantially improving the accuracy of unsupervised word segmentation, this work is interesting because it suggests a connection between unsupervised word segmentation and the induction of syntactic structure.
p1622
aVIt is reasonable to expect that hierarchical non-parametric Bayesian models such as Adaptor Grammars may be useful tools for exploring such a connection.
p1623
ag106
asg107
S'P14-1027'
p1624
sg109
(lp1625
VInspired by experimental psychological findings suggesting that function words play a special role in word learning, we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic function words at the beginnings and endings of collocations of (possibly multi-syllabic) words.
p1626
aVThis modification improves unsupervised word segmentation on the standard corpus of child-directed English by more than 4% token f-score compared to a model identical except that it does not special-case function words , setting a new state-of-the-art of 92.4% token f-score.
p1627
aVOur function word model assumes that function words appear at the left periphery, and while this is true of languages such as English, it is not true universally.
p1628
aVWe show that a learner can use Bayesian model selection to determine the location of function words in their language, even though the input to the model only consists of unsegmented sequences of phones.
p1629
aVThus our computational models support the hypothesis that function words play a special role in word learning.
p1630
aVSIL/I .
p1631
aVX.
p1632
aVt 1.
p1633
aVt n.
p1634
aV.
p1635
ag106
asba(icmyPackage
FText
p1636
(dp1637
g3
(lp1638
VBy exploiting tag embeddings and tensor-based transformation, MMTNN has the ability to model complicated interactions between tags and context characters
p1639
aVUnlike English and other western languages, Chinese do not delimit words by white-space
p1640
aVTherefore, word segmentation is a preliminary and important pre-process for Chinese language processing
p1641
aVThese systems are effective because researchers can incorporate a large body of handcrafted features into the models
p1642
aVHowever, the ability of these models is restricted by the design of features and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus
p1643
aV[ 6 ] to Chinese word segmentation and POS tagging and proposed a perceptron-style algorithm to speed up the training process with negligible loss in performance
p1644
aVWorkable as previous neural network models seem, a limitation of them to be pointed out is that the tag-tag interaction, tag-character interaction and character-character interaction are not well modeled
p1645
aVIn conventional feature-based linear (log-linear) models, these interactions are explicitly modeled as features
p1646
aVTake phrase u'\u005cu201c' æç¯®ç(play basketball) u'\u005cu201d' as an example, assuming we are labeling character C 0 = u'\u005cu201c' ç¯® u'\u005cu201d' , possible features could be
p1647
aVTo capture more interactions, researchers have designed a large number of features based on linguistic intuition and statistical information
p1648
aVIn order to address this problem, we propose a new model called Max-Margin Tensor Neural Network ( MMTNN ) that explicitly models the interactions between tags and context characters by exploiting tag embeddings and tensor-based transformation
p1649
aV[ 15 ] , we wonder how well our model can perform with minimal feature engineering
p1650
aVTherefore, we integrate additional simple character bigram features into our model and the result shows that our model can achieve a competitive performance that other systems hardly achieve unless they use more complex task-specific features
p1651
aVWe propose a new tensor factorization approach that models each tensor slice as the product of two low-rank matrices
p1652
aVNot only does this approach improve the efficiency of our model but also it avoids the risk of overfitting
p1653
aVThe conclusions are given in Section 6
p1654
aVThe idea of distributed representation for symbolic data is one of the most important reasons why the neural network works
p1655
aVEach character c u'\u005cu2208' D is represented as a real-valued vector (character embedding) E u'\u005cu2062' m u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' d u'\u005cu2062' ( c ) u'\u005cu2208' u'\u005cu211d' d where d is the dimensionality of the vector space
p1656
aVThe Lookup Table layer can be seen as a simple projection layer where the character embedding for each context character is achieved by table lookup operation according to their indices
p1657
aVWe set w = 5 in all experiments
p1658
aVAs shown in Figure 1, at position c i , 1 u'\u005cu2264' i u'\u005cu2264' n , the context characters are fed into the Lookup Table layer
p1659
aVDespite sharing commonalities mentioned above, previous work models the segmentation task differently and therefore uses different training and inference procedure
p1660
aV[ 15 ] modeled Chinese word segmentation as a series of classification task at each position of the sentence in which the tag score is transformed into probability using softmax function
p1661
aVThe model is then trained in MLE-style which maximizes the log-likelihood of the tagged data
p1662
aVTo model the tag dependency, previous neural network models [ 6 , 35 ] introduce a transition score A i u'\u005cu2062' j for jumping from tag i u'\u005cu2208' T to tag j u'\u005cu2208' T
p1663
aVwhere f u'\u005cu0398' ( t i c [ i - 2 i + 2 ] ) indicates the score output for tag t i at the i -th character by the network with parameters u'\u005cu0398' = ( M , A , W 1 , b 1 , W 2 , b 2
p1664
aV[ 15 ] , their model is a global one where the training and inference is performed at sentence-level
p1665
aVWorkable as these methods seem, one of the limitations of them is that the tag-tag interaction and the neural network are modeled seperately
p1666
aVThe simple tag-tag transition neglects the impact of context characters and thus limits the ability to capture flexible interactions between tags and context characters
p1667
aVIn this way, the tag representation can be directly incorporated in the neural network so that the tag-tag interaction and tag-character interaction can be explicitly modeled in deeper layers (See Section 3.2
p1668
aVMoreover, the transition score in equation (4) is not necessary in our model, because, by incorporating tag embedding into the neural network, the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model
p1669
aV[ 35 ] , our model is also trained at sentence-level and carries out inference globally
p1670
aVIt can be represented as a multi-dimensional array of numerical values
p1671
aVAn advantage of the tensor is that it can explicitly model multiple interactions in data
p1672
aVAs a result, tensor-based model have been widely used in a variety of tasks [ 20 , 12 , 23 ]
p1673
aVIn linear models, these kinds of interactions are usually modeled as features
p1674
aVIn conventional neural network models, however, the input embeddings only implicitly interact through the non-linear function which can hardly model the complexity of the interactions
p1675
aVSince vector a is the concatenation of character embeddings and the tag embedding, equation (7) can be written in the following form
p1676
aVwhere E j [ p ] is the j -th element of the p -th embedding in Lookup Table layer and V ( p , q , j , k ) [ i ] is the corresponding coefficient for E j [ p ] and E k [ q ] in V [ i ]
p1677
aVAs we can see, in each tensor slice i , the embeddings are explicitly related in a bilinear form which captures the interactions between characters and tags
p1678
aVThe multiplicative operations between tag embeddings and character embeddings can somehow be seen as u'\u005cu201c' feature combination u'\u005cu201d' , which are hand-designed in feature-based models
p1679
aVIntuitively, we can interpret each slice of the tensor as capturing a specific type of tag-character interaction and character-character interaction
p1680
aVCombining the tensor product with linear transformation, the tensor-based transformation in Layer 2 is defined as
p1681
aVTo remedy this, we propose a tensor factorization approach that factorizes each tensor slice as the product of two low-rank matrices
p1682
aVFormally, each tensor slice V [ i ] u'\u005cu2208' u'\u005cu211d' H 1 × H 1 is factorized into two low rank matrix P [ i ] u'\u005cu2208' u'\u005cu211d' H 1 × r and Q [ i ] u'\u005cu2208' u'\u005cu211d' r × H 1
p1683
aVSubstituting equation (9) into equation (8), we get the factorized tensor function
p1684
aVIntuitively, the Max-Margin criterion provides an alternative to probabilistic, likelihood-based estimation methods by concentrating directly on the robustness of the decision boundary of a model [ 27 ]
p1685
aVWe first define a structured margin loss u'\u005cu25b3' ( y i , y ^ ) for predicting a tag sequence y ^ for a given correct tag sequence y i
p1686
aVThe object of Max-Margin training is that the highest scoring tag sequence is the correct one y * = y i and its score will be larger up to a margin to other possible tag sequences y ^ u'\u005cu2208' Y u'\u005cu2062' ( x i )
p1687
aVThis leads to the regularized objective function for m training examples
p1688
aVBy minimizing this object, the score of the correct tag sequence y i is increased and score of the highest scoring incorrect tag sequence y ^ is decreased
p1689
aVThe objective function is not differentiable due to the hinge loss
p1690
aV[ 22 ] , we use the diagonal variant of AdaGrad [ 8 ] with minibatchs to minimize the objective
p1691
aVFor model selection, we use the first 90 u'\u005cu2062' % sentences in the training data for training and the rest 10 u'\u005cu2062' % sentences as development data
p1692
aVThe minibatch size is set to 20
p1693
aVWe hypothesize that larger factor size results in too many parameters to train and hence perform worse
p1694
aVThe final hyperparameters of our model are set as in Table 2
p1695
aVWe first perform a close test 3 3 No other material or knowledge except the training data is allowed on the PKU dataset to show the effect of different model configurations
p1696
aVAs we can see, by using Tag embedding, the F-score is improved by +0.6% and OOV recall is improved by +1.0%, which shows that tag embeddings succeed in modeling the tag-tag interaction and tag-character interaction
p1697
aVThe F-score is improved by +0.6% while OOV recall is improved by +3.2%, which denotes that tensor-based transformation captures more interactional information than simple non-linear transformation
p1698
aVAnother important result in Table 3 is that our neural network models perform much better than CRF-based model when only unigram features are used
p1699
aVIn each column, we list the top 5 characters that are nearest (measured by Euclidean distance) to the corresponding character in the first row according to their embeddings
p1700
aVAs we can see, characters in the first column are all Chinese number characters and characters in the second column and the third column are all Chinese family names and Chinese punctuations respectively
p1701
aVTherefore, compared with discrete feature representations, distributed representation can capture the syntactic and semantic similarity between characters
p1702
aVAs a result, the model can still perform well even if some words do not appear in the training cases
p1703
aV[ 35 ] did not report the results on the these datasets, we re-implemented their model and tested it on the test data
p1704
aVPrevious work found that the performance can be improved by pre-training the character embeddings on large unlabeled data and using the obtained embeddings to initialize the character lookup table instead of random initialization [ 15 , 35 ]
p1705
aV[ 1 ] which learns the embeddings based on neural language model
p1706
aV[ 16 ] proposed a faster skip-gram model word2vec 5 5 https://code.google.com/p/word2vec/â which tries to maximize classification of a word based on another word in the same sentence
p1707
aVIn this paper, we use word2vec because preliminary experiments did not show differences between performances of these models but word2vec is much faster to train
p1708
aVAs shown in Table 5 (last three rows), both the F-score and OOV recall of our model boost by using pre-training
p1709
aV[ 15 ] , the bigram embeddings are pre-trained on unlabeled data with character embeddings, which significantly improves the model performance
p1710
aVFurther improvement could be obtained if the bigram embeddings are also pre-trained
p1711
aVSince feature engineering is not the main focus of this paper, we did not experiment with more features
p1712
aVOur study is consistent with this line of research, however, our model explicitly models the interactions between tags and context characters and accordingly captures more semantic information
p1713
aVTensor-based transformation was also used in other neural network models for its ability to capture multiple interactions in data
p1714
aVHowever, given the small size of their tensor matrix, they do not have the problem of high time cost and overfitting problem as we faced in modeling a sequence labeling task like Chinese word segmentation
p1715
aVBy introducing tensor factorization into the neural network model for sequence labeling tasks, the model training and inference are speeded up and overfitting is prevented
p1716
asg88
(lp1717
sg90
(lp1718
sg92
(lp1719
VIn this paper, we propose a new model called Max-Margin Tensor Neural Network that explicitly models the interactions between tags and context characters.
p1720
aVMoreover, we propose a tensor factorization approach that effectively improves the model efficiency and avoids the risk of overfitting.
p1721
aVExperiments on the benchmark datasets show that our model achieve better results than previous neural network models and that our model can achieve a competitive result with minimal feature engineering.
p1722
aVIn the future, we plan to further extend our model and apply it to other structure prediction problems.
p1723
ag106
asg107
S'P14-1028'
p1724
sg109
(lp1725
VRecently, neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering.
p1726
aVIn this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network ( MMTNN.
p1727
aVBy exploiting tag embeddings and tensor-based transformation, MMTNN has the ability to model complicated interactions between tags and context characters.
p1728
aVFurthermore, a new tensor factorization approach is proposed to speed up the model and avoid overfitting.
p1729
aVExperiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering.
p1730
aVDespite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks.
p1731
aVUTF8gbsn.
p1732
ag106
asba(icmyPackage
FText
p1733
(dp1734
g3
(lp1735
VWe will refer to a negation word as the negator and the text span within the scope of the negator as the argument
p1736
aVCommonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument (and not on the negator or the argument itself
p1737
aVIn this paper, we refer to a negation word as the negator (e.g.,, isn u'\u005cu2019' t ), a text span being modified by and composed with a negator as the argument (e.g.,, very good ), and entire phrase (e.g.,, isn u'\u005cu2019' t very good ) as the negated phrase
p1738
aVThis corpus provides us with the data to further understand the quantitative behavior of negators, as the effect of negators can now be studied with arguments of rich syntactic and semantic variety
p1739
aVWe regard the negators u'\u005cu2019' behavior as an underlying function embedded in annotated data; we aim to model this function from different aspects
p1740
aVBy examining sentiment compositions of negators and arguments, we model the quantitative behavior of negators in changing sentiment
p1741
aVEarly work on automatic sentiment analysis includes the widely cited work of [] , among others
p1742
aVSince then, there has been an explosion of research addressing various aspects of the problem, including detecting subjectivity, rating and classifying sentiment, labeling sentiment-related semantic roles (e.g.,, target of sentiment), and visualizing sentiment (see surveys by and
p1743
aVFor example, not alive has the same meaning as dead , however, not tall does not always mean short
p1744
aVFor example, in the work of [] , a feature not_good will be created if the word good is encountered within a predefined range after a negator
p1745
aVThe more recent work of [] proposed models based on recursive neural networks that do not rely on any heuristic rules
p1746
aVSuch models work in a bottom-up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition of the sentiment expressed by its constituting parts
p1747
aVIn this paper, we call a model based on such assumptions a non-lexicalized model
p1748
aVThat is, the model parameters are only based on the sentiment value of the arguments
p1749
aVwhere s i g n is the standard sign function which determines if the constant C should be added to or deducted from s u'\u005cu2062' ( w n the constant is added to a negative s u'\u005cu2062' ( w u'\u005cu2192' ) but deducted from a positive one
p1750
aVPolarity-based shifting As will be shown in our experiments, negators can have different shifting power when modifying a positive or a negative phrase
p1751
aVThus, we explore the use of two different constants for these two situations, i.e.,, f u'\u005cu2062' ( s u'\u005cu2062' ( w u'\u005cu2192' ) ) = s u'\u005cu2062' ( w u'\u005cu2192' ) - s u'\u005cu2062' i u'\u005cu2062' g u'\u005cu2062' n u'\u005cu2062' ( s u'\u005cu2062' ( w u'\u005cu2192' ) ) * C u'\u005cu2062' ( s u'\u005cu2062' i u'\u005cu2062' g u'\u005cu2062' n u'\u005cu2062' ( s u'\u005cu2062' ( w u'\u005cu2192' ) )
p1752
aVThis shifting model is based on negators and the polarity of the text they modify constants can be different for each negator-polarity pair
p1753
aVA recursive neural tensor network (RNTN) is a specific form of feed-forward neural network based on syntactic (phrasal-structure) parse tree to conduct compositional sentiment analysis
p1754
aVTo this end, we make use of the sentiment class information of p 1 , noted as p 1 s u'\u005cu2062' e u'\u005cu2062' n
p1755
aVAs a result, the vector of p 2 is calculated as follows
p1756
aVAs shown in Equation 6 , for the node vector p 1 u'\u005cu2208' u'\u005cu211d' d × 1 , we employ a matrix, namely W s u'\u005cu2062' e u'\u005cu2062' n u'\u005cu2208' u'\u005cu211d' d × ( d + m ) and a tensor, V s u'\u005cu2062' e u'\u005cu2062' n u'\u005cu2208' u'\u005cu211d' ( d + m ) × ( d + m ) × d , aiming at explicitly capturing the interplays between the sentiment class of p 1 , denoted as p 1 s u'\u005cu2062' e u'\u005cu2062' n ( u'\u005cu2208' u'\u005cu211d' m × 1 ), and the negator a
p1757
aVFollowing the idea of , we regard the sentiment of p 1 as a prior sentiment as it has not been affected by the specific context (negators), so we denote our method as prior sentiment-enriched tensor network (PSTN
p1758
aVHowever, in our current study, we focus on exploring the behavior of negators
p1759
aVAs we have discussed above, we will use the human annotated sentiment for the arguments, same as in the models discussed in Section 3
p1760
aVWith the new matrix and tensor, we then have u'\u005cu0398' = ( V , V s u'\u005cu2062' e u'\u005cu2062' n , W , W s u'\u005cu2062' e u'\u005cu2062' n , W l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l , L ) as the PSTN model u'\u005cu2019' s parameters
p1761
aVHere, L denotes the vector representations of the word dictionary
p1762
aVTo minimize E u'\u005cu2062' ( u'\u005cu0398' ) , the gradient of the objective function with respect to each of the parameters in u'\u005cu0398' is calculated efficiently via backpropagation through structure, as proposed by
p1763
aVSpecifically, we first compute the prediction errors in all tree nodes bottom-up
p1764
aVDuring the derivative computation, the two errors will be summed up as the complement incoming error for the node
p1765
aVWe denote the complete incoming error and the softmax error vector for node i as u'\u005cu0394' i , c u'\u005cu2062' o u'\u005cu2062' m u'\u005cu2208' u'\u005cu211d' d × 1 and u'\u005cu0394' i , s u'\u005cu2208' u'\u005cu211d' d × 1 , respectively
p1766
aVNow, let u'\u005cu2019' s form the equations for computing the error for the two children of the p 2 node
p1767
aVWe denote the error passing down as u'\u005cu0394' p 2 , d u'\u005cu2062' o u'\u005cu2062' w u'\u005cu2062' n , where the left child and the right child of p 2 take the 1 s u'\u005cu2062' t and 2 n u'\u005cu2062' d half of the error u'\u005cu0394' p 2 , d u'\u005cu2062' o u'\u005cu2062' w u'\u005cu2062' n , namely u'\u005cu0394' p 2 , d u'\u005cu2062' o u'\u005cu2062' w u'\u005cu2062' n [ 1 d ] and u'\u005cu0394' p 2 , d u'\u005cu2062' o u'\u005cu2062' w u'\u005cu2062' n [ d + 1
p1768
aVThe original RNTN and the PSTN predict 5-class sentiment for each negated phrase; we map the output to real-valued scores based on the scale that used to map real-valued sentiment scores to sentiment categories
p1769
aVFor example, if y i = [ 0.5 u'\u005cu2004' 0.5 u'\u005cu2004' 0 u'\u005cu2004' 0 u'\u005cu2004' 0 ] , meaning this phrase has a 0.5 probability to be in the first category (strong negative) and 0.5 for the second category (weak negative), the resulting p i r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' l will be 0.2 (0.5*0.1+0.5*0.3
p1770
aVData As described earlier, the Stanford Sentiment Treebank [] has manually annotated, real-valued sentiment values for all phrases in parse trees
p1771
aVThe table shows that the basic reversing and shifting heuristics do capture negators u'\u005cu2019' behavior to some degree, as their MAE scores are higher than that of the baseline
p1772
aVThis could suggest that additional external knowledge, e.g.,, that from human-built resources or automatically learned from other data (e.g.,, as in [] ), including sentiment that cannot be inferred from its constituent expressions, might be incorporated to benefit the current neural-network-based models as prior knowledge
p1773
aVNote that the two neural network based models incorporate the syntax and semantics by representing each node with a vector
p1774
aVFor example, if a phrase very good modified by a negator not appears in the training and test data, the system can simply memorize the sentiment score of not very good in training and use this score at testing
p1775
aVAlternatively, if the modeling has to be done in groups, one should consider clustering valence shifters by their shifting abilities in training or external data
p1776
aVThe depth here is defined as the longest distance between the root of a negator-phrase pair u'\u005cu27e8' w n , w u'\u005cu2192' u'\u005cu27e9' and their descendant leafs
p1777
aVNegators appearing at deeper levels of the tree tend to have more complicated syntax and semantics
p1778
aVThe errors made by model 6 is bumpy, as the model considers no semantics and hence its errors are not dependent on the depths
p1779
asg88
(lp1780
sg90
(lp1781
sg92
(lp1782
VNegation plays a fundamental role in modifying sentiment.
p1783
aVIn the process of semantic composition, the impact of negators is complicated by the syntax and semantics of the text spans they modify.
p1784
aVThis paper provides a comprehensive and quantitative study of the behavior of negators through a unified view of fitting human annotation.
p1785
aVWe first measure the modeling capabilities of two influential heuristics on a sentiment treebank and find that they capture some effect of negation; however, extending these non-lexicalized models to be dependent on the negators improves the performance statistically significantly.
p1786
aVThe detailed analysis reveals the differences in the behavior among negators, and we argue that they should always be modeled separately.
p1787
aVWe further make the models to be dependent on the text being modified by negators, through adaptation of a state-of-the-art recursive neural network to incorporate the syntax and semantics of the arguments; we discover this further reduces fitting errors.
p1788
ag106
asg107
S'P14-1029'
p1789
sg109
(lp1790
VNegation words, such as no and not , play a fundamental role in modifying sentiment of textual expressions.
p1791
aVWe will refer to a negation word as the negator and the text span within the scope of the negator as the argument.
p1792
aVCommonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument (and not on the negator or the argument itself.
p1793
aVWe use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment.
p1794
aVWe then modify these heuristics to be dependent on the negators and show that this improves prediction.
p1795
aVNext, we evaluate a recently proposed composition model [] that relies on both the negator and the argument.
p1796
aVThis model learns the syntax and semantics of the negator s argument with a recursive neural network.
p1797
aVWe show that this approach performs better than those mentioned above.
p1798
aVIn addition, we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors.
p1799
ag106
asba(icmyPackage
FText
p1800
(dp1801
g3
(lp1802
VThis paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences
p1803
aVThe ability to extract sentiment from text is crucial for many opinion-mining applications such as opinion summarization, opinion question answering and opinion retrieval
p1804
aVAccordingly, extracting sentiment at the fine-grained level (e.g., at the sentence- or phrase-level) has received increasing attention recently due to its challenging nature and its importance in supporting these opinion analysis tasks [ 18 ]
p1805
aVStill, their methods can encounter difficulty when the sentence on its own does not contain strong enough sentiment signals (due to the lack of statistical evidence or the requirement for background knowledge
p1806
aVExisting feature-based classifiers may be effective in identifying the positive sentiment of the first sentence due to the use of the word revelation , but they could be less effective in the last two sentences due to the lack of explicit sentiment signals
p1807
aVHowever, if we examine these sentences within the discourse context, we can see that the second sentence expresses sentiment towards the same aspect u'\u005cu2013' the music u'\u005cu2013' as the first sentence; the third sentence expands the second sentence with the discourse connective In fact
p1808
aVHowever, the discourse relations were obtained from fine-grained annotations and implemented as hard constraints on polarity
p1809
aVObtaining sentiment labels at the fine-grained level is costly
p1810
aVIn this paper, we propose a sentence-level sentiment classification method that can (1) incorporate rich discourse information at both local and global levels; (2) encode discourse knowledge as soft constraints during learning; (3) make use of unlabeled data to enhance learning
p1811
aVSpecifically, we use the Conditional Random Field (CRF) model as the learner for sentence-level sentiment classification, and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization (PR) [ 7 ]
p1812
aVAs a framework for structured learning with constraints, PR has been successfully applied to many structural NLP tasks [ 6 , 7 , 5 ]
p1813
aVWe also show that discourse knowledge is highly useful for improving sentence-level sentiment classification
p1814
aVExisting machine learning approaches for the task can be classified based on the use of two ideas
p1815
aVThe first idea is to exploit sentiment signals at the sentence level by learning the relevance of sentiment and words while taking into account the context in which they occur
p1816
aV2010 ) uses tree-CRF to model word interactions based on dependency tree structures; Choi and Cardie ( 2008 ) applies compositional inference rules to handle polarity reversal; Socher et al
p1817
aV2013 ) explored the use of explanatory discourse relations as soft constraints in a Markov Logic Network framework for extracting subjective text segments
p1818
aVLeveraging both ideas, our approach exploits sentiment signals from both intra-sentential and inter-sentential context
p1819
aVIt has the advantages of utilizing rich discourse knowledge at different levels of context and encoding it as soft constraints during learning
p1820
aVOur approach is also semi-supervised
p1821
aVWe also show that constraints derived from the discourse context can be highly useful for disambiguating sentence-level sentiment
p1822
aVThen we introduce the context-aware constraints derived based on intuitive discourse and lexical knowledge
p1823
aVIn this work, we apply PR in the context of CRFs for sentence-level sentiment classification
p1824
aVDenote u'\u005cud835' u'\u005cudc31' as a sequence of sentences within a document and u'\u005cud835' u'\u005cudc32' as a vector of sentiment labels associated with u'\u005cud835' u'\u005cudc31'
p1825
aVPR makes the assumption that the labeled data we have is not enough for learning good model parameters, but we have a set of constraints on the posterior distribution of the labels
p1826
aVWe focus on the equality constraints since we found them to express the sentiment-relevant constraints well
p1827
aVThe PR objective can be written as the original model objective penalized with a regularization term, which minimizes the KL-divergence between the desired model posteriors and the learned model posteriors with an L2 penalty 2 2 Other convex functions can be used for the penalty
p1828
aVWe use L2 norm because it works well in practice u'\u005cu0392' is a regularization constant for the constraint violations
p1829
aVSolving the minimization problem is equivalent to solving its dual since the objective is convex
p1830
aVWe develop a rich set of context-aware posterior constraints for sentence-level sentiment analysis by exploiting lexical and discourse knowledge
p1831
aVSpecifically, we construct the lexical constraints by extracting sentiment-bearing patterns within sentences and construct the discourse-level constraints by extracting discourse relations that indicate sentiment coherence or sentiment changes both within and across sentences
p1832
aVEach constraint can be formulated as equality between the expectation of a constraint function value and a desired value set by prior knowledge
p1833
aVThe equality is not strictly enforced (due to the regularization in the PR objective 2
p1834
aVTherefore all the constraints are applied as soft constraints
p1835
aVLexical Patterns The existence of a polarity-carrying word alone may not correctly indicate the polarity of the sentence, as the polarity can be reversed by other polarity-reversing words
p1836
aVWe extract lexical patterns that consist of polar words and negators 3 3 The polar words are identified using the MPQA lexicon and the negators are identified using a handful of seed words extended by the General Inquirer dictionary and WordNet as described in [ 2 ] and apply the heuristics based on compositional semantics [ 2 ] to assign a sentiment value to each pattern
p1837
aVWe encode the extracted lexical patterns along with their sentiment values as feature-label constraints
p1838
aVThe constraint function can be written as
p1839
aVNote that sentences with neutral sentiment can also contain such lexical patterns
p1840
aVTherefore we allow the lexical patterns to be assigned a neutral sentiment with a prior probability r 0 (we compute this value as the empirical probability of neutral sentiment in the training documents
p1841
aVUsing the polarity indicated by lexical patterns to constrain the sentiment of sentences is quite aggressive
p1842
aVTherefore we only consider lexical patterns that are strongly discriminative (many opinion words in the lexicon only indicate sentiment with weak strength
p1843
aVLexical patterns can be limited in capturing contextual information since they only look at interactions between words within an expression
p1844
aVTo capture context at the clause or sentence level, we consider discourse connectives, which are cue phrases or words that indicate discourse relations between adjacent sentences or clauses
p1845
aVTo identify discourse connectives, we apply a discourse tagger trained on the Penn Discourse Treebank [ 20 ] 4 4 http://www.cis.upenn.edu/~epitler/discourse.html to our data
p1846
aVWe consider a discourse connective to be intra-sentential if it has the Comparison sense and connects two polar clauses with opposite polarities (determined by the lexical patterns
p1847
aVIntuitively, discourse connectives with the senses of Expansion (e.g., also, for example, furthermore) and Contingency (e.g., as a result, hence, because) are likely to indicate sentiment coherence; discourse connectives with the sense of Comparison (e.g., but, however, nevertheless) are likely to indicate sentiment changes
p1848
aVIn general, discourse connectives can also be used to connect non-polar (neutral) sentences
p1849
aVThus it is hard to directly constrain the posterior expectation for each type of sentiment transitions using inter-sentential discourse connectives
p1850
aVInstead, we impose constraints on the model posteriors by reducing constraint violations
p1851
aVThe desired value for the constraint expectation is set to 0 so that the model is encouraged to have less constraint violations
p1852
aVWe consider a set of polar sentences to be linked by the opinion coreference relation if they contain coreferring opinion-related entities
p1853
aVFor example, the following sentences express opinions towards u'\u005cu201c' the speaker phone u'\u005cu201d' , u'\u005cu201c' The speaker phone u'\u005cu201d' and u'\u005cu201c' it u'\u005cu201d' respectively
p1854
aVAs these opinion targets are coreferential (referring to the same entity u'\u005cu201c' the speaker phone u'\u005cu201d' ), they are linked by the opinion coreference relation 5 5 In general, the opinion-related entities include both the opinion targets and the opinion holders
p1855
aVIn this work, we only consider the targets since we experiment with single-author product reviews
p1856
aVThe opinion holders can be included in a similar way as the opinion targets
p1857
aVMy favorite features are the speaker phone and the radio
p1858
aV1) we encode the coreference relations as soft constraints during learning instead of applying them as hard constraints during inference time; (2) our constraints can apply to both polar and non-polar sentences; (3) our identification of coreference relations is automatic without any fine-grained annotations for opinion targets
p1859
aVListing Patterns Another type of coherence relations we observe in online reviews is listing, where a reviewer expresses his/her opinions by listing a series of statements followed by a sequence of numbers
p1860
aVIn this work, we also take into account this information and encode it as posterior constraints
p1861
aVNote that these constraints are not necessary for our model and can be applied when the document-level sentiment labels are naturally available
p1862
aVBased on an analysis of the Amazon review data, we observe that sentence-level sentiment usually doesn u'\u005cu2019' t conflict with the document-level sentiment in terms of polarity
p1863
aVWe can derive q by solving the dual problem in 3
p1864
aVMost of our constraints can be factorized in the same way as factorizing the model features in the first-order CRF model, and we can compute the expectations under q very efficiently using the forward-backward algorithm
p1865
aVSince the possible label assignments only differ at position i , we can make the computation efficient by maintaining the structure of the coreference clusters and precomputing the constraint function for different types of violations
p1866
aVFor documents where the higher-order constraints apply, we use the same Gibbs sampler as described above to infer the most likely label assignment, otherwise, we use the Viterbi algorithm
p1867
aVIn the supervised setting, we treated the test data as unlabeled data and performed transductive learning
p1868
aVIn the semi-supervised setting, our unlabeled data consists of both the available unlabeled data and the test data
p1869
aVWe use accuracy as the performance measure
p1870
aVIn our tables, boldface numbers are statistically significant by paired t-test for p 0.05 against the best baseline developed in this paper 7 7 Significance test was not conducted over the previous methods as we do not have their results for each fold
p1871
aVIn addition, we include the discourse connectives as local or transition features and the document-level sentiment labels as features (only available in the MD dataset
p1872
aVWe set the CRF regularization parameter u'\u005cu03a3' = 1 and set the posterior regularization parameter u'\u005cu0392' and u'\u005cu0393' (a trade-off parameter we introduce to balance the supervised objective and the posterior regularizer in 2 ) by using grid search 8 8 We conducted 10-fold cross-validation on each training fold with the parameter space u'\u005cu0392'
p1873
aVWe can incorporate the proposed constraints (constraints derived from lexical patterns and discourse connectives) as hard constraints into CRF during inference by manually setting u'\u005cu039b' in equation 4 to a large value, 9 9 We set u'\u005cu039b' to 1000 for the lexical constraints and -1000 to the discourse connective constraints in the experiments
p1874
aVWhen u'\u005cu039b' is large enough, it is equivalent to adding hard constraints to the viterbi inference
p1875
aVTo better understand the different effects of lexical and discourse constraints, we report results for applying only the lexical constraints ( CRF-inf l u'\u005cu2062' e u'\u005cu2062' x ) as well as results for applying only the discourse constraints ( CRF-inf d u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' c
p1876
aVThe poor performance of CRF-inf l u'\u005cu2062' e u'\u005cu2062' x indicates that directly applying lexical constraints as hard constraints during inference could only hurt the performance
p1877
aVIn contrast, both PR l u'\u005cu2062' e u'\u005cu2062' x and PR significantly outperform CRF , which implies that incorporating lexical and discourse constraints as posterior constraints is much more effective
p1878
aVBy introducing the u'\u005cu201c' neutral u'\u005cu201d' category, the sentiment classification problem becomes harder
p1879
aVThe rule-based baseline VoteFlip gave the weakest performance because it has no prediction power on sentences with no opinion words
p1880
aVDocOracle performs much better than VoteFlip and performs especially well on the Music domain
p1881
aVThis indicates that the document-level sentiment is a very strong indicator of the sentence-level sentiment label
p1882
aVFor the CRF baseline and its invariants, we observe a similar performance trend as in the two-way classification task there is nearly no performance improvement from applying the lexical and discourse-connective-based constraints during CRF inference
p1883
aVThis confirms that encoding lexical and discourse knowledge as posterior constraints allows the feature-based model to gain additional learning power for sentence-level sentiment prediction
p1884
aVIn particular, incorporating discourse constraints leads to consistent improvements to our model
p1885
aVThis demonstrates that our modeling of discourse information is effective and that taking into account the discourse context is important for improving sentence-level sentiment analysis
p1886
aVThis is because it over-predicts the polar sentences in the polar documents, and predicts no polar sentences in the neutral documents
p1887
aVHowever, the improvement on the neutral category is modest
p1888
aVA plausible explanation is that most of our constraints focus on discriminating polar sentences
p1889
aVIn contrast, the PR model is able to associate stronger sentiment signals to these features by leveraging unlabeled data for indirect supervision
p1890
aVHowever, hard-constraint baselines can hardly improve the performance in general because the contributions of different constraints are not learned and their combination may not lead to better predictions
p1891
aVThe lexical constraints alone are often not sufficient since their coverage is limited by the sentiment lexicon and they can only constrain sentiment locally
p1892
aVOne reason is that they do not constrain the neutral sentiment
p1893
aVAs a result they could not help disambiguate neutral sentiment from polar sentiment, such as the third example in Table 5
p1894
aVIn the MD dataset, a neutral label may be given because the sentence contains mixed sentiment or no sentiment or it is off-topic
p1895
aVIn this paper, we propose a context-aware approach for learning sentence-level sentiment
p1896
asg88
(lp1897
sg90
(lp1898
sg92
(lp1899
VIn this paper, we propose a context-aware approach for learning sentence-level sentiment.
p1900
aVOur approach incorporates intuitive lexical and discourse knowledge as expressive constraints while training a conditional random field model via posterior regularization.
p1901
aVWe explore a rich set of context-aware constraints at both intra- and inter-sentential levels, and demonstrate their effectiveness in the analysis of sentence-level sentiment.
p1902
aVWhile we focus on the sentence-level task, our approach can be easily extended to handle sentiment analysis at finer levels of granularity.
p1903
aVOur experiments show that our model achieves better accuracy than existing supervised and semi-supervised models for the sentence-level sentiment classification task.
p1904
ag106
asg107
S'P14-1031'
p1905
sg109
(lp1906
VThis paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences.
p1907
aVMost existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture non-local contextual cues that are important for sentiment interpretation.
p1908
aVIn contrast, our approach allows structured modeling of sentiment while taking into account both local and global contextual information.
p1909
aVSpecifically, we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization.
p1910
aVThe context-aware constraints provide additional power to the CRF model and can guide semi-supervised learning when labeled data is limited.
p1911
aVExperiments on standard product review datasets show that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings.
p1912
ag106
asba(icmyPackage
FText
p1913
(dp1914
g3
(lp1915
VUsing parse accuracy in a simple reranking strategy for self-monitoring, we find that with a state-of-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make
p1916
aVHowever, by using an SVM ranker to combine the realizer u'\u005cu2019' s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved
p1917
aVRajkumar White [ 28 , 36 ] have recently shown that some rather egregious surface realization errors u'\u005cu2014' in the sense that the reader would likely end up with the wrong interpretation u'\u005cu2014' can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model [ 35 ] , as reviewed in the next section
p1918
aVHowever, one is apt to wonder could one use a parser to check whether the intended interpretation is easy to recover, either as an alternative or to catch additional mistakes
p1919
aVDoing so would be tantamount to self-monitoring in Levelt u'\u005cu2019' s [ 21 ] model of language production
p1920
aVNeumann van Noord [ 22 ] pursued the idea of self-monitoring for generation in early work with reversible grammars
p1921
aVAs Neumann van Noord observed, a simple, brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form, then to parse each realization to see how many interpretations it has, keeping only those that have a single reading; they then went on to devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences, targeted to the ambiguous portion of the output
p1922
aVWe might question, however, whether it is really possible to avoid ambiguity entirely in the general case, since Abney [ 1 ] and others have argued that nearly every sentence is potentially ambiguous, though we (as human comprehenders) may not notice the ambiguities if they are unlikely
p1923
aVIn this paper, we investigate whether Neumann van Noord u'\u005cu2019' s brute-force strategy for avoiding ambiguities in surface realization can be updated to only avoid vicious ambiguities, extending (and revising) Van Deemter u'\u005cu2019' s general strategy to all kinds of structural ambiguity, not just the one investigated by Khan et al
p1924
aVTo do so u'\u005cu2014' in a nutshell u'\u005cu2014' we enumerate an n -best list of realizations and rerank them if necessary to avoid vicious ambiguities, as determined by one or more automatic parsers
p1925
aVA potential obstacle, of course, is that automatic parsers may not be sufficiently representative of human readers, insofar as errors that a parser makes may not be problematic for human comprehension; moreover, parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length, even with carefully edited news text
p1926
aVConsequently, we examine two reranking strategies, one a simple baseline approach and the other using an SVM reranker [ 17 ]
p1927
aVWith this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with White Rajkumar u'\u005cu2019' s [ 28 , 36 ] baseline generative model, but not with their averaged perceptron model
p1928
aVIn inspecting the results of reranking with this strategy, we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities, common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers
p1929
aVTherefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n -best parsing results, and per-label precision and recall for each type of dependency, together with the realizer u'\u005cu2019' s normalized perceptron model score as a feature
p1930
aVIn Section 2 , we review the realization ranking models that serve as a starting point for the paper
p1931
aVIn Section 3 , we report on our experiments with the simple reranking strategy, including a discussion of the ways in which this method typically fails
p1932
aVTo select preferred outputs from the chart, we use White Rajkumar u'\u005cu2019' s [ 35 , 36 ] realization ranking model, recently augmented with a large-scale 5-gram model based on the Gigaword corpus
p1933
aVThe model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier Steedman u'\u005cu2019' s [ 14 ] generative model and Clark Curran u'\u005cu2019' s [ 7 ] normal-form model
p1934
aVUsing the averaged perceptron algorithm [ 8 ] , White Rajkumar [ 35 ] trained a structured prediction ranking model to combine these existing syntactic models with several n -gram language models
p1935
aVTo improve word ordering decisions, White Rajkumar [ 36 ] demonstrated that incorporating a feature into the ranker inspired by Gibson u'\u005cu2019' s [ 12 ] dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation
p1936
aVSupporting Gibson u'\u005cu2019' s theory, comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices; see Temperley ( 2007 ) and Gildea and Temperley ( 2010 ) for an overview
p1937
aVGenerally, inserting a complementizer makes the onset of a complement clause more predictable, and thus less information dense, thereby avoiding a potential spike in information density that is associated with comprehension difficulty
p1938
aVRajkumar White u'\u005cu2019' s experiments confirmed the efficacy of the features based on Jaeger u'\u005cu2019' s work, including information density u'\u005cu2013' based features, in a local classification model
p1939
aV2 2 Note that the features from the local classification model for that -complementizer choice have not yet been incorporated into OpenCCG u'\u005cu2019' s global realization ranking model, and thus do not inform the baseline realization choices in this work
p1940
aVFor example, in (1), the presence of that avoids a local ambiguity, helping the reader to understand that for the second month in a row modifies the reporting of the shortage; without that , it is very easy to mis-parse the sentence as having for the second month in a row modifying the saying event
p1941
aVHe said that / u'\u005cu2205' for the second month in a row, food processors reported a shortage of nonfat dry milk
p1942
aVSimple ranking with the Berkeley parser of the generative model u'\u005cu2019' s n -best realizations raised the BLEU score from 85.55 to 86.07, well below the averaged perceptron model u'\u005cu2019' s BLEU score of 87.93
p1943
aVHowever, as shown in Table 2 , none of the parsers yielded significant improvements on the top of the perceptron model
p1944
aVThe simple ranker ends up choosing (b) as the best realization because it has the most accurate parse compared to the reference sentence, given the mistake with (c
p1945
aVHere, (b) ends up getting chosen by the simple ranker as the realization with the most accurate parse given the failures in (c), where the additional technology, personnel training is mistakenly analyzed as one noun phrase, a reading unlikely to be considered by human readers
p1946
aVIn sum, although simple ranking helps to avoid vicious ambiguity in some cases, the overall results of simple ranking are no better than the perceptron model (according to BLEU, at least), as parse failures that are not reflective of human intepretive tendencies too often lead the ranker to choose dispreferred realizations
p1947
aVAs such, we turn now to a more nuanced model for combining the results of multiple parsers in a way that is less sensitive to such parsing mistakes, while also letting the perceptron model have a say in the final ranking
p1948
aVSince different parsers make different errors, we conjectured that dependencies in the intersection of the output of multiple parsers may be more reliable and thus may more reliably reflect human comprehension preferences
p1949
aVSimilarly, we conjectured that large differences in the realizer u'\u005cu2019' s perceptron model score may more reliably reflect human fluency preferences than small ones, and thus we combined this score with features for parser accuracy in an SVM ranker
p1950
aVAdditionally, given that parsers may more reliably recover some kinds of dependencies than others, we included features for each dependency type, so that the SVM ranker might learn how to weight them appropriately
p1951
aVFinally, since the differences among the n -best parses reflect the least certain parsing decisions, and thus ones that may require more common sense inference that is easy for humans but not machines, we conjectured that including features from the n -best parses may help to better match human performance
p1952
aVFinally, since the Berkeley parser yielded the best results on its own, we also tested models using all the feature classes but only using this parser by itself
p1953
aVWe then confirmed this result on the final test set, Section 23 of the CCGbank, as shown in Table 4 ( p 0.02 as well
p1954
aVIn order to gain a better understanding of the successes and failures of our SVM ranker, we present here a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, carried out by the second author (a native speaker
p1955
aVIn this analysis, we consider whether the reranked realization improves upon or detracts from realization quality u'\u005cu2014' in terms of adequacy, fluency, both or neither u'\u005cu2014' along with a linguistic categorization of the differences between the reranked realization and the original top-ranked realization according to the averaged perceptron model
p1956
aVIn this comparison, items where both fluency and adequacy were affected were counted as adequacy cases
p1957
aVThe table also shows that differences in the order of VP constituents usually led to a change in adequacy or fluency, as did ordering changes within NPs, with noun-noun compounds and named entities as the most frequent subcategories of NP-ordering changes
p1958
aVWith wsj_0041.18, the SVM ranker unfortunately prefers a realization where presumably seems to modify shows rather than of two politicians as in the original, which the averaged perceptron model prefers
p1959
aVFinally, wsj_0044.111 is an example where a subject-inversion makes no difference to adequacy or fluency
p1960
aVA limitation of the experiments reported in this paper is that OpenCCG u'\u005cu2019' s input semantic dependency graphs are not the same as the Stanford dependencies used with the Treebank parsers, and thus we have had to rely on the gold parses in the PTB to derive gold dependencies for measuring accuracy of parser dependency recovery
p1961
aVTo our knowledge, however, a comprehensive investigation of avoiding vicious structural ambiguities with broad coverage statistical parsers has not been previously explored
p1962
aVAs our SVM ranking model does not make use of CCG-specific features, we would expect our self-monitoring method to be equally applicable to realizers using other frameworks
p1963
aVAdditionally, via a targeted manual analysis, we showed that the SVM reranker frequently manages to avoid egregious errors involving u'\u005cu201c' vicious u'\u005cu201d' ambiguities, of the kind that would mislead human readers as to the intended meaning
p1964
aVAs noted in Reiter u'\u005cu2019' s [ 30 ] survey, many NLG systems use surface realizers as off-the-shelf components
p1965
aVIn future work, we also plan to investigate ways that self-monitoring might be implemented more efficiently as a combined process, rather than running independent parsers as a post-process following realization
p1966
aVWe thank Mark Johnson, Micha Elsner, the OSU Clippers Group and the anonymous reviewers for helpful comments and discussion
p1967
asg88
(lp1968
sg90
(lp1969
sg92
(lp1970
VIn this paper, we have shown that while using parse accuracy in a simple reranking strategy for self-monitoring fails to improve BLEU scores over a state-of-the-art averaged perceptron realization ranking model, it is possible to significantly increase BLEU scores using an SVM ranker that combines the realizer s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes that human readers would be unlikely to make.
p1971
aVAdditionally, via a targeted manual analysis, we showed that the SVM reranker frequently manages to avoid egregious errors involving vicious ambiguities, of the kind that would mislead human readers as to the intended meaning.
p1972
aVAs noted in Reiter s [ 30 ] survey, many NLG systems use surface realizers as off-the-shelf components.
p1973
aVIn this paper, we have focused on broad coverage surface realization using widely-available PTB data where there are many sentences of varying complexity with gold-standard annotations following the common assumption that experiments with broad coverage realization are (or eventually will be) relevant for NLG applications.
p1974
aVOf course, the kinds of ambiguity that can be problematic in news text may or may not be the same as the ones encountered in particular applications.
p1975
aVMoreover, for certain applications (e.g., ones with medical or legal implications), it may be better to err on the side of ambiguity avoidance, even at some expense to fluency, thereby requiring training data reflecting the desired trade-off to adapt the methods described here.
p1976
aVWe leave these application-centered issues for investigation in future work.
p1977
aVThe current approach is primarily suitable for offline use, for example in report generation where there are no real-time interaction demands.
p1978
aVIn future work, we also plan to investigate ways that self-monitoring might be implemented more efficiently as a combined process, rather than running independent parsers as a post-process following realization.
p1979
ag106
asg107
S'P14-1039'
p1980
sg109
(lp1981
VWe investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving vicious ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones.
p1982
aVUsing parse accuracy in a simple reranking strategy for self-monitoring, we find that with a state-of-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make.
p1983
aVHowever, by using an SVM ranker to combine the realizer s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved.
p1984
aVMoreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy.
p1985
ag106
asba(icmyPackage
FText
p1986
(dp1987
g3
(lp1988
VThis paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training
p1989
aVInstead of only using 1-best parse trees in previous work, our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data
p1990
aVAll above work leads to significant improvement on parsing accuracy
p1991
aVBoth work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical
p1992
aVIn this way, the auto-parsed unlabeled data becomes more reliable
p1993
aVTo solve above issues, this paper proposes a more general and effective framework for semi-supervised dependency parsing, referred to as ambiguity-aware ensemble training
p1994
aVDifferent from traditional self/co/tri-training which only use 1-best parse trees on unlabeled data, our approach adopts ambiguous labelings, represented by parse forest, as gold-standard for unlabeled sentences
p1995
aVThe upper tree take u'\u005cu201c' deer u'\u005cu201d' as the subject of u'\u005cu201c' riding u'\u005cu201d' , whereas the lower one indicates that u'\u005cu201c' he u'\u005cu201d' rides the bicycle
p1996
aVThe other difference is where the preposition phrase (PP) u'\u005cu201c' in the park u'\u005cu201d' should be attached, which is also known as the PP attachment problem, a notorious challenge for parsing
p1997
aVReserving such uncertainty has three potential advantages
p1998
aVFirst, noise in unlabeled data is largely alleviated, since parse forest encodes only a few highly possible parse trees with high oracle score
p1999
aVFinally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees
p2000
aVTo construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser [] , and a generative constituent parser []
p2001
aVFinally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings
p2002
aVExperiments show that the constituent parser is very helpful since it produces more divergent structures for our semi-supervised parser than discriminative dependency parsers
p2003
aVUsing the probabilistic parser, we benchmark and conduct systematic comparisons among ours and all previous bootstrapping methods, including self/co/tri-training
p2004
aVGiven an input sentence u'\u005cud835' u'\u005cudc31' = w 0 u'\u005cu2062' w 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' w n , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1 , denoted by u'\u005cud835' u'\u005cudc1d' = { ( h , m
p2005
aV0 u'\u005cu2264' h u'\u005cu2264' n , 0 m u'\u005cu2264' n } , where ( h , m ) indicates a directed arc from the head word w h to the modifier w m , and w 0 is an artificial node linking to the root of the sentence
p2006
aVThe graph-based method views the problem as finding an optimal tree from a fully-connected directed graph [] , while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree []
p2007
aVIn this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree u'\u005cud835' u'\u005cudc1d' given a sentence u'\u005cud835' u'\u005cudc31' , which is required to compute likelihood of both labeled and unlabeled data
p2008
aVWe adopt the second-order graph-based dependency parsing model of as our core parser, which incorporates features from the two kinds of subtrees in Fig
p2009
aVwhere the first term is the empirical counts and the second term is the model expectations
p2010
aVSince u'\u005cud835' u'\u005cudcb4' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' i ) contains exponentially many dependency trees, direct calculation of the second term is prohibitive
p2011
aVIntuitively, if several parsers disagree on an unlabeled sentence, it implies that the unlabeled sentence contains some difficult syntactic phenomena which are not sufficiently covered in manually labeled data
p2012
aVTherefore, exploiting such unlabeled data may introduce more discriminative syntactic knowledge, largely compensating labeled training data
p2013
aVTo address above issues, we propose ambiguity-aware ensemble training , which can be interpreted as a generalized tri-training framework
p2014
aVThe key idea is the use of ambiguous labelings for the purpose of aggregating multiple 1-best parse trees produced by several diverse parsers
p2015
aVHere, u'\u005cu201c' ambiguous labelings u'\u005cu201d' mean an unlabeled sentence may have multiple parse trees as gold-standard reference, represented by parse forest (see Figure 1
p2016
aV3 ) can be efficiently computed by running the inside-outside algorithm in the constrained search space u'\u005cud835' u'\u005cudcb1' i
p2017
aVMoreover, it is very convenient to parallel SGD since computations among examples in the same batch is mutually independent
p2018
aVTraining with the combined labeled and unlabeled data, the objective is to maximize the mixed likelihood
p2019
aVSince u'\u005cud835' u'\u005cudc9f' u'\u005cu2032' contains much more instances than u'\u005cud835' u'\u005cudc9f' (1.7M vs
p2020
aV16K for Chinese), it is likely that the unlabeled data may overwhelm the labeled data during SGD training
p2021
aVTherefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 3.2 , where u'\u005cud835' u'\u005cudc9f' i , k b is the subset of training data used in k t u'\u005cu2062' h update and b is the batch size; u'\u005cu0397' k is the update step, which is adjusted following the simulated annealing procedure []
p2022
aVOnce the feature weights u'\u005cud835' u'\u005cudc30' are learnt, we can parse the test data to find the optimal parse tree
p2023
aVFor the semi-supervised parsers trained with Algorithm 3.2 , we use N 1 = 20 K and M 1 = 50 K for English, and N 1 = 15 K and M 1 = 50 K for Chinese, based on a few preliminary experiments
p2024
aVFor Berkeley Parser, we use the model after 5 split-merge iterations to avoid over-fitting the training data according to the manual
p2025
aVUsing three supervised parsers, we have many options to construct parse forest on unlabeled data
p2026
aVWhen using the outputs of GParser itself ( u'\u005cu201c' Unlabeled u'\u005cu2190' G u'\u005cu201d' ), the experiment reproduces traditional self-training
p2027
aVThe results on both English and Chinese re-confirm that self-training may not work for dependency parsing , which is consistent with previous studies []
p2028
aVWe believe the reason is that being a generative model designed for constituent parsing, Berkeley Parser is more different from discriminative dependency parsers, and therefore can provide more divergent syntactic structures
p2029
aVThis kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective also show that the diversity of parsers is important for performance improvement when integrating different parsers in the supervised track
p2030
aVTherefore, we can conclude that co-training helps dependency parsing, especially when using a more divergent parser
p2031
aVThe last experiment in the second major row is known as tri-training , which only uses unlabeled sentences on which Berkeley Parser and ZPar produce identical outputs ( u'\u005cu201c' Unlabeled u'\u005cu2190' B=Z u'\u005cu201d'
p2032
aVCombining the outputs of Berkeley Parser and GParser ( u'\u005cu201c' Unlabeled u'\u005cu2190' B+G u'\u005cu201d' ), we get higher oracle score (96.37% on English and 89.72% on Chinese) and higher syntactic divergence (1.085 candidate heads per word on English, and 1.188 on Chinese) than u'\u005cu201c' Unlabeled u'\u005cu2190' Z+G u'\u005cu201d' , which verifies our earlier discussion that Berkeley Parser produces more different structures than ZPar
p2033
aVHowever, it leads to slightly worse accuracy than co-training with Berkeley Parser ( u'\u005cu201c' Unlabeled u'\u005cu2190' B u'\u005cu201d'
p2034
aVCombining the outputs of Berkeley Parser and ZPar ( u'\u005cu201c' Unlabeled u'\u005cu2190' B+Z u'\u005cu201d' ), we get the best performance on English, which is also significantly better than both co-training ( u'\u005cu201c' Unlabeled u'\u005cu2190' B u'\u005cu201d' ) and tri-training ( u'\u005cu201c' Unlabeled u'\u005cu2190' B=Z u'\u005cu201d' ) on both English and Chinese
p2035
aVDuring experimental trials, we find that u'\u005cu201c' Unlabeled u'\u005cu2190' B+(Z u'\u005cu2229' G) u'\u005cu201d' can further boost performance on Chinese
p2036
aVA possible explanation is that by using the intersection of the outputs of GParser and ZPar, the size of the parse forest is better controlled, which is helpful considering that ZPar performs worse on this data than both Berkeley Parser and GParser
p2037
aVAdding the output of GParser itself ( u'\u005cu201c' Unlabeled u'\u005cu2190' B+Z+G u'\u005cu201d' ) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than u'\u005cu201c' Unlabeled u'\u005cu2190' B+Z u'\u005cu201d'
p2038
aVWe suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting
p2039
aVAppropriately composing the forest parse, our approach outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and 0.92% (84.26-83.34) on Chinese
p2040
aVMoreover, our method may be combined with other semi-supervised approaches, since they are orthogonal in methodology and utilize unlabeled data from different perspectives
p2041
aVWe divide the unlabeled data into three sets according to the divergence of the 1-best outputs of Berkeley Parser and ZPar
p2042
aVOther sentences are split into two sets according to averaged number of heads per word in parse forests, denoted by u'\u005cu201c' low divergence u'\u005cu201d' and u'\u005cu201c' high divergence u'\u005cu201d' respectively
p2043
aVEspecially, the unlabeled data with highly divergent structures leads to slightly higher improvement
p2044
aVThey first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resource-poor target language by making use of source-language treebanks
p2045
aVStacked learning uses one parser u'\u005cu2019' s outputs as guide features for another parser, leading to improved performance []
p2046
aVRe-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree []
p2047
aVThis paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings
p2048
asg88
(lp2049
sg90
(lp2050
sg92
(lp2051
VThis paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings.
p2052
aVFor each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest.
p2053
aVThe training objective is to maximize the mixed likelihood of both the labeled data and the auto-parsed unlabeled data with ambiguous labelings.
p2054
aVExperiments show that our framework can make better use of the unlabeled data, especially those with divergent outputs from different parsers, than traditional tri-training.
p2055
aVDetailed analysis demonstrates the effectiveness of our approach.
p2056
aVSpecifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data.
p2057
aVFor future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easy-first non-directional dependency parser [] and other constituent parsers [].
p2058
ag106
asg107
S'P14-1043'
p2059
sg109
(lp2060
VThis paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training.
p2061
aVInstead of only using 1-best parse trees in previous work, our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data.
p2062
aVWith a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings.
p2063
aVThis framework offers two promising advantages.
p2064
aV1) ambiguity encoded in parse forests compromises noise in 1-best parse trees.
p2065
aVDuring training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves.
p2066
aV2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser.
p2067
aVExperimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training.
p2068
ag106
asba(icmyPackage
FText
p2069
(dp2070
g3
(lp2071
VOwing to its ability to bring together features like multilinguality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Parsing [ 33 ] , Semantic Role Labeling [ 28 ] , and Word Sense Disambiguation [ 25 ]
p2072
aVNevertheless, when it comes to aligning textual definitions in different resources, the lexical approach [ 32 , 5 , 11 ] falls short because of the potential use of totally different wordings to define the same concept
p2073
aVHowever, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wiktionary have first to be transformed into semantic graphs before such graph-based approaches can be applied to them
p2074
aVHowever, this alignment method still involves tuning of parameters which are highly dependent on the characteristics of the generated graphs and, hence, requires hand-crafted sense alignments for the specific pair of resources to be aligned, a task which has to be replicated every time the resources are updated
p2075
aVTherefore, we assume that a lexical resource L can be represented as an undirected graph G = ( V , E ) where V is the set of nodes, i.e.,, the concepts defined in the resource, and E is the set of undirected edges, i.e.,, semantic relations between concepts
p2076
aVFor instance, WordNet can be readily represented as an undirected graph G whose nodes are synsets and edges are modeled after the relations between synsets defined in WordNet (e.g.,, hypernymy, meronymy, etc.), and u'\u005cu2112' G is the mapping between each synset node and the set of synonyms which express the concept
p2077
aVHowever, other resources such as Wiktionary do not provide semantic relations between concepts and, therefore, have first to be transformed into semantic networks before they can be aligned using our alignment algorithm
p2078
aVGiven a pair of lexical resources L 1 and L 2 , we align each concept in L 1 by mapping it to its corresponding concept(s) in the target lexicon L 2
p2079
aVThe algorithm iterates over all concepts c 1 u'\u005cu2208' V 1 and, for each of them, obtains the set of concepts C u'\u005cu2282' V 2 , which can be considered as alignment candidates for c 1 (line 2
p2080
aVFor a concept c 1 , alignment candidates in G 2 usually consist of every concept c 2 u'\u005cu2208' V 2 that shares at least one lexicalization with c 1 in the same part of speech tag, i.e.,, u'\u005cu2112' G 1 u'\u005cu2062' ( c 1 ) u'\u005cu2229' u'\u005cu2112' G 2 u'\u005cu2062' ( c 2 ) u'\u005cu2260' u'\u005cu2205' [ 31 , 20 ]
p2081
aVIn the following, we present our novel approach for measuring the similarity of concept pairs
p2082
aV[t!] Lexical Resource Aligner {algorithmic} [1] \u005cREQUIRE graphs H = ( V H , E H ) , G 1 = ( V 1 , E 1 ) and G 2 = ( V 2 , E 2 ) , the similarity threshold u'\u005cu0398' , and the combination parameter u'\u005cu0392' \u005cENSURE A , the set of all aligned concept pairs
p2083
aVFigure 1 illustrates the procedure underlying our cross-resource concept similarity measurement technique
p2084
aVAs can be seen, the approach consists of two main components definitional similarity and structural similarity
p2085
aVEach of these components gets, as its input, a pair of concepts belonging to two different semantic networks and produces a similarity score
p2086
aVThe definitional similarity component computes the similarity of two concepts in terms of the similarity of their definitions, a method that has also been used in previous work for aligning lexical resources [ 27 , 12 ]
p2087
aVIn spite of its simplicity, the mere calculation of the similarity of concept definitions provides a strong baseline, especially for cases where the definitional texts for a pair of concepts to be aligned are lexically similar, yet distinguishable from the other definitions
p2088
aVHowever, as mentioned in the introduction, definition similarity-based techniques fail at identifying the correct alignments in cases where different wordings are used or definitions are not of high quality
p2089
aVThe structural similarity component, instead, is a novel graph-based similarity measurement technique which calculates the similarity between a pair of concepts across the semantic networks of the two resources by leveraging the semantic structure of those networks
p2090
aVThis component goes beyond the surface realization of concepts, thus providing a deeper measure of concept similarity
p2091
aVThe aim of this stage is to model a given concept or set of concepts through a vectorial semantic representation, which we refer to as the semantic signature of the input
p2092
aVWe utilized Personalized PageRank [ 10 , ppr ] , a random walk graph algorithm, for calculating semantic signatures
p2093
aVWhen applied to a semantic graph by initializing the random walks from a set of concepts (nodes), ppr yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts
p2094
aVFormally, we first represent a semantic network consisting of N concepts as a row-stochastic transition matrix u'\u005cud835' u'\u005cudc0c' u'\u005cu2208' u'\u005cu211d' N × N
p2095
aVThe cell ( i , j ) in the matrix denotes the probability of moving from a concept i to j in the graph
p2096
aVIn this component the personalization vector u'\u005cud835' u'\u005cudc2f' i is set by uniformly distributing the probability mass over the nodes corresponding to the senses of all the content words in the extended definition of d i according to the sense inventory of a semantic network H
p2097
aVWe use the same semantic graph H for computing the semantic signatures of both definitions
p2098
aVFor this purpose we used the WordNet [ 7 ] graph which was further enriched by connecting each concept to all the concepts appearing in its disambiguated gloss
p2099
aVIn the structural similarity component (Figure 1 (b), bottom), the semantic signature for each concept c i is computed by running the ppr algorithm on its corresponding graph G i , hence a different u'\u005cud835' u'\u005cudc0c' i is built for each of the two concepts
p2100
aVAs mentioned earlier, semantic signatures are vectors with dimension equal to the number of nodes in the semantic graph
p2101
aVSince the structural similarity signatures u'\u005cud835' u'\u005cudcae' u'\u005cud835' u'\u005cudc2f' 1 and u'\u005cud835' u'\u005cudcae' u'\u005cud835' u'\u005cudc2f' 2 are calculated on different graphs and thus have different dimensions, we need to make them comparable by unifying them
p2102
aVWe therefore propose an approach (part (c) of Figure 1 ) that finds a common ground between the two signatures to this end we consider all the concepts associated with monosemous words in the two signatures as landmarks and restrict the two signatures exclusively to those common concepts
p2103
aVLeveraging monosemous words as bridges between two signatures is a particularly reliable technique as typically a significant portion of all words in a lexicon are monosemous
p2104
aVThen, given two signatures u'\u005cud835' u'\u005cudcae' u'\u005cud835' u'\u005cudc2f' 1 and u'\u005cud835' u'\u005cudcae' u'\u005cud835' u'\u005cudc2f' 2 , computed on the respective graphs G 1 and G 2 , we first obtain the set u'\u005cu2133' of words that are monosemous according to both semantic networks, i.e.,, u'\u005cu2133' = { w u'\u005cu2110' G 1 u'\u005cu2062' ( w
p2105
aVWe then transform each of the two signatures u'\u005cud835' u'\u005cudcae' u'\u005cud835' u'\u005cudc2f' i into a new sub-signature u'\u005cud835' u'\u005cudcae' u'\u005cud835' u'\u005cudc2f' i u'\u005cu2032' whose dimension is u'\u005cu2133' the k t u'\u005cu2062' h component of u'\u005cud835' u'\u005cudcae' u'\u005cud835' u'\u005cudc2f' i u'\u005cu2032' corresponds to the weight in u'\u005cud835' u'\u005cudcae' u'\u005cud835' u'\u005cudc2f' i of the only concept of w k in u'\u005cu2110' G i u'\u005cu2062' ( w k
p2106
aVAs an example, assume we are given two semantic signatures computed for two concepts in WordNet and Wiktionary
p2107
aVAlso, consider the noun tradeoff which is monosemous according to both these resources
p2108
aVAs a result of the unification process, we obtain a pair of equally-sized semantic signatures with comparable components
p2109
aVHaving at hand the semantic signatures for the two input concepts, we proceed to comparing them (part (d) in Figure 1
p2110
aVIn Section 2 , we presented our approach for aligning lexical resources
p2111
aVIn order to address this issue and hence generalize our alignment approach to any given lexical resource, we propose a method for transforming a given machine-readable dictionary into a semantic network, a process we refer to as ontologization
p2112
aVOur ontologization algorithm takes as input a lexicon L and outputs a semantic graph G = ( V , E ) where, as already defined in Section 2 , V is the set of concepts in L and E is the set of semantic relations between these concepts
p2113
aVBoth words in these relations, however, should be disambiguated according to the given lexicon [ 29 ] , making the task particularly prone to mistakes due to the high number of possible sense pairings
p2114
aVHere, we take an alternative approach which requires disambiguation on the target side only, hence reducing the size of the search space significantly
p2115
aVWe first create the empty undirected graph G L = ( V , E ) such that V is the set of concepts in L and E = u'\u005cu2205'
p2116
aVFor each source concept c u'\u005cu2208' V we create a bag of content words W = { w 1 , u'\u005cu2026' , w n } which includes all the content words in its definition d and, if available, additional related words obtained from lexicon relations (e.g.,, synonyms in Wiktionary
p2117
aVThe problem is then cast as a disambiguation task whose goal is to identify the intended sense of each word w i u'\u005cu2208' W according to the sense inventory of L if w i is monosemous, i.e
p2118
aVIn this latter case, we choose the most appropriate concept c i u'\u005cu2208' u'\u005cu2110' G L u'\u005cu2062' ( w i ) by finding the maximal similarity between the definition of c and the definitions of each sense of w i
p2119
aVHaving found the intended sense c ^ w i of w i , we add the edge { c , c ^ w i } to E
p2120
aVAs a result of this procedure, we obtain a semantic graph representation G for the lexicon L
p2121
aVAs an example, consider the 4 t u'\u005cu2062' h sense of the noun cone in Wiktionary (i.e.,, cone 4 n ) which is defined as u'\u005cu201c' The fruit of a conifer u'\u005cu201d'
p2122
aVThe latter word is monosemous in Wiktionary, hence we directly connect cone 4 n to the only sense of conifer n
p2123
aVThe noun fruit , however, has 5 senses in Wiktionary
p2124
aVWe therefore measure the similarity between the definition of cone 4 n and all the 5 definitions of fruit and introduce a link from cone 4 n to the sense of fruit which yields the maximal similarity value (defined as u'\u005cu201c' (botany) The seed-bearing part of a plant u'\u005cu2026' u'\u005cu201d'
p2125
aVWe also report results for accuracy which, in addition to true positives, takes into account true negatives, i.e.,, pairs which are correctly judged as unaligned
p2126
aVHere, we describe how the four semantic graphs for our four lexical resources (i.e.,, wn , wp , wt , ow ) were constructed
p2127
aVAs mentioned in Section 2.1.1 , we build the wn graph by including all the synsets and semantic relations defined in WordNet (e.g.,, hypernymy and meronymy) and further populate the relation set by connecting a synset to all the other synsets that appear in its disambiguated gloss
p2128
aVThe other two resources, i.e.,, wt and ow , do not provide a reliable network of semantic relations, therefore we used our ontologization approach to construct their corresponding semantic graphs
p2129
aVFor ontologizing wt and ow , the bag of content words W is given by the content words in sense definitions and, if available, additional related words obtained from lexicon relations (see Section 3
p2130
aVIn wt , both of these are in word surface form and hence had to be disambiguated
p2131
aVFor ow , however, the encoded relations, though relatively small in number, are already disambiguated and, therefore, the ontologization was just performed on the definition u'\u005cu2019' s content words
p2132
aVThe edges obtained from unambiguous entries are essentially sense disambiguated on both sides whereas those obtained from ambiguous terms are a result of our similarity-based disambiguation
p2133
aVHence, given that a large portion of edges came from ambiguous words (see Table 1 ), we carried out an experiment to evaluate the accuracy of our disambiguation method
p2134
aVTo this end, we took as our benchmark the dataset provided by Meyer and Gurevych ( 2010 ) for evaluating relation disambiguation in wt
p2135
aVWe compared our similarity-based disambiguation approach against the state of the art on this dataset, i.e.,, the wktwsd system, which is a wt relation disambiguation algorithm based on a series of rules [ 22 ]
p2136
aVThe u'\u005cu201c' Human u'\u005cu201d' row corresponds to the inter-rater F1 and accuracy scores, i.e.,, the upperbound performance on this dataset, as calculated by Meyer and Gurevych ( 2010
p2137
aVAs can be seen, our method proves to be very accurate, surpassing the performance of the wktwsd system in terms of precision, F1, and accuracy
p2138
aVThis is particularly interesting as the wktwsd system uses a rule-based technique specific to relation disambiguation in wt , whereas our method is resource independent and can be applied to arbitrary words in the definition of any concept
p2139
aVOur approach, however, thanks to the connections obtained through ambiguous words, can provide graphs with significantly higher coverage
p2140
aVAs an example, for wt , Matuschek and Gurevych ( 2013 ) generated a graph where around 30% of the nodes were in isolation, whereas this number drops to around 5% in our corresponding graph
p2141
aVNow that all the four resources are transformed into semantic graphs, we move to our alignment experiments
p2142
aVAs our benchmark we tested on the gold standard datasets used in Matuschek and Gurevych ( 2013 ) for three alignment tasks
p2143
aVHowever, the dataset for wn - ow was originally built for the German language and, hence, was missing many English ow concepts that could be considered as candidate target alignments
p2144
aVWe therefore fixed the dataset for the English language and reproduced the performance of previous work on the new dataset
p2145
aVUnsupervised , where the two parameters are set to their middle values (i.e.,, 0.5), hence, no tuning is performed for either of the parameters
p2146
aVIn this case, both the definitional and structural similarity scores are treated as equally important and two concepts are aligned if their overall similarity exceeds the middle point of the similarity scale
p2147
aVWe also show the results for this system as sb+dwsa in the table
p2148
aVWe show the results for this setting in the bottom part of the table (last three lines
p2149
aVThe main feature worth remarking upon is the consistency in the results across different resource pairs the unsupervised system gains the best recall among the three configurations (with the improvement over sb+dwsa being always statistically significant 4 4 All significance tests are done using z-test at p 0.05 whereas tuning, both on a subset or through cross-validation, consistently leads to the best performance in terms of F1 and accuracy (with the latter being statistically significant with respect to sb+dwsa on wn - wp and wn - wt
p2150
aVMoreover, the unsupervised system proves to be very robust inasmuch as it provides competitive results on all the three datasets, while it surpasses the performance of sb+dwsa on wn - wt
p2151
aVThis is particularly interesting as the latter system involves tuning of several parameters, whereas SemAlign, in its unsupervised configuration, does not need any training data nor does it involve any tuning
p2152
aVThe consistency in the performance of SemAlign in its different configurations and across different resource pairs indicates its robustness and shows that our system can be utilized effectively for aligning any pair of lexical resources, irrespective of their structure or availability of training data
p2153
aVTo gain more insight into the effectiveness of our structural similarity measure in comparison to the Dijkstra-WSA method, we carried out an experiment where our alignment system used only the structural similarity component, a variant of our system we refer to as SemAlign s u'\u005cu2062' t u'\u005cu2062' r
p2154
aVWe show in Table 4 the performance of the two systems on our three datasets
p2155
aVAs can be seen in the table, SemAlign s u'\u005cu2062' t u'\u005cu2062' r consistently improves over Dijkstra-WSA according to recall, F1 and accuracy with all the differences in recall and accuracy being statistically significant (p 0.05
p2156
aVIn addition, as we mentioned earlier, for wn - wp we used the same graph as that of Dijkstra-WSA, since both wn and wp provide a full-fledged semantic network and thus neither needed to be ontologized
p2157
aVTherefore, the considerable performance improvement over Dijkstra-WSA on this resource pair shows the effectiveness of our novel concept similarity measure independently of the underlying semantic network
p2158
aVHaving lexical resources represented as semantic networks is highly beneficial
p2159
aVA good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks [ 7 ]
p2160
aVA recent prominent case is Wikipedia [ 18 , 13 ] which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information [ 2 , 34 , 23 , 8 ]
p2161
aVMeyer and Gurevych ( 2012a ) and Matuschek and Gurevych ( 2013 ) provided approaches for building graph representations of Wiktionary and OmegaWiki
p2162
aVOur approach, in contrast, aims at transforming a lexical resource into a full-fledged semantic network, hence providing a denser graph with most of its nodes connected
p2163
aVAligning lexical resources has been a very active field of research in the last decade
p2164
aVOne of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources
p2165
aVAs a matter of fact, most efforts have been concentrated on aligning the de facto community standard sense inventory, i.e., WordNet, to other resources
p2166
aVLast year Matuschek and Gurevych ( 2013 ) proposed Dijkstra-WSA, a graph-based approach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking
p2167
aVInstead of measuring the similarity of two concepts on the basis of their distance in the combined graph, our approach models each concept through a rich vectorial representation we refer to as semantic signature and compares the two concepts in terms of the similarity of their semantic signatures
p2168
aVThis rich representation leads to our approach having a good degree of robustness such that it can achieve competitive results even in the absence of training data
p2169
aVThis enables our system to be applied effectively for aligning new pairs of resources for which no training data is available, with state-of-the-art performance
p2170
aVThis paper presents a unified approach for aligning lexical resources
p2171
aVWe also show that our approach is robust across its different configurations, even when the training data is absent, enabling it to be used effectively for aligning new pairs of lexical resources for which no resource-specific training data is available
p2172
aVWe would like to thank Michael Matuschek for providing us with Wikipedia graphs and alignment datasets
p2173
asg88
(lp2174
sg90
(lp2175
sg92
(lp2176
VThis paper presents a unified approach for aligning lexical resources.
p2177
aVOur method leverages a novel similarity measure which enables a direct structural comparison of concepts across different lexical resources.
p2178
aVThanks to an effective ontologization method, our alignment approach can be applied to any pair of lexical resources independently of whether they provide a full-fledged network structure.
p2179
aVWe demonstrate that our approach achieves state-of-the-art performance on aligning WordNet to three collaboratively-constructed resources with different characteristics, i.e.,, Wikipedia, Wiktionary, and OmegaWiki.
p2180
aVWe also show that our approach is robust across its different configurations, even when the training data is absent, enabling it to be used effectively for aligning new pairs of lexical resources for which no resource-specific training data is available.
p2181
aVIn future work, we plan to extend our concept similarity measure across different natural languages.
p2182
aVWe release all our data at http://lcl.uniroma1.it/semalign.
p2183
ag106
asg107
S'P14-1044'
p2184
sg109
(lp2185
VLexical resource alignment has been an active field of research over the last decade.
p2186
aVHowever, prior methods for aligning lexical resources have been either specific to a particular pair of resources, or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned.
p2187
aVHere we present a unified approach that can be applied to an arbitrary pair of lexical resources, including machine-readable dictionaries with no network structure.
p2188
aVOur approach leverages a similarity measure that enables the structural comparison of senses across lexical resources, achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources.
p2189
aVWikipedia, Wiktionary and OmegaWiki.
p2190
ag106
asba(icmyPackage
FText
p2191
(dp2192
g3
(lp2193
VIn a discourse, we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant
p2194
aVThis way of building a semantic network has been very popular since [] , even though the nature of the information it contains is hard to define, and its evaluation is far from obvious
p2195
aVIntrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects []
p2196
aVThey are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard []
p2197
aVOne advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations
p2198
aVThe method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what [] call u'\u005cu201c' non classical lexical semantic relations u'\u005cu201d'
p2199
aVAt the same time, we want to filter associations that can be considered as accidental in a semantic perspective (e.g., flag and composer are similar because they appear a lot with nationality names
p2200
aVWe do this by judging the relevance of a lexical relation in a context where both elements of a lexical pair occur
p2201
aVIn the rest of this paper, we describe the resource we used as a case study, and the data we collected to evaluate its content (section 2
p2202
aVThis is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a lexical item can appear in many predicates and as an argument (e.g., interest as argument, interest_for as one predicate
p2203
aVTo ease the use of lexical neighbours in our experiments, we merged together predicates that include the same lexical unit, a posteriori
p2204
aVThus there is no need for a syntactic analysis of the context considered when exploiting the resource, and sparsity is less of an issue 1 1 Whenever two predicates with the same lemma have common neighbours, we average the score of the pairs
p2205
aVIl a également des coussinets noirs situés, à l u'\u005cu2019' arrière de ses pattes
p2206
aVThis seems to validate the feasability of a reliable annotation of relatedness in context, so we went on for a larger annotation with two of the previous annotators
p2207
aVIt turns out that the kappa score ( 0.80 ) shows a better inter-annotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrences of each element in the pair to judge, and also because the annotators were more experienced after the preliminary test
p2208
aVAgreement measures are summed-up table 1
p2209
aVAn excerpt of an example text, as it was presented to the annotators, is shown figure 2
p2210
aVIt is not easy to decide if the non-relevant pairs are just noise, or context-dependent associations that were not present in the actual text considered (for polysemy reasons for instance), or just low-level associations
p2211
aVAn important aspect is thus to guarantee that there is a correlation between the similarity score (Lin u'\u005cu2019' s score here), and the evaluated relevance of the neighbour pairs
p2212
aVThe produced annotation 2 2 Freely available, and distributed with this submission can be used as a reference to explore various aspects of distributional resources, with the caveat that it is as such a bit dependent on the particular resource used
p2213
aVWe nonetheless assume that some of the relevant pairs would appear in other thesauri, or would be of interest in an evaluation of another resource
p2214
aVThe resource itself is built by choosing a cut-off which is supposed to keep pairs with a satisfactory similarity, but this threshold is rather arbitrary
p2215
aVThis can be considered as a baseline for extraction of relevant lexical pairs, to which we turn in the following section
p2216
aVThe outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links, and we showed these linguistic judgments to be reliable
p2217
aVThey can be divided in three groups, according to their origin they are computed from the whole corpus, gathered from the distributional resource, or extracted from the considered text which contains the semantic pair to be evaluated
p2218
aVThis is a rather large window, and thus gives a good coverage with respect to the neighbour database (70% of all pairs
p2219
aVeach neighbour productivity p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d a and p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d b are defined as the numbers of neighbours of respectively neighbour a and neighbour b in the database (thus related tokens with a similarity above the threshold), from which we derive three features as for frequencies the min, the max, and the log of the product
p2220
aVthe ranks of tokens in other related items neighbours r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' k a - b is defined as the rank of neighbour a among neighbours of neighbour b u'\u005cu2009' ordered with respect to Lin u'\u005cu2019' s score; r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' g b - a is defined similarly and again we consider as features the min, max and log-product of these ranks
p2221
aVFirst, we take into account the frequencies of items within the text, with three features as before the min of the frequencies of the two related items, the max, and the log-product
p2222
aVThen we consider a tf u'\u005cu22c5' idf [] measure, to evaluate the specificity and arguably the importance of a word in a document or within a document
p2223
aVWe similarly defined a tf u'\u005cu22c5' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text
p2224
aVThe resulting feature we used is the product of this measure for neighbour a and neighbour b
p2225
aVFinally, we took into account the network of related lexical items, by considering the largest sets of words present in the text and connected in the database (self-connected components), by adding the following features
p2226
aVthe degree of each lemma, seen as a node in this similarity graph, combined as above in minimal degree of the pair, maximal degree, and product of degrees ( p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t min , p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t max , p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t ×
p2227
aVWe have seen that the relevant/not relevant classification is very imbalanced, biased towards the u'\u005cu201c' not relevant u'\u005cu201d' category (about 11%/89%), so we applied methods dedicated to counter-balance this, and will focus on the precision and recall of the predicted relevant links
p2228
aVOther popular methods (maximum entropy, SVM) have shown slightly inferior combined F-score, even though precision and recall might yield more important variations
p2229
aVAs a baseline, we can also consider a simple threshold on the lexical similarity score, in our case Lin u'\u005cu2019' s measure, which we have shown to yield the best F-score of 24% when set at 0.22
p2230
aVWe tested the two strategies, by applying the classical Smote method of [] as a kind of resampling, and the ensemble method MetaCost of [] as a cost-aware learning method
p2231
aVSmote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling
p2232
aVMetaCost is an interesting meta-learner that can use any classifier as a base classifier
p2233
aVWe used Weka u'\u005cu2019' s implementations of these methods [] , and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by refinements of these techniques
p2234
aVWe chose the following settings for the different models naive bayes uses a kernel density estimation for numerical features, as this generally improves performance
p2235
aVFor cost-aware learning, a sensible choice is to invert the class ratio for the cost ratio, i.e., here the cost of a mistake on a relevant link (false negative) is exactly 8.5 times higher than the cost on a non-relevant link (false positive), as non-relevant instances are 8.5 times more present than relevant ones
p2236
aVIf we take the best simple classifier (random forests), the precision and recall are 68.1 u'\u005cu2062' % and 24.2 u'\u005cu2062' % for an F-score of 35.7 u'\u005cu2062' % , and this is significantly beaten by the Naive Bayes method as precision and recall are more even (F-score of 41.5%
p2237
aVAlso note that predicting every link as relevant would result in a 2.6% precision, and thus a 5% F-score
p2238
aVThe random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3% is reached with Random Forests and the cost-aware learning method
p2239
aVWe analysed the learning curve by doing a cross-validation on reduced set of instances (from 10% to 90%); F1-scores range from 37.3% with 10% of instances and stabilize at 80%, with small increment in every case
p2240
aVThe filtering approach we propose seems to yield good results, by augmenting the similarity built on the whole corpus with signals from the local contexts and documents where related lexical items appear together
p2241
aVEvaluating distributional resources is the subject of a lot of methodological reflection [] , and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations
p2242
aVWe differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an u'\u005cu201c' existential u'\u005cu201d' view of semantic relatedness
p2243
aVAs for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built [] , or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target []
p2244
aVThey still use u'\u005cu201c' essential u'\u005cu201d' evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate
p2245
aVWe are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in
p2246
aVWe proposed a method to reliably evaluate distributional semantic similarity in a broad sense by considering the validation of lexical pairs in contexts where they both appear
p2247
aVThis can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level [] or other macro-textual level [] , since these are always aggregation functions of word similarities
p2248
aVWe are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the particular distributional thesaurus we used, and the way the similarities are computed
p2249
asg88
(lp2250
sg90
(lp2251
sg92
(lp2252
VWe proposed a method to reliably evaluate distributional semantic similarity in a broad sense by considering the validation of lexical pairs in contexts where they both appear.
p2253
aVThis helps cover non classical semantic relations which are hard to evaluate with classical resources.
p2254
aVWe also presented a supervised learning model which combines global features from the corpus used to built a distributional thesaurus and local features from the text where similarities are to be judged as relevant or not to the coherence of a document.
p2255
aVIt seems from these experiments that the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the similarity score and the ranks of items as neighbours of other items.
p2256
aVThis can hopefully help filter out lexical pairs when word lexical similarity is used as an information source where context is important lexical disambiguation [] , topic segmentation [].
p2257
aVThis can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level [] or other macro-textual level [] , since these are always aggregation functions of word similarities.
p2258
aVThere are limits to what is presented here we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built.
p2259
aVOur starting corpus is relatively small compared to current efforts in this framework.
p2260
aVWe are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the particular distributional thesaurus we used, and the way the similarities are computed.
p2261
ag106
asg107
S'P14-1045'
p2262
sg109
(lp2263
VUsing distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP.
p2264
aVThe resulting relations are often noisy or difficult to interpret in general.
p2265
aVThis paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains, but instead of considering it in abstracto, we focus on pairs of words in context.
p2266
aVIn a discourse, we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant.
p2267
aVWe first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity, and to assess the prevalence of actual semantic relations between word tokens.
p2268
aVWe then built an experiment to automatically predict this relevance, evaluated on the reliable reference data set which was the outcome of the first annotation.
p2269
aVWe show that in-document information greatly improve the prediction made by the similarity level alone.
p2270
ag106
asba(icmyPackage
FText
p2271
(dp2272
g3
(lp2273
VWe use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario
p2274
aVTwo agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from
p2275
aVBuilding a dialogue policy can be a challenging task especially for complex applications
p2276
aVFor this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies [ 40 , 34 , 22 ]
p2277
aVBoth agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus
p2278
aV1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior (see section 6
p2279
aVAs we discuss below, concurrent learning could potentially be used for learning via live interaction with human users
p2280
aVMoreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU
p2281
aVThey are both negotiators, thus building a good SU is as difficult as building a good system policy
p2282
aVSingle-agent RL methods make the assumption that the system learns by interacting with a stationary environment, i.e.,, an environment that does not change over time
p2283
aVImagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her
p2284
aVTherefore it is important to investigate RL approaches that do not make such assumptions about the user/environment
p2285
aVIn this case the environment of a learning agent is one or more other agents that can also be learning at the same time
p2286
aVTherefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction, and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios
p2287
aVThis ability of multi-agent RL can also have important implications for learning via live interaction with human users
p2288
aVImagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants
p2289
aVWe vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate
p2290
aVOur research contributions are as follows
p2291
aV1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters
p2292
aVThen the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues
p2293
aVDepending on the application, building a realistic SU can be just as difficult as building a good dialogue policy
p2294
aVFurthermore, it is not clear what constitutes a good SU for dialogue policy learning
p2295
aVShould the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns
p2296
aVTypically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system
p2297
aVTypically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs
p2298
aVGaussian processes have been shown to speed up learning
p2299
aVSpace constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management
p2300
aVThus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains
p2301
aVIn either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work
p2302
aVHowever, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold
p2303
aVHere two or more agents learn simultaneously
p2304
aVThus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time
p2305
aVTherefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all
p2306
aVWe chose these two algorithms because, unlike other multi-agent RL methods [ 26 , 21 ] , they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale
p2307
aVFurthermore, Cuayáhuitl and Dethlefs ( 2012 ) used hierarchical multi-agent RL for co-ordinating the verbal and non-verbal actions of a robot
p2308
aVWith regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling
p2309
aVThen Heeman ( 2009 ) extended this work by experimenting with different representations of the RL state in the same domain (this time learning against a hand-crafted SU
p2310
aVBecause it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts
p2311
aVAn MDP is defined as a tuple ( S , A , T , R , u'\u005cu0393' ) where S is the set of states (representing different contexts) which the agent may be in, A is the set of actions of the agent, T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking an action, R is the reward function S × A u'\u005cu2192' u'\u005cu211c' which defines the reward received when taking an action from the given state, and u'\u005cu0393' is a factor that discounts future rewards
p2312
aVA stochastic game is defined as a tuple ( n , S , A 1 u'\u005cu2062' n , T , R 1 u'\u005cu2062' n , u'\u005cu0393' ) where n is the number of agents, S is the set of states, A i is the set of actions available for agent i (and A is the joint action space A 1 × A 2 × u'\u005cu2026' × A n ), T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking a joint action, R i is the reward function for the i th agent S × A u'\u005cu2192' u'\u005cu211c' , and u'\u005cu0393' is a factor that discounts future rewards
p2313
aVS × A i u'\u005cu2192' [0, 1] that maps states to mixed strategies, which are probability distributions over the agent u'\u005cu2019' s actions, so that the agent u'\u005cu2019' s expected discounted (with discount factor u'\u005cu0393' ) future reward is maximized
p2314
aVIn each step the mixed policy is updated by increasing the probability of selecting the highest valued action according to a learning rate u'\u005cu0394' (see equations (2), (3), and (4) below
p2315
aVThe main idea is that when the agent is u'\u005cu201c' winning u'\u005cu201d' the learning rate u'\u005cu0394' W should be low so that the opponents have more time to adapt to the agent u'\u005cu2019' s policy, which helps with convergence
p2316
aVOn the other hand when the agent is u'\u005cu201c' losing u'\u005cu201d' the learning rate u'\u005cu0394' L u'\u005cu2062' F should be high so that the agent has more time to adapt to the other agents u'\u005cu2019' policies, which also facilitates convergence
p2317
aVThus PHC-WoLF uses two learning rates u'\u005cu0394' W and u'\u005cu0394' L u'\u005cu2062' F
p2318
aVPHC-WoLF determines whether the agent is u'\u005cu201c' winning u'\u005cu201d' or u'\u005cu201c' losing u'\u005cu201d' by comparing the current policy u'\u005cu2019' s u'\u005cu03a0' u'\u005cu2062' ( s , a ) expected payoff with that of the average policy u'\u005cu03a0' ~ u'\u005cu2062' ( s , a ) over time
p2319
aVIf the current policy u'\u005cu2019' s expected payoff is greater then the agent is u'\u005cu201c' winning u'\u005cu201d' , otherwise it is u'\u005cu201c' losing u'\u005cu201d'
p2320
aVNevertheless, despite its shortcomings Q-learning has been used successfully for multi-agent RL [ 7 ]
p2321
aVIndeed, as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces
p2322
aVHowever, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF
p2323
aVAlso, they have human-like constraints of imperfect information about each other; they do not know each other u'\u005cu2019' s reward function or degree of rationality (during learning our agents can be irrational
p2324
aVThus a Nash equilibrium (if there exists one) cannot be computed in advance
p2325
aVFor all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome (see Table 1
p2326
aVThus the two agents have different reward functions
p2327
aVAlso, to avoid long dialogues, if none of the agents accepts the other agent u'\u005cu2019' s offers, the negotiation finishes after 20 pairs of exchanges between the two agents (20 offers from Agent 1 and 20 offers from Agent 2
p2328
aVAn example interaction between the two agents is shown in Figure 1
p2329
aVAs we can see, each agent can offer any combination of apples and oranges
p2330
aVSo if we have X apples and Y oranges for sharing, there can be ( X + 1 ) × ( Y + 1 ) possible offers
p2331
aVFor example if we have 2 apples and 2 oranges for sharing, there can be 9 possible offers u'\u005cu201c' offer-0-0 u'\u005cu201d' , u'\u005cu201c' offer-0-1 u'\u005cu201d' , u'\u005cu2026' , u'\u005cu201c' offer-2-2 u'\u005cu201d'
p2332
aVTable 3 also shows the number of state-action pairs (Q-values
p2333
aVAs we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds
p2334
aVIn the first case which from now on will be referred to as PHC-W, we set u'\u005cu0394' to be equal to u'\u005cu0394' W (also used for PHC-WoLF
p2335
aVIn the second case which from now on will be referred to as PHC-LF, we set u'\u005cu0394' to be equal to u'\u005cu0394' L u'\u005cu2062' F (also used for PHC-WoLF
p2336
aVSo unlike PHC-WoLF, PHC-W and PHC-LF do not use a variable learning rate
p2337
aVFor example, if the policies were learned for 5 epochs with each epoch containing 25,000 episodes, then for testing the two policies will interact for another 25,000 episodes
p2338
aVTo ensure that the policies do not converge by chance, we run the training and test sessions 20 times each and we report averages
p2339
aVThus all results presented in section 5 are averages of 20 runs
p2340
aVGiven that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer
p2341
aVThus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer
p2342
aVFor 5 fruits the average reward should be 1500 minus 10, and so forth
p2343
aVWe call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty
p2344
aVAgent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts
p2345
aVThus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190
p2346
aVThis is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes
p2347
aVAlso, n r is the number of runs (in our case always equal to 20
p2348
aVThus in the case of 4 fruits, we will have C u'\u005cu2062' R 1 = C u'\u005cu2062' R 2 =1200, and if for all runs R 1 u'\u005cu2062' j = R 2 u'\u005cu2062' j =1190, then A u'\u005cu2062' D =10
p2349
aVIt is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence
p2350
aVAlso, for 1, 2, and 3 fruits all algorithms converge and perform comparably
p2351
aVAs the number of fruits increases, Q-learning starts performing worse than the multi-agent RL algorithms
p2352
aVFor 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence
p2353
aVThus after only 100,000 episodes per epoch all policies still behave somewhat randomly
p2354
aVFigures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively
p2355
aVClearly having a constant exploration rate in all epochs is problematic
p2356
aVWe used single-agent RL and multi-agent RL for learning dialogue policies in a resource allocation negotiation scenario
p2357
aVAnother interesting question is whether corpora or SUs may still be required for designing the state and action spaces and the reward functions of the interlocutors, bootstrapping the policies, and ensuring that information about the behavior of human users is encoded in the resulting learned policies
p2358
aVHowever, abstraction is not trivial because the agents have no guarantee that the value of a deal is a simple function of the value of its parts, and values may differ for different agents
p2359
asg88
(lp2360
sg90
(lp2361
sg92
(lp2362
VWe used single-agent RL and multi-agent RL for learning dialogue policies in a resource allocation negotiation scenario.
p2363
aVTwo agents interacted with each other and both learned at the same time.
p2364
aVThe advantage of this approach is that it does not require SUs to train against or corpora to learn from.
p2365
aVWe compared a traditional single-agent RL algorithm (Q-learning) against two multi-agent RL algorithms (PHC and PHC-WoLF) varying the scenario complexity (state space size), the number of training episodes, and the learning and exploration rates.
p2366
aVOur results showed that Q-learning is not suitable for concurrent learning given that it is designed for learning against a stationary environment.
p2367
aVQ-learning failed to converge in all cases, except for very small state space sizes.
p2368
aVOn the other hand, both PHC and PHC-WoLF always converged (or in the case of 7 fruits they needed more training episodes) and performed similarly.
p2369
aVWe also showed that in concurrent learning very high gradually decreasing exploration rates are required for convergence.
p2370
aVWe conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.
p2371
aVThe focus of this paper is on comparing single-agent RL and multi-agent RL for concurrent learning, and studying the implications for convergence and exploration/learning rates.
p2372
aVOur next step is testing with human users.
p2373
aVWe are particularly interested in users whose behavior changes during the interaction and continuous testing against expert repeat users, which has never been done before.
p2374
aVAnother interesting question is whether corpora or SUs may still be required for designing the state and action spaces and the reward functions of the interlocutors, bootstrapping the policies, and ensuring that information about the behavior of human users is encoded in the resulting learned policies.
p2375
aVGa i et al.
p2376
aV2013 ) showed that it is possible to learn full dialogue policies just via interaction with human users (without any bootstrapping using corpora or SUs.
p2377
aVSimilarly, concurrent learning could be used in an on-line fashion via live interaction with human users.
p2378
aVOr alternatively concurrent learning could be used off-line to bootstrap the policies and then these policies could be improved via live interaction with human users (again using concurrent learning to address possible changes in user behavior.
p2379
aVThese are open research questions for future work.
p2380
aVFurthermore, we intend to apply multi-agent RL to more complex negotiation domains, e.g.,, experiment with more than two types of resources (not just apples and oranges) and more types of actions (not just offers and acceptances.
p2381
aVWe would also like to compare policies learned with multi-agent RL techniques with policies learned with SUs or from corpora both in simulation and with human users.
p2382
aVFinally, we aim to experiment with different feature-based representations of the state and action spaces.
p2383
aVCurrently all possible deal combinations are listed as possible actions and as elements of the state, which can quickly lead to very large state and action spaces as the application becomes more complex (in our case as the number of fruits increases.
p2384
aVHowever, abstraction is not trivial because the agents have no guarantee that the value of a deal is a simple function of the value of its parts, and values may differ for different agents.
p2385
ag106
asg107
S'P14-1047'
p2386
sg109
(lp2387
VWe use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario.
p2388
aVTwo agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from.
p2389
aVIn particular, we compare the Q-learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate.
p2390
aVOur results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly.
p2391
aVWe also show that very high gradually decreasing exploration rates are required for convergence.
p2392
aVWe conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.
p2393
ag106
asba(icmyPackage
FText
p2394
(dp2395
g3
(lp2396
VHowever, their model has a high order of time complexity, and thus cannot be applied in practice
p2397
aVOur model adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers
p2398
aVTo enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF
p2399
aVMoreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy
p2400
aVWhile research in discourse parsing can be partitioned into several directions according to different theories and frameworks, Rhetorical Structure Theory (RST) [ 12 ] is probably the most ambitious one, because it aims to identify not only the discourse relations in a small local context, but also the hierarchical tree structure for the full text from the relations relating the smallest discourse units (called elementary discourse units, EDUs), to the ones connecting paragraphs
p2401
aV4 are further related by a multi-nuclear relation Sequence , with both spans as the nucleus
p2402
aVConventionally, there are two major sub-tasks related to text-level discourse parsing
p2403
aVSince the first sub-task is considered relatively easy, with the state-of-art accuracy at above 90% [ 7 ] , the recent research focus is on the second sub-task, and often uses manual EDU segmentation
p2404
aVCKY parsing is a bottom-up parsing algorithm which searches all possible parsing paths by dynamic programming
p2405
aVTherefore, despite its superior performance, their model is infeasible in most realistic situations
p2406
aVFirst, with a greedy bottom-up strategy, we develop a discourse parser with a time complexity linear in the total number of sentences in the document
p2407
aVAs a result of successfully avoiding the expensive non-greedy parsing algorithms, our discourse parser is very efficient in practice
p2408
aVSecond, by using two linear-chain CRFs to label a sequence of discourse constituents, we can incorporate contextual information in a more natural way, compared to using traditional discriminative classifiers, such as SVMs
p2409
aVThird, after a discourse (sub)tree is fully built from bottom up, we perform a novel post-editing process by considering information from the constituents on upper levels
p2410
aVHowever, HILDA u'\u005cu2019' s approach also has obvious weakness the greedy algorithm may lead to poor performance due to local optima, and more importantly, the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account
p2411
aVDue to the O u'\u005cu2062' ( n 3 ) time complexity, where n is the number of input discourse units, for large documents, the parsing simply takes too long 1 1 The largest document in the RST-DT contains over 180 sentences, i.e.,, n 180 for their multi-sentential CKY parsing
p2412
aVIt is possible to optimize Joty et al u'\u005cu2019' s CKY-like parsing by replacing their CRF-based computation for upper-level constituents with some local computation based on the probabilities of lower-level constituents
p2413
aVEach sentence S i , after being segmented into EDUs (not shown in the figure), goes through an intra-sentential bottom-up tree-building model M i u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' a , to form a sentence-level discourse tree T S i , with the EDUs as leaf nodes
p2414
aVAfter that, we apply the intra-sentential post-editing model P i u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' a to modify the generated tree T S i to T S i p , by considering upper-level information
p2415
aVSimilar to sentence-level parsing, we also post-edit T D using P m u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' i to produce the final discourse tree T D p
p2416
aVIn this way, we are able to take into account the sequential information from contextual discourse constituents, which cannot be naturally represented in HILDA with SVMs as local classifiers
p2417
aVTherefore, our model incorporates the strengths of both HILDA and Joty et al u'\u005cu2019' s model, i.e.,, the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs
p2418
aVAs shown by Feng and Hirst ( 2012 ) , for a pair of discourse constituents of interest, the sequential information from contextual constituents is crucial for determining structures
p2419
aVTherefore, it is well motivated to use Conditional Random Fields (CRFs) [ 10 ] , which is a discriminative probabilistic graphical model, to make predictions for a sequence of constituents surrounding the pair of interest
p2420
aVThen, in the tree-building process, we will have to deal with the situations where the joint model yields conflicting predictions it is possible that the model predicts S j = 1 and R j = NO-REL , or vice versa, and we will have to decide which node to trust (and thus in some sense, the structure and the relation is no longer jointly modeled
p2421
aVSecondly, as a joint model, it is mandatory to use a dynamic CRF, for which exact inference is usually intractable or slow
p2422
aV5 , u'\u005cu2026' , e m } , and so on
p2423
aVBecause the structure model is the first component in our pipeline of local models, its accuracy is crucial
p2424
aVTherefore, to improve its accuracy, we enforce additional commonsense constraints in its Viterbi decoding
p2425
aVSince the computation of E i does not depend on a particular pair of constituents, we can use the same sequence E i to compute structural probabilities for all adjacent constituents
p2426
aVIn contrast, Joty et al u'\u005cu2019' s computation of intra-sentential sequences depends on the particular pair of constituents the sequence is composed of the pair in question, with other EDUs in the sentence, even if those EDUs have already been merged
p2427
aVThus, different CRF chains have to be formed for different pairs of constituents
p2428
aVIn addition to efficiency, our use of a single CRF chain for all constituents can better capture the sequential dependencies among context, by taking into account the information from partially built discourse constituents, rather than bottom-level EDUs only
p2429
aVInstead, we choose to take a sliding-window approach to form CRF chains for a particular pair of constituents, as shown in Figure 6
p2430
aVFor example, suppose we wish to compute the structural probability for the pair U j - 1 and U j , we form three chains, each of which contains two contextual constituents
p2431
aVSimilar to M i u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' a s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' u u'\u005cu2062' c u'\u005cu2062' t , for M m u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' i s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' u u'\u005cu2062' c u'\u005cu2062' t , we also include additional constraints in the Viterbi decoding, by disallowing transitions between two ones, and disallowing the sequence to be all zeros if it contains all the remaining constituents in the document
p2432
aVRather, each relation node R j attempts to model the relation of one single constituent U j , by taking U j u'\u005cu2019' s left and right subtrees U j , L and U j , R as its first-layer nodes; if U j is a single EDU, then the first-layer node of R j is simply U j , and R j is a special relation symbol LEAF 3 3 These leaf constituents are represented using a special feature vector is_leaf = True ; thus the CRF never labels them with relations other than LEAF
p2433
aVSince we know, a priori, that the constituents in the chains are either leaf nodes or the ones that have been merged by our structure model, we never need to worry about the NO-REL issue as outlined in Section 4.1
p2434
aVIn fact, by performing inference on this chain, we produce predictions not only for R j , but also for all other R nodes in the chain, which correspond to all other constituents in the sentence
p2435
aVSince those non-leaf constituents are already labeled in previous steps in the tree-building, we can now re-assign their relations if the model predicts differently in this step
p2436
aVTherefore, this re-labeling procedure can compensate for the loss of accuracy caused by our greedy bottom-up strategy to some extent
p2437
aVSimilar to M m u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' i s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' u u'\u005cu2062' c u'\u005cu2062' t introduced in Section 4.2.2 , M m u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' i r u'\u005cu2062' e u'\u005cu2062' l also takes a sliding-window approach to predict labels for constituents in a local context
p2438
aVAfter an intra- or multi-sentential discourse tree is fully built, we perform a post-editing to consider possible modifications to the current tree, by considering useful information from the discourse constituents on upper levels, which is unavailable in the bottom-up tree-building process
p2439
aVThe motivation for post-editing is that, some particular discourse relations, such as Textual-Organization , tend to occur on the top levels of the discourse tree; thus, information such as the depth of the discourse constituent can be quite indicative
p2440
aVHowever, the exact depth of a discourse constituent is usually unknown in the bottom-up tree-building process; therefore, it might be beneficial to modify the tree by including top-down information after the tree is fully built
p2441
aVIdentify the lowest level of T on which the constituents can be modified according to the post-editing structure component, P s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' u u'\u005cu2062' c u'\u005cu2062' t
p2442
aVTo do so, we maintain a list L to store the discourse constituents that need to be examined
p2443
aVIf the predicted pair is not merged in the original tree T , then a possible modification is located; otherwise, we merge the pair, and proceed to the next iteration
p2444
aVIf modifications have been proposed in the previous step, we build a new tree T p using P s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' u u'\u005cu2062' c u'\u005cu2062' t as the structure model, and P r u'\u005cu2062' e u'\u005cu2062' l as the relation model, from the constituents on which modifications are proposed
p2445
aV1 \u005cState \u005cReturn T \u005cComment Do nothing if it is a single EDU
p2446
aVThe following analysis is focused on the bottom-up tree-building process, but a similar analysis can be carried out for the post-editing process
p2447
aVSince the number of operations in the post-editing process is roughly the same (1.5 times in the worst case) as in the bottom-up tree-building, post-editing shares the same complexity as the tree-building
p2448
aVFor each sentence S k with m k EDUs, the overall time complexity to perform intra-sentential parsing is O u'\u005cu2062' ( m k 2
p2449
aVThe reason is the following
p2450
aVStarting from the EDUs on the bottom level, we need to perform inference for one chain on each level during the bottom-up tree-building, and thus the total time complexity is u'\u005cu03a3' i = 1 m k u'\u005cu2062' O u'\u005cu2062' ( m k - i ) = O u'\u005cu2062' ( m k 2 )
p2451
aVIt is fairly safe to assume that each m k is a constant, in the sense that m k is independent of the total number of sentences in the document
p2452
aVTherefore, the total time complexity u'\u005cu03a3' k = 1 n u'\u005cu2062' O u'\u005cu2062' ( m k 2 ) u'\u005cu2264' n × O u'\u005cu2062' ( max 1 u'\u005cu2264' j u'\u005cu2264' n u'\u005cu2061' ( m j 2 ) ) = n × O u'\u005cu2062' ( 1 ) = O u'\u005cu2062' ( n ) , i.e.,, linear in the total number of sentences
p2453
aVFor multi-sentential models, M m u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' i s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' u u'\u005cu2062' c u'\u005cu2062' t and M m u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' i r u'\u005cu2062' e u'\u005cu2062' l , as shown in Figures 6 and 9 , for a pair of constituents of interest, we generate multiple chains to predict the structure or the relation
p2454
aVBy including a constant number k of discourse units in each chain, and considering a constant number l of such chains for computing each adjacent pair of discourse constituents ( k = 4 for M m u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' i s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' u u'\u005cu2062' c u'\u005cu2062' t and k = 3 for M m u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' i r u'\u005cu2062' e u'\u005cu2062' l ; l = 3 ), we have an overall time complexity of O u'\u005cu2062' ( n
p2455
aVAdopting a greedy approach, on an arbitrary level during the tree-building, once we decide to merge a certain pair of constituents, say U j and U j + 1 , we only need to recompute a small number of chains, i.e.,, the chains which originally include U j or U j + 1 , and inference on each chain takes O u'\u005cu2062' ( 1
p2456
aVTherefore, the total time complexity is ( n - 1 ) × O u'\u005cu2062' ( 1 ) + ( n - 1 ) × O u'\u005cu2062' ( 1 ) = O u'\u005cu2062' ( n ) , where the first term in the summation is the complexity of computing all chains on the bottom level, and the second term is the complexity of computing the constant number of chains on higher levels
p2457
aVWe have thus showed that the time complexity is linear in n , which is the number of sentences in the document
p2458
aVNevertheless, a careful caching strategy can accelerate feature computation, since a large number of multi-sentential chains overlap with each other
p2459
aVwhether each unit corresponds to a single syntactic subtree, and if so, the top PoS tag of the subtree; the distance of each unit to their lowest common ancestor in the syntax tree (intra-sentential only
p2460
aVThe cue phrase list is based on the connectives collected by Knott and Dale ( 1994
p2461
aVFor evaluating relations, since there is a skewed distribution of different relation types in the corpus, we also include the macro-averaged F1-score (MAFS) 7 7 MAFS is the F1-score averaged among all relation classes by equally weighting each class
p2462
aVTherefore, we cannot conduct significance test between different MAFS as another metric, to emphasize the performance of infrequent relation types
p2463
aVWe report the MAFS separately for the correctly retrieved constituents (i.e.,, the span boundary is correct) and all constituents in the reference tree
p2464
aVAs demonstrated by Table 1 , our greedy CRF models perform significantly better than the other two models
p2465
aVSince we do not have the actual output of Joty et al u'\u005cu2019' s model, we are unable to conduct significance testing between our models and theirs
p2466
aVThese four relation classes, apart from their infrequency in the corpus, are more abstractly defined, and thus are particularly challenging
p2467
aVWe further illustrate the efficiency of our parser by demonstrating the time consumption of different models
p2468
aVFirst, as shown in Table 2 , the average number of sentences in a document is 26.11, which is already too large for optimal parsing models, e.g.,, the CKY-like parsing algorithm in j CRF, let alone the fact that the largest document contains several hundred of EDUs and sentences
p2469
aVTherefore, it should be seen that non-optimal models are required in most cases
p2470
aVIn Table 3 , we report the parsing time 8 8 Tested on a Linux system with four duo-core 3.0GHz processors and 16G memory for the last three models, since we do not know the time of j CRF
p2471
aVAs can be seen, our g CRF model is considerably faster than g SVM F u'\u005cu2062' H , because, on one hand, feature computation is expensive in g SVM F u'\u005cu2062' H , since g SVM F u'\u005cu2062' H utilizes a rich set of features; on the other hand, in g CRF, we are able to accelerate decoding by multi-threading MALLET (we use four threads
p2472
aVOur approach was to adopt a greedy bottom-up tree-building, with two linear-chain CRFs as local probabilistic models, and enforce reasonable constraints in the first CRF u'\u005cu2019' s Viterbi decoding
p2473
aVIn addition, we propose a novel idea of post-editing, which modifies a fully-built discourse tree by considering information from upper-level constituents
p2474
aVIn future work, we wish to further explore the idea of post-editing, since currently we use only the depth of the subtrees as upper-level information
p2475
aVMoreover, we wish to study whether we can incorporate constraints into the relation models, as we do to the structure models
p2476
asg88
(lp2477
sg90
(lp2478
sg92
(lp2479
VIn this paper, we presented an efficient text-level discourse parser with time complexity linear in the total number of sentences in the document.
p2480
aVOur approach was to adopt a greedy bottom-up tree-building, with two linear-chain CRFs as local probabilistic models, and enforce reasonable constraints in the first CRF s Viterbi decoding.
p2481
aVWhile significantly outperforming the state-of-the-art model by Joty et al.
p2482
aV2013 ) , our parser is much faster in practice.
p2483
aVIn addition, we propose a novel idea of post-editing, which modifies a fully-built discourse tree by considering information from upper-level constituents.
p2484
aVWe show that, although doubling the time consumption, post-editing can further boost the parsing performance to close to 90% of human performance.
p2485
aVIn future work, we wish to further explore the idea of post-editing, since currently we use only the depth of the subtrees as upper-level information.
p2486
aVMoreover, we wish to study whether we can incorporate constraints into the relation models, as we do to the structure models.
p2487
aVFor example, it might be helpful to train the relation models using additional criteria, such as Generalized Expectation [ 11 ] , to better take into account some prior knowledge about the relations.
p2488
aVLast but not least, as reflected by the low MAFS in our experiments, some particularly difficult relation types might need specifically designed features for better recognition.
p2489
ag106
asg107
S'P14-1048'
p2490
sg109
(lp2491
VText-level discourse parsing remains a challenge.
p2492
aVThe current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al.
p2493
aV2013.
p2494
aVHowever, their model has a high order of time complexity, and thus cannot be applied in practice.
p2495
aVIn this work, we develop a much faster model whose time complexity is linear in the number of sentences.
p2496
aVOur model adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers.
p2497
aVTo enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF.
p2498
aVIn addition to efficiency, our parser also significantly outperforms the state of the art.
p2499
aVMoreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy.
p2500
aV*EndWhile \u005calgtext *EndIf \u005calgtext *EndFor.
p2501
ag106
asba(icmyPackage
FText
p2502
(dp2503
g3
(lp2504
VIn the previous example, u'\u005cu201d' u'\u005cu7ed9' u'\u005cu529b' (cool; powerful) u'\u005cu201d' is a strong feature for classification models while each single character is not
p2505
aVAdding new words as feature in classification models will improve the performance of polarity classification, as demonstrated later in this paper
p2506
aVMoreover, existing lexical resources never have adequate and timely coverage since new words appear constantly
p2507
aVPeople thus resort to statistical methods such as Pointwise Mutual Information [ 6 ] , Symmetrical Conditional Probability [ 7 ] , Mutual Expectation [ 8 ] , Enhanced Mutual Information [ 22 ] , and Multi-word Expression Distance [ 2 ]
p2508
aVWe will describe the proposed method in Section 3, including definitions, the overview of the algorithm, and the statistical measures for addressing the two key issues
p2509
aVIn these works, new word detection is considered as an integral part of segmentation, where new words are identified as the most probable segments inferred by the probabilistic models; and the detected new word can be further used to improve word segmentation
p2510
aVTypical models include conditional random fields proposed by [ 14 ] , and a joint model trained with adaptive online gradient descent based on feature frequency information [ 18 ]
p2511
aVAnother line is to treat new word detection as a separate task, usually preceded by part-of-speech tagging
p2512
aVThe first genre of such studies is to leverage complex linguistic rules or knowledge
p2513
aVThe second genre of the studies is to treat new word detection as a classification problem
p2514
aVZhou [ 25 ] proposed a discriminative Markov Model to detect new words by chunking one or more separated words
p2515
aVIn [ 12 ] , new word detection was viewed as a binary classification problem
p2516
aVHowever, these supervised models requires not only heavy engineering of linguistic features, but also expensive annotation of training data
p2517
aVUser behavior data has recently been explored for finding new words
p2518
aVHowever, both of the work are limited due to the public unavailability of expensive commercial resources
p2519
aVIn this setting, new word detection is mostly known as multi-word expression extraction
p2520
aVTo measure multi-word association, the first model is Pointwise Mutual Information (PMI) [ 6 ]
p2521
aVSince then, a variety of statistical methods have been proposed to measure b u'\u005cu2062' i -gram association, such as Log-likelihood [ 9 ] and Symmetrical Conditional Probability (SCP) [ 7 ]
p2522
aVIn order to measure arbitrary n -grams, most common strategies are to separate n -gram into two parts X and Y so that existing b u'\u005cu2062' i -gram methods can be used [ 7 , 8 , 16 ]
p2523
aVIn Chinese, such words are like u'\u005cu201d' u'\u005cu7740' , u'\u005cu4e86' , u'\u005cu5566' , u'\u005cu7684' , u'\u005cu554a' u'\u005cu201d' , and punctuation marks include u'\u005cu201d' u'\u005cuff0c' u'\u005cu3002' u'\u005cuff01' u'\u005cuff1f' u'\u005cuff1b' u'\u005cuff1a' u'\u005cu201d' and so on
p2524
aVA lexical pattern is a triplet A D , * , A U , where A u'\u005cu2062' D is an adverbial word, the wildcard * means an arbitrary number of words 1 1 We set the number to 3 words in this work considering computation costs and A u'\u005cu2062' U denotes an auxiliary word
p2525
aVIn order to obtain lexical patterns, we can define regular expressions with POS tags 2 2 Such expressions are very simple and easy to write because we only need to consider POS tags of adverbial and auxiliary word and apply the regular expressions on POS tagged texts
p2526
aVSince the tags of adverbial and auxiliary words are relatively static and can be easily identified, such a method can safely obtain lexical patterns
p2527
aVNote that we do not augment the pattern set ( u'\u005cud835' u'\u005cudcab' ) at each iteration, instead, we keep a fixed small number of patterns during iteration because this strategy produces optimal results
p2528
aVFrom linguistic perspectives, new sentiment words are commonly modified by adverbial words and thus can be extracted by lexical patterns
p2529
aVThis is the reason why the algorithm will work
p2530
aVIn our problem, LRT computes a contingency table of a pattern p and a word w , derived from the corpus statistics, as given in Table 3 , where k 1 u'\u005cu2062' ( w , p ) is the number of documents that w matches pattern p , k 2 u'\u005cu2062' ( w , p ¯ ) is the number of documents that w occurs while p does not, k 3 u'\u005cu2062' ( w ¯ , p ) is the number of documents that p occurs while w does not, and k 4 u'\u005cu2062' ( w ¯ , p ¯ ) is the number of documents containing neither p nor w
p2531
aVBased on the statistics shown in Table 3 , the likelihood ratio tests (LRT) model captures the statistical association between a pattern p and a word w by employing the following formula
p2532
aVL u'\u005cu2062' ( u'\u005cu03a1' , k , n ) = u'\u005cu03a1' k * ( 1 - u'\u005cu03a1' ) n - k ; n 1 = k 1 + k 3 ; n 2 = k 2 + k 4 ; u'\u005cu03a1' 1 = k 1 / n 1 ; u'\u005cu03a1' 2 = k 2 / n 2 ; u'\u005cu03a1' = ( k 1 + k 2 ) / ( n 1 + n 2 )
p2533
aVThus, the utility of a pattern can be measured as follows
p2534
aVThis has linguistic interpretations because new sentiment words are commonly modified by adverbial words and thus should have close association with lexical patterns
p2535
aVIf a candidate word is a new word, it will be more commonly used with diversified lexical patterns since the non-compositionality of new word means that the word can be used in many different linguistic scenarios
p2536
aVNote that we use u'\u005cud835' u'\u005cudcab' c , instead of u'\u005cud835' u'\u005cudcab' , because the latter set is very small while computing entropy needs a large number of patterns
p2537
aVFor example, u'\u005cu201d' u'\u005cu7231' u'\u005cu5403' (love to eat) u'\u005cu201d' and u'\u005cu201d' u'\u005cu7231' u'\u005cu8bf4' (love to talk) u'\u005cu201d' can be matched by many lexical patterns, however, they are not new words due to the lack of non-compositionality
p2538
aVIn such words, each single character has high probability to be a word
p2539
aVThus, we design the following measure to favor this observation
p2540
aVwhere w = w 1 u'\u005cu2062' w 2 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' w n , each w i is a single character, and p u'\u005cu2062' ( w i ) is the probability of the character w i being a word, as computed as follows
p2541
aVwhere a u'\u005cu2062' l u'\u005cu2062' l u'\u005cu2062' ( w i ) is the total frequency of w i , and s u'\u005cu2062' ( w i ) is the frequency of w i being a single character word
p2542
aVNew words are usually multi-word expressions, where a variety of statistical measures have been proposed to detect multi-word expressions
p2543
aVThus, such measures can be naturally incorporated into our algorithm
p2544
aVwhere u'\u005cu039c' u'\u005cu2062' ( w ) is the set of documents in which all single words in w = w 1 u'\u005cu2062' w 2 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' w n co-occur, u'\u005cu03a6' u'\u005cu2062' ( w ) is the set of documents in which word w occurs as a whole, and N is the total number of documents
p2545
aVDifferent from EMI, this measure is a strict distance metric, meaning that a smaller value indicates a larger possibility of being a multi-word expression
p2546
aVAs can be seen from the formula, the key idea of this metric is to compute the ratio of the co-occurrence of all words in a multi-word expressions to the occurrence of the whole expression
p2547
aVTaking into account the aforementioned factors, we have different settings to score a new word, as follows
p2548
aVIf there is a disagreement on either of the two tasks, discussions are required to make the final decision
p2549
aVThe annotation led to 323 new words, among which there are 116 positive words, 112 negative words, and 95 neutral words 3 3 All the resources are available upon request
p2550
aVAs our algorithm outputs a ranked list of words, we adapt average precision to evaluate the performance of new sentiment word detection
p2551
aVwhere P u'\u005cu2062' ( k ) is the precision at cut-off k , r u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( k ) is 1 if the word at position k is a new word and 0 otherwise, and K is the number of words in the ranked list
p2552
aVFirst, we assess the influence of likelihood ratio test, which measures the association of a word to the pattern set
p2553
aVAs can be seen from Table 4 , the association model (LRT) remarkably boosts the performance of new word detection, indicating LRT is a key factor for new sentiment word extraction
p2554
aVFrom linguistic perspectives, new sentiment words are commonly modified by adverbial words and thus should have close association with lexical patterns
p2555
aVThe results are shown in Figure 1
p2556
aVAs can be seen, all the proposed measures outperform the two baselines ( E u'\u005cu2062' M u'\u005cu2062' I and N u'\u005cu2062' M u'\u005cu2062' E u'\u005cu2062' D ) remarkably and consistently
p2557
aVAdding N u'\u005cu2062' M u'\u005cu2062' E u'\u005cu2062' D or E u'\u005cu2062' M u'\u005cu2062' I leads to remarkable improvements because of their capability of measuring non-compositionality of new words
p2558
aVOnly using L u'\u005cu2062' R u'\u005cu2062' T can obtain a fairly good results when K is small, however, the performance drops sharply because it u'\u005cu2019' s unable to measure non-compositionality
p2559
aVThe measure setting we take here is F N u'\u005cu2062' M u'\u005cu2062' E u'\u005cu2062' D u'\u005cu2062' ( w ) , as shown in Formula ( 12
p2560
aVAgain, we choose only one seed word u'\u005cu201d' u'\u005cu5751' u'\u005cu7239' (reverse one u'\u005cu2019' s expectation) u'\u005cu201d' , and the number of words returned is set to K = 300
p2561
aVNote that at the early stage of Algorithm 1, larger k p (perhaps with noisy patterns) may lead to lower quality of new words; while larger k w (perhaps with noisy seed words) may lead to lower quality of lexical patterns
p2562
aVTherefore, we choose the optimal setting to small numbers, as k p = 5 , k w = 10
p2563
aVBy looking into the pattern set and the selected words at each iteration, we found that the pattern set ( u'\u005cud835' u'\u005cudcab' ) converges soon to the same set after a few iterations; and at the beginning several iterations, the selected words are almost the same although the order of adding the words is different
p2564
aVSince the algorithm will finally sort the words at step (11) and u'\u005cud835' u'\u005cudcab' is the same, the ranking of the words becomes all the same
p2565
aVLastly, we need to decide the optimal number of patterns in u'\u005cud835' u'\u005cudcab' c (that is, k c in Algorithm 1) because the set has been used in computing left pattern entropy, see Formula ( 4
p2566
aVToo small size of u'\u005cud835' u'\u005cudcab' c may lead to insufficient estimation of left pattern entropy
p2567
aVResults in Table 7 shows that larger u'\u005cud835' u'\u005cudcab' c decrease the performance, particularly when the number of words returned ( K ) becomes larger
p2568
aVTherefore, we set u'\u005cud835' u'\u005cudcab' c
p2569
aVThe polarity is judged according to this rule if M u'\u005cu2062' V u'\u005cu2062' ( w ) t u'\u005cu2062' h 1 , the word w is positive; if M u'\u005cu2062' V u'\u005cu2062' ( w ) - t u'\u005cu2062' h 1 the word negative; otherwise neutral
p2570
aVThe polarity is judged according to the rule if P u'\u005cu2062' M u'\u005cu2062' I u'\u005cu2062' ( w ) t u'\u005cu2062' h 2 , w is positive; if P u'\u005cu2062' M u'\u005cu2062' I u'\u005cu2062' ( w ) - t u'\u005cu2062' h 2 negative; otherwise neutral
p2571
aVThe threshold t u'\u005cu2062' h 2 is manually tuned
p2572
aVAs for the resources P u'\u005cu2062' W and N u'\u005cu2062' W , we have three settings
p2573
aVWe conjecture that this may be because new sentiment words are more frequently co-occurring with emoticons than with these opinion words
p2574
aVThe second observation is that three-class polarity classification is much more difficult than two-class polarity classification because many extracted new words are nouns such as u'\u005cu201d' u'\u005cu57fa' u'\u005cu53cb' (gay) u'\u005cu201d' , u'\u005cu201d' u'\u005cu83c7' u'\u005cu51c9' (girl) u'\u005cu201d' , and u'\u005cu201d' u'\u005cu76c6' u'\u005cu53cb' (friend) u'\u005cu201d'
p2575
aVThe first model is a lexicon-based model (denoted by L u'\u005cu2062' e u'\u005cu2062' x u'\u005cu2062' i u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' n ) that counts the number of positive and negative opinion words in a post respectively, and classifies a post to be positive if there are more positive words than negative ones, and to be negative otherwise
p2576
aVThe second model is a SVM model in which opinion words are used as feature, and 5-fold cross validation is conducted
p2577
aVAll words in the top 100 words can be used as feature we thus manually label the polarity of all top 100 words (we did NOT remove incorrect new word
p2578
aVNote, that T u'\u005cu2062' 100 is automatically obtained from Algorithm 1 so that it may contain words that are not new sentiment words, but the resource also improves performance remarkably
p2579
aVFrom linguistic perspectives, our framework is capable to extract adjective new words because the lexical patterns usually modify adjective words
p2580
aVAs future work, we are considering how to extract other types of new sentiment words, such as nounal new words that can express sentiment
p2581
asg88
(lp2582
sg90
(lp2583
sg92
(lp2584
VIn order to extract new sentiment words from large-scale user-generated content, this paper proposes a fully unsupervised, purely data-driven, and almost knowledge-free (except POS tags) framework.
p2585
aVWe design statistical measures to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word, respectively.
p2586
aVThe method is almost free of linguistic resources (except POS tags), and does not rely on elaborated linguistic rules.
p2587
aVWe conduct extensive experiments to reveal the influence of different statistical measures in new word finding.
p2588
aVComparative experiments show that our proposed method outperforms baselines remarkably.
p2589
aVExperiments also demonstrate that inclusion of new sentiment words benefits sentiment classification definitely.
p2590
aVFrom linguistic perspectives, our framework is capable to extract adjective new words because the lexical patterns usually modify adjective words.
p2591
aVAs future work, we are considering how to extract other types of new sentiment words, such as nounal new words that can express sentiment.
p2592
ag106
asg107
S'P14-1050'
p2593
sg109
(lp2594
VAutomatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation, named entity extraction, and sentiment analysis.
p2595
aVThis paper aims at extracting new sentiment words from large-scale user-generated content.
p2596
aVWe propose a fully unsupervised, purely data-driven framework for this purpose.
p2597
aVWe design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word.
p2598
aVThe method is almost free of linguistic resources (except POS tags), and requires no elaborated linguistic rules.
p2599
aVWe also demonstrate how new sentiment word will benefit sentiment analysis.
p2600
aVExperiment results demonstrate the effectiveness of the proposed method.
p2601
aV zh \u005cXeTeXlinebreakskip =0ptplus1pt.
p2602
ag106
asba(icmyPackage
FText
p2603
(dp2604
g3
(lp2605
VWe study the problem of generating an English sentence given an underlying probabilistic grammar, a world and a communicative goal
p2606
aVWe model the generation problem as a Markov decision process with a suitably defined reward function that reflects the communicative goal
p2607
aVThe problem is restricted because in our work, we do not consider the issue of how to fragment a complex goal into multiple sentences (discourse planning
p2608
aVOther approaches view NLG as a planning problem [ 10 ]
p2609
aVHere, the communicative goal is treated as a predicate to be satisfied, and the grammar and vocabulary are suitably encoded as logical operators
p2610
aVA key limitation is the logical nature of automated planning systems, which do not handle probabilistic grammars, or force ad-hoc approaches for doing so [ 1 ]
p2611
aVA second limitation comes from restrictions on the goal it may be difficult to ensure that some specific piece of information should not be communicated, or to specify preferences over communicative goals, or specify general conditions, like that the sentence should be readable by a sixth grader
p2612
aVA third limitation comes from the search process without strong heuristics, most planners get bogged down when given communicative goals that require chaining together long sequences of operators [ 11 ]
p2613
aVThis setting allows us to address the limitations outlined above it is naturally probabilistic, and handles probabilistic grammars; we are able to specify complex communicative goals and general criteria through a suitably-defined reward function; and, as we show in our experiments, recent developments in fast planning in large MDPs result in a generation system that can rapidly deal with very specific communicative goals
p2614
aVFinally, the decision-theoretic setting allows for a precise tradeoff between exploration of the grammar and vocabulary to find a better solution and exploitation of the current most promising (partial) solution, instead of a heuristic search through the solution space as performed by standard planning approaches
p2615
aVBelow, we first describe related work, followed by a detailed description of our approach
p2616
aVOne direction can be thought of as u'\u005cu201c' overgeneration and ranking u'\u005cu201d' Here some (possibly probabilistic) structure is used to generate multiple candidate sentences, which are then ranked according to how well they satisfy the generation criteria
p2617
aVThis includes work based on chart generation and parsing [ 17 , 6 ]
p2618
aVThese generators assign semantic meaning to each individual token, then use a set of rules to decide if two words can be combined
p2619
aVUsing dynamic programming, the highest ranked sentence from this structure is then output
p2620
aVA second line of attack formalizes NLG as an AI planning problem
p2621
aVSPUD [ 18 ] , a system for NLG through microplanning, considers NLG as a problem which requires realizing a deliberative process of goal-directed activity
p2622
aVThis chain is sometimes referred to as the u'\u005cu201c' NLG Pipeline u'\u005cu201d' [ 16 ]
p2623
aVAnother approach, called integrated generation , considers both sentence generation portions of the pipeline together [ 10 ]
p2624
aVThese generators generate parses for the sentence at the same time as the sentence, which keeps them from generating realizations that are grammatically incorrect, and keeps them from generating grammatical structures that cannot be realized properly
p2625
aVThe first step finds an adjoining location by searching through our sentence to find any subtree with a root whose label matches the root node of the auxiliary tree
p2626
aVIn the second step, the target subtree is removed from the sentence tree, and placed in the auxiliary tree as a direct replacement for the foot node
p2627
aVFinally, the modified auxiliary tree is placed back in the sentence tree in the original target location
p2628
aVRecent approaches such as PCRISP [ 1 ] attempt to remedy this, but do so in a somewhat ad-hoc way, by transforming the probabilities into costs, because they rely on deterministic planning to actually realize the output
p2629
aVIn this work, we directly address this by using a more expressive underlying formalism, a Markov decision process (MDP
p2630
aVEntities are defined as any element anchored by precisely one node in the tree which can appear in a statement representing the semantic content of the tree
p2631
aVWe refer to this list as a lexicon
p2632
aVEach word in the lexicon is annotated with its first-order logic semantics with any number of entities present in its subtree as the arguments
p2633
aVA world specification is simply a list of all statements which are true in the world surrounding our generation
p2634
aVBefore execution begins, our grammar is pruned to remove entries which cannot possibly be used in generation for the given problem, by transitively discovering all predicates that hold about the entities mentioned in the goal in the world, and eliminating all trees not about any of these
p2635
aVThis often allows STRUCT to be resilient to large grammar sizes, as our experiments will show
p2636
aVWe formulate NLG as a planning problem on a Markov decision process (MDP) [ 15 ]
p2637
aVIn the MDP we use for NLG, we must define each element of the tuple in such a way that a plan in the MDP becomes a sentence in a natural language
p2638
aVOur set of states, therefore, will be partial sentences which are in the language defined by our PLTAG input
p2639
aVThere are an infinite number of these states, since TAG adjoins can be repeated indefinitely
p2640
aVNonetheless, given a specific world and communicative goal, only a fraction of this MDP needs to be explored, and, as we show below, a good solution can often be found quickly using a variation of the UCT algorithm [ 9 ]
p2641
aVSince we are using PLTAGs in this work, this means every action adds a word to the partial sentence
p2642
aVBased on these state and action definitions, the transition function takes a mapping between a partial sentence / action pair and the partial sentences which can result from one particular PLTAG adjoin / substitution, and returns the probability of that rule in the grammar
p2643
aVIn order to control the search space, we restrict the structure of the MDP so that while substitutions are available, only those operations are considered when determining the distribution over the next state, without any adjoins
p2644
aVThis allows STRUCT to operate as an anytime algorithm, described further below
p2645
aVThe immediate value of a state, intuitively, describes closeness of an arbitrary partial sentence to our communicative goal
p2646
aVEach partial sentence is annotated with its semantic information, built up using the semantic annotations associated with the PLTAG trees
p2647
aVThus we use as a reward a measure of the match between the semantic annotation of the partial tree and the communicative goal
p2648
aVFor an exact reward signal, when checking this overlap, we need to substitute each combination of entities in the goal into predicates in the sentence so we can return a high value if there are any mappings which are both possible (contain no statements which are not present in the grounded world) and mostly fulfill the goal (contain most of the goal predicates
p2649
aVHowever, this is combinatorial; also, most entities within sentences do not interact (e.g., if we say u'\u005cu201c' the white rabbit jumped on the orange carrot, u'\u005cu201d' the whiteness of the rabbit has nothing to do with the carrot), and finally, an approximate reward signal generally works well enough unless we need to emit nested subclauses
p2650
aVThus as an approximation, we use a reward signal where we simply count how many individual predicates overlap with the goal with some entity substitution
p2651
aVWe generally use a discount factor of 1; this is because we are willing to generate lengthy sentences in order to ensure we match our goal
p2652
aVA discount factor of 1 can be problematic in general since it can cause rewards to diverge, but since there are a finite number of terms in our reward function (determined by the communicative goal and the fact that because of lexicalization we do not loop), this is not a problem for us
p2653
aVUsing the UCT approach with a suitably defined MDP (explained above) allows us to naturally handle probabilistic grammars as well as formulate NLG as a planning problem, unifying the distinct lines of attack described in Section 2
p2654
aVFurther, the theoretical guarantees of UCT translate into fast generation in many cases, as we demonstrate in our experiments
p2655
aVIn order to build a lookahead tree, we use a u'\u005cu201c' rollout policy u'\u005cu201d' This policy has two components if it encounters a state already in the tree, it follows a u'\u005cu201c' tree policy, u'\u005cu201d' discussed further below
p2656
aVIf it encounters a new state, the policy reverts to a u'\u005cu201c' default u'\u005cu201d' policy that randomly samples an action
p2657
aVBecause this is a Monte Carlo estimate, typically, we run several simultaneous trials, and we keep track of the rewards received by each choice and use this to select the best action at the root
p2658
aVHere Q u'\u005cu2062' ( s , a ) is the estimated value of a as observed in the tree search, computed as a sum over future rewards observed after ( s , a
p2659
aVN u'\u005cu2062' ( s ) and N u'\u005cu2062' ( s , a ) are visit counts for the state and state-action pair
p2660
aVThus the second term is an exploration term that biases the algorithm towards visiting actions that have not been explored enough c is a constant that trades off exploration and exploitation
p2661
aVFirst, because we receive frequent, reasonably accurate feedback, we favor breadth over depth in the tree search
p2662
aVSecond, UCT was originally used in an adversarial environment, and so is biased to select actions leading to the best average reward rather than the action leading to the best overall reward
p2663
aVThis is not true for us, however, so we choose the latter action instead
p2664
aVAfter every action is selected and applied, we check to see if we are in a state in which the algorithm could terminate (i.e., the sentence has no nonterminals yet to be expanded
p2665
aVIf so, we determine if this is the best possibly-terminal state we have seen so far
p2666
aVIf so, we store it, and continue the generation process
p2667
aVWhenever we reach a terminal state, we begin again from the start state of the MDP
p2668
aVBecause of the structure restriction above (substitution before adjoin), STRUCT generates a valid sentence quickly
p2669
aVThis enables STRUCT to perform as an anytime algorithm, which if interrupted will return the highest-value complete and valid sentence it has found
p2670
aVThis also allows partial completion of communicative goals if not all goals can be achieved simultaneously in the time given
p2671
aVIn this section, we compare STRUCT to a state-of-the-art NLG system, CRISP, 1 1 We were unfortunately unable to get the PCRISP system to compile, and so we could not evaluate it and evaluate three hypotheses i) STRUCT is comparable in speed and generation quality to CRISP as it generates increasingly large referring expressions, (ii) STRUCT is comparable in speed and generation quality to CRISP as the size of the grammar which they use increases, and (iii) STRUCT is capable of communicating complex propositions, including multiple concurrent goals, negated goals, and nested subclauses
p2672
aVThe times reported are from the start of the generation process, eliminating variations due to interpreter startup, input parsing, etc
p2673
aVWe begin by describing experiments comparing STRUCT to CRISP
p2674
aVIn this experiment, m u'\u005cu2062' a u'\u005cu2062' x u'\u005cu2062' D u'\u005cu2062' e u'\u005cu2062' p u'\u005cu2062' t u'\u005cu2062' h was set equal to 1, since each action taken improved the sentence in a way measurable by our reward function n u'\u005cu2062' u u'\u005cu2062' m u'\u005cu2062' T u'\u005cu2062' r u'\u005cu2062' i u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' s was set equal to k u'\u005cu2062' ( k + 1 ) , since this is the number of adjoining sites available in the final step of generation, times the number of potential words to adjoin
p2675
aVLater experiments had successful referring expression generation of lengths as high as 25
p2676
aVThe u'\u005cu201c' STRUCT_initial u'\u005cu201d' curve shows the time taken by STRUCT to come up with the first complete sentence, which partially solves the goal and which (at least) could be output if generation was interrupted and no better alternative was found
p2677
aVAs can be seen, this always happens very quickly
p2678
aVThis experiment is set up in the same way as the one above, with the exception of l u'\u005cu201c' distracting u'\u005cu201d' words, words which are not useful in the sentence to be generated l is defined as j - k
p2679
aVIf we do not prune the grammar (as described in Section 3.1 ), STRUCT u'\u005cu2019' s performance is similar to CRISP using the FF planner [ 5 ] , also profiled in [ 11 ] , which increased from 27 ms to 4.4 seconds over the interval from j = 1 to j = 10
p2680
aVThis is due almost entirely to the required increase in the value of n u'\u005cu2062' u u'\u005cu2062' m u'\u005cu2062' T u'\u005cu2062' r u'\u005cu2062' i u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' s as the grammar size increases
p2681
aVAt the low end, we can use n u'\u005cu2062' u u'\u005cu2062' m u'\u005cu2062' T u'\u005cu2062' r u'\u005cu2062' i u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' s = 20 , but at l = 50 , we must use n u'\u005cu2062' u u'\u005cu2062' m u'\u005cu2062' T u'\u005cu2062' r u'\u005cu2062' i u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' s = 160 in order to ensure perfect generation as soon as possible
p2682
aVNote that, as STRUCT is an anytime algorithm, valid sentences are available very early in the generation process, despite the size of the set of adjoining trees
p2683
aVSTRUCT u'\u005cu2019' s performance improves significantly if we allow for pruning
p2684
aVOur experiments do not show any significant impact on runtime due to the pruning procedure itself, even on large grammars
p2685
aVIn that section, the referred-to dog was unique, and it was therefore possible to produce a referring expression which identified it unambiguously
p2686
aVIn this experiment, we remove this condition by creating a situation in which the generator will be forced to ambiguously refer to several dogs
p2687
aVSince these adjectives do not further disambiguate their subject, our generator should not use them in its output
p2688
aVWe then encode these adjectives into communicative goals, so that they will be included in the output of the generator despite not assisting in the accomplishment of disambiguation
p2689
aVWe would have as our goal both u'\u005cu201c' sleeps(c) u'\u005cu201d' and u'\u005cu201c' black(c) u'\u005cu201d'
p2690
aVWe find that, in all cases, these otherwise useless adjectives are included in the output of our generator, indicating that STRUCT is successfully balancing multiple communicative goals
p2691
aVAs we show in figure 6 (the u'\u005cu201c' Positive Goals u'\u005cu201d' curve) , the presence of additional satisfiable semantic goals does not substantially affect the time required for generation
p2692
aVWe again modify the problem used earlier by adding to our lexicon several new adjectives, each applicable only to the target of our referring expression
p2693
aVSince our target can now be referred to unambiguously using only one adjective, our generator should just select one of these new adjectives (we experimentally confirmed this
p2694
aVWe then encode these adjectives into negated communicative goals, so that they will not be included in the output of the generator, despite allowing a much shorter referring expression
p2695
aVSince this experiment alters the grammar size, we see the time to final generation growing linearly with grammar size
p2696
aVThis is a case where pruning does not help us in reducing the grammar size; we cannot optimistically prune out words that we do not plan to use
p2697
aVDoing so might reduce the ability of STRUCT to produce a sentence which partially fulfills its goals
p2698
aVNotably, we must adjoin the word u'\u005cu201c' which u'\u005cu201d' to u'\u005cu201c' the dog u'\u005cu201d' during the portion of generation where the sentence reads u'\u005cu201c' the dog chased the cat u'\u005cu201d'
p2699
aVThis decision requires us to do planning deeper than one level in the MDP, which increases the number of simulations STRUCT requires in order to get the correct result
p2700
aVNotice that, because the exact reward function is being used, the time to generate is longer in this experiment
p2701
aVTo the best of our knowledge, CRISP is not able to generate sentences of this form due to an insufficiency in the way it handles TAGs, and consequently we present our results without this baseline
p2702
aVGiven sufficient depth for the search ( d = 3 was sufficient for our experiments, as our reward signal is fine-grained), STRUCT will produce two sentences joined by the conjunction u'\u005cu201c' and u'\u005cu201d'
p2703
aVAgain, we follow prior work in our experiment design [ 11 ]
p2704
aVAs we can see in Figures 10 , 10 , and 10 , STRUCT successfully generates results for conjunctions of up to five sentences
p2705
aVThey attempted to generate sentences with three entities and failed to find a result within their 4 GB memory limit
p2706
aVAs we can see, CRISP generates a result slightly faster than STRUCT when we are working with a single entity, but works much much slower for two entities and cannot generate results for a third entity
p2707
aVAccording to Koller u'\u005cu2019' s findings, this is because the search space grows by a factor of the universe size with the addition of another entity [ 11 ]
p2708
aVSTRUCT formalizes the generation problem as an MDP and applies a version of the UCT algorithm, a fast online MDP planner, to solve it
p2709
aVThus, STRUCT naturally handles probabilistic grammars
p2710
aVA second direction is that, due to the nature of the approach, STRUCT is highly amenable to parallelization
p2711
asg88
(lp2712
sg90
(lp2713
sg92
(lp2714
VWe have proposed STRUCT, a general-purpose natural language generation system which is comparable to current state-of-the-art generators.
p2715
aVSTRUCT formalizes the generation problem as an MDP and applies a version of the UCT algorithm, a fast online MDP planner, to solve it.
p2716
aVThus, STRUCT naturally handles probabilistic grammars.
p2717
aVWe demonstrate empirically that STRUCT is anytime, comparable to existing generation-as-planning systems in certain NLG tasks, and is also capable of handling other, more complex tasks such as negated communicative goals.
p2718
aVThough STRUCT has many interesting properties, many directions for exploration remain.
p2719
aVAmong other things, it would be desirable to integrate STRUCT with discourse planning and dialog systems.
p2720
aVFortunately, reinforcement learning has already been investigated in such contexts [ 13 ] , indicating that an MDP-based generation procedure could be a natural fit in more complex generation systems.
p2721
aVThis is a primary direction for future work.
p2722
aVA second direction is that, due to the nature of the approach, STRUCT is highly amenable to parallelization.
p2723
aVNone of the experiments reported here use parallelization, however, to be fair to CRISP.
p2724
aVWe plan to parallelize STRUCT in future work, to take advantage of current multicore architectures.
p2725
aVThis should obviously further reduce generation time.
p2726
aVSTRUCT is open source and available from github.com upon request.
p2727
ag106
asg107
S'P14-1052'
p2728
sg109
(lp2729
VWe study the problem of generating an English sentence given an underlying probabilistic grammar, a world and a communicative goal.
p2730
aVWe model the generation problem as a Markov decision process with a suitably defined reward function that reflects the communicative goal.
p2731
aVWe then use probabilistic planning to solve the MDP and generate a sentence that, with high probability, accomplishes the communicative goal.
p2732
aVWe show empirically that our approach can generate complex sentences with a speed that generally matches or surpasses the state of the art.
p2733
aVFurther, we show that our approach is anytime and can handle complex communicative goals, including negated goals.
p2734
ag106
asba(icmyPackage
FText
p2735
(dp2736
g3
(lp2737
VIt is inattentive to structure
p2738
aVTherefore, segmenting and parsing Chinese are more difficult and less accurate
p2739
aVThe Omni-word feature uses every potential word in a sentence as lexicon feature, reducing errors caused by word segmentation
p2740
aVThe lack of delimiter also causes the Out-of-Vocabulary problem (OOV, also known as new word detection ) []
p2741
aVThese problems are worsened by the fact that Chinese has a large number of characters and words
p2742
aVThe errors caused by segmentation and OOV will accumulate and propagate to subsequent processing (e.g., part-of-speech (POS) tagging or parsing
p2743
aVTherefore, the Chinese relation extraction is more difficult
p2744
aVAccording to our survey, compared to the same work in English, the Chinese relation extraction researches make less significant progress
p2745
aVBased on the characteristics of Chinese, in this paper, an Omni-word feature and a soft constraint method are proposed for Chinese relation extraction
p2746
aVUnlike the traditional segmentation based method, which is a partition of the sentence, the Omni-word feature uses every potential word in a sentence as lexicon feature
p2747
aVAiming at the Chinese inattentive structure, we utilize the soft constraint to capture the local dependency in a relation instance
p2748
aVBased on massive and heterogeneous corpora, the ORE systems deal with millions or billions of documents
p2749
aVThe TRE paradigm takes hand-tagged examples as input, extracting predefined relation types []
p2750
aVCombining with external semantic resources, a better performance was achieved
p2751
aV[] also pointed out that, due to the inaccuracy of Chinese word segmentation and parsing, the tree kernel based approach is inappropriate for Chinese relation extraction
p2752
aVThe reason of the tree kernel based approach not achieve the same level of accuracy as that from English may be that segmenting and parsing Chinese are more difficult and less accurate than processing English
p2753
aVBoth approaches are based on the Chinese characteristics
p2754
aVTherefore, better performance is expected
p2755
aVThe soft constraint is the method to generate the combine features 1 1 If without ambiguity, we also use the terminology of u'\u005cu201c' soft constraint u'\u005cu201d' denoting features generated by the employed constraint conditions
p2756
aVThe entity mention is annotated with its full extent and its head , referred to as the extend mention and the head mention respectively
p2757
aVThe extent mention includes both the head and its modifiers
p2758
aVEach relation has two entities as arguments
p2759
aVArg-1 and Arg-2, referred to as E1 and E2
p2760
aVRelation identification is handled as a classification problem
p2761
aVEntity-related information (e.g., head noun, entity type, subtype, CLASS, LDCTYPE, etc.) are supposed to be known and provided by the corpus
p2762
aVBecause the entity mentions can be nested, two entity mentions may have four coarse structures u'\u005cu201c' E1 is before E2 u'\u005cu201d' , u'\u005cu201c' E1 is after E2 u'\u005cu201d' , u'\u005cu201c' E1 nests in E2 u'\u005cu201d' and u'\u005cu201c' E2 nests in E1 u'\u005cu201d' , encoded as u'\u005cu2018' E1_B_E2 u'\u005cu2019' , u'\u005cu2018' E1_A_E2 u'\u005cu2019' , u'\u005cu2018' E1_N_E2 u'\u005cu2019' and u'\u005cu2018' E2_N_E1 u'\u005cu2019'
p2763
aVIt is encoded by combining the POS tag with the adjacent entity mention information
p2764
aVThe word-formation of Chinese also implies that the meanings of a compound word are made up, usually, by the meanings of words that contained in it []
p2765
aVSo, fragments of phrase are also informative
p2766
aVBecause high precision can be received by using simple lexical features []
p2767
aVIn consideration of the Chinese characteristics, we use every potential word in a relation mention as the lexical features
p2768
aV{CJK} UTF8gbsn For example, relation mention u'\u005cu2018' å°åå¤§å®æ£®æå¬å­ u'\u005cu2019' (Taipei Daan Forest Park) has a u'\u005cu201d' PART-WHOLE u'\u005cu201d' relation type
p2769
aVMost of these features are nested or overlapped mutually
p2770
aVSo, the traditional character-based or word-based feature is only a subset of the Omni-word feature
p2771
aVBecause the number of lexicon entry determines the dimension of the feature space, performance of Omni-word feature is influenced by the lexicon being employed
p2772
aVIn this paper, we generate the lexicon by merging two lexicons
p2773
aVThe first lexicon is obtained by segmenting every relation instance using the ICTCLAS package, collecting very word produced by ICTCLAS
p2774
aVBecause the ICTCLAS package was trained on annotated corpus containing many meaningful lexicon entries
p2775
aVDespite the Omni-word can be seen as a subset of n-Gram feature
p2776
aVIt is not the same as the n-Gram feature
p2777
aVIn most of the instances, the n-Gram features have no semantic meanings attached to them, thus have varied distributions
p2778
aVBecause Chinese has plenty of characters 5 5 Currently, at least 13000 characters are used by native Chinese people
p2779
aVHowever, even in English, u'\u005cu201c' deeper u'\u005cu201d' analysis (e.g., logical syntactic relations or predicate-argument structure) may suffer from a worse performance caused by inaccurate chunking or parsing
p2780
aVHence, the local dependency contexts around the relation arguments are more helpful []
p2781
aVFor example, Agichtein and Gravano [] generates patterns according to a confidence threshold ( u'\u005cu03a4' t
p2782
aVDeleting of relation instances is acceptable for open relation extraction because it always deals with a big data set
p2783
aVUtilizing the notion of combined feature [] , we replace the hard constraint by the soft constraint
p2784
aVNo subjective or priori judgement is adopted to delete any potential determinative constraint (except for the reason of dimensionality reduction
p2785
aVMost of the researches make use of the combined feature, but rarely analyze the influence of the approaches we combine them
p2786
aVA feature is employed as a singleton feature when it is used without combining with any information
p2787
aVIn our experiment, the Head noun and POS Tag are utilized as position sensitive features, which has been introduced in Section 3.1
p2788
aVFor example, u'\u005cu2018' å°å_E1 u'\u005cu2019' means that the head noun u'\u005cu2018' å°å u'\u005cu2019' depend on the first entity mention
p2789
aVSemantic pair is generated by combining two semantic units
p2790
aVThose are generated by combining two entity types or two entity subtypes into a semantic pair
p2791
aVIn our study, Omni-word feature is not added as u'\u005cu201c' bag of words u'\u005cu201d'
p2792
aVTo use the Omni-word feature, we segment each relation mention by two entity mentions
p2793
aVTogether with the two entity mentions, we get five parts u'\u005cu201c' FIRST u'\u005cu201d' , u'\u005cu201c' MIDDLE u'\u005cu201d' , u'\u005cu201c' END u'\u005cu201d' , u'\u005cu201c' E1 u'\u005cu201d' and u'\u005cu201c' E2 u'\u005cu201d' (or less, if the two entity mentions are nested
p2794
aVEach part is taken as an independent bin
p2795
aVA flag is used to distinguish them
p2796
aVThey will be used as three independent features
p2797
aVTo sum up, among the five candidate feature sets, the position feature is used as a singleton feature
p2798
aVBoth head noun and POS tag are position sensitive
p2799
aVEntity types and subtypes are employed as semantic pair
p2800
aVOnly Omni-word feature is bin sensitive
p2801
aVAfter deleting 5 documents containing wrong annotations 6 6 DAVYZW_{20041230.1024, 20050110.1403, 20050111.1514, 20050127.1720, 20050201.1538} we keep 9,244 relation mentions as positive instances
p2802
aVUTF8gbsn To get the negative instances, each document is segmented into sentences 7 7 The five punctuations are used as sentence boundaries
p2803
aVFor each of the remained sentences, we iteratively extract every entity mention pair as the arguments of relation instances for predicting
p2804
aVFor example, suppose a sentence has three entity mentions
p2805
aVBecause the relation arguments are order sensitive, six entity mention pairs can be generated
p2806
aVAfter discarding the entity mention pairs that were used as positive instances, we generated 93,283 negative relation instances labelled as u'\u005cu201c' OTHER u'\u005cu201d'
p2807
aVBecause we are interested in the 6 annotated major relation types and the 18 subtypes, we average the results of five runs on the 6 positive relation types (and 18 subtypes) as the final performance
p2808
aVThe entity type and subtype , head noun , position feature are referred to as u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p 8 8 u'\u005cu201c' thp u'\u005cu201d' is an acronym of u'\u005cu201c' t ype, h ead, p osition u'\u005cu201d'
p2809
aVThe POS tags are referred to as u'\u005cu2131' p u'\u005cu2062' o u'\u005cu2062' s
p2810
aVIn Row 1, because u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p are features directly obtained from annotated corpus, we take this performance as our referential performance
p2811
aVSegmentation of bins bears the sentence structure information
p2812
aVTherefore, the Owni-word feature with bin information can make a better use of both the syntactic information and the local dependency
p2813
aV[] was based on the ACE 2005 corpus with 75% data for training and 25% for testing
p2814
aVIn order to give a better comparison with the state-of-the-art methods, based on our experiment settings and data, we implement the two feature based methods proposed by Che et al
p2815
aVBecause features may interact mutually in an indirect way, even with the same feature set, different constraint conditions can have significant influences on the final performance
p2816
aVIn Section 3 , we introduced five candidate feature sets
p2817
aVInstead of using them as independent features, we combined them with additional information
p2818
aVThe first observation is that the combined features are more powerful than used as singletons
p2819
aVModel parameters are increased by the combined features
p2820
aVExcept in Row 8 and Row 11, when two head nouns of entity pair were combined as semantic pair and when POS tag were combined with the entity type, the performances are decreased
p2821
aVIn Row 4, 10 and 13, these features are used as singleton , the performance degrades considerably
p2822
aVThis means that, the missing of sentence structure information on the employed features can lead to a bad performance
p2823
aVComparing the reference set (5) with the reference set (3), the Head noun and adjacent entity POS tag get a better performance when used as singletons
p2824
aVIn this paper, for a better demonstration of the constraint condition, we still use the Position Sensitive as the default setting to use the Head noun and the adjacent entity POS tag
p2825
aVConventionally, if a sentence is perfectly segmented, By-Segmentation is straightforward and effective
p2826
aVIn short, from Table 4 we have seen that the entity type and subtype maximize the performance when used as semantic pair
p2827
aVHead noun and adjacent entity POS tag are employed to combine with positional information
p2828
asg88
(lp2829
sg90
(lp2830
sg92
(lp2831
VIn this paper, We proposed a novel Omni-word feature taking advantages of Chinese sub-phrases.
p2832
aVWe also introduced the soft constraint method for Chinese relation recognition.
p2833
aVThe soft constraint utilizes four constraint conditions to catch the structure information in a relation instance.
p2834
aVBoth the Omni-word feature and soft constrain make better use of information a sentence has, and minimize the deficiency caused by Chinese segmentation and parsing.
p2835
aVThe size of the employed lexicon determines the dimension of the feature space.
p2836
aVThe first impression is that more lexicon entries result in more power.
p2837
aVHowever, more lexicon entries also increase the computational complexity and bring in noises.
p2838
aVIn our future work, we will study this issue.
p2839
aVThe notion of soft constraints can also be extended to include more patterns, rules, regexes or syntactic constraints that have been used for information extraction.
p2840
aVThe usability of these strategies is also left for future work.
p2841
ag106
asg107
S'P14-1054'
p2842
sg109
(lp2843
VChinese is an ancient hieroglyphic.
p2844
aVIt is inattentive to structure.
p2845
aVTherefore, segmenting and parsing Chinese are more difficult and less accurate.
p2846
aVIn this paper, we propose an Omni-word feature and a soft constraint method for Chinese relation extraction.
p2847
aVThe Omni-word feature uses every potential word in a sentence as lexicon feature, reducing errors caused by word segmentation.
p2848
aVIn order to utilize the structure information of a relation instance, we discuss how soft constraint can be used to capture the local dependency.
p2849
aVBoth Omni-word feature and soft constraint make a better use of sentence information and minimize the influences caused by Chinese word segmentation and parsing.
p2850
aVWe test these methods on the ACE 2005 RDC Chinese corpus.
p2851
aVThe results show a significant improvement in Chinese relation extraction, outperforming other methods in F-score by 10% in 6 relation types and 15% in 18 relation subtypes.
p2852
ag106
asba(icmyPackage
FText
p2853
(dp2854
g3
(lp2855
VAccurately segmenting a citation string into fields for authors, titles, etc is a challenging task because the output typically obeys various global constraints
p2856
aVThis task is important because citation data is often provided only in plain text; however, having an accurate structured database of bibliographic information is necessary for many scientometric tasks, such as mapping scientific sub-communities, discovering research trends, and analyzing networks of researchers
p2857
aVAutomated citation field extraction needs further research because it has not yet reached a level of accuracy at which it can be practically deployed in real-world systems
p2858
aVSince linear-chain models are unable to capture more than Markov dependencies, the models sometimes mislabel the editor as a second author
p2859
aVIf we could enforce the global constraint that there should be only one author section, accuracy could be improved
p2860
aVWhen hard constraints can be encoded as linear equations on the output variables, and the underlying model u'\u005cu2019' s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) [ 15 ]
p2861
aVOn the other hand, recent work has demonstrated improvements in citation field extraction by imposing soft constraints [ 4 ]
p2862
aVThis paper introduces a novel method for imposing soft constraints via dual decomposition
p2863
aVWe also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints
p2864
aVBecause our learning method drives many penalties to zero, it allows practitioners to perform u'\u005cu2018' constraint selection, u'\u005cu2019' in which a large number of automatically-generated candidate global constraints can be considered and automatically culled to a smaller set of useful constraints, which can be run quickly at test time
p2865
aVUsing our new method, we are able to incorporate not only all the soft global constraints of Chang et al
p2866
aVFor this underlying model, we employ a chain-structured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction [ 14 ]
p2867
aVWe produce a prediction by performing MAP inference [ 10 ]
p2868
aVThe MAP inference task in a CRF be can expressed as an optimization problem with a linear objective [ 21 , 20 ]
p2869
aVHere, we define a binary indicator variable for each candidate setting of each factor in the graphical model
p2870
aVSince the log probability of some y in the CRF is proportional to sum of the scores of all the factors, we can concatenate the indicator variables as a vector y and the scores as a vector w and write the MAP problem as
p2871
aVThe algorithms we present in later sections for handling soft global constraints and for learning the penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference
p2872
aVDual Decomposition is a popular method for performing MAP inference in this scenario, since it leverages known algorithms for MAP in the base problem where these extra constraints have not been added [ 11 , 20 , 18 ]
p2873
aVRegrouping terms and maximizing over the primal variables, we have the dual problem
p2874
aVFor any u'\u005cu039b' , we can evaluate the dual objective D u'\u005cu2062' ( u'\u005cu039b' ) , since the maximization in ( 4 ) is of the same form as the original problem ( 1 ), and we assumed we had a method for performing MAP in this
p2875
aVFurthermore, a subgradient of D u'\u005cu2062' ( u'\u005cu039b' ) is A u'\u005cu2062' y * - b , for an y * which maximizes this inner optimization problem
p2876
aVTherefore, we can minimize D u'\u005cu2062' ( u'\u005cu039b' ) with the projected subgradient method [ 2 ] , and the optimal y can be obtained when evaluating D u'\u005cu2062' ( u'\u005cu039b' *
p2877
aVThis is necessary because u'\u005cu039b' is a vector of dual variables for inequality constraints
p2878
aVThe algorithm has converged when each constraint is either satisfied by y ( t ) with equality or its corresponding component of u'\u005cu039b' is 0, due to complimentary slackness [ 2 ]
p2879
aVIn our formulation, a soft-constrained model imposes a penalty for each unsatisfied constraint, proportional to the amount by which it is violated
p2880
aVTherefore, our derivation parallels how soft-margin SVMs are derived from hard-margin SVMs by introducing auxiliary slack variables [ 7 ]
p2881
aVNote that when performing MAP subject to soft constraints, optimal solutions might not satisfy some constraints, since doing so would reduce the model u'\u005cu2019' s score by too much
p2882
aVFor positive c i , it is clear that an optimal z i will be equal to the degree to which a i T u'\u005cu2062' y u'\u005cu2264' b i is violated
p2883
aVTherefore, we pay a cost c i times the degree to which the i th constraint is violated, which mirrors how slack variables are used to represent the hinge loss for SVMs
p2884
aVNote that c i has to be positive, otherwise this linear program is unbounded and an optimal value can be obtained by setting z i to infinity
p2885
aVUsing a similar construction as in section 2.2 we write the Lagrangian as
p2886
aVThe optimality constraints with respect to z tell us that - c - u'\u005cu039b' - u'\u005cu039c' = 0 , hence u'\u005cu039c' = - c - u'\u005cu039b'
p2887
aVSubstituting, we have
p2888
aVSince this Lagrangian has the same form as equation ( 3 ), we can also derive a dual problem, which is the same as in equation ( 4 ), with the additional constraint that each u'\u005cu039b' i can not be bigger than its cost c i
p2889
aVIn other words, the dual problem can not penalize the violation of a constraint more than the soft constraint model in the primal would penalize you if you violated it
p2890
aVNow, we check for the KKT conditions of ( 5 ), where for every constraint i , either the constraint is satisfied with equality, u'\u005cu039b' i = 0 , or u'\u005cu039b' i = c i
p2891
aVTherefore, implementing soft-constrained dual decomposition is as easy as implementing hard-constrained dual decomposition, and the per-iteration complexity is the same
p2892
aVOne consideration when using soft v.s hard constraints is that soft constraints present a new training problem, since we need to choose the vector c , the penalties for violating the constraints
p2893
aVAn important property of problem ( 5 ) in the previous section is that it corresponds to a structured linear model over y and z
p2894
aVTherefore, we can apply known training algorithms for estimating the parameters of structured linear models to choose c
p2895
aVAll we need to employ the structured perceptron algorithm [ 6 ] or the structured SVM algorithm [ 23 ] is a black-box procedure for performing MAP inference in the structured linear model given an arbitrary cost vector
p2896
aVIntuitively, the perceptron update increases the penalty for a constraint if it is satisfied in the ground truth and not in an inferred prediction, and decreases the penalty if the constraint is satisfied in the prediction and not the ground truth
p2897
aVSince we truncate penalties at 0, this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth, constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model, and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints
p2898
aVA similar analysis holds for the structured SVM approach
p2899
aVTherefore, we can view learning the values of the penalties not just as parameter tuning, but as a means to perform u'\u005cu2018' constraint selection, u'\u005cu2019' since constraints that have a penalty of 0 can be ignored
p2900
aVWe found it beneficial, though it is not theoretically necessary, to learn the constraints on a held-out development set, separately from the other model parameters, as during training most constraints are satisfied due to overfitting, which leads to an underestimation of the relevant penalties
p2901
aVNote these constraints are all linear, since they depend only on the counts of each possible conditional random field label
p2902
aVMoreover, since our labels are BIO-encoded, it is possible, by counting B tags, to count how often each citation tag itself appears in a sentence
p2903
aVWe denote [ [ y k = i ] ] as the function that outputs 1 if y k has a 1 at index i and 0 otherwise
p2904
aVHere, y k represents an output tag of the CRF, so if [ [ y k = i ] ] = 1, then we have that y k was given a label with index i
p2905
aVHowever, we are using them as soft constraints, so these constraints will not necessarily be satisfied by the output of the model, which eliminates concern over enforcing logically impossible outputs
p2906
aVFurthermore, in section 3.1 we described how our procedure for learning penalties will drive some penalties to 0, which effectively removes them from our set of constraints we consider
p2907
aVWe define C u'\u005cu2062' ( x , i ) as the function that returns 1 if the output x contains the label i in the hierarchy and 0 otherwise
p2908
aVThis both improves performance of the underlying model when used without global constraints, as well as ensures the validity of the global constraints we impose, since they operate only on B labels
p2909
aVRather than enforcing these constraints using dual decomposition, they can be enforced directly when performing MAP inference in the CRF by modifying the dynamic program of the Viterbi algorithm to only allow valid pairs of adjacent labels
p2910
aVWhile the techniques from section 3.1 can easily cope with a large numbers of constraints at training time, this can be computationally costly, specially if one is considering very large constraint families
p2911
aVThis is problematic because the size of some constraint families we consider grows quadratically with the number of candidate labels, and there are about 100 in the UMass dataset
p2912
aVSuch a family consists of constraints that the sum of the counts of two different label types has to be bounded (a useful example is that there can u'\u005cu2019' t be more than one out of u'\u005cu201c' phd thesis u'\u005cu201d' and u'\u005cu201c' journal u'\u005cu201d'
p2913
aVTherefore, quickly pruning bad constraints can save a substantial amount of training time, and can lead to better generalization
p2914
aVTo do so, we calculate a score that estimates how useful each constraint is expected to be
p2915
aVNote that it may make sense to consider a constraint that is sometimes violated in the ground truth, as the penalty learning algorithm can learn a small penalty for it, which will allow it to be violated some of the time
p2916
aVwhere [[ y ]] c is 1 if the constraint is violated on output y and 0 otherwise
p2917
aVWe prune constraints by picking a cutoff value for i u'\u005cu2062' m u'\u005cu2062' p u'\u005cu2062' ( c
p2918
aVA value of i u'\u005cu2062' m u'\u005cu2062' p u'\u005cu2062' ( c ) above 1 implies that the constraint is more violated on the predicted examples than on the ground truth, and hence that we might want to keep it
p2919
aVThere are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs
p2920
aVBelief propagation is prohibitively expensive in our model due to the high cardinalities of the output variables and of the global factors, which involve all output variables simultaneously
p2921
aVThere are various methods for exploiting the combinatorial structure of these factors, but performance would still have higher complexity than our method
p2922
aVWhile Gibbs sampling has been shown to work well tasks such as named entity recognition [ 8 ] , our previous experiments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF
p2923
aVRecently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP [ 12 , 17 , 18 , 13 , 5 ]
p2924
aVSoft constraints can be implemented inefficiently using hard constraints and dual decomposition u'\u005cu2014' by introducing copies of output variables and an auxiliary graphical model, as in Rush et al
p2925
aVFurthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence
p2926
aVThis approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA [ 14 ]
p2927
aVWe use the same features as Anzaroot and McCallum [ 1 ] , which include word type, capitalization, binned location in citation, regular expression matches, and matches into lexicons
p2928
aVIn addition, we use a rule-based segmenter that segments the citation string based on punctuation as well as probable start or end segment words (e.g., u'\u005cu2018' in u'\u005cu2019' and u'\u005cu2018' volume u'\u005cu2019'
p2929
aVThis final feature improves the F1 score on the cleaned test set from 94.0 F1 to 94.44 F1, which we use as a baseline score
p2930
aVWe then use the development set to learn the penalties for the soft constraints, using the perceptron algorithm described in section 3.1
p2931
aVIn the all-constraints settings, 32.96% of the constraints have a learned parameter of 0 , and therefore only 421 constraints are active
p2932
aVSoft-DD converges, and thus solves the constrained inference problem exactly, for all test set examples after at most 41 iterations
p2933
aVSince performing inference in the CRF is by far the most computationally intensive step in the iterative algorithm, this means our procedure requires approximately twice as much work as running the baseline CRF on the dataset
p2934
aVFor 11.99% of the examples, the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference, while in the remaining 11.72% Soft-DD converges with some constraints left unsatisfied, which is possible since we are imposing them as soft constraints
p2935
aVWe could have enforced these constraints as hard constraints rather than soft ones
p2936
aVThis experiment is shown in the last row of Table 1 , where F1 only improves to 94.6
p2937
aVWe find that a large portion of our gain in accuracy can be obtained when we allow ourselves as few as 2 dual decomposition iterations
p2938
aVHowever, this only amounts to 1.24 times as much work as running the baseline CRF on the dataset, since the constraints are satisfied immediately for many examples
p2939
aVWe train and evaluate on the UMass dataset instead of CORA, because it is significantly larger, has a useful finer-grained labeling schema, and its annotation is more consistent
p2940
aVFurthermore, since the dataset is so small, learning the penalties for our large collection of constraints is difficult, and test set results are unreliable
p2941
aV[ 4 ] via results on CORA, we apply their constraints on the UMass data using Soft-DD and demonstrate accuracy gains, as discussed above
p2942
aVThe importance score of a constraint provides information about how often it is violated by the CRF, but holds in the ground truth, and a non-zero penalty implies we enforce it as a soft constraint at test time
p2943
aVThe two singleton constraints with highest importance score are that there should only be at most one title segment in a citation and that there should be at most one author segment in a citation
p2944
aVThe only one author constraint is particularly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them as author segments
p2945
aVAs can be seen in Table 3 , editor fields are among the most improved with our new method, largely due to this constraint
p2946
aVThis penalization leads allows the constrained inference to correctly label the booktitle segment as a title segment
p2947
aVThe above example constraints are almost always satisfied on the ground truth, and would be useful to enforce as hard constraints
p2948
aVHowever, there are a number of learned constraints that are often violated on the ground truth but are still useful as soft constraints
p2949
aVTake, for example, the constraint that the number of number segments does not exceed the number of booktitle segments, as well as the constraint that it does not exceed the number of journal segments
p2950
aVIt is still useful to impose these soft constraints, as strong evidence from the CRF allows us to violate them, and they can guide the model to good predictions when the CRF is unconfident
p2951
aVWe introduce a novel modification to the standard projected subgradient dual decomposition algorithm for performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints
p2952
aVIn addition, we offer an easy-to-implement procedure for learning the penalties on soft constraints
p2953
aVWe show via experiments on a recent substantial dataset that using soft constraints, and selecting which constraints to use with our penalty-learning procedure, can lead to significant gains in accuracy
p2954
asg88
(lp2955
sg90
(lp2956
sg92
(lp2957
VWe introduce a novel modification to the standard projected subgradient dual decomposition algorithm for performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints.
p2958
aVIn addition, we offer an easy-to-implement procedure for learning the penalties on soft constraints.
p2959
aVThis method drives many penalties to zero, which allows users to automatically discover discriminative constraints from large families of candidates.
p2960
aVWe show via experiments on a recent substantial dataset that using soft constraints, and selecting which constraints to use with our penalty-learning procedure, can lead to significant gains in accuracy.
p2961
aVWe achieve a 17% gain in accuracy over a chain-structured CRF model, while only needing to run MAP in the CRF an average of less than 2 times per example.
p2962
aVThis minor incremental cost over Viterbi, plus the fact that we obtain certificates of optimality on 100% of our test examples in practice, suggests the usefulness of our algorithm for large-scale applications.
p2963
aVWe encourage further use of our Soft-DD procedure for other structured prediction problems.
p2964
ag106
asg107
S'P14-1056'
p2965
sg109
(lp2966
VAccurately segmenting a citation string into fields for authors, titles, etc is a challenging task because the output typically obeys various global constraints.
p2967
aVPrevious work has shown that modeling soft constraints , where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance.
p2968
aVOn the other hand, for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference.
p2969
aVWe extend dual decomposition to perform prediction subject to soft constraints.
p2970
aVMoreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training.
p2971
aVThis allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset.
p2972
aValgorithmttop.
p2973
aV4 ref-marker [ J D Monk , ] authors [ Cardinal Functions on Boolean Algebra , ] title [ Lectures in Mathematics , ETH Zurich , BirkhÂ¨ause Verlag , Basel , Boston , Berlin , 1990 ] venue.
p2974
ag106
asba(icmyPackage
FText
p2975
(dp2976
g3
(lp2977
VOne commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation
p2978
aVIn particular, we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts
p2979
aVWith the increasing use of electronic health records, it becomes urgent to leverage this rich information resource about patients u'\u005cu2019' health conditions to transform research in health and medicine
p2980
aVAs an example, when developing a cohort for a clinical trial, researchers need to identify patients matching a set of clinical criteria based on their medical records during their hospital visits [ 22 , 8 ]
p2981
aVIntuitively, to better solve this domain-specific retrieval problem, we need to understand the requirements specified in a query and identify the documents satisfying these requirements based on their semantic meanings
p2982
aVThe basic idea is to represent both queries and documents as u'\u005cu201c' bags of concepts u'\u005cu201d' , where the concepts are identified based on the information from the knowledge bases
p2983
aVThis method has been shown to be more effective than traditional term-based representation in the medical record retrieval because of its ability to handle the ambiguity in the medical terminology
p2984
aVHowever, this method also suffers the limitation that its effectiveness depends on the accuracy of the concept mapping results
p2985
aVAs a result, directly applying existing weighting strategies might lead to non-optimal retrieval performance
p2986
aVSpecifically, by applying the axiomatic approaches [ 7 ] , we analyze the retrieval functions with concept-based representation and find that they may violate some reasonable retrieval constraints
p2987
aVWe then propose two concept-based weighting regularization methods so that the regularized retrieval functions would satisfy the retrieval constraints and achieve better retrieval performance
p2988
aVHowever, due to the inherent ambiguity of natural languages, the results of NLP tools are not perfect
p2989
aVOne of our contributions is to present a general methodology that can be used to adjust existing IR techniques based on the inaccurate NLP results
p2990
aVTo further improve the performance, Limsopatham et al proposed a task-specific representation, i.e.,, using only four types of concepts (symptom, diagnostic test, diagnosis and treatment) in the concept-based representation and a query expansion method based on the relationships among the medical concepts [ 12 , 13 ]
p2991
aVHowever, none of the previous work has studied how to regularize the weight of concepts based on their relations
p2992
aVSince each visit can be associated with multiple medical records, the relevance of a visit is related to the relevance of individual associated medical records
p2993
aVExisting studies computed the relevance scores at either visit-level, where all the medical records of a visit are merged into a visit document [ 5 , 15 ] , or record-level, where we can first compute the relevance score of individual records and then aggregate their scores as the relevance score of a visit [ 15 , 30 , 12 ]
p2994
aVIn this paper, we focus on the visit-level relevance because of its simplicity
p2995
aVIn particular, given a patient u'\u005cu2019' s visit, all the medical records generated from this visit are merged as a document
p2996
aVNote that our proposed concept-weighting strategies can also be easily applied to record-level relevance modeling
p2997
aVSince the goal is to retrieve medical records of patients that satisfying requirements specified in a query, the relevance of medical records should be modeled based on how well they match all the requirements (i.e.,, aspects) specified in the queries
p2998
aVIn particular, MetaMap [ 3 ] can take a text string as the input, segment it into phrases, and then map each phrase to multiple UMLS CUIs with confidence scores
p2999
aVTraditional retrieval models are based on u'\u005cu201c' bag of terms u'\u005cu201d' representation
p3000
aVOne limitation of this representation is that relevance scores are computed based on the matching of terms rather than the meanings
p3001
aVAs a result, the system may fail to retrieve the relevant documents that do not contain any query terms
p3002
aVIn particular, MetaMap is used to map terms from queries and documents (e.g.,, medical records) to the semantic concepts from biomedical knowledge bases such as UMLS
p3003
aVWithin the concept-based representation, the query can then be represented as a bag of all the generated CUIs in the MetaMap results
p3004
aVAlthough existing retrieval functions can be directly applied to concept-based representation, they may lead to non-optimal performance
p3005
aVUnder such a situation, traditional retrieval functions would likely work well and generate satisfying retrieval performance since the relations among concepts are independent which is consistent with the assumptions made in traditional IR [ 18 ]
p3006
aVIn particular, our preliminary results show that turning on the disambiguation functionality provided by MetaMap (i.e.,, returning only the most likely concept for each query) could lead to worse retrieval performance than using all the candidate mappings
p3007
aVThus, we use the one-to-many mapping results generated by MetaMap, in which each aspect can be mapped to multiple concepts
p3008
aVThe multiple concepts generated from the same aspect are related, which is inconsistent with the independence assumption made in the existing retrieval functions [ 18 ]
p3009
aVFor example, as shown in Figure 1 , u'\u005cu201c' dental caries u'\u005cu201d' is mapped to three concepts
p3010
aVThe one-to-many mapping results generated by MetaMap could arbitrarily inflate the weights of some query aspects
p3011
aVFor example, as shown in Figure 1 , query aspect u'\u005cu201c' children u'\u005cu201d' is mapped to 2 concepts while u'\u005cu201c' dental caries u'\u005cu201d' is mapped to 3 concepts
p3012
aVHowever, when converting the text to concepts representation using MetaMap, the occurrences of the concepts are determined by not only the original term occurrences, a good indicator of relevance, but also the number of mapped concepts, which is determined by MetaMap and has nothing to do with the relevance status
p3013
aVAs a result, the occurrences of concepts might not be a very accurate indicator of importance of the corresponding query aspect
p3014
aVS u'\u005cu2062' ( Q , D ) is the relevance score of D with respect to Q e i denotes a concept, and u'\u005cud835' u'\u005cudc9c' u'\u005cu2062' ( e ) denotes the query aspect associated with e , i.e.,, a set of concepts that are mapped to the same phrases as e by using MetaMap i u'\u005cu2062' ( e ) is the normalized confidence score of the mapping for concept e generated by MetaMap c u'\u005cu2062' ( e , D ) denotes the occurrences of concept e in document D , d u'\u005cu2062' f u'\u005cu2062' ( e ) denotes the number of documents containing e
p3015
aVWe now discuss how to address the first challenge, i.e how to regularize the weighting strategy so that we can take into consideration the fact that concepts associated with the same query aspect are not independent
p3016
aVWe call a concept is a variant of another one if both of them are associated with the same aspect
p3017
aVIf we know that c u'\u005cu2062' ( e 1 , D 1 ) = c u'\u005cu2062' ( e 3 , D 2 ) 0 , c u'\u005cu2062' ( e 1 , D 2 ) = c u'\u005cu2062' ( e 3 , D 1 ) = 0 and c u'\u005cu2062' ( e 2 , D 1 ) = c u'\u005cu2062' ( e 2 , D 2 ) 0 , then S u'\u005cu2062' ( Q , D 1 ) S u'\u005cu2062' ( Q , D 2 )
p3018
aVIt is clear that existing retrieval functions would violate this constraint since they ignore the relations among concepts
p3019
aVBy merging the concepts together, we are aiming to purify the concepts and make the similar concepts centralized so that the assumption that all the concepts are independent would hold
p3020
aVwhere c u'\u005cu2062' ( e , D ) is the original occurrence of concept e in document D , E u'\u005cu2062' C u'\u005cu2062' ( e ) denotes a set of all the variants of e including itself (i.e.,, all the concepts with the same preferred name as e ), and R u'\u005cu2062' e u'\u005cu2062' p u'\u005cu2062' ( E u'\u005cu2062' C u'\u005cu2062' ( e ) ) denotes the representative concept from E u'\u005cu2062' C u'\u005cu2062' ( e )
p3021
aVIt is trivial to prove that, with such changes, existing retrieval functions would satisfy the above constraint since the constraint implies TFC2 constraint defined in the previous study [ 6 ]
p3022
aVIt is clear that the right plot (i.e.,, selecting the concept with the maximum IDF as the representative concept) is the best choice since the changes make the points less scattered
p3023
aVIn fact, this can also be confirmed by experimental results as reported in Table 5
p3024
aVThus, we use the concept with the maximum IDF value as the representative concept of all the variants
p3025
aVThe arbitrary inflation could impact the importance of the query aspects
p3026
aVFor example, as shown in Figure 1 , one aspect is mapped to two concepts while the other is mapped to three
p3027
aVTo fix the negative impact on query aspects, we could leverage the findings in the previous study [ 28 ] and regularize the weighting strategy based on the length of query aspects to favor documents covering more query aspects
p3028
aVSince each concept mapping is associated with a confidence score, we can incorporate them into the regularization function as follows
p3029
aVThis regularization function aims to penalize the weight of concept e based on its variants as well as the concepts from other aspects
p3030
aVTo fix the negative impact on the concept IDF values, we propose to regularize the weighting based on the importance of the query aspect
p3031
aVIf we know I u'\u005cu2062' m u'\u005cu2062' p c u'\u005cu2062' ( e 1 ) = I u'\u005cu2062' m u'\u005cu2062' p c u'\u005cu2062' ( e 2 ) and I u'\u005cu2062' m u'\u005cu2062' p A u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9c' u'\u005cu2062' ( e 1 ) ) I u'\u005cu2062' m u'\u005cu2062' p A u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9c' u'\u005cu2062' ( e 2 ) ) , then we have S u'\u005cu2062' ( Q , D 1 ) S u'\u005cu2062' ( Q , D 2 )
p3032
aVIn a way, the constraint aims to counteract the arbitrary statistics inflation caused by MetaMap results and balance the weight among concepts based on the importance of the associated query aspects
p3033
aVNote that I u'\u005cu2062' m u'\u005cu2062' p A u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9c' u'\u005cu2062' ( e ) ) is the importance of a query aspect and can be estimated based on the terms from the query aspect
p3034
aVWith the unified concept weighting regularization, the revised function based on F2-EXP function, i.e.,, Unified , is shown as follows
p3035
aVSince the task is to retrieve relevant visits, we merged all the records from a visit to form a single document for the visit, which leads to 17,198 documents in the collection
p3036
aVThese queries were developed by domain experts based on the u'\u005cu201c' inclusion criteria u'\u005cu201d' of a clinical study [ 25 , 24 ]
p3037
aVSince documents are often much longer, we can first segment them into sentences, get the mapping results for each sentence, and then merge them together to generate the concept-based representation for the documents
p3038
aVFollowing the evaluation methodology used in the medical record track, we use MAP@1000 as the primary measure for Med11 and also report bpref
p3039
aVFor Med12 , we take infNDCG@100 as the primary measure and also report infAP@100
p3040
aVDifferent measures were chosen for these two sets mainly because different pooling strategies were used to create the judgment pools [ 24 ]
p3041
aVNote that T , C and T u'\u005cu2062' S indicate improvement over Term-BL, Concept-BL and TSConcept-BL is statistically significant at 0.05 level based on Wilcoxon signed-rank test, respectively
p3042
aVIt is not surprising to see that Balanced method is more effective than Unified since the former satisfies both of the proposed retrieval constraints while the latter satisfies only one
p3043
aVMoreover, our performance might be further improved if we apply the result filtering methods used by many TREC participants [ 11 ]
p3044
aVIn the Unified method, we chose the concept with the maximum IDF as the representative concept among all the variants
p3045
aVWe now conduct experiments on Med11 to compare its performance with those of using average IDF and minimum IDF ones as the representative concept
p3046
aVTable 6 summarizes the results, and shows that using the maximum IDF value performs better than the other choices
p3047
aVAs shown in Equation ( 3 ), the Balanced method regularizes the weights through two components
p3048
aVIn this paper, we present a general methodology that can use axiomatic approaches as guidance to regularize the concept weighting strategies to address the limitations caused by the inaccurate concept mapping and improve the retrieval performance
p3049
aVIn particular, we proposed two weighting regularization methods based on the relations among concepts
p3050
aVFirst, we plan to study how to automatically predict whether to use concept-based indexing based on the quality of MetaMap results, and explore whether the proposed methods are applicable for other entity linking methods
p3051
asg88
(lp3052
sg90
(lp3053
sg92
(lp3054
VMedical record retrieval is an important domain-specific IR problem.
p3055
aVConcept-based representation is an effective approach to dealing with ambiguity terminology in medical domain.
p3056
aVHowever, the results of the NLP tools used to generate the concept-based representation are often not perfect.
p3057
aVIn this paper, we present a general methodology that can use axiomatic approaches as guidance to regularize the concept weighting strategies to address the limitations caused by the inaccurate concept mapping and improve the retrieval performance.
p3058
aVIn particular, we proposed two weighting regularization methods based on the relations among concepts.
p3059
aVExperimental results show that the proposed methods can significantly outperform existing retrieval functions.
p3060
aVThere are many interesting directions for our future work.
p3061
aVFirst, we plan to study how to automatically predict whether to use concept-based indexing based on the quality of MetaMap results, and explore whether the proposed methods are applicable for other entity linking methods.
p3062
aVSecond, we will study how to leverage other information from knowledge bases to further improve the performance.
p3063
aVThird, more experiments could be conducted to examine the effectiveness of the proposed methods when using other ranking strategies.
p3064
aVFinally, it would be interesting to study how to follow the proposed methodology to study other domain-specific IR problems.
p3065
ag106
asg107
S'P14-1057'
p3066
sg109
(lp3067
VAn important search task in the biomedical domain is to find medical records of patients who are qualified for a clinical trial.
p3068
aVOne commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation.
p3069
aVHowever, the mapping results are not perfect, and none of previous work studied how to deal with them in the retrieval process.
p3070
aVIn this paper, we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the retrieval performance of the concept-based ranking methods.
p3071
aVIn particular, we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts.
p3072
aVExperimental results show that the proposed methods are effective to improve the retrieval performance, and their performances are comparable to other top-performing systems in the TREC Medical Records Track.
p3073
ag106
asba(icmyPackage
FText
p3074
(dp3075
g3
(lp3076
VAlthough the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word u'\u005cu2019' s predominant meaning changes
p3077
aVHowever, if it were possible to predict how the distribution of a word changes from one domain to another, the predictions could be used to adapt a system trained in one domain to work in another
p3078
aVFor example, in the domain of portable computer reviews the word lightweight is often associated with positive sentiment bearing words such as sleek or compact , whereas in the movie review domain the same word is often associated with negative sentiment-bearing words such as superficial or formulaic
p3079
aVConsequently, the distributional representations of the word lightweight will differ considerably between the two domains
p3080
aVIn this paper, given the distribution w u'\u005cu2192' \u005ccS of a word w in the source domain \u005ccS , we propose an unsupervised method for predicting its distribution w u'\u005cu2192' \u005ccT in a different target domain \u005ccT
p3081
aVDomain adaptation (DA) of sentiment classification becomes extremely challenging when the distributions of words in the source and the target domains are very different, because the features learnt from the source domain labeled reviews might not appear in the target domain reviews that must be classified
p3082
aVBy predicting the distribution of a word across different domains, we can find source domain features that are similar to the features in target domain reviews, thereby reducing the mismatch of features between the two domains
p3083
aVUsing two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain
p3084
aVBecause our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction [] or dependency parsing []
p3085
aVGiven the distribution w u'\u005cu2192' \u005ccS of a word w in a source domain \u005ccS , we propose a method for learning its distribution w u'\u005cu2192' \u005ccT in a target domain \u005ccT
p3086
aVUsing the learnt distribution prediction model, we propose a method to learn a cross-domain POS tagger
p3087
aVUsing the learnt distribution prediction model, we propose a method to learn a cross-domain sentiment classifier
p3088
aVLearning semantic representations for words using documents from a single domain has received much attention lately []
p3089
aVAs we have already discussed, the semantics of a word varies across different domains, and such variations are not captured by models that only learn a single semantic representation for a word using documents from a single domain
p3090
aVFor example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) []
p3091
aVConsequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE append the source domain labeled data with predicted pivots (i.e., words that appear in both the source and target domains) to adapt a POS tagger to a target domain propose a cross-domain POS tagging method by training two separate models a generalised model and a domain-specific model
p3092
aVAdding latent states to the smoothing model further improves the POS tagging accuracy [] propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words
p3093
aVHowever, if a small amount of labeled data is available for the target domain, it can be used to further improve the performance of DA tasks []
p3094
aVUsing a standard stop word list, we filter out frequent non-content unigrams and select the remainder as unigram features to represent a sentence
p3095
aVUsing data from a single domain, we construct a feature co-occurrence matrix \u005cmat u'\u005cu2062' A in which columns correspond to unigram features and rows correspond to either unigram or bigram features
p3096
aVMoreover, co-occurrences of bigrams are rare compared to co-occurrences of unigrams, and co-occurrences involving a unigram and a bigram
p3097
aVConsequently, in matrix \u005cmat u'\u005cu2062' A , we consider co-occurrences only between unigrams vs unigrams, and bigrams vs unigrams
p3098
aVMatrix \u005cmat u'\u005cu2062' F has the same number of rows, n r , and columns, n c , as the raw co-occurrence matrix \u005cmat u'\u005cu2062' A
p3099
aVNote that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain []
p3100
aVSince the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular feature representation method, any of these alternative methods could be used
p3101
aVEach row in \u005cmat u'\u005cu2062' F ^ is considered as representing a word in a lower k ( u'\u005cu226a' n c ) dimensional feature space corresponding to a particular domain
p3102
aVDistribution prediction in this lower dimensional feature space is preferrable to prediction over the original feature space because there are reductions in overfitting, feature sparseness, and the learning time
p3103
aVIn the literature, such features are often referred to as pivots , and they have been shown to be useful for DA, allowing the weights learnt to be transferred from one domain to another
p3104
aVVarious criteria have been proposed for selecting a small set of pivots for DA, such as the mutual information of a word with the two domains []
p3105
aVNote that the dimensionality of w u'\u005cu2192' \u005ccS ( i ) and w u'\u005cu2192' \u005ccT ( i ) need not be equal, and we may select different numbers of singular vectors to approximate \u005cmat u'\u005cu2062' F ^ \u005ccS and \u005cmat u'\u005cu2062' F ^ \u005ccT
p3106
aVWe model distribution prediction as a multivariate regression problem where, given a set { ( w u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccT ( i ) ) } i = 1 n consisting of pairs of feature vectors selected from each domain for the pivots in \u005ccW , we learn a mapping from the inputs ( w u'\u005cu2192' \u005ccS ( i ) ) to the outputs ( w u'\u005cu2192' \u005ccT ( i )
p3107
aVLet \u005cmat u'\u005cu2062' X and \u005cmat u'\u005cu2062' Y denote matrices formed by arranging respectively the vectors w u'\u005cu2192' \u005ccS ( i ) s and w u'\u005cu2192' \u005ccT ( i ) in rows
p3108
aVThe matrices, u'\u005cud835' u'\u005cudeb2' , u'\u005cud835' u'\u005cudeaa' , \u005cmat u'\u005cu2062' P , and \u005cmat u'\u005cu2062' Q are constructed respectively by arranging u'\u005cu039b' u'\u005cu2192' l , u'\u005cu0393' u'\u005cu2192' l , p u'\u005cu2192' l , and q u'\u005cu2192' l vectors as columns
p3109
aVIt is based on the two block NIPALS routine [] and iteratively discovers L pairs of vectors ( u'\u005cu039b' u'\u005cu2192' l , u'\u005cu0393' u'\u005cu2192' l ) such that the covariances, Cov u'\u005cu2062' ( u'\u005cu039b' u'\u005cu2192' l , u'\u005cu0393' u'\u005cu2192' l ) , are maximised under the constraint \u005cnorm u'\u005cu2062' p u'\u005cu2192' l = \u005cnorm u'\u005cu2062' q u'\u005cu2192' l = 1
p3110
aVThis is an important point, and means that the distribution prediction method is independent of the task to which it may subsequently be applied
p3111
aVAs we go on to show in Section 6 , this enables us to use the same distribution prediction method for both POS tagging and sentiment classification
p3112
aVNext, for each word w in a source domain labeled (i.e., manually POS tagged) sentence, we select its neighbours u ( i ) in the source domain as additional features
p3113
aVSpecifically, we measure the similarity, sim u'\u005cu2062' ( u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccS ) , between the source domain distributions of u ( i ) and w , and select the top r similar neighbours u ( i ) for each word w as additional features for w
p3114
aVWe refer to such features as distributional features in this work
p3115
aVThe value of a neighbour u ( i ) selected as a distributional feature is set to its similarity score sim u'\u005cu2062' ( u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccS
p3116
aVAt test time, for each word w that appears in a target domain test sentence, we measure the similarity, sim u'\u005cu2062' ( \u005cmat u'\u005cu2062' M u'\u005cu2062' u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccT ) , and select the most similar r words u ( i ) in the source domain labeled sentences as the distributional features for w , with their values set to sim u'\u005cu2062' ( \u005cmat u'\u005cu2062' M u'\u005cu2062' u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccT
p3117
aVFirst, we lemmatise each word in a source domain labeled review x u'\u005cu2192' \u005ccS ( i ) , and extract both unigrams and bigrams as features to represent x u'\u005cu2192' \u005ccS ( i ) by a binary-valued feature vector
p3118
aVNext, we train a PLSR model, \u005cmat u'\u005cu2062' M , as described in Section 3.2 using unlabeled reviews in the source and target domains
p3119
aVAt test time, we represent a test target review H using a binary-valued feature vector h u'\u005cu2192' of unigrams and bigrams of lemmas of the words in H , as we did for source domain labeled train reviews
p3120
aVIn particular, we do not find distributional features independently for each word in the test review
p3121
aVThis enables us to find distributional features that are consistent with all the features in a test review
p3122
aVFor representation, we considered distributional features u ( i ) in descending order of their scores given by Equation 4 , and then taking the inverse-rank as the values for the distributional features []
p3123
aVHowever, none of these alternatives resulted in performance gains
p3124
aVWe observed that setting r larger than 10 did not result in significant improvements in tagging accuracy, but only increased the train time due to the larger feature space
p3125
aVConsequently, we set r = 10 in POS tagging
p3126
aVTo evaluate DA for POS tagging, following , we use sections 2 - 21 from Wall Street Journal (WSJ) as the source domain labeled data
p3127
aVAn additional 100 , 000 WSJ sentences from the 1988 release of the WSJ corpus are used as the source domain unlabeled data
p3128
aVFollowing , we use the POS labeled sentences in the SACNL dataset [] for the five target domains
p3129
aVWe use the standard split of 800 positive and 800 negative labeled reviews from each domain as training data, and the remainder for testing
p3130
aVFor each domain \u005ccD in the SANCL (POS tagging) and Amazon review (sentiment classification) datasets, we create a PPMI weighted co-occurrence matrix \u005cmat u'\u005cu2062' F \u005ccD
p3131
aVIn cross-domain POS tagging, WSJ is always the source domain, whereas the five domains in SANCL dataset are considered as the target domains
p3132
aVFor this setting we have 9822 pivots on average
p3133
aVBy limiting the evaluation to unseen words instead of all words, we can evaluate the gain in POS tagging accuracy solely due to DA
p3134
aVIf w does not appear in the target domain, then w u'\u005cu2192' \u005ccT is set to the zero vector
p3135
aVThe DA method proposed in Section 4.1 is shown as the Proposed method
p3136
aVFilter denotes the training set filtering method proposed by for the DA of POS taggers
p3137
aVRecall that the \u005ccT p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d baseline cannot find source domain words that do not appear in the target domain as distributional features for the words in the target domain test reviews
p3138
aVTherefore, when the overlap between the vocabularies used in the source and the target domains is small, \u005ccT p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d cannot reduce the mismatch between the feature spaces
p3139
aVThe improvements of Proposed over the previously proposed Filter are statistically significant in all domains except the Emails domain (denoted by u'\u005cu2020' in Table 2 according to the Binomial exact test at 95 u'\u005cu2062' % confidence
p3140
aVThe baselines NA , \u005ccS u'\u005cud835' u'\u005cudc29' u'\u005cud835' u'\u005cudc2b' u'\u005cud835' u'\u005cudc1e' u'\u005cud835' u'\u005cudc1d' , and \u005ccT u'\u005cud835' u'\u005cudc29' u'\u005cud835' u'\u005cudc2b' u'\u005cud835' u'\u005cudc1e' u'\u005cud835' u'\u005cudc1d' are defined similarly as in Section 6.1
p3141
aVSST is the Sentiment Sensitive Thesaurus proposed by
p3142
aVAll methods are evaluated under the same settings, including train/test split, feature spaces, pivots, and classification algorithms so that any differences in performance can be directly attributable to their domain adaptability
p3143
aVThis upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for the target domain
p3144
aVIn contrast, our Proposed method predicts the distribution of a word in the target domain, given its distribution in the source domain, thereby explicitly translating the source domain reviews to the target
p3145
aVThis property enables us to apply the proposed distribution prediction method to tasks other than sentiment analysis such as POS tagging where we must identify distributional features for individual words
p3146
aVTo evaluate the effect of the PLSR dimensions, we fixed k = 1000 and measured the cross-domain sentiment classification accuracy over a range of L values
p3147
aVAs shown in Figure 2 , accuracy remains stable across a wide range of PLSR dimensions
p3148
aVBecause the time complexity of Algorithm 3.2 increases linearly with L , it is desirable that we select smaller L values in practice
p3149
aVBecause the dimensionality of the source and target domain feature spaces is equal to k , the complexity of the least square regression problem increases with k
p3150
aVTherefore, larger k values result in overfitting to the train data and classification accuracy is reduced on the target test data
p3151
aVAs an example of the distribution prediction method, in Table 3 we show the top 3 similar distributional features u in the books (source) domain, predicted for the electronics (target) domain word w = u'\u005cud835' u'\u005cudc59' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc54' u'\u005cu210e' u'\u005cud835' u'\u005cudc61' u'\u005cud835' u'\u005cudc64' u'\u005cud835' u'\u005cudc52' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc54' u'\u005cu210e' u'\u005cud835' u'\u005cudc61' , by different similarity measures
p3152
aVInterestingly, we see that by using the distributions predicted by the proposed method (i.e., sim u'\u005cu2062' ( \u005cmat u'\u005cu2062' M u'\u005cu2062' u \u005ccS , w \u005ccT ) ) we overcome this problem and find relevant distributional features from the source domain
p3153
aVAlthough for illustrative purposes we used the word lightweight , which occurs in both the source and the target domains, our proposed method does not require the source domain distribution w \u005ccS for a word w in a target domain document
p3154
aVTherefore, it can find distributional features even for words occurring only in the target domain, thereby reducing the feature mismatch between the two domains
p3155
asg88
(lp3156
sg90
(lp3157
sg92
(lp3158
VWe proposed a method to predict the distribution of a word across domains.
p3159
aVWe first create a distributional representation for a word using the data from a single domain, and then learn a Partial Least Square Regression (PLSR) model to predict the distribution of a word in a target domain given its distribution in a source domain.
p3160
aVWe evaluated the proposed method in two domain adaptation tasks cross-domain POS tagging and cross-domain sentiment classification.
p3161
aVOur experiments show that without requiring any task-specific customisations to our distribution prediction method, it outperforms competitive baselines and achieves comparable results to the current state-of-the-art domain adaptation methods.
p3162
ag106
asg107
S'P14-1058'
p3163
sg109
(lp3164
VAlthough the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word s predominant meaning changes.
p3165
aVHowever, if it were possible to predict how the distribution of a word changes from one domain to another, the predictions could be used to adapt a system trained in one domain to work in another.
p3166
aVWe propose an unsupervised method to predict the distribution of a word in one domain, given its distribution in another domain.
p3167
aVWe evaluate our method on two tasks cross-domain part-of-speech tagging and cross-domain sentiment classification.
p3168
aVIn both tasks, our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods, while requiring no task-specific customisations.
p3169
ag106
asba(icmyPackage
FText
p3170
(dp3171
g3
(lp3172
VFor instance, successfully interpreting a sentence such as
p3173
aVrequires the knowledge that the semantic connotations of u'\u005cu2018' kicking the bucket u'\u005cu2019' as a unit are the same as those for u'\u005cu2018' dying u'\u005cu2019'
p3174
aVThat is, a non-compositional phrase such as u'\u005cu2018' kick the bucket u'\u005cu2019' is likely to persist in common parlance only if it is frequently used with its associated semantic mapping
p3175
aVIn particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below
p3176
aVDistributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics [ 26 ]
p3177
aVMost significantly, word tokens that act as latent dimensions are often derived from arbitrary tokenization
p3178
aVWhile helpful, the representation seems unsatisfying since words such as u'\u005cu2018' press u'\u005cu2019' , u'\u005cu2018' wake u'\u005cu2019' and u'\u005cu2018' shores u'\u005cu2019' seem to have little to do with a crisis
p3179
aVThis is the overarching theme of this work we present a frequency driven paradigm for extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive, recurrent lexical units or motifs
p3180
aVThrough exploiting regularities in language usage, the framework can efficiently account for both compositional and non-compositional word usage, while avoiding the issue of data-sparsity by design
p3181
aVWe present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens
p3182
aVSection 3 describes our methodology, which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications
p3183
aVWhile word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings
p3184
aVStructural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels
p3185
aVWhile this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between word-nodes very weakly
p3186
aVFigure 1 shows an example of the shortcomings of this general approach
p3187
aVWhile existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current framework, and the feature-set for our motif segmentation model is designed to subsume most of these ideas
p3188
aV1 1 We note that since we take motifs as lineal units, the current method doesn u'\u005cu2019' t subsume several common non-contiguous MWEs such as u'\u005cu2018' let off u'\u005cu2019' in u'\u005cu2018' let him off u'\u005cu2019'
p3189
aVThe features are chosen so as to best represent frequency-based, statistical as well as linguistic considerations for treating a segment as an agglutinative unit, or a motif
p3190
aVThe model is an instantiation of a simple featurized HMM, and the weighted sum of features corresponding to a segment is cognate with an affinity score for the u'\u005cu2018' stickiness u'\u005cu2019' of the segment, i.e.,, the affinity for the segment to be treated as holistic unit or a single motif
p3191
aVWe also associate a penalizing cost for each non unary-motif to avoid aggressive agglutination of tokens
p3192
aVIn particular, for an ngram occurrence to be considered a motif, the marginal contribution due to the affinity of the prospective motif should at minimum exceed this penalty
p3193
aVWe describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision
p3194
aVUpdates are run for a large number of iterations until the change in objective drops below a threshold, and the learning rate u'\u005cu0391' is adaptively modified as described in Collins et al
p3195
aVImplicitly, the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring (Viterbi) state sequences, and the label state sequences
p3196
aVWhile the Viterbi algorithm can be used for tagging optimal state-sequences given the weights, the structured perceptron can learn optimal model weights given gold-standard sequence labels
p3197
aVHence, in this case, we use a variation of the hard EM algorithm for learning
p3198
aVThis would involve initializing the weights prior to the semi-supervised procedure with the weights from the supervised learning model, so as to seed the semi-supervised approach with reasonable model, and use the partially annotated data to fine-tune the supervised model
p3199
aVSince our state definitions preclude certain transitions (such as from state T 2 to T 1 ), these weights are initialized to - u'\u005cu221e' to expedite training
p3200
aVN-gram penalties f n u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m We define a penalty for tagging each non-unary motif as described before
p3201
aVFor a motif to be tagged, the improvement in objective score should at least exceed the corresponding penalty e.g.,, f q u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m u'\u005cu2062' ( y i ) = I y i = Q 4 denotes the penalty for tagging a tetragram
p3202
aVThis feature is associated with a particular token-sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence matches the feature token-sequence, and is marked as with a matching tag e.g.,, f b u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m ( x i - 1 = l o v e , x i = story , y i = B 2 )
p3203
aVThis feature is associated with a particular POS-tag sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence gets a matching tag, and is marked as with a matching ngram tag e.g.,, f b u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m ( p i - 1 = V B , p i = N N , y i = B 2 )
p3204
aVAdditionally, a few feature for the segmentations model contained minor orthographic features based on word shape (length and capitalization patterns
p3205
aVSince the segmentation model accounts for the contexts of the entire sentence in determining motifs, different instances of the same token could evoke different meaning representations
p3206
aVConsider the following sentences tagged by the segmentation model, that would correspond to different representations of the token u'\u005cu2018' remains u'\u005cu2019' once as a standalone motif, and once as part of an encompassing bigram motif ( u'\u005cu2018' remains classified u'\u005cu2019'
p3207
aVEven with the release of such documents, questions are not answered, since only the agency knows what remains classified
p3208
aVGiven constituent motifs of each sentence in the data, we can now define neighbourhood distributions for unary or phrasal motifs in terms of other motifs (as envisioned in Table 1
p3209
aVIn our experiments, we use a window-length of 5 adjoining motifs on either side to define the neighbourhood of a constituent
p3210
aVRecent work [ 13 ] has shown that the Hellinger distance is an especially effective measure in learning distributional embeddings, with Hellinger PCA being much more computationally inexpensive than neural language modeling approaches, while performing much better than standard PCA, and competitive with the state-of-the-art in downstream evaluations
p3211
aVHence, we use the Hellinger measure between neighbourhood motif distributions in learning representations
p3212
aVThe Hellinger measure has intuitively desirable properties specifically, it can be seen as the Euclidean distance between the square-roots transformed distributions, where both vectors P and Q are length-normalized under the same(Euclidean) norm
p3213
aVIn an evaluation of the motif segmentations model within the perspective of our framework, we believe that exact correspondence to human judgment is unrealistic, since guiding principles for defining motifs, such as semantic cohesion, are hard to define and only serve as working principles
p3214
aVAlso, since a majority of motifs are unary tokens, including them into consideration artificially boosts the accuracy, whereas we are more interested in the prediction of larger n-gram tokens
p3215
aVHence we report results on the performance on only non-unary motifs
p3216
aVWe observe that this model has a very high precision (since many token sequences marked as motifs would recur in similar contexts, and would thus have the same motif boundaries
p3217
aVHowever, the rule-based method has a very row recall due to lack of generalization capabilities
p3218
aVThis is not unexpected the supervision provided to the model is very weak due to a lack of negative examples (which leads to spurious motif taggings, leading to a low precision), as well as no examples of transitions between adjacent motifs (to learn transitional weights and penalties
p3219
aVFor sentence polarity, we consider the Cornell Sentence Polarity corpus by Pang and Lee ( 2005 ) , where the task is to classify the polarity of a sentence as positive or negative
p3220
aVThe data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative
p3221
aVFor composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences, using a vector representation at each graph node that representing a single lexical token
p3222
aVFor our purposes, we modify the approach to merge the nodes of all tokens that constitute a motif occurrence, and use the motif representation as the vector associated with the node
p3223
aVFor this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al
p3224
aVThe data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal
p3225
aVFor this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model
p3226
aVThe qualitative and quantitative analyis of results from our preliminary motif segmentation model indicate that such motifs can help to disambiguate contexts of single tokens, and provide cleaner, more interpretable representations
p3227
aVThe flexibility of having separate representations to model different semantic senses has considerable valuable, as compared with extant approaches that assign a single representation to each token, and are hence constrained to conflate several semantic senses into a common representation
p3228
asg88
(lp3229
sg90
(lp3230
sg92
(lp3231
VWe have presented a new frequency-driven framework for distributional semantics of not only lexical items but also longer cohesive motifs.
p3232
aVThe theme of this work is a general paradigm of seeking motifs that are recurrent in common parlance, are semantically coherent, and are possibly non-compositional.
p3233
aVSuch a framework for distributional models avoids the issue of data sparsity in learning of representations for larger linguistic structures.
p3234
aVThe approach depends on drawing features from frequency statistics, statistical correlations, and linguistic theories; and this work provides a computational framework to jointly model recurrence and semantic cohesiveness of motifs through compositional penalties and affinity scores in a data driven way.
p3235
aVWhile being deliberately vague in our working definition of motifs, we have presented simple efficient formulations to extract such motifs that uses both annotated as well as partially unannotated data.
p3236
aVThe qualitative and quantitative analyis of results from our preliminary motif segmentation model indicate that such motifs can help to disambiguate contexts of single tokens, and provide cleaner, more interpretable representations.
p3237
aVFinally, we obtain motif representations in form of low-dimensional vector-space embeddings, and our experimental findings indicate value of the learnt representations in downstream applications.
p3238
aVWe believe that the approach has considerable theoretical as well as practical merits, and provides a simple and clean formulation for modeling phrasal and sentential semantics.
p3239
aVIn particular, we believe that ours is the first method that can invoke different meaning representations for a token depending on textual context of the sentence.
p3240
aVThe flexibility of having separate representations to model different semantic senses has considerable valuable, as compared with extant approaches that assign a single representation to each token, and are hence constrained to conflate several semantic senses into a common representation.
p3241
aVThe approach also elegantly deals with the problematic issue of differential compositional and non-compositional usage of words.
p3242
aVFuture work can focus on a more thorough quantitative evaluation of the paradigm, as well as extension to model non-contiguous motifs.
p3243
ag106
asg107
S'P14-1060'
p3244
sg109
(lp3245
VTraditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items.
p3246
aVIn this work, we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents, or motifs.
p3247
aVThe framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents, and circumvents some problems of data sparsity by design.
p3248
aVWe design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated.
p3249
aVHellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.
p3250
ag106
asba(icmyPackage
FText
p3251
(dp3252
g3
(lp3253
VSince individual sentences are rarely observed or not observed at all, one must represent a sentence in terms of features that depend on the words and short n -grams in the sentence that are frequently observed
p3254
aVA central class of models are those based on neural networks
p3255
aVThey can be trained to obtain generic vectors for words and phrases by predicting, for instance, the contexts in which the words and phrases occur
p3256
aVThrough supervised training, neural sentence models can fine-tune these vectors to information that is specific to a certain task
p3257
aVBesides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word []
p3258
aVSecondly, the pooling parameter k can be dynamically chosen by making k a function of other aspects of the network or the input
p3259
aVLike in the convolutional networks for object recognition [] , we enrich the representation in the first layer by computing multiple feature maps with different filters applied to the input sentence
p3260
aVSubsequent layers also have multiple feature maps computed by convolving filters with all the maps from the layer below
p3261
aVThe network matches the accuracy of other state-of-the-art methods that are based on large sets of engineered features and hand-coded knowledge resources
p3262
aVThe network is trained on 1.6 million tweets labelled automatically according to the emoticon that occurs in them
p3263
aVBy adding a max pooling layer to the network, the TDNN can be adopted as a sentence model []
p3264
aVVarious neural sentence models have been described
p3265
aVThe RNN is primarily used as a language model, but may also be viewed as a sentence model with a linear structure
p3266
aVFinally, a further class of neural sentence models is based on the convolution operation and the TDNN architecture []
p3267
aVConcretely, we think of u'\u005cud835' u'\u005cudc2c' as the input sentence and u'\u005cud835' u'\u005cudc2c' i u'\u005cu2208' u'\u005cu211d' is a single feature value associated with the i -th word in the sentence
p3268
aVOut-of-range input values u'\u005cud835' u'\u005cudc2c' i where i 1 or i s are taken to be zero
p3269
aVThe result of the narrow convolution is a subsequence of the result of the wide convolution
p3270
aVA TDNN convolves a sequence of inputs u'\u005cud835' u'\u005cudc2c' with a set of weights u'\u005cud835' u'\u005cudc26'
p3271
aVAs in the TDNN for phoneme recognition [] , the sequence u'\u005cud835' u'\u005cudc2c' is viewed as having a time dimension and the convolution is applied over the time dimension
p3272
aVEach u'\u005cud835' u'\u005cudc2c' j is often not just a single value, but a vector of d values so that u'\u005cud835' u'\u005cudc2c' u'\u005cu2208' u'\u005cu211d' d × s
p3273
aVMultiple convolutional layers may be stacked by taking the resulting sequence u'\u005cud835' u'\u005cudc1c' as input to the next layer
p3274
aVThe Max-TDNN sentence model is based on the architecture of a TDNN []
p3275
aVThe fixed-sized vector u'\u005cud835' u'\u005cudc1c' m u'\u005cu2062' a u'\u005cu2062' x is then used as input to a fully connected layer for classification
p3276
aVThe Max-TDNN model has many desirable properties
p3277
aVIncreasing m or stacking multiple convolutional layers of the narrow type makes the range of the feature detectors larger; at the same time it also exacerbates the neglect of the margins of the sentence and increases the minimum size s of the input sentence required by the convolution
p3278
aVFor this reason higher-order and long-range feature detectors cannot be easily incorporated into the model
p3279
aVIn the network the width of a feature map at an intermediate layer varies depending on the length of the input sentence; the resulting architecture is the Dynamic Convolutional Neural Network
p3280
aVA convolutional layer in the network is obtained by convolving a matrix of weights u'\u005cud835' u'\u005cudc26' u'\u005cu2208' u'\u005cu211d' d × m with the matrix of activations at the layer below
p3281
aVFor example, the second layer is obtained by applying a convolution to the sentence matrix u'\u005cud835' u'\u005cudc2c' itself
p3282
aVThis guarantees that the input to the fully connected layers is independent of the length of the input sentence
p3283
aVBut, as we see next, at intermediate convolutional layers the pooling parameter k is not fixed, but is dynamically selected in order to allow for a smooth extraction of higher-order and longer-range features
p3284
aVFor an example in sentiment prediction, according to the equation a first order feature such as a positive word occurs at most k 1 times in a sentence of length s , whereas a second order feature such as a negated phrase or clause occurs at most k 2 times
p3285
aVIf we temporarily ignore the pooling layer, we may state how one computes each d -dimensional column a in the matrix u'\u005cud835' u'\u005cudc1a' resulting after the convolutional and non-linear layers
p3286
aVSecond order features are similarly obtained by applying Eq
p3287
aVWe denote a feature map of the i -th order by u'\u005cud835' u'\u005cudc05' i
p3288
aVAs in convolutional networks for object recognition, to increase the number of learnt feature detectors of a certain order, multiple feature maps u'\u005cud835' u'\u005cudc05' 1 i , u'\u005cu2026' , u'\u005cud835' u'\u005cudc05' n i may be computed in parallel at the same layer
p3289
aVEach feature map u'\u005cud835' u'\u005cudc05' j i is computed by convolving a distinct set of filters arranged in a matrix u'\u005cud835' u'\u005cudc26' j , k i with each feature map u'\u005cud835' u'\u005cudc05' k i - 1 of the lower order i - 1 and summing the results
p3290
aVwhere * indicates the wide convolution
p3291
aVFull dependence between different rows could be achieved by making u'\u005cud835' u'\u005cudc0c' in Eq
p3292
aVFor a map of d rows, folding returns a map of d / 2 rows, thus halving the size of the representation
p3293
aVWith a folding layer, a feature detector of the i -th order depends now on two rows of feature values in the lower maps of order i - 1
p3294
aVWe describe some of the properties of the sentence model based on the DCNN
p3295
aVThe filters u'\u005cud835' u'\u005cudc26' of the wide convolution in the first layer can learn to recognise specific n -grams that have size less or equal to the filter width m ; as we see in the experiments, m in the first layer is often set to a relatively large value such as 10
p3296
aVA sentence model based on a recurrent neural network is sensitive to word order, but it has a bias towards the latest words that it takes as input []
p3297
aVThis gives the RNN excellent performance at language modelling, but it is suboptimal for remembering at once the n -grams further back in the input sentence
p3298
aVA node from a layer is connected to a node from the next higher layer if the lower node is involved in the convolution that computes the value of the higher node
p3299
aVHigher-order features have highly variable ranges that can be either short and focused or global and long as the input sentence
p3300
aVLikewise, the edges of a subgraph in the induced graph reflect these varying ranges
p3301
aVThe feature extraction function as stated in Eq
p3302
aV6 has a more general form than that in a RecNN, where the value of m is generally 2
p3303
aVWe begin by specifying aspects of the implementation and the training of the network
p3304
aVUsing the well-known convolution theorem, we can compute fast one-dimensional linear convolutions at all rows of an input matrix by using Fast Fourier Transforms
p3305
aVLikewise, in the fine-grained case, we use the standard 8544/1101/2210 splits
p3306
aVLabelled phrases that occur as subparts of the training sentences are treated as independent training instances
p3307
aVThe binary result is based on a DCNN that has a wide convolutional layer followed by a folding layer, a dynamic k -max pooling layer and a non-linearity; it has a second wide convolutional layer followed by a folding layer, a k -max pooling layer and a non-linearity
p3308
aVThe network is topped by a softmax classification layer
p3309
aVThe DCNN for the fine-grained result has the same architecture, but the filters have size 10 and 7, the top pooling parameter k is 5 and the number of maps is, respectively, 6 and 12
p3310
aVThe Max-TDNN performs worse than the NBoW likely due to the excessive pooling of the max pooling operation; the latter discards most of the sentiment features of the words in the input sentence
p3311
aVIn the next experiment we compare the performance of the DCNN with those of methods that use heavily engineered resources
p3312
aVAs an aid to question answering, a question may be classified as belonging to one of many question types
p3313
aVThe difference in performance between the DCNN and the NBoW further suggests that the ability of the DCNN to both capture features based on long n -grams and to hierarchically combine these features is highly beneficial
p3314
aVSince the filters have width 7, for each of the 288 feature detectors we rank all 7 -grams occurring in the validation and test sets according to their activation of the detector
p3315
aVWe find detectors for multiple other notable constructs including u'\u005cu2018' all u'\u005cu2019' , u'\u005cu2018' or u'\u005cu2019' , u'\u005cu2018' with u'\u005cu2026' that u'\u005cu2019' , u'\u005cu2018' as u'\u005cu2026' as u'\u005cu2019'
p3316
aVThe feature detectors learn to recognise not just single n -grams, but patterns within n -grams that have syntactic, semantic or structural significance
p3317
asg88
(lp3318
sg90
(lp3319
sg92
(lp3320
VWe have described a dynamic convolutional neural network that uses the dynamic k -max pooling operator as a non-linear subsampling function.
p3321
aVThe feature graph induced by the network is able to capture word relations of varying size.
p3322
aVThe network achieves high performance on question and sentiment classification without requiring external features as provided by parsers or other resources.
p3323
ag106
asg107
S'P14-1062'
p3324
sg109
(lp3325
VThe ability to accurately represent sentences is central to language understanding.
p3326
aVWe describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences.
p3327
aVThe network uses Dynamic k -Max Pooling, a global pooling operation over linear sequences.
p3328
aVThe network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations.
p3329
aVThe network does not rely on a parse tree and is easily applicable to any language.
p3330
aVWe test the DCNN in four experiments small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision.
p3331
aVThe network achieves excellent performance in the first three tasks and a greater than 25 % error reduction in the last task with respect to the strongest baseline.
p3332
ag106
asba(icmyPackage
FText
p3333
(dp3334
g3
(lp3335
VWe propose an online learning algorithm based on tensor-space models
p3336
aVWe apply with the proposed algorithm to a parsing task, and show that even with very little training data the learning algorithm based on a tensor model performs well, and gives significantly better results than standard learning algorithms based on traditional vector-space models
p3337
aVMany NLP applications use models that try to incorporate a large number of linguistic features so that as much human knowledge of language can be brought to bear on the (prediction) task as possible
p3338
aVThis also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets the feature weights cannot be estimated reliably
p3339
aVMost traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space
p3340
aVMany learning algorithms applied to NLP problems, such as the Perceptron [ Collins2002 ] , MIRA [ Crammer et al.2006 , McDonald et al.2005 , Chiang et al.2008 ] , PRO [ Hopkins and May2011 ] , RAMPION [ Gimpel and Smith2012 ] etc., are based on vector-space models
p3341
aVSuch models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set
p3342
aVData can be represented in a compact and structured way using tensors as containers
p3343
aVTensor representations have been applied to computer vision problems [ Hazan et al.2005 , Shashua and Hazan2005 ] and information retrieval [ Cai et al.2006a ] a long time ago
p3344
aVA linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors
p3345
aVMost of the learning algorithms for NLP problems are based on vector space models, which represent data as vectors u'\u005cu03a6' u'\u005cu2208' u'\u005cu211d' n , and try to learn feature weight vectors u'\u005cud835' u'\u005cudc98' u'\u005cu2208' u'\u005cu211d' n such that a linear model y = u'\u005cud835' u'\u005cudc98' u'\u005cu22c5' u'\u005cu03a6' is able to discriminate between, say, good and bad hypotheses
p3346
aVSpecifically, a vector is a 1 st order tensor, a matrix is a 2 nd order tensor, and data organized as a rectangular cuboid is a 3 rd order tensor etc
p3347
aVIn general, a D th order tensor is represented as u'\u005cud835' u'\u005cudcaf' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062' n D , and an entry in u'\u005cud835' u'\u005cudcaf' is denoted by u'\u005cud835' u'\u005cudcaf' i 1 , i 2 , u'\u005cu2026' , i D
p3348
aVUsing a D th order tensor as container, we can assign each feature of the task a D -dimensional index in the tensor and represent the data as tensors
p3349
aVA D th order tensor u'\u005cud835' u'\u005cudc9c' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062' n D is rank-1 if it can be written as the outer product of D vectors, i.e
p3350
aVwhere u'\u005cud835' u'\u005cudc1a' i u'\u005cu2208' u'\u005cu211d' n d , 1 u'\u005cu2264' d u'\u005cu2264' D
p3351
aVIf u'\u005cud835' u'\u005cudc7e' is further decomposed as the sum of H major component rank-1 tensors, i.e., u'\u005cud835' u'\u005cudc7e' u'\u005cu2248' u'\u005cu2211' h = 1 H u'\u005cud835' u'\u005cudc98' h 1 u'\u005cu2297' u'\u005cud835' u'\u005cudc98' h 2 u'\u005cu2297' , u'\u005cu2026' , u'\u005cu2297' u'\u005cud835' u'\u005cudc98' h D , then
p3352
aVThe linear tensor model is illustrated in Figure 1
p3353
aVSo what is the advantage of learning with a tensor model instead of a vector model
p3354
aVHowever if we use a 2 nd order tensor model, organize the features into a 1000 × 1000 matrix u'\u005cud835' u'\u005cudebd' , and use just one rank-1 matrix to approximate the weight tensor, then the linear model becomes
p3355
aVIn general, if V features are defined for a learning problem, and we (i) organize the feature set as a tensor u'\u005cud835' u'\u005cudebd' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062' n D and (ii) use H component rank-1 tensors to approximate the corresponding target weight tensor
p3356
aVThen the total number of parameters to be learned for this tensor model is H u'\u005cu2062' u'\u005cu2211' d = 1 D n d , which is usually much smaller than V = u'\u005cu220f' d = 1 D n d for a traditional vector space model
p3357
aVTherefore we expect the tensor model to be more effective in a low-resource training environment
p3358
aVSpecifically, a vector space model assumes each feature weight to be a u'\u005cu201c' free u'\u005cu201d' parameter, and estimating them reliably could therefore be hard when training data are not sufficient or the feature set is huge
p3359
aVThis approximation can be treated as a form of model regularization, since the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation
p3360
aVOf course, as we reduce the model complexity, e.g., by choosing a smaller and smaller H , the model u'\u005cu2019' s expressive ability is weakened at the same time
p3361
aVOnce the structure of u'\u005cud835' u'\u005cudebd' is determined, the structure of u'\u005cud835' u'\u005cudc7e' is fixed as well
p3362
aVAs mentioned in Section 2.1 , a tensor model has many more degrees of u'\u005cu201c' design freedom u'\u005cu201d' than a vector model, which makes the problem of finding a good tensor structure a nontrivial one
p3363
aVWe assume H = 1 in the analysis below, noting that one can always add as many rank-1 component tensors as needed to approximate a tensor with arbitrary precision
p3364
aVObviously, the 1 st order tensor (vector) model is the most expressive, since it is structureless and any arbitrary set of numbers can always be represented exactly as a vector
p3365
aVThe 2 nd order rank-1 tensor (rank-1 matrix) is less expressive because not every set of numbers can be organized into a rank-1 matrix
p3366
aVIn general, a D th order rank-1 tensor is more expressive than a ( D + 1 ) th order rank-1 tensor, as a lower-order tensor imposes less structural constraints on the set of numbers it can express
p3367
aVAssuming that a D th order has equal size on each mode (we will elaborate on this point in Section 3.2 ) and the volume (number of entries) of the tensor is fixed as V , then the total number of parameters of the model is D u'\u005cu2062' V 1 D
p3368
aVThis is a convex function of D , and the minimum 2 2 The optimal integer solution can be determined simply by comparing the two function values is reached at either D u'\u005cu2217' = \u005cfloor u'\u005cu2062' ln u'\u005cu2061' V or D u'\u005cu2217' = \u005cceil u'\u005cu2062' ln u'\u005cu2061' V
p3369
aVTherefore, as D increases from 1 to D * , we lose more and more of the expressive power of the model but reduce the number of parameters to be trained
p3370
aVFor example, if the tensor order is 2 and the volume V is 12, then we can either choose n 1 = 3 , n 2 = 4 or n 1 = 2 , n 2 = 6
p3371
aVHowever it is hard to know the structure of target feature weights before learning, and it would be impractical to try every possible combination of mode sizes, therefore we choose the criterion of determining the mode sizes as minimization of the total number of parameters, namely we solve the problem
p3372
aVOf course it is not guaranteed that V 1 D is an integer, therefore we choose n d = \u005cfloor u'\u005cu2062' V 1 D or \u005cceil u'\u005cu2062' V 1 D , d = 1 , u'\u005cu2026' , D such that u'\u005cu220f' d = 1 D n d u'\u005cu2265' V and [ u'\u005cu220f' d = 1 D n d ] - V is minimized
p3373
aVSince for each n d there are only two possible values to choose, we can simply enumerate all the possible 2 D (which is usually a small number) combinations of values and pick the one that matches the conditions given above
p3374
aVWhile this strategy might work well with small amount of training data, it is not guaranteed to be the best strategy in all cases, especially when more data is available we might want to increase the number of parameters, making the model more complex so that the data can be more precisely modeled
p3375
aVThe impact of using H 1 rank-1 tensors is obvious a larger H increases the model complexity and makes the model more expressive, since we are able to approximate target weight tensor with smaller error
p3376
aVAs a trade-off, the number of parameters and training complexity will be increased
p3377
aVUnfortunately we have no knowledge about the target weights in advance, since that is what we need to learn after all
p3378
aVAs a way out, we first run a simple vector-model based learning algorithm (say the Perceptron) on the training data and estimate a weight vector, which serves as a u'\u005cu201c' surrogate u'\u005cu201d' weight vector
p3379
aVHowever matrix rank minimization is in general a hard problem [ Fazel2002 ]
p3380
aVTherefore, we follow an approximate algorithm given in Figure 4 , whose main idea is illustrated via an example in Figure 4
p3381
aVBasically, what the algorithm does is to divide the surrogate weights into hierarchical groups such that groups on the same level are approximately proportional to each other
p3382
aVUsing these groups as units we are able to u'\u005cu201c' fill u'\u005cu201d' the tensor in a hierarchical way
p3383
aVThis of course ignores correlation between features since the original feature order in the vector could be totally meaningless, and this strategy is not expected to be a good solution for vector to tensor mapping
p3384
aVThis problem setting follows the same u'\u005cu201c' passive-aggressive u'\u005cu201d' strategy as in the original MIRA
p3385
aVTo optimize the vectors u'\u005cud835' u'\u005cudc98' h d , h = 1 , u'\u005cu2026' , H , d = 1 , u'\u005cu2026' , D , we use a similar iterative strategy as proposed in [ Cai et al.2006b ]
p3386
aVBasically, the idea is that instead of optimizing u'\u005cud835' u'\u005cudc98' h d all together, we optimize u'\u005cud835' u'\u005cudc98' 1 1 , u'\u005cud835' u'\u005cudc98' 1 2 , u'\u005cu2026' , u'\u005cud835' u'\u005cudc98' H D in turn
p3387
aVFor the problem setting given above, each of the sub-problems that need to be solved is convex, and according to [ Cai et al.2006b ] the objective function value will decrease after each individual weight update and eventually this procedure will converge
p3388
aVOnce a vector has been updated, it is fixed for future updates
p3389
aVThe initial vectors u'\u005cud835' u'\u005cudc98' h , 1 i cannot be made all zero, since otherwise the l -mode product in Equation ( 9 ) would yield all zero u'\u005cu03a6' h , t d u'\u005cu2062' ( x , y ) and the model would never get a chance to be updated
p3390
aVTherefore, we initialize the entries of u'\u005cud835' u'\u005cudc98' h , 1 i uniformly such that the Frobenius-norm of the weight tensor u'\u005cud835' u'\u005cudc7e' is unity
p3391
aVWe would like to observe from the experiments how the amount of training data as well as different settings of the tensor degrees of freedom affects the algorithm performance
p3392
aVTherefore we tried all combinations of the following experimental parameters
p3393
aVAccording to the strategy given in 3.2 , once the tensor order and number of features are fixed, the sizes of modes and total number of parameters to estimate are fixed as well, as shown in the tables below
p3394
aVWhen very few labeled data are available for training (compared with the number of features), T-MIRA performs much better than the vector-based models MIRA and Perceptron
p3395
aVHowever as the amount of training data increases, the advantage of T-MIRA fades away, and vector-based models catch up
p3396
aVThis is because the weight tensors learned by T-MIRA are highly structured, which significantly reduces model/training complexity and makes the learning process very effective in a low-resource environment, but as the amount of data increases, the more complex and expressive vector-based models adapt to the data better, whereas further improvements from the tensor model is impeded by its structural constraints, making it insensitive to the increase of training data
p3397
aVNevertheless, with a huge number of parameters to fit a limited amount of data, they tend to over-fit and give much worse results on the held-out set than T-MIRA does
p3398
aVAs an aside, observe that MIRA consistently outperformed Perceptron, as expected
p3399
aVFrom the contrast between the largest and the 2 nd -largest singular values, it can be seen that the matrix generated by the first strategy approximates a low-rank structure much better than the second strategy
p3400
aVTherefore, the performance of T-MIRA is influenced significantly by the way features are mapped to the tensor
p3401
aVIf the corresponding target weight tensor has internal structure that makes it approximately low-rank, the learning procedure becomes more effective
p3402
aVThe best results are consistently given by 2 nd order tensor models, and the differences between the 3 rd and 4 th order tensors are not significant
p3403
aVAs discussed in Section 3.1 , although 3 rd and 4 th order tensors have less parameters, the benefit of reduced training complexity does not compensate for the loss of expressiveness
p3404
aVA 2 nd order tensor has already reduced the number of parameters from the original 1.33 million to only 2310, and it does not help to further reduce the number of parameters using higher order tensors
p3405
aVAs the amount of training data increases, there is a trend that the best results come from models with more rank-1 component tensors
p3406
aVA tensor-space model is a compact representation of data, and via rank-1 tensor approximation, the weight tensor can be made highly structured hence the number of parameters to be trained is significantly reduced
p3407
aVFor D = 1 , it is obvious that if a set of real numbers { x 1 , u'\u005cu2026' , x n } can be represented by a rank-1 matrix, it can always be represented by a vector, but the reverse is not true
p3408
aVand this representation is unique for a given D (up to the ordering of u'\u005cud835' u'\u005cudc29' j and s j d in u'\u005cud835' u'\u005cudc29' j , which simply assigns { x 1 , u'\u005cu2026' , x n } with different indices in the tensor), due to the pairwise proportional constraint imposed by x i / x j , i , j = 1 , u'\u005cu2026' , n
p3409
aVThen it must be the case that
p3410
aVsince otherwise { x 1 , u'\u005cu2026' , x n } would be represented by a different set of factors than those given in Equation ( 11
p3411
aVTherefore, in order for tensor u'\u005cud835' u'\u005cudcac' to represent the same set of real numbers that u'\u005cud835' u'\u005cudcab' represents, there needs to exist a vector [ s 1 d , u'\u005cu2026' , s n d d ] that can be represented by a rank-1 matrix as indicated by Equation ( 12 ), which is in general not guaranteed
p3412
asg88
(lp3413
sg90
(lp3414
sg92
(lp3415
VIn this paper, we reformulated the traditional linear vector-space models as tensor-space models, and proposed an online learning algorithm named Tensor-MIRA.
p3416
aVA tensor-space model is a compact representation of data, and via rank-1 tensor approximation, the weight tensor can be made highly structured hence the number of parameters to be trained is significantly reduced.
p3417
aVThis can be regarded as a form of model regularization.Therefore, compared with the traditional vector-space models, learning in the tensor space is very effective when a large feature set is defined, but only small amount of training data is available.
p3418
aVOur experimental results corroborated this argument.
p3419
aVAs mentioned in Section 3.2 , one interesting problem that merits further investigation is how to determine optimal mode sizes.
p3420
aVThe challenge of applying a tensor model comes from finding a proper tensor structure for a given problem, and the key to solving this problem is to find a balance between the model complexity (indicated by the order and sizes of modes) and the number of parameters.
p3421
aVDeveloping a theoretically guaranteed approach of finding the optimal structure for a given task will make the tensor model not only perform well in low-resource environments, but adaptive to larger data sets.
p3422
ag106
asg107
S'P14-1063'
p3423
sg109
(lp3424
VWe propose an online learning algorithm based on tensor-space models.
p3425
aVA tensor-space model represents data in a compact way, and via rank-1 approximation the weight tensor can be made highly structured, resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models.
p3426
aVThis regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training.
p3427
aVWe apply with the proposed algorithm to a parsing task, and show that even with very little training data the learning algorithm based on a tensor model performs well, and gives significantly better results than standard learning algorithms based on traditional vector-space models.
p3428
ag106
asba(icmyPackage
FText
p3429
(dp3430
g3
(lp3431
VIn this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data
p3432
aVOur work introduces a new take on the problem using graph-based semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources
p3433
aVOn the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§ 2.2
p3434
aVUnlike previous work [ 11 , 22 ] , we use higher order n -grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text
p3435
aVIf a source phrase is found in the baseline phrase table it is called a labeled phrase its conditional empirical probability distribution over target phrases (estimated from the parallel data) is used as the label, and is subsequently never changed
p3436
aVThe label space is thus the phrasal translation inventory, and like the source side it can also be represented in terms of a graph, initially consisting of target phrase nodes from the parallel corpus
p3437
aVFor the unlabeled phrases, the set of possible target translations could be extremely large (e.g.,, all target language n -grams
p3438
aVTherefore, we first generate and fix a list of possible target translations for each unlabeled source phrase
p3439
aVThe generation component is based on the observation that for structured label spaces, such as translation candidates for source phrases in SMT, even similar phrases have slightly different labels (target translations
p3440
aVThe exponential dependence of the sizes of these spaces on the length of instances is to blame
p3441
aVThus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances
p3442
aVWe therefore need to enrich the target or label space for unknown phrases
p3443
aVA naïve way to achieve this goal would be to extract all n -grams, from n = 1 to a maximum n -gram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases
p3444
aVWe refer to these additional candidates as u'\u005cu201c' generated u'\u005cu201d' candidates
p3445
aVBased on these functions, source and target sequences of words can be mapped to sequences of stems
p3446
aVThe morphological generation step adds to the target graph all target word sequences from the monolingual data that map to the same stem sequence as one of the target phrases occurring in the baseline phrase table
p3447
aVIn other words, this step adds phrases that are morphological variants of existing phrases, differing only in their affixes
p3448
aVTo determine pairwise phrase similarities in order to embed these nodes in their graphs, we utilize the monolingual corpora on both the source and target sides to extract distributional features based on the context surrounding each phrase
p3449
aV1 1 The q most frequent words in the monolingual corpus were removed as keys from this mapping, as these high entropy features do not provide much information
p3450
aVThe inverted index structure reduces the graph construction cost from u'\u005cu0398' u'\u005cu2062' ( n 2 ) , by only computing similarities for a subset of all possible pairs of phrases, namely other phrases that have at least one feature in common
p3451
aVAs mentioned previously, we construct and fix a set of translation candidates, i.e.,, the label set for each unlabeled source phrase
p3452
aV1 , this source would yield the cat and cat , among others, as candidates
p3453
aVThe generated candidates for the unlabeled phrase u'\u005cu2013' the ones from the baseline system u'\u005cu2019' s decoder output, or from a morphological generator (e.g.,, a cat and catlike in Fig
p3454
aVThe morphologically-generated candidates for a given source unlabeled phrase are initially defined as the target word sequences in the monolingual data that have the same stem sequence as one of the baseline u'\u005cu2019' s target translations for a source phrase which has the same stem sequence as the unlabeled source phrase
p3455
aVA graph propagation algorithm transfers label information from labeled nodes to unlabeled nodes by following the graph u'\u005cu2019' s structure
p3456
aVThis quantity is also known as the propagation probability, and its exact form will depend on the type of graph propagation algorithm used
p3457
aVFor our purposes, node f is a source phrasal node, the set u'\u005cud835' u'\u005cudca9' u'\u005cu2062' ( f ) refers to other source phrases that are neighbors of f (restricted to the k -nearest neighbors as in § 2.2 ), and the aim is to estimate P ( e f ) , the probability of target phrase e being a phrasal translation of source phrase f
p3458
aVAs a result, LP is suboptimal for our needs, since it is unable to appropriately handle generated translation candidates for the unlabeled phrases
p3459
aVThese translation candidates are usually not present as translations for the labeled phrases (or for the labeled phrases that neighbor the unlabeled one in question
p3460
aVWhen propagating information from the labeled phrases, such candidates will obtain no probability mass since e u'\u005cu2260' e u'\u005cu2032'
p3461
aVThus, due to the setup of the problem, LP naturally biases away from translation candidates produced during the generation step (§ 2.1
p3462
aVIn particular, the definition of target similarity is similar to that of source similarity
p3463
aVTherefore, the final update equation in SLP is
p3464
aVWith this formulation, even if e u'\u005cu2260' e u'\u005cu2032' , the similarity T t ( e u'\u005cu2032' e ) as determined by the target phrase graph will dictate propagation probability
p3465
aVWe re-normalize the probability distributions after each propagation step to sum to one over the fixed list of translation candidates, and run the SLP algorithm to convergence
p3466
aVWe utilize the graph propagation-estimated forward phrasal probabilities u'\u005cu2119' ( e f ) as the forward likelihood probabilities for the acquired phrases; to obtain the backward phrasal probability for a given phrase pair, we make use of Bayes u'\u005cu2019' Theorem
p3467
aVBaseline phrasal systems are used both for comparison and for generating translation candidates for unlabeled phrases as described in § 2.1
p3468
aVThe baseline is a state-of-the-art phrase-based system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic [ 14 ]
p3469
aVThe NIST MT06 and MT08 Arabic-English evaluation sets (combining the newswire and weblog domains for both sets), with four references each, were used as tuning and testing sets respectively
p3470
aVFor Urdu-English, the training corpus was provided by the LDC for the NIST Urdu-English MT evaluation, and most of the data was automatically acquired from the web, making it quite noisy
p3471
aVIn addition, we obtained a corpus from the ELRA 5 5 ELRA-W0038 , which contains a mix of parallel and monolingual data; based on timestamps, we extracted a comparable English corpus for the ELRA Urdu monolingual data to form a roughly 470k-sentence u'\u005cu201c' noisy parallel u'\u005cu201d' set
p3472
aVIn our first set of experiments, we looked at the impact of choosing bigrams over unigrams as our basic unit of representation, along with performance of LP (Eq
p3473
aV2 ) compared to SLP (Eq
p3474
aVRecall that LP only takes into account source similarity; since the vast majority of generated candidates do not occur as labeled neighbors u'\u005cu2019' labels, restricting propagation to the source graph drastically reduces the usage of generated candidates as labels, but does not completely eliminate it
p3475
aVTable 4 presents the results of these variations; overall, by taking into account generated candidates appropriately and using bigrams ( u'\u005cu201c' SLP 2-gram u'\u005cu201d' ), we obtained a 1.13 BLEU gain on the test set
p3476
aVWe used a simple hand-built Arabic morphological analyzer that segments word types based on regular expressions, and an English lexicon-based morphological analyzer
p3477
aVThe morphological candidates add a small amount of improvement, primarily by targeting genuine OOVs
p3478
aVIn this set of experiments, we examined if the improvements in § 3.2 can be explained primarily through the extraction of language model characteristics during the semi-supervised learning phase, or through orthogonal pieces of evidence
p3479
aVTable 5 presents the results of using this language model
p3480
aVFurther examination of the differences between the two systems yielded that most of the improvements are due to better bigrams and trigrams, as indicated by the breakdown of the BLEU score precision per n -gram, and primarily leverages higher quality generated candidates from the baseline system
p3481
aVEnglish monolingual u'\u005cu201c' En II Noisy Parallel u'\u005cu201d' + u'\u005cu201c' En II Non-Comparable u'\u005cu201d'
p3482
aVThe results from this setup are presented as u'\u005cu201c' Baseline u'\u005cu201d' and u'\u005cu201c' SLP+Noisy u'\u005cu201d' in Table 6
p3483
aVEnglish monolingual u'\u005cu201c' En II Non-Comparable u'\u005cu201d'
p3484
aVThe results from this setup are presented as u'\u005cu201c' Baseline+Noisy u'\u005cu201d' and u'\u005cu201c' SLP u'\u005cu201d' in Table 6
p3485
aVThe two setups allow us to examine how effectively our method can learn from the noisy parallel data by treating it as monolingual (i.e.,, for graph construction), compared to treating this data as parallel, and also examines the realistic scenario of using completely non-comparable monolingual text for graph construction as in the second setup
p3486
aVThe third and fourth examples represent bigram phrases with much better translations compared to backing off to the lexical translations as in the baseline
p3487
aVThe fifth Arabic-English example demonstrates the pitfalls of over-reliance on the distributional hypothesis the source bigram corresponding to the name u'\u005cu201c' abd almahmood u'\u005cu201d' is distributional similar to another named entity u'\u005cu201c' mahmood u'\u005cu201d' and the English equivalent is offered as a translation
p3488
aVThe sixth example shows how morphological information can propose novel candidates an OOV word is broken down to its stem via the analyzer and candidates are generated based on the stem
p3489
aVBy leveraging the monolingual corpus to understand the context of this unlabeled bigram, we can utilize the graph structure to propose a syntactically correct form, also resulting in a more fluent and correct sentence as determined by the language model
p3490
aVExamples 8 9 show cases where the baseline deletes words or translates them into more common words e.g.,, u'\u005cu201c' conversation u'\u005cu201d' to u'\u005cu201c' the u'\u005cu201d' , while our system proposes reasonable candidates
p3491
aVThe idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context
p3492
aVThis line of work, initiated by Rapp ( 1995 ) and continued by others [ 7 , 13 ] ( inter alia ) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only
p3493
aVRecent improvements to BLI [ 24 , 10 ] have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams
p3494
aV2013 ) and Irvine and Callison-Burch ( 2013 ) conduct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e.,, unigrams, and not on enriching the entire translation model
p3495
aVAs with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and in the latter work, on a subset of language pairs evaluated) are obtained
p3496
aVAdditionally, because of our structured propagation algorithm, our approach is better at handling multiple translation candidates and does not need to restrict itself to the top translation
p3497
aVIn our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role in good performance of the method
p3498
aVHowever, the former work operates only at the level of sentences, and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding
p3499
aVThe goal of leveraging non-parallel data in machine translation has been explored from several different angles
p3500
asg88
(lp3501
sg90
(lp3502
sg92
(lp3503
VIn this work, we presented an approach that can expand a translation model extracted from a sentence-aligned, bilingual corpus using a large amount of unstructured, monolingual data in both source and target languages, which leads to improvements of 1.4 and 1.2 BLEU points over strong baselines on evaluation sets, and in some scenarios gains in excess of 4 BLEU points.
p3504
aVIn the future, we plan to estimate the graph structure through other learned, distributed representations.
p3505
ag106
asg107
S'P14-1064'
p3506
sg109
(lp3507
VStatistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates.
p3508
aVIn this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data.
p3509
aVThe proposed technique first constructs phrase graphs using both source and target language monolingual corpora.
p3510
aVNext, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations.
p3511
aVWe report results on a large Arabic-English system and a medium-sized Urdu-English system.
p3512
aVOur proposed approach significantly improves the performance of competitive phrase-based systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets.
p3513
ag106
asba(icmyPackage
FText
p3514
(dp3515
g3
(lp3516
VRather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics
p3517
aVFrom its foundations, Statistical Machine Translation (SMT) had two defining characteristics first, translation was modeled as a generative process at the sentence-level
p3518
aVSecond, it was purely statistical over words or word sequences and made little to no use of linguistic information
p3519
aVAlthough modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models
p3520
aVRecently, there have been two promising research directions for improving SMT and its evaluation a )  by using more structured linguistic information, such as syntax [] , hierarchical structures [] , and semantic roles [] , and ( b )  by going beyond the sentence-level, e.g.,, translating at the document level []
p3521
aVGoing beyond the sentence-level is important since sentences rarely stand on their own in a well-written text
p3522
aVThe logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts
p3523
aVNote that sentences can be made of several clauses, which in turn can be interrelated through the same logical relations
p3524
aVThus, in a coherent text, discourse units (sentences or clauses) are logically connected the meaning of a unit relates to that of the previous and the following units
p3525
aVIn this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored
p3526
aVThese metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties
p3527
aVFortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses
p3528
aVThus, although limited, this setting is able to demonstrate the potential of discourse-level information for MT evaluation
p3529
aVAddressing discourse-level phenomena in machine translation is relatively new as a research direction
p3530
aVSome recent work has looked at anaphora resolution [ 5 ] and discourse connectives [ 1 ] , to mention two examples
p3531
aVA common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality [ 5 ]
p3532
aVThus, there is consensus that discourse-informed MT evaluation metrics are needed in order to advance research in this direction
p3533
aVThe field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others
p3534
aVFor example, at WMT12, 12 metrics were compared [] , most of them new
p3535
aV2010 ) , which use the Discourse Representation Theory [] and tree-based discourse representation structures (DRS) produced by a semantic parser
p3536
aVThey calculate the similarity between the MT output and references based on DRS subtree matching, as defined in [ 7 ] , DRS lexical overlap, and DRS morpho-syntactic overlap
p3537
aVHowever, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset
p3538
aVCompared to the previous work, ( i )  we use a different discourse representation (RST), ( ii )  we compare discourse parses using all-subtree kernels [] , ( iii )  we evaluate on much larger datasets, for several language pairs and for multiple metrics, and ( iv )  we do demonstrate better correlation with human judgments
p3539
aVOur working hypothesis is that the similarity between the discourse structures of an automatic and of a reference translation provides additional information that can be valuable for evaluating MT systems
p3540
aVIn particular, we believe that good translations should tend to preserve discourse relations
p3541
aVAs an example, consider the three discourse trees (DTs) shown in Figure 4 a ) for a reference (human) translation, and ( b ) and ( c ) for translations of two different systems on the WMT12 test dataset
p3542
aVDiscourse units linked by a relation are further distinguished based on their relative importance in the text nuclei are the core parts of the relation while satellites are supportive ones
p3543
aVNote that the nuclearity and relation labels in the reference translation are also realized in the system translation in (b), but not in (c), which makes (b) a better translation compared to (c), according to our hypothesis
p3544
aVAs shown in Figure 7 , DR does not include any lexical item, and therefore measures the similarity between two translations in terms of their discourse structures only
p3545
aVIn order to allow the tree kernel to find subtree matches at the word level, we include an additional layer of dummy leaves as was done in [] ; not shown in Figure 7 , for simplicity
p3546
aVTo do so, we contrast different MT evaluation metrics with and without discourse information
p3547
aVThe individual metrics combined in Asiya- all can be naturally categorized according to the type of linguistic information they use to compute the quality scores
p3548
aVCombination of three metric variants based on predicate argument structures (semantic role labeling u'\u005cu2018' SR-Mr(*) u'\u005cu2019' , u'\u005cu2018' SR-Or(*) u'\u005cu2019' , and u'\u005cu2018' SR-Or u'\u005cu2019'
p3549
aVCombination of two metrics variants based on semantic parsing
p3550
aV6 6 In Asiya the metrics from this family are referred to as u'\u005cu201c' Discourse Representation u'\u005cu201d' metrics
p3551
aVHowever, the structures they consider are actually very different from the discourse structures exploited in this paper
p3552
aVFor clarity, we will refer to them as semantic parsing metrics u'\u005cu2018' DR-Or(*) u'\u005cu2019' and u'\u005cu2018' DR-Orp(*) u'\u005cu2019'
p3553
aVAnnotators rank the output of five systems according to perceived translation quality
p3554
aVThe organizers relied on a random selection of systems, and a large number of comparisons between pairs of them, to make comparisons across systems feasible []
p3555
aVAs a result, for each source sentence, only relative rankings were available
p3556
aVAs in the WMT12 experimental setup, we use these rankings to calculate correlation with human judgments at the sentence-level, i.e., Kendall u'\u005cu2019' s Tau; see [] for details
p3557
aVIn order to use the WMT12 data for training a learning-to-rank model, we transformed the five-way relative rankings into ten pairwise comparisons
p3558
aVFor instance, if a judge ranked the output of systems A , B , C , D , E as A B C D E , this would entail that A B , A C , A D and A E , etc
p3559
aVTo determine the relative weights for the tuned combinations, we followed a similar approach to the one used by PRO to tune the relative weights of the components of a log-linear SMT model [] , also using Maximum Entropy as the base learning algorithm
p3560
aVUnlike PRO, ( i )  we use human judgments , not automatic scores, and ( ii )  we train on all pairs , not on a subsample
p3561
aVThis made TerrorCat u'\u005cu2019' s score negative, as we present it in Table 3
p3562
aVAs expected, DR- lex performs better than DR since it is lexicalized (at the unigram level), and also gives partial credit to correct structures
p3563
aVIndividually, DR- lex outperforms most of the metrics from group II, and ranks as the second best metric in that group
p3564
aVFurthermore, when combined with individual metrics in group II, DR- lex is able to improve consistently over each one of them
p3565
aVHowever, over all metrics and all language pairs, DR- lex is able to obtain an average improvement in correlation of +.035, which is remarkably higher than that of DR
p3566
aVThus, we can conclude that at the system-level, adding discourse information to a metric, even using the simplest of the combination schemes, is a good idea for most of the metrics, and can help to significantly improve the correlation with human judgments
p3567
aVConversely, ties and incomplete discourse analysis were not a problem at the system-level, where evidence from all 3,003 test sentences is aggregated, and allows to rank systems more precisely
p3568
aVDue to the low score of DR as an individual metric, it fails to yield improvements when uniformly combined with other metrics
p3569
aVAgain, DR- lex is better than DR; with a positive Tau of +.133, yet as an individual metric, it ranks poorly compared to other metrics in group II
p3570
aVFor cross-validation in WMT12, we used ten folds of approximately equal sizes, each containing about 300 sentences we constructed the folds by putting together entire documents, thus not allowing sentences from a document to be split over two different folds
p3571
aVThe results are shown in Table 4
p3572
aVAs in previous sections we present the average results over all four language pairs
p3573
aVInterestingly, the tuned combinations that include the much weaker metric DR now improve over 12 out of 13 of the individual metrics in groups II and III, and only slightly degrades the score of the 13th one ( spede07pP
p3574
aVNote that the Asiya metrics are combinations of several metrics, and these combinations (which exclude DR and DR- lex ) can be also tuned; this yields sizable improvements over the untuned versions as column three in the table shows
p3575
aVSince the metrics that participated in WMT11 and WMT12 are different (and even when they have the same name, there is no guarantee that they have not changed from 2011 to 2012), we only report results for the versions of NIST, Rouge , TER, and BLEU available in Asiya , as well as for the Asiya metrics, thus ensuring that the metrics in the experiments are consistent for 2011 and 2012
p3576
aVThis shows that the weights learned on WMT12 generalize well, as they are also good for WMT11
p3577
aVThis is remarkable given that DR has a strong negative Tau as an individual metric at the sentence-level
p3578
aVThis suggests that both DR and DR- lex contain information that is complementary to that of the individual metrics that we experimented with
p3579
aVOverall, from the experimental results in this section, we can conclude that discourse structure is an important information source to be taken into account in the automatic evaluation of machine translation output
p3580
aVYet, many of the ongoing efforts have been moderately successful according to traditional evaluation metrics
p3581
aVIn the future, we plan to work on integrated representations of syntactic, semantic and discourse-based structures, which would allow us to train evaluation metrics based on more fine-grained features
p3582
aVFirst, at the sentence-level, we can use discourse information to re-rank alternative MT hypotheses; this could be applied either for MT parameter tuning, or as a post-processing step for the MT output
p3583
aVSecond, we propose to move in the direction of using discourse information beyond the sentence-level
p3584
asg88
(lp3585
sg90
(lp3586
sg92
(lp3587
VIn this paper we have shown that discourse structure can be used to improve automatic MT evaluation.
p3588
aVFirst, we defined two simple discourse-aware similarity metrics (lexicalized and un-lexicalized), which use the all-subtree kernel to compute similarity between discourse parse trees in accordance with the Rhetorical Structure Theory.
p3589
aVThen, after extensive experimentation on WMT12 and WMT11 data, we showed that a variety of existing evaluation metrics can benefit from our discourse-based metrics, both at the segment- and the system-level, especially when the discourse information is incorporated in an informed way (i.e., using supervised tuning.
p3590
aVOur results show that discourse-based metrics can improve the state-of-the-art MT metrics, by increasing correlation with human judgments, even when only sentence-level discourse information is used.
p3591
aVAddressing discourse-level phenomena in MT is a relatively new research direction.
p3592
aVYet, many of the ongoing efforts have been moderately successful according to traditional evaluation metrics.
p3593
aVThere is a consensus in the MT community that more discourse-aware metrics need to be proposed for this area to move forward.
p3594
aVWe believe this work is a valuable contribution towards this longer-term goal.
p3595
aVThe tuned combined metrics tested in this paper are just an initial proposal, i.e., a simple adjustment of the relative weights for the individual metrics in a linear combination.
p3596
aVIn the future, we plan to work on integrated representations of syntactic, semantic and discourse-based structures, which would allow us to train evaluation metrics based on more fine-grained features.
p3597
aVAdditionally, we propose to use the discourse information for MT in two different ways.
p3598
aVFirst, at the sentence-level, we can use discourse information to re-rank alternative MT hypotheses; this could be applied either for MT parameter tuning, or as a post-processing step for the MT output.
p3599
aVSecond, we propose to move in the direction of using discourse information beyond the sentence-level.
p3600
ag106
asg107
S'P14-1065'
p3601
sg109
(lp3602
VWe present experiments in using discourse structure for improving machine translation evaluation.
p3603
aVWe first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory.
p3604
aVThen, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level.
p3605
aVRather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.
p3606
ag106
asba(icmyPackage
FText
p3607
(dp3608
g3
(lp3609
VSuch variability is due to two main reasons
p3610
aVSince the quality standards of individual users may vary considerably ( e.g., according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user;
p3611
aVSince data from a new job may differ from those used to train the QE model, its estimates on the new instances might result to be biased or uninformative
p3612
aVOur approach is based on the online learning paradigm and exploits a key difference between such framework and the batch learning methods currently used
p3613
aVOn the other side, online learning techniques are designed to learn in a stepwise manner (either from scratch, or by refining an existing model) from new, unseen test instances by taking advantage of external feedback
p3614
aVTo develop our approach, different online algorithms have been embedded in the backbone of a QE system
p3615
aVThis required the adaptation of its standard batch learning workflow to
p3616
aVGather user feedback for the instance ( i.e., calculating a u'\u005cu201c' true label u'\u005cu201d' based on the amount of user post-editions);
p3617
aVFocusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach
p3618
aVThe evaluation is carried out by measuring the global error of each algorithm on test sets featuring different degrees of similarity with the data used for training
p3619
aVQE is generally cast as a supervised machine learning task, where a model trained from a collection of ( source, target, label ) instances is used to predict labels 1 1 Possible label types include post-editing effort scores ( e.g., 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values [ 28 ] , and post-editing time ( e.g., seconds per word for new, unseen test items [ 31 ]
p3620
aVCurrent approaches to the tasks proposed at WMT have mainly focused on three main directions, namely i) feature engineering, as in [ 12 , 10 , 11 , 26 ] , ii) model learning with a variety of classification and regression algorithms, as in [ 3 , 1 , 29 ] , and iii) feature selection as a way to overcome sparsity and overfitting issues, as in [ 29 ]
p3621
aVBeing optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored
p3622
aVAmong these, the necessity to model the diversity of human quality judgements and correction strategies [ 16 , 15 ] calls for solutions that i) account for annotator-specific behaviour, thus being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items
p3623
aVThe first aspect, modelling annotators u'\u005cu2019' individual behaviour and interdependences, has been addressed by Cohn and Specia [ 8 ] , who explored multi-task Gaussian Processes as a way to jointly learn from the output of multiple annotations
p3624
aVThis technique is suitable to cope with the unbalanced distribution of training instances and yields better models when heterogeneous training datasets are available
p3625
aVAs regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements
p3626
aVIn the online framework, differently from the batch mode, the learning algorithm sequentially processes an unknown sequence of instances X = x 1 , x 2 , u'\u005cu2026' , x n , returning a prediction p u'\u005cu2062' ( x i ) as output at each step
p3627
aVDifferences between p u'\u005cu2062' ( x i ) and the true label p ^ u'\u005cu2062' ( x i ) obtained as feedback are used by the learner to refine the next prediction p u'\u005cu2062' ( x i + 1 )
p3628
aV5 5 Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the reference
p3629
aVLower HTER values indicate better translations
p3630
aVAt each step of the process, the goal of the learner is to exploit user post-editions to reduce the difference between the predicted HTER values and the true labels for the following ( source, target ) pairs
p3631
aVAs depicted in Figure 1 , this is done as follows
p3632
aVBased on the post-edition done by the user, the true HTER label p ^ u'\u005cu2062' ( x i ) is calculated by means of the TERCpp 7 7 goo.gl/nkh2rE open source tool;
p3633
aVFor the sake of clarity it is worth observing that, at least in principle, a model built in a batch fashion could also be adapted to new test data
p3634
aVFor instance, this could be done by running periodic re-training routines once a certain amount of new labelled instances has been collected ( de facto mimicking an online process
p3635
aVIn our experiments, the degree of similarity is measured in terms of u'\u005cu0394' HTER, which is computed as the absolute value of the difference between the average HTER of the training and test sets
p3636
aVLarge values indicate a low similarity between training and test data and a more challenging scenario for the learning algorithms
p3637
aVThe batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances
p3638
aVThis baseline ( u'\u005cu039c' henceforth) is calculated by labelling each instance of the test set with the mean HTER score of the training set
p3639
aVThe MAE is the average of the absolute errors e i f i - y i where f i is the prediction of the model and y i is the true value for the i t u'\u005cu2062' h instance
p3640
aVAs our focus is on the algorithmic aspect, in all experiments we use the same feature set, which consists of the seventeen features proposed in [ 30 ]
p3641
aVIn our experiments we evaluate two online algorithms, OnlineSVR [ 24 ] 8 8 http://www2.imperial.ac.uk/~gmontana/onlinesvr.htm and Passive-Aggressive Perceptron [ 9 ] , 9 9 https://code.google.com/p/sofia-ml/ by comparing their performance with a batch learning strategy based on the Scikit-learn implementation of Support Vector Regression (SVR
p3642
aVThe choice of the OnlineSVR and Passive-Aggressive (OSVR and PA henceforth) is motivated by different considerations
p3643
aVFrom a performance point of view, as an adaptation of u'\u005cu0395' -SVR which proved to be one of the top performing algorithms in the regression QE tasks at WMT, OSVR seems to be the best candidate
p3644
aVFor this reason, we use the online adaptation of u'\u005cu0395' -SVR proposed by [ 18 ]
p3645
aVThe goal of OnlineSVR is to find a way to add each new sample to one of three sets (support, empty, error) maintaining the consistency of a set of conditions known as Karush-Kuhn Tucker (KKT) conditions
p3646
aVFor each new point, OSVR starts a cycle where the samples are moved across the three sets until the KKT conditions are verified and the new point is assigned to one of the sets
p3647
aVIf the point is identified as a support vector, the parameters of the model are updated
p3648
aVIf its value is larger than the tolerance parameter ( u'\u005cu0395' ), the weights of the model are updated as much as the aggressiveness parameter C allows
p3649
aVAlthough it makes PA faster than OSVR, this is a riskier strategy because it may lead the algorithm to change the model to adapt to outlier points
p3650
aVFirst, since in this artificial scenario adaptation capabilities are not required for the QE component, batch methods operate in the ideal conditions (as training and test are independent and identically distributed
p3651
aVSecond, this scenario provides the fairest conditions for such comparison because, in principle, online algorithms are not favoured by the possibility to learn from the diversity of the test instances
p3652
aVEvaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set
p3653
aVTable 1 reports the results achieved by the best performing algorithm for each type of model ( batch , adaptive , empty
p3654
aVAs can be seen, close MAE values show a similar behaviour for the three types of models
p3655
aVThis demonstrates that, as expected, the online algorithms do not take advantage of test data with a label distribution similar to the training set
p3656
aVAll the models outperform the baseline, even if the minimal differences confirm the competitiveness of such a simple approach
p3657
aVThe source sentences were translated with two SMT systems built by training the Moses toolkit [ 14 ] on parallel data from the two domains (about 2M sentences for IT and 1.5M for L
p3658
aVAccording to the way they are created, the two datasets allow us to evaluate the adaptability of different QE models with respect to user changes within the same domain (§ 6.1 ), as well as user and domain changes at the same time (§ 6.2
p3659
aVFor each document D (L or IT), these two scenarios are obtained by dividing D into two parts of equal size (80 instances for L and 140 for IT
p3660
aVThe result is one training set and one test set for each post-editor within the same domain
p3661
aVFor each domain, these respectively involve the most dissimilar and the most similar post-editors according to the u'\u005cu0394' HTER
p3662
aVThe first scenario defines a challenging situation where two post-editors ( rad and cons ) are characterized by opposite behaviour
p3663
aVAs evidenced by the high u'\u005cu0394' HTER values, one of them ( rad ) is the most u'\u005cu201c' radical u'\u005cu201d' post-editor (performing more corrections) while the other ( cons ) is the most u'\u005cu201c' conservative u'\u005cu201d' one
p3664
aVAs shown in Table 2 , global MAE scores for the online algorithms (both adaptive and empty ) indicate their good adaptation capabilities
p3665
aVThese results (MAE reductions are always statistically significant) suggest that, when dealing with datasets with very different label distributions, the evident limitations of batch methods are more easily overcome by learning from scratch from the feedback of a new post-editor
p3666
aVThis also holds when the amount of test points to learn from is limited, as in the L domain where the test set contains only 80 instances
p3667
aVFrom the application-oriented perspective that motivates our work, considering the high costs of acquiring large and representative QE training data, this is an important finding
p3668
aVA closer look at the behaviour of the online algorithms in the two domains leads to other observations
p3669
aVFor OSVR the addition of new points to the support set may have a limited effect on the whole model, in particular if the number of points in the set is large
p3670
aVIn the table, results are ordered according to the u'\u005cu0394' HTER computed between the selected post-editor in the training domain ( e.g., L cons ) and the selected post-editor in the test domain ( e.g., IT rad
p3671
aVFor the sake of comparison, we also report (grey rows) the results of the experiments within the same domain presented in § 6.1
p3672
aVThis is a strong evidence of the fact that, in case of domain changes, online models can still learn from new test instances even if they have a label distribution similar to the training set
p3673
aVThis suggests that, although PA is potentially capable of achieving higher results and better adapt to the new test points, its instability makes it less reliable for practical use
p3674
aVAs a final analysis of our results, we investigated how the performance of the different types of models ( batch , adaptive , empty ) relates to the distance between training and test sets
p3675
aVAs expected, the results of the empty models are completely uncorrelated with the u'\u005cu0394' HTER since they only use the test set
p3676
aVIn the CAT scenario, each translation job can be seen as a complex situation where the user (his personal style and background), the source document (the language and the domain) and the underlying technology (the translation memory and the MT engine that generate translation suggestions) contribute to make the task unique
p3677
aVSo far, the adaptability to such specificities (a major challenge for CAT technology) has been mainly supported by the evolution of translation memories, which incrementally store translated segments incorporating the user style
p3678
aVOur results show that the wealth of dynamic knowledge brought by user corrections can be exploited to refine in a stepwise fashion the quality judgements in different testing conditions (user changes as well as simultaneous user and domain changes
p3679
aVAs an additional contribution, to spark further research on this facet of the QE problem, our adaptive QE infrastructure (integrating all the components and the algorithms described in this paper) has been released as open source
p3680
asg88
(lp3681
sg90
(lp3682
sg92
(lp3683
VIn the CAT scenario, each translation job can be seen as a complex situation where the user (his personal style and background), the source document (the language and the domain) and the underlying technology (the translation memory and the MT engine that generate translation suggestions) contribute to make the task unique.
p3684
aVSo far, the adaptability to such specificities (a major challenge for CAT technology) has been mainly supported by the evolution of translation memories, which incrementally store translated segments incorporating the user style.
p3685
aVThe wide adoption of translation memories demonstrates the importance of capitalizing on such information to increase translators productivity.
p3686
aVWhile this lesson recently motivated research on adaptive MT decoders that learn from user corrections, nothing has been done to develop adaptive QE components.
p3687
aVIn the first attempt to address this problem, we proposed the application of the online learning protocol to leverage users feedback and to tailor QE predictions to their quality standards.
p3688
aVBesides highlighting the limitations of current batch methods to adapt to user and domain changes, we performed an application-oriented analysis of different online algorithms focusing on specific aspects relevant to the CAT scenario.
p3689
aVOur results show that the wealth of dynamic knowledge brought by user corrections can be exploited to refine in a stepwise fashion the quality judgements in different testing conditions (user changes as well as simultaneous user and domain changes.
p3690
aVAs an additional contribution, to spark further research on this facet of the QE problem, our adaptive QE infrastructure (integrating all the components and the algorithms described in this paper) has been released as open source.
p3691
aVIts C++ implementation is available at http://hlt.fbk.eu/technologies/aqet.
p3692
ag106
asg107
S'P14-1067'
p3693
sg109
(lp3694
VThe automatic estimation of machine translation (MT) output quality is a hard task in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role.
p3695
aVWhen moving from controlled lab evaluations to real-life scenarios the task becomes even harder.
p3696
aVFor current MT quality estimation (QE) systems, additional complexity comes from the difficulty to model user and domain changes.
p3697
aVIndeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions.
p3698
aVTo tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes.
p3699
aVContrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach.
p3700
ag106
asba(icmyPackage
FText
p3701
(dp3702
g3
(lp3703
VThe two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively
p3704
aVIn general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis words that appear in similar linguistic contexts are likely to have related meanings
p3705
aVThese models learn the meaning of words based on textual and perceptual input
p3706
aVDespite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities
p3707
aVThese models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g.,, text and images
p3708
aVIn this work, we introduce a model, illustrated in Figure 1 , which learns grounded meaning representations by mapping words and images into a common embedding space
p3709
aVUnlike most previous work, our model is defined at a finer level of granularity u'\u005cu2014' it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities
p3710
aVWe follow in arguing that an attribute-centric representation is expedient for several reasons
p3711
aVFirstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g.,, ) where humans often employ attributes when asked to describe a concept
p3712
aVSecondly, from a modeling perspective, attributes allow for easier integration of different modalities, since these are rendered in the same medium, namely, language
p3713
aVWe performed a large-scale evaluation on a new dataset consisting of human similarity judgments for 7,576 word pairs
p3714
aVUnlike previous efforts such as the widely used WordSim353 collection () , our dataset contains ratings for visual and textual similarity, thus allowing to study the two modalities (and their contribution to meaning representation) together and in isolation
p3715
aVSeveral other models have been extensions of Latent Dirichlet Allocation () where topic distributions are learned from words and other perceptual units use visual words which they extract from a corpus of multimodal documents (i.e.,, BBC news articles and their associated images), whereas others () use feature norms obtained in longitudinal elicitation studies (see for an example) as an approximation of the visual environment
p3716
aVMore recently, topic models which combine both feature norms and visual words have also been introduced
p3717
aVDrawing inspiration from the successful application of attribute classifiers in object recognition, show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss
p3718
aVThe use of stacked autoencoders to extract a shared lexical meaning representation is new to our knowledge, although, as we explain below related to a large body of work on deep learning
p3719
aVFirstly, most of these approaches aim to learn a shared representation between modalities so as to infer some missing modality from others (e.g.,, to infer text from images and vice versa); in contrast, we aim to learn an optimal representation for each modality and their optimal combination
p3720
aVAutoencoders are a means to learn representations of some input by retaining useful features in the encoding phase which help to reconstruct the input, whilst discarding useless or noisy ones
p3721
aVThe underlying idea is that the learned latent representation is good if the autoencoder is capable of reconstructing the actual input from its corruption
p3722
aVSeveral (denoising) autoencoders can be used as building blocks to form a deep neural network
p3723
aVFor that purpose, the autoencoders are pre-trained layer by layer, with the current layer being fed the latent representation of the previous autoencoder as input
p3724
aVUsing this unsupervised pre-training procedure, initial parameters are found which approximate a good solution
p3725
aVThen, we join these two SAEs by feeding their respective second coding simultaneously to another autoencoder, whose hidden layer thus yields the fused meaning representation
p3726
aVFor both modalities, we use the hyperbolic tangent function as activation function for encoder f u'\u005cu0398' and decoder g u'\u005cu0398' u'\u005cu2032' and an entropic loss function for L
p3727
aVThe weights of each autoencoder are tied, i.e.,, u'\u005cud835' u'\u005cudc16' u'\u005cu2032' = u'\u005cud835' u'\u005cudc16' T
p3728
aVRegarding the visual autoencoder, we derive a new ( u'\u005cu2018' denoised u'\u005cu2019' ) target vector to be reconstructed for each input vector u'\u005cud835' u'\u005cudc31' ( u'\u005cud835' u'\u005cudc22' ) , and treat u'\u005cud835' u'\u005cudc31' ( u'\u005cud835' u'\u005cudc22' ) itself as corrupted input
p3729
aVThe unimodal autoencoder is thus trained to denoise a given input
p3730
aVWe also encourage the autoencoder to detect dependencies between the two modalities while learning the mapping to the bimodal hidden layer
p3731
aVWe therefore apply masking noise to one modality with a masking factor v (see Section 3.1 ), so that the corrupted modality optimally has to rely on the other modality in order to reconstruct its missing input features
p3732
aVThe overall objective to be minimized is therefore the weighted sum of the reconstruction error L r and the classification error L c
p3733
aVFor example, by setting the corruption parameter v for the textual modality to one and u'\u005cu0394' r to zero, a standard object classification model for images can be trained
p3734
aVSetting v close to one for either modality enables the model to infer the other (missing) modality
p3735
aVAs our input consists of natural language attributes, the model would infer textual attributes given visual attributes and vice versa
p3736
aVIn this section we present our experimental setup for assessing the performance of our model
p3737
aVThe norms were elicited by asking participants to list properties (e.g.,, barks , an_animal , has_legs ) describing the nouns they were presented with
p3738
aVAs shown in Figure 1 , our model takes as input two (real-valued) vectors representing the visual and textual modalities
p3739
aVTextual attributes were extracted by running Strudel () on a 2009 dump of the English Wikipedia
p3740
aVStrudel is a fully automatic method for extracting weighted word-attribute pairs (e.g.,, bat u'\u005cu2013' species:n , bat u'\u005cu2013' bite:v ) from a lemmatized and POS-tagged corpus
p3741
aVWe use the visual vectors of the training and development set for training the autoencoders, and the vectors for the test set for evaluation
p3742
aVThese were established by presenting participants with a cue word (e.g.,, canary ) and asking them to name an associate word in response (e.g.,, bird, sing, yellow
p3743
aVSome performance gains could be expected if parameter optimization took place separately for each task
p3744
aVAlthough several relevant datasets exist, such as the widely used WordSim353 () or the more recent Rel-122 norms () , they contain many abstract words, (e.g.,, love u'\u005cu2013' sex or arrest u'\u005cu2013' detention ) which are not covered in
p3745
aVThis is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon
p3746
aVWe thus created a new dataset consisting exclusively of nouns which we hope will be useful for the development and evaluation of grounded semantic space models
p3747
aVWe opted for this specific measure as it achieves high correlation with human ratings and has a high coverage on our nouns
p3748
aVThe similarity data was post-processed so as to identify and remove outliers
p3749
aVWe measured inter-subject agreement as the average pairwise correlation coefficient (Spearman u'\u005cu2019' s u'\u005cu03a1' ) between the ratings of all annotators for each task
p3750
aVFor semantic similarity, the mean correlation was 0.76 (Min = 0.34, Max = 0.97, StD = 0.11) and for visual similarity 0.63 (Min = 0.19, Max = 0.90, SD = 0.14
p3751
aVThese results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency
p3752
aVWe also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see
p3753
aVWe evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task () ; it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively
p3754
aVThe first one is based on kernelized canonical correlation (kCCA, ) with a linear kernel which was the best performing model in
p3755
aV7 7 We thank Elia Bruni for providing us with their data
p3756
aVWe report results with the 640-dimensional embeddings as they performed best
p3757
aVWe report correlation coefficients of model predictions against similarity ratings
p3758
aVAs an indicator to how well automatically extracted attributes can approach the performance of clean human generated attributes, we also report results of a distributional model induced from McRae et al u'\u005cu2019' s ( ) norms (see the row labeled McRae in the table
p3759
aVEach noun is represented as a vector with dimensions corresponding to attributes elicited by participants of the norming study
p3760
aVVector components are set to the (normalized) frequency with which participants generated the corresponding attribute
p3761
aVWe show results for three models, using all attributes except those classified as visual (T), only visual attributes (V), and all available attributes (V+T
p3762
aV9 9 Classification of attributes into categories is provided by in their dataset
p3763
aVAs baselines, we also report the performance of a model based solely on textual attributes (which we obtain from Strudel), visual attributes (obtained from our classifiers), and their concatenation (see row Attributes in Table 2 , and columns T, V, and T+V, respectively
p3764
aVThe automatically obtained textual and visual attribute vectors serve as input to SVD, kCCA, and our stacked autoencoder (SAE
p3765
aVThe third row in the table presents three variants of our model trained on textual and visual attributes only (T and V, respectively) and on both modalities jointly (T+V
p3766
aVThis suggests that both modalities contribute complementary information and that the SAE model is able to extract a shared representation which improves generalization performance across tasks by learning them jointly
p3767
aVTable 3 shows examples of word pairs with highest semantic and visual similarity according to the SAE model
p3768
aVWe also observe that simply concatenating textual and visual attributes (Attributes, T+V) performs competitively with SVD and better than kCCA
p3769
aVThis indicates that the attribute-based representation is a powerful predictor on its own
p3770
aVIt is interesting to note that the unimodal SAEs are in most cases better than the raw textual or visual attributes
p3771
aVThis indicates that higher level embeddings may be beneficial to NLP tasks in general, not only to those requiring multimodal information
p3772
aVThe two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data
p3773
asg88
(lp3774
sg90
(lp3775
sg92
(lp3776
VIn this paper, we presented a model that uses stacked autoencoders to learn grounded meaning representations by simultaneously combining textual and visual modalities.
p3777
aVThe two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data.
p3778
aVTo the best of our knowledge, our model is novel in its use of attribute-based input in a deep neural network.
p3779
aVExperimental results in two tasks, namely simulation of word similarity and word categorization, show that our model outperforms competitive baselines and related models trained on the same attribute-based input.
p3780
aVOur evaluation also reveals that the bimodal models are superior to their unimodal counterparts and that higher-level unimodal representations are better than the raw input.
p3781
aVIn the future, we would like to apply our model to other tasks, such as image and text retrieval () , zero-shot learning () , and word learning ().
p3782
aVWe would like to thank Vittorio Ferrari, Iain Murray and members of the ILCC at the School of Informatics for their valuable feedback.
p3783
aVWe acknowledge the support of EPSRC through project grant EP/I037415/1.
p3784
ag106
asg107
S'P14-1068'
p3785
sg109
(lp3786
VIn this paper we address the problem of grounding distributional representations of lexical meaning.
p3787
aVWe introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input.
p3788
aVThe two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively.
p3789
aVWe evaluate our model on its ability to simulate similarity judgments and concept categorization.
p3790
aVOn both tasks, our approach outperforms baselines and related models.
p3791
ag106
asba(icmyPackage
FText
p3792
(dp3793
g3
(lp3794
VThis can be useful for capturing the various degrees of semantic compositionality of MWEs
p3795
aVA real-life parsing system should comprise the recognition of multi-word expressions (MWEs 1 1 Multiword expressions can be roughly defined as continuous or discontinuous sets of tokens, which either do not exhibit full freedom in lexical selection or whose meaning is not fully compositional
p3796
aVWe focus in this paper on contiguous multiword expressions, also known as u'\u005cu201c' words with spaces u'\u005cu201d' first because downstream semantic-oriented applications need some marking in order to distinguish between regular semantic composition and the typical semantic non-compositionality of MWEs
p3797
aVThe trees show syntactic dependencies between semantically sound units (made of one or several tokens), and are thus particularly appealing for downstream semantic-oriented applications, as dependency trees are considered to be closer to predicate-argument structures
p3798
aVIn this paper, we investigate various strategies for predicting from a tokenized sentence both MWEs and syntactic dependencies, using the French dataset of the SPMRL 13 Shared Task
p3799
aVIt uses no feature nor treatment specific to MWEs as it focuses on the general aim of the Shared Task, namely coping with prediction of morphological and syntactic analysis
p3800
aVThe first component of a MWE is taken as the head of the MWE
p3801
aVAll subsequent components of the MWE depend on the first one, with the special label dep_cpd (hence the name flat representation
p3802
aVFurthermore, the first MWE component bears a feature mwehead equal to the POS of the MWE
p3803
aVIn the alternative representation we propose, irregular MWEs are unchanged and appear as flat MWEs (e.g., en vain in Figure 1 has pattern preposition+adjective, which is not considered regular for an adverb, and is thus unchanged
p3804
aVRegular MWEs appear with u'\u005cu2019' structured u'\u005cu2019' syntax we modify the tree structure to recover the regular syntactic dependencies
p3805
aVFurthermore, some sequences are both syntactically and semantically regular, but encoded as MWE due to frozen lexical selection
p3806
aVThis is the case for déficit budgétaire (lit budgetary deficit , meaning u'\u005cu2019' budget deficit u'\u005cu2019' ), because it is not possible to use déficit du budget ( budget deficit
p3807
aVThis renders the syntactic description more uniform and it provides an internal structure for regular MWEs, which is meaningful if the MWE is fully or partially compositional
p3808
aVMoreover, such a distinction opens the way to a non-binary classification of MWE status the various criteria leading to classify a sequence as MWE could be annotated separately and using nominal or scaled categories for each criteria
p3809
aVThe representation we propose is thus a first step towards a uniform handling of MWEs and discontinuous non-compositional phenomena like light-verb constructions or verbal frozen idioms
p3810
aVWe developed an ad hoc program for structuring the regular MWEs in gold data
p3811
aVMWEs are first classified as regular or irregular, using regular expressions over the sequence of parts-of-speech within the MWE
p3812
aVTo define the regular expressions, we grouped gold MWEs according to the pair [global POS of the MWE + sequence of POS of the MWE components], and designed regular expressions to match the most frequent patterns that looked regular according to our linguistic knowledge
p3813
aV113 of these were classified as regular, and we judged that all of them were actually regular, and were correctly structured
p3814
aVAmong the 87 classified as irregular, 7 should have been tagged as regular and structured
p3815
aVFor 4 of them, the classification error is due to errors on the (gold) POS of the MWE components
p3816
aV\u005cdraftreplace Counts of MWEs automatically structures because classified as regular are shown in Table 1 Table 1 shows the proportions of MWEs classified as regular, and thus further structured
p3817
aVAbout half MWEs are structured, and about two thirds of structured MWEs are nouns
p3818
aVIn some experiments, we make use of alternative representations, which we refer later as u'\u005cu201c' labeled representation u'\u005cu201d' , in which the MWE features are incorporated in the dependency labels, so that MWE composition and/or the POS of the MWE be totally contained in the tree topology and labels, and thus predictable via dependency parsing
p3819
aVFigure shows the labeled representation for the sentence of Figure 1
p3820
aVFor instance, in bottom tree of Figure 1 , arcs pointing to the non-head components ( de, biens, sociaux ) are suffixed with _r to mark them as belonging to a structured MWE, and with _N since the MWE is a noun
p3821
aVIRREG-MERGED gold irregular MWEs are merged for training; for parsing, irregular MWEs are predicted externally, merged into one token at parsing time, and re-expanded into several tokens for evaluation;
p3822
aVIRREG-BY-PARSER the MWE status, flat topology and POS are all predicted via dependency parsing, using representations for training and parsing, with all information for irregular MWEs encoded in topology and labels (as for in vain in Figure 2
p3823
aVFor regular MWEs, their internal structure is always predicted by the parser
p3824
aVMWE lexicons are exploited as sources of features for both the dependency parser and the external MWE analyzer
p3825
aVIn particular, two large-coverage general-language lexicons are used the Lefff 6 6 We use the version available in the POS tagger MElt [ 14 ] lexicon [ 21 ] , which contains approximately half a million inflected word forms, among which approx
p3826
aVIn order to compare the MWEs present in the lexicons and those encoded in the French treebank, we applied the following procedure (hereafter called lexicon lookup in a given sentence, the maximum number of non overlapping MWEs according to the lexicons are systematically marked as such
p3827
aVWe obtain about 70% recall and 50% precision with respect to MWE spanning
p3828
aVMWE information can be integrated as features to be used by the dependency parser
p3829
aVWe tested to incorporate the MWE-specific features as defined in the gold flat representation (section 3.1 the mwehead=POS feature for the MWE head token, POS being the part-of-speech of the MWE; the component=y feature for the non-first MWE component
p3830
aVThe intuition behind this feature is that for an irregular MWE, the POS of the linearly first component, which serves as head, is not always representative of the external distribution of the MWE
p3831
aVFor regular MWEs, the usefulness of such a trick is less obvious
p3832
aVThe first component of a regular MWE is not necessarily its head (for instance for a nominal MWE with internal pattern adjective+noun), so the switch trick could be detrimental in such cases
p3833
aVPredicted lemmas, POS and morphology features are computed with Morfette version 0.3.5 [ 8 , 22 ] 13 13 https://sites.google.com/site/morfetteweb/ , using 10 iterations for the tagging perceptron, 3 iterations for the lemmatization perceptron, default beam size for the decoding of the joint prediction, and the Lefff [ 21 ] as external lexicon used for out-of-vocabulary words
p3834
aVEvaluation metrics we evaluate our parsing systems by using the standard metrics for dependency parsing
p3835
aVFor the flat MWE features, we experimented both with features predicted by the MWE analyzer, and with features predicted using the external lexicons mentioned in section 5.1 (using the lexicon lookup procedure
p3836
aVBoth kinds of prediction lead to fairly comparable results, so in all the following, the MWE features, when used, are predicted using the external lexicons
p3837
aVFor LAS, in both cases the three last tokens will count as incorrect if the wrong MWE status is predicted
p3838
aVSo to sum up on the u'\u005cu201c' labeled evaluation u'\u005cu201d' , we obtain a LAS evaluation for the whole task of parsing plus MWE recognition, but an UAS evaluation that penalizes less errors on MWE status, while keeping a representation that is richer predicted parses contain not only the syntactic dependencies and MWE information, but also a classification of MWEs into regular and irregular, and the internal syntactic structure of regular MWEs
p3839
aVThe evaluation on u'\u005cu201c' structured representation u'\u005cu201d' can be interpreted as an evaluation of the parsing task plus the recognition of irregular MWEs only both LAS and UAS are measured independently of errors on regular MWE status (note the UAS is exactly the same than in the u'\u005cu201c' labeled u'\u005cu201d' case
p3840
aVThe best score for each metric is in bold.The JOINT baseline corresponds to a u'\u005cu201c' pure u'\u005cu201d' joint system without external MWE resources (hence the minus sign for the first three columns
p3841
aV\u005cdraftremove Note that columns 6 and 8 are identical, since the UAS scores for u'\u005cu201c' labeled u'\u005cu201d' and structured representations are the same
p3842
aVConcerning the tuning of parameters, it appears that the best setting is to use MWE-features, and switch for both regular and irregular MWEs, except for the pipeline architecture for which results without MWE features are slightly better
p3843
aVSo overall, informing the parser with independently predicted POS of MWE has positive impact
p3844
aVIt can be noted though, that JOINT-IRREG performs overall better on MWEs (last two columns of table 3 ), whereas JOINT performs better on irregular MWEs the \u005cdraftreplace full joint architecturelatter seems \u005cdraftreplace on the one handto be beneficial for parsing, but is less efficient to correctly spot the regular MWEs
p3845
aVConcerning the three distinct representations, evaluating on structured representation (hence without looking at regular MWE status) leads to a rough 2 point performance increase for the LAS and a one point increase for the UAS, with respect to the evaluation against flat representation
p3846
aV16 16 The slight differences in LAS between the labeled and the flat representations are due to side effects of errors on MWE status some wrong reattachments performed to obtain flat representation decrease the UAS, but also in some cases the LAS
p3847
aVWe observe the same general trend as in the development corpus, but with tinier differences
p3848
aVJOINT and JOINT-IRREG significantly outperform the baseline and the PIPELINE, on labeled representation and flat representation
p3849
aVMoreover, we provide in table 5 a comparison of our best architecture with reg/irregular MWE distinction with other architectures that do not make this distinction, namely the two best comparable systems designed for the SPMRL Shared Task [ 23 ] the pipeline simple parser based on Mate-tools of Constant et al
p3850
aVResults can only be compared on the flat representation, because the other systems output poorer linguistic information
p3851
aVWe computed statistical significance of differences between our systems and Const13
p3852
aVBut on the test corpus (which is twice bigger), the best system is Const13 pipeline, with statistically significant differences over our joint systems
p3853
aVSo the first observation is that our architectures that distinguish between reg/irreg MWEs do not outperform uniform architectures
p3854
aVBut we note that the differences are slight, and the output we obtain is enhanced with regular MWE internal structure
p3855
aVIt can thus be noted that the increased syntactic uniformity obtained by our MWE representation is mitigated so far by the additional complexity of the task
p3856
aVThe second observation is that currently the best system on this dataset is a pipeline system, as results on test set show (and somehow contrary to results on dev set
p3857
aVIn this section, we propose to better evaluate the difficulty of combining the tasks of MWE analysis and dependency parsing by comparing our systems with systems performing simpler tasks i.e., MWE recognition without parsing, and parsing with no or limited MWE recognition, simulated by using gold MWEs
p3858
aVNext, we compare the best JOINT-REG system with the one based on the same architecture but where the irregular MWEs are perfectly pre-identified, in order to quantify the difficulty added by the irregular MWEs
p3859
aVIn the JOINT-REG architecture, assuming gold irregular MWE identification, increases LAS by 1.3 point
p3860
aVIn terms of MWE recognition, as compared with the CRF-based analyzer, our best system is around 2 points below
p3861
aVThe u'\u005cu201d' weak point u'\u005cu201d' of our system is therefore the identification of regular MWEs
p3862
aVWe experimented strategies to predict both MWE analysis and dependency structure, and tested them on the dependency version of French Treebank [ 1 ] , as instantiated in the SPMRL Shared Task [ 23 ]
p3863
aVThis can be useful for capturing the various degrees of semantic compositionality of MWEs
p3864
asg88
(lp3865
sg90
(lp3866
sg92
(lp3867
VWe experimented strategies to predict both MWE analysis and dependency structure, and tested them on the dependency version of French Treebank [ 1 ] , as instantiated in the SPMRL Shared Task [ 23 ].
p3868
aVOur work focused on using an alternative representation of syntactically regular MWEs, which captures their syntactic internal structure.
p3869
aVWe obtain a system with comparable performance to that of previous works on this dataset, but which predicts both syntactic dependencies and the internal structure of MWEs.
p3870
aVThis can be useful for capturing the various degrees of semantic compositionality of MWEs.
p3871
aVThe main weakness of our system comes from the identification of regular MWEs, a property which is highly lexical.
p3872
aVOur current use of external lexicons does not seem to suffice, and the use of data-driven external information to better cope with this identification can be envisaged.
p3873
ag106
asg107
S'P14-1070'
p3874
sg109
(lp3875
VIn this paper, we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression (MWE) recognition, testing them on the dependency version of French Treebank [ 1 ] , as instantiated in the SPMRL Shared Task [ 23 ].
p3876
aVOur work focuses on using an alternative representation of syntactically regular MWEs, which captures their syntactic internal structure.
p3877
aVWe obtain a system with comparable performance to that of previous works on this dataset, but which predicts both syntactic dependencies and the internal structure of MWEs.
p3878
aVThis can be useful for capturing the various degrees of semantic compositionality of MWEs.
p3879
ag106
asba(icmyPackage
FText
p3880
(dp3881
g3
(lp3882
VIn this paper, we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data
p3883
aVVariation poses a serious challenge for determining who or what a name refers to
p3884
aVAnother option is to train a model like stochastic edit distance from known pairs of similar names [] , but this requires supervised data in the test domain
p3885
aVEven the best model of name similarity is not enough by itself, since two names that are similar u'\u005cu2014' even identical u'\u005cu2014' do not necessarily corefer
p3886
aVOur model is an evolutionary generative process based on the name variation model of , which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits
p3887
aVTheir inference procedure only clustered types (distinct names) rather than tokens (mentions in context), and relied on expensive matrix inversions for learning
p3888
aV\u005ctodo [author=jason,color=RedOrange,fancyline,size= ,]summarize results and add John Smith here if you do it in the conclusions
p3889
aVCross-document coreference resolution (CDCR) was first introduced by
p3890
aVMost approaches since then are based on the intuitions that coreferent names tend to have u'\u005cu201c' similar u'\u005cu201d' spellings and tend to appear in u'\u005cu201c' similar u'\u005cu201d' contexts
p3891
aVTo apply our model to the CDCR task, we observe that the probability that two name mentions are coreferent is the probability that they arose from a common ancestor in the phylogeny
p3892
aVSo we design a Monte Carlo sampler to reconstruct likely phylogenies
p3893
aVWhile our model is capable of generating each name independently, a phylogeny will generally achieve higher probability if it explains similar names as being similar by mutation (rather than by coincidence
p3894
aVThus, our sampled phylogenies tend to make similar names coreferent u'\u005cu2014' especially long or unusual names that would be expensive to generate repeatedly, and especially in contexts that are topically similar and therefore have a higher prior probability of coreference
p3895
aVRather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies
p3896
aVName similarity is also an important component of within -document coreference resolution, and efforts in that area bear resemblance to our approach describe an u'\u005cu201c' entity-centered u'\u005cu201d' model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document
p3897
aVSimilarly, learn a mention similarity model based on labeled data
p3898
aVLike and we use topics as the contexts, but learn mention topics jointly with other model parameters
p3899
aVWe assume that each document has a (single) known \u005ctodo [author=mark,color=RoyalBlue,fancyline,size=,]We got dinged in the last submission for trying to generalize the model to language without any data to back it up
p3900
aV\u005ctodo [author=jason,color=RedOrange,fancyline,size=,]As you see, I already amplified the footnote to give our defense of this
p3901
aVOur model generates an ordered sequence u'\u005cud835' u'\u005cudc99' although we do not observe its order
p3902
aVThus each mention x has latent position x i (e.g.,, x 729 i = 729
p3903
aVThe entire corpus, including these entities, is generated according to standard topic model assumptions; we first generate a topic distribution for a document, then sample topics and words for the document [ ]
p3904
aVTo mitigate this unrealistic assumption, we allow any ordering u'\u005cud835' u'\u005cudc99' of the observed mentions, not respecting document timestamps or forcing the mentions from a given document to be generated as a contiguous subsequence of u'\u005cud835' u'\u005cudc99'
p3905
aVAlternatively, the model may manufacture a name for a new person, though the name itself may not be new
p3906
aVIf all previous mentions were equally likely, this would be a Chinese Restaurant Process (CRP) in which frequently mentioned entities are more likely to be mentioned again ( u'\u005cu201c' the rich get richer u'\u005cu201d'
p3907
aVWe refine that idea by saying that the current topic, language, and document influence the choice of which previous mention to copy, similar to the distance-dependent CRP [ ]
p3908
aV2 2 Unlike the ddCRP, our generative story is careful to prohibit derivational cycles each mention is copied from a previous mention in the latent ordering
p3909
aVThis is why our phylogeny is a tree , and why our sampler is more complex
p3910
aVAlso unlike the ddCRP, we permit asymmetric u'\u005cu201c' distances u'\u005cu201d' if a certain topic or language likes to copy mentions from another, the compliment is not necessarily returned
p3911
aVThis will help distinguish multiple John Smith entities if they tend to appear in different contexts
p3912
aVWhile u'\u005cud835' u'\u005cudc8a' and u'\u005cud835' u'\u005cudc9b' are not necessary for creating coref clusters, they are needed to produce u'\u005cud835' u'\u005cudc91'
p3913
aVThe distributions used to choose these are unimportant because these variables are always observed
p3914
aV\u005ctodo [author=jason,color=RedOrange,fancyline,size=,]If we u'\u005cu2019' re using [ ] , then don u'\u005cu2019' t we have to say how u'\u005cud835' u'\u005cudc8e' is sampled
p3915
aVI simplified it from u'\u005cu03a4' u'\u005cu2062' u'\u005cud835' u'\u005cudc8e' here and in the main document
p3916
aVSo now u'\u005cud835' u'\u005cudc8e' is unnormalized
p3917
aV\u005ctodo [author=jason,color=RedOrange,fancyline,size=,]if we restore u'\u005cu0391' then it should be mentioned here
p3918
aVNotice that the tokens w d u'\u005cu2062' k in document d are exchangeable by collapsing out u'\u005cud835' u'\u005cudf4d' d , we can regard them as having been generated from a CRP
p3919
aVThus, for fixed values of the non-mention tokens and their topics, the probability of generating the mention sequence u'\u005cud835' u'\u005cudc99' is proportional to the product of the probabilities of the choices in step 3 at the positions d u'\u005cu2062' k where mentions were generated
p3920
aVTo select a parent for a mention x of type t = x e t , a simple model (as mentioned above) would be a CRP each previous mention of the same type is selected with probability proportional to 1, and u'\u005cu2662' is selected with probability proportional to u'\u005cu0391' t 0
p3921
aVA larger choice of u'\u005cu0391' t results in smaller entity clusters, because it prefers to create new entities of type t rather than copying old ones
p3922
aVWe modify this story by re-weighting u'\u005cu2662' and previous mentions according to their relative suitability as the parent of x
p3923
aV\u005ctodo [author=noa,color=SeaGreen,fancyline,size=,]I have features on u'\u005cu2662' here since e.g., position in document may be predictive of if a new entity is being started
p3924
aV\u005ctodo [author=jason,color=RedOrange,fancyline,size=,]Ok, but then the factors of u'\u005cu0391' t n u'\u005cu2062' ( x ) + u'\u005cu0391' t should no longer be used, so I deleted them and simplified
p3925
aVIn fact, we no longer have any u'\u005cu0391' parameter
p3926
aVwhere x p ranges over u'\u005cu2662' and all previous mentions of the same type as x , that is, mentions p such that p i x i and p e t = x e t
p3927
aVThe normalizing constant Z ( x ) = def u'\u005cu2211' p exp ( u'\u005cu03a6' u'\u005cu22c5' u'\u005cud835' u'\u005cudc1f' ( x p , x ) ) is chosen so that the probabilities sum to 1
p3928
aVThis binary feature has a high weight if authors mainly choose mentions from the same topic
p3929
aVOne could also make more specific versions of any feature by conjoining it with the entity type t
p3930
aVLet x denote a mention with parent p = x p
p3931
aVAs in , its name x n is a stochastic transduction of its parent u'\u005cu2019' s name p n
p3932
aVMore generally, the probability ( 2 ) may also be conditioned on other variables such as on the languages p u'\u005cu2113' and x u'\u005cu2113' u'\u005cu2014' this leaves room for a transliteration model when x u'\u005cu2113' u'\u005cu2260' p u'\u005cu2113' u'\u005cu2014' and on the entity type x t
p3933
aVWe use the same model, taking u'\u005cu2662' n to be the empty string (but with # u'\u005cu2662' rather than # as the end-of-string symbol
p3934
aVThis yields a feature-based unigram language model (whose character probabilities may differ from usual insertion probabilities because they see # u'\u005cu2662' as the lookahead character
p3935
aVThe edit model thinks that Pr u'\u005cud835' u'\u005cudf3d' ( u'\u005cud835' u'\u005cudda2' u'\u005cud835' u'\u005cudda8' u'\u005cud835' u'\u005cudda0' u'\u005cu2223' u'\u005cu2662' ) is relatively high (because CIA is a short string) and so is Pr u'\u005cud835' u'\u005cudf3d' ( u'\u005cud835' u'\u005cudda2' u'\u005cud835' u'\u005cudda8' u'\u005cud835' u'\u005cudda0' u'\u005cu2223' Chuck u'\u005cu2019' s Ice Art
p3936
aVBut in fact, if CIA has already been frequently used to refer to the Central Intelligence Agency, then an author is unlikely to use it for a different entity
p3937
aVWe take this to be the probability that a reader who knows our sub-models would guess some parent having the correct entity (or u'\u005cu2662' if x is a first mention u'\u005cu2211' p u'\u005cu2032' p u'\u005cu2032' e = x e w u'\u005cu2062' ( p u'\u005cu2032' , x ) / u'\u005cu2211' p u'\u005cu2032' w u'\u005cu2062' ( p u'\u005cu2032' , x
p3938
aVOutput the current sample s t = ( u'\u005cud835' u'\u005cudc91' , u'\u005cud835' u'\u005cudc8a' , u'\u005cud835' u'\u005cudc9b' )
p3939
aVIt is difficult to draw exact samples at steps 1 and 2
p3940
aVThus, we sample u'\u005cud835' u'\u005cudc8a' or u'\u005cud835' u'\u005cudc9b' from a simpler proposal distribution, but correct the discrepancy using the Independent Metropolis-Hastings (IMH) strategy with an appropriate probability, reject the proposed new value and instead use another copy of the current value [ ]
p3941
aVThe current phylogeny u'\u005cud835' u'\u005cudc91' already defines a partial order on u'\u005cud835' u'\u005cudc99' , since each parent must precede its children
p3942
aVWe provide details about this procedure in Appendix A .) 7 7 The full version of this paper is available at http://cs.jhu.edu/~noa/publications/phylo-acl-14.pdf However, such orderings are not in fact equiprobable given the other variables u'\u005cu2014' some orderings better explain why that phylogeny was chosen in the first place, according to our competitive parent selection model (§ 4.1
p3943
aVThe topics of context words are assumed exchangeable, and so we resample them using Gibbs sampling [ ]
p3944
aVThis probability is expensive to evaluate because changing x z will change the probability of many edges in the current phylogeny u'\u005cud835' u'\u005cudc91'
p3945
aVEquation ( 1 ) puts x is in competition with other parents, so every mention y that follows x must recompute how happy it is with its current parent y p
p3946
aVAs detailed below, a proposal can be sampled from Q u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9b' ) in time O u'\u005cu2062' u'\u005cud835' u'\u005cudc9b' u'\u005cu2062' K 2 ) where K is the number of topics, because the only interactions among topics are along the edges of the tree u'\u005cud835' u'\u005cudc91'
p3947
aVWe sample from Q using standard methods, \u005ctodo [author=jason,color=RedOrange,fancyline,size=,]can we cite a section of Koller Friedman similar to sampling from a linear-chain CRF by running the backward algorithm followed by forward sampling
p3948
aVWhile Pr ( u'\u005cud835' u'\u005cudc91' , u'\u005cud835' u'\u005cudc8a' , u'\u005cud835' u'\u005cudc9b' ^ , u'\u005cud835' u'\u005cudc99' u'\u005cu2223' u'\u005cud835' u'\u005cudf3d' , u'\u005cu03a6' ) might seem slow to compute because it contains many factors ( 1 ) with different denominators Z u'\u005cu2062' ( x ) , one can share work by visiting the mentions x in their order u'\u005cud835' u'\u005cudc8a'
p3949
aVMost summands in Z u'\u005cu2062' ( x ) were already included in Z u'\u005cu2062' ( x u'\u005cu2032' ) , where x u'\u005cu2032' is the latest previous mention having the same attributes as x (e.g.,, same topic
p3950
aV\u005ctodo [author=jason,color=RedOrange,fancyline,size=,]Nick, I assume you use this trick
p3951
aV[author=jason,color=RedOrange,fancyline,size=,]cut out a speedup here for space
p3952
aVSince the ordering u'\u005cud835' u'\u005cudc8a' prevents cycles, the resulting phylogeny u'\u005cud835' u'\u005cudc91' is indeed a tree
p3953
aVGiven the topics u'\u005cud835' u'\u005cudc9b' , the ordering u'\u005cud835' u'\u005cudc8a' , and the observed names, we choose an x p value according to its posterior probability
p3954
aVWith the pragmatic model (section 4.2 ), the parent choices are no longer independent; then the samples of u'\u005cud835' u'\u005cudc91' should be corrected by IMH as usual
p3955
aVThe initial sampler state ( u'\u005cud835' u'\u005cudc9b' 0 , u'\u005cud835' u'\u005cudc91' 0 , u'\u005cud835' u'\u005cudc8a' 0 ) is obtained as follows
p3956
aV\u005ctodo [author=jason,color=RedOrange,fancyline,size=,]nick, a LaTeXnote you can use inline lists with itemize* if you want to compactify like this (we u'\u005cu2019' re already using enumitem package) (1) We fix topics u'\u005cud835' u'\u005cudc9b' 0 via collapsed Gibbs sampling [ ]
p3957
aVThis process treats all topics as exchangeable, including those associated with named entities
p3958
aV\u005ctodo [author=jason,color=RedOrange,fancyline,size=,]Well, maybe they u'\u005cu2019' re exchangeable if you u'\u005cu2019' re ignoring the names at this step and just treating the types as words
p3959
aVAre you
p3960
aVThe weight w ( x p , x ) is defined as in section 5.3 u'\u005cu2014' except that since we do not yet have an ordering u'\u005cud835' u'\u005cudc8a' , we do not restrict the possible values of x p to mentions p with p i x p i
p3961
aVEvaluating the likelihood and its partial derivatives with respect to the parameters of the model requires marginalizing over our latent variables
p3962
aVAs this marginalization is intractable, we resort to Monte Carlo EM procedure [ ] which iterates the following two steps
p3963
aVImprove u'\u005cud835' u'\u005cudf3d' and u'\u005cu03a6' to increase 8 8 We actually do MAP-EM, which augments ( 7 ) by adding the log-likelihoods of u'\u005cu0398' and u'\u005cu03a6' under a Gaussian prior
p3964
aVIt is not necessary to locally maximize u'\u005cu2112' at each M-step, merely to improve it if it is not already at a local maximum [ ]
p3965
aVAlso, should u'\u005cu03a3' t be described as a (diagonal) matrix where u'\u005cu0395' is a fixed scaling term and u'\u005cu03a3' t is an adaptive learning rate given by AdaGrad [ ]
p3966
aVThat edge explains a mention x as a mutation of some parent p in the context of a particular sample ( u'\u005cud835' u'\u005cudc91' s , u'\u005cud835' u'\u005cudc8a' s , u'\u005cud835' u'\u005cudc9b' s
p3967
aVThe possible parents p u'\u005cu2032' range over u'\u005cu2662' and the mentions that precede x according to the ordering u'\u005cud835' u'\u005cudc8a' s , while the features u'\u005cud835' u'\u005cudc87' and distribution Pr u'\u005cu03a6' depend on the topics u'\u005cud835' u'\u005cudc9b' s
p3968
aVFrom a single phylogeny u'\u005cud835' u'\u005cudc91' , we deterministically obtain a clustering u'\u005cud835' u'\u005cudc86' by removing the root u'\u005cu2662'
p3969
aVOur model gives a distribution over phylogenies u'\u005cud835' u'\u005cudc91' (given observations u'\u005cud835' u'\u005cudc99' and learned parameters u'\u005cu03a6' ) u'\u005cu2014' and thus gives a posterior distribution over clusterings u'\u005cud835' u'\u005cudc86' , which can be used to answer various queries
p3970
aVIn practice, we again estimate the expectation by sampling u'\u005cud835' u'\u005cudc86' values
p3971
aVwhere the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) use the clustering u'\u005cud835' u'\u005cudc86' to evaluate how well u'\u005cud835' u'\u005cudc86' u'\u005cu2032' classifies the ( N 2 ) mention pairs as coreferent or not
p3972
aVMore similar clusterings achieve larger R , with R u'\u005cu2062' ( u'\u005cud835' u'\u005cudc86' u'\u005cu2032' , u'\u005cud835' u'\u005cudc86' ) = 1 iff u'\u005cud835' u'\u005cudc86' u'\u005cu2032' = u'\u005cud835' u'\u005cudc86'
p3973
aVwhere u'\u005cu223c' denotes coreference according to u'\u005cud835' u'\u005cudc86' u'\u005cu2032'
p3974
aVAs explained above, the s i u'\u005cu2062' j are coreference probabilities s i u'\u005cu2062' j that can be estimated from a sample of clusterings u'\u005cud835' u'\u005cudc86'
p3975
aVFor the political blog dataset, the reference does not consist of entity annotations, and so we follow the evaluation procedure of
p3976
aVTheir method uses Jaro-Winkler string similarity to match names, then clusters mentions with matching names (for disambiguation) by comparing their unigram context distributions using the Jenson-Shannon metric
p3977
aVFor our model, we tune only the fixed weight of the root feature, which determines the precision/recall trade-off (larger values of this feature result in more attachments to u'\u005cu2662' and hence more entities
p3978
aVWe leave other hyperparameters fixed
p3979
aVThe baseline system took the first mention from each (gold) within-document coreference chain as the canonical mention, ignoring other mentions in the chain; we follow the same procedure in our experiments
p3980
aVWe use the same baselines as in § 8.1
p3981
aVOn development data, modeling pragmatics as in § 4.2 gave large improvements for organizations (8 points in F-measure), \u005ctodo [author=jason,color=RedOrange,fancyline,size=,]but maybe just for org correcting the tendency to assume that short names like CIA were coincidental homonyms
p3982
aVHence we allowed u'\u005cu0393' 0 and tuned it on development data
p3983
aVThe other results we report do not use pragmatics at all, since we found that it gave only a slight improvement on Twitter
p3984
aVThe CMU political blogs dataset consists of 3000 documents about U.S politics [ ]
p3985
aVPreprocessed as described in , the data consists of 10647 entity mentions
p3986
aVLike the output of our model, the output of their hierarchical clustering baseline is a mention clustering, and therefore must be mapped to a table of canonical entity names to compare to the reference table
p3987
aVThe tuned model then produced a mention clustering on the full political blog corpus
p3988
aVAs the mapping from clusters to a table is not fully detailed in , we used a simple heuristic the most frequent name in each cluster is taken as the canonical name, augmented by any titles from a predefined list appearing in any other name in the cluster
p3989
aVThough not state-of-the-art, this result is close to the score of the u'\u005cu201c' EEA u'\u005cu201d' system of , as reported in Figure 2 of , \u005ctodo [author=jason,color=RedOrange,fancyline,size=,]so we lose slightly to EEA but EEA loses big to Yogotama which is specifically designed for the task of canonicalization
p3990
aVFor instance, we see several instances of variation due to transliteration that were all correctly grouped together, such as Megawati Soekarnoputri and Megawati Sukarnoputri
p3991
aVWe found that multiple samples tend to give different phylogenies (so the sampler is mobile), but essentially the same clustering into entities (which is why consensus clustering did not improve much over simply using the last sample
p3992
aVRandom restarts of EM might create more variety by choosing different locally optimal parameter settings
p3993
aVHowever, the true tree must include many names that fall outside our small observed corpora, so our model would be a more appropriate fit for a far larger corpus
p3994
aVLarger corpora also offer stronger signals that might enable our Monte Carlo methods to mix faster and detect regularities more accurately
p3995
aVNote that in the base case where x is a leaf (so M = 0 ), this procedure terminates immediately, having printed the empty ordering
p3996
aVUsing the Twitter 1% streaming API, we collected all tweets during the 2013 Grammy music awards ceremony, which occurred on Feb 10, 2013 between 8pm eastern (1:00am GMT) and 11:30pm (4:30 GMT
p3997
aVIf the extracted mention was incorrect or referred to a non-person, it was removed
p3998
aVIf it was mostly correct, but omitted/excluded a token, the annotator corrected it
p3999
asg88
(lp4000
sg90
(lp4001
sg92
(lp4002
VOur primary contribution consists of new modeling ideas, and associated inference techniques, for the problem of cross-document coreference resolution.
p4003
aVWe have described how writers systematically plunder ( ) and then systematically modify ( ) the work of past writers.
p4004
aVInference under such models could also play a role in tracking evolving memes and social influence, not merely in establishing strict coreference.
p4005
aVOur model also provides an alternative to the distance-dependent CRP.
p4006
aV3.
p4007
aVOur implementation is available for research use at https://bitbucket.org/noandrews/phyloinf.
p4008
ag106
asg107
S'P14-1073'
p4009
sg109
(lp4010
VEntity clustering must determine when two named-entity mentions refer to the same entity.
p4011
aVTypical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity.
p4012
aVIn this paper, we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data.
p4013
aVThe generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context.
p4014
aVClustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process.
p4015
aVWe present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets.
p4016
ag106
asba(icmyPackage
FText
p4017
(dp4018
g3
(lp4019
VIn the literature, relation extraction (RE) is usually investigated in a classification style, where relations are simply treated as isolated class labels, while their definitions or background information are sometimes ignored
p4020
aVTake the relation Capital as an example, we can imagine that this relation will expect a country as its subject and a city as object, and in most cases, a city can be the capital of only one country
p4021
aVSimilarly, the cardinality requirements of arguments, e.g.,, a person can have only one birthdate and a city can only be labeled as capital of one country, should be considered as a strong indicator to eliminate wrong predictions, but has to be coded manually as well
p4022
aVOn the other hand, most previous relation extractors process each entity pair (we will use entity pair and entity tuple exchangeably in the rest of the paper) locally and individually, i.e.,, the extractor makes decisions solely based on the sentences containing the current entity pair and ignores other related pairs, therefore has difficulties to capture possible disagreements among different entity pairs
p4023
aVHowever, when looking at the output of a multi-class relation predictor globally, we can easily find possible incorrect predictions such as a university locates in two different cities, two different cities have been labeled as capital for one country, a country locates in a city and so on
p4024
aVIn this paper, we will address how to derive and exploit two categories of these clues the expected types and the cardinality requirements of a relation u'\u005cu2019' s arguments, in the scenario of relation extraction
p4025
aVWe propose to perform joint inference upon multiple local predictions by leveraging implicit clues that are encoded with relation specific requirements and can be learnt from existing knowledge bases
p4026
aVSpecifically, the joint inference framework operates on the output of a sentence level relation extractor as input, derives 5 types of constraints from an existing KB to implicitly capture the expected type and cardinality requirements for a relation u'\u005cu2019' s arguments, and jointly resolve the disagreements among candidate predictions
p4027
aVWe formalize this procedure as a constrained optimization problem, which can be solved by many optimization frameworks
p4028
aVWe use integer linear programming (ILP) as the solver and evaluate our framework on English and Chinese datasets
p4029
aVWe conclude this paper in Section 5
p4030
aVSince traditional supervised relation extraction methods [ 12 , 20 ] require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods [ 1 , 5 ]
p4031
aVSince the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data [ 16 ]
p4032
aVThese can be considered as the latent type information of the relations u'\u005cu2019' arguments, which is learnt from various data sources
p4033
aVIn contrast, our approach learn implicit clues from existing KBs, and jointly optimize local predictions among different entity tuples to capture both relation argument type clues and cardinality clues
p4034
aVOur framework takes a set of entity pairs and their supporting sentences as its input
p4035
aVWe first train a preliminary sentence level extractor which can output confidence scores for its predictions, e.g.,, a maximum entropy or logistic regression model, and use this local extractor to produce local predictions
p4036
aVSince we will focus on the open domain relation extraction, we still follow the distant supervision paradigm to collect our training data guided by a KB, and train the local extractor accordingly
p4037
aVKeep in mind that our local extractor is trained on noisy training data, which, we admit, is not fully reliable
p4038
aVAs we observed in a pilot experiment that there is a good chance that the predictions ranked in the second or third may still be correct, we select top three predictions as the candidate relations for each mention in order to introduce more potentially correct output
p4039
aVFor a tuple t , we obtain its candidate relation set by combining the candidate relations of all its mentions, and represent it as R t
p4040
aVFor a candidate relation r u'\u005cu2208' R t and a tuple t , we define M t r as all t u'\u005cu2019' s mentions whose candidate relations contain r
p4041
aVGenerally, we expect more potentially correct relations to be put into the candidate relation set for further consideration
p4042
aVSo in addition to lexical and syntactic features, we also use n-gram features to train our preliminary relation extraction model
p4043
aVN-gram features are considered as more ambiguous compared to traditional lexical and syntactic features, and may introduce incorrect predictions, thus improving the recall at the cost of precision
p4044
aVThe candidate relations we obtained in the previous subsection inevitably include many incorrect predictions
p4045
aVIdeally we should discard those wrong predictions to produce more accurate results
p4046
aVAs discussed earlier, we will exploit from the knowledge base two categories of clues that implicitly capture relations u'\u005cu2019' backgrounds their expected argument types and argument cardinalities, based on which we can discover two categories of disagreements among the candidate predictions, summarized as argument type inconsistencies and violations of arguments u'\u005cu2019' uniqueness, which have been rarely considered before
p4047
aVIf the predictions among different entity tuples require the same entity to belong to different types, we call this an argument type inconsistency
p4048
aVIn Figure 1 , USA, New York has a candidate relation LargestCity which restricts USA to be either countries or states, while USA, Washington D.C has a prediction LocationCity which indicates a disagreement in terms of USA u'\u005cu2019' s type because the latter prediction expects USA to be an organization located in a city
p4049
aVWe represent the relation pairs ( r i , r j ) that are inconsistent in terms of subjects as u'\u005cud835' u'\u005cudc9e' s u'\u005cu2062' r , the relations pairs that are inconsistent in terms of objects as u'\u005cud835' u'\u005cudc9e' r u'\u005cu2062' o , the relation pairs that are inconsistent in terms of one u'\u005cu2019' s subject and the other one u'\u005cu2019' s object as u'\u005cud835' u'\u005cudc9e' r u'\u005cu2062' e u'\u005cu2062' r
p4050
aVThey implicitly consider USA as u'\u005cu201c' country u'\u005cu201d' and u'\u005cu201c' organization u'\u005cu201d' , respectively
p4051
aVThe previous categories of disagreements are all based on the implicit type information of the relations u'\u005cu2019' arguments, Now we make use of the clues of argument cardinality requirements
p4052
aVFor example, in Figure 1 , given USA as the subject of the relation Capital , we can only accept one possible object, because there is great chance that a country only have one capital
p4053
aVOn the other hand, given Washington D.C as the object of the relation Capital , we can only accept one subject, since usually a city can only be the capital of one country or state
p4054
aVIf these are violating in the candidates, we could know that there may be some incorrect predictions
p4055
aVWe represent the relations expecting unique objects as u'\u005cud835' u'\u005cudc9e' o u'\u005cu2062' u , and the relations expecting unique subjects as u'\u005cud835' u'\u005cudc9e' s u'\u005cu2062' u
p4056
aVMost existing knowledge bases represent their knowledge facts in the form of ( subject, relation, object ) triple, which can be seen as relational facts between entity tuples
p4057
aVUsually the triples in a KB are carefully defined by experts
p4058
aVThe clues are therefore learnt from KBs, and further refined manually if needed
p4059
aVThis threshold is set to 0.8 in this paper
p4060
aVAs discussed above, given a set of entity pairs and their candidate relations output by a preliminary extractor, our goal is to find an optimal configuration for all those entities pairs jointly, solving the disagreements among those candidate predictions and maximizing the overall confidence of the selected predictions
p4061
aVIn this paper, we propose to solve the problem by using an ILP tool, IBM ILOG Cplex 1 1 www.cplex.com
p4062
aVFor the sake of clarity, we describe the constraints derived from each scenario of the two categories of disagreements separately
p4063
aVThe relation-entity-relation constraints ensure that if an entity works as subject and object in two tuples t i and t j respectively, their relations agree with each other
p4064
aVBy adopting ILP, we can combine the local information including MaxEnt confidence scores and the implicit relation backgrounds that are embedded into global consistencies of the entity tuples together
p4065
aVIt uses Freebase as the knowledge base and New York Time corpus as the text corpus, including about 60,000 entity tuples in the training set, and about 90,000 entity tuples in the testing set
p4066
aVWe generate the second English dataset, DBpedia dataset, by mapping the triples in DBpedia [ 2 ] to the sentences in New York Time corpus
p4067
aVWe collect four national economic newspapers in 2009 as our corpus
p4068
aV28 different relations are mapped to the corpus and this results in 60,000 entity tuples, 120,000 sentences for training and 40,000 tuples, 83,000 sentences for testing
p4069
aVThe model predicts for each mention separately, and allows multi-label outputs for an entity tuple by OR-ing the outputs of its mentions
p4070
aVAs we described in Section 3.1 , originally we select the top three predicted relations as the candidates for each mention
p4071
aVWe tune the models of MultiR and MIML-RE so that they fit our datasets
p4072
aVWhat is worse, we cannot find any clues from the top three relations because their arguments u'\u005cu2019' types are too general
p4073
aVAlthough we may find some clues any way, they are too few to make any improvement
p4074
aVHence, our framework does not perform well due to the poor performance of MaxEnt extractor and the lack of clues
p4075
aVCompared to ILP-2cand and original ILP, ILP-1cand leads to slightly lower precision but much lower recall, showing that selecting more candidates may help us collect more potentially correct predictions
p4076
aVComparing ILP-2cand and original ILP, the latter hardly makes any improvement in precision, but is slightly longer in recall, indicating using three candidates can still collect some more potentially correct predictions, although the number may be limited
p4077
aVDifferent from [ 15 ] , we use all the entity pairs instead of the ones with more than 10 mentions
p4078
aVWe also investigate the impacts of the constraints used in ILP, which are derived based on the two kinds of clues and can encode relation definition information into our framework
p4079
aVIn the Riedel u'\u005cu2019' s dataset we do not see any improvements since there are almost no clues
p4080
aVWe also fit MultiR u'\u005cu2019' s mention level extractor into our framework
p4081
aVAs shown in Figure 3 , in the DBpedia dataset and the Chinese dataset, in most parts of the curve, ILP optimized MultiR outperforms original MultiR
p4082
aVWe think the reason is that our framework make use of global clues to discard the incorrect predictions
p4083
aVWe think one reason is that MultiR does not perform well in these two datasets
p4084
aVAs a result, we only use the top one result as the candidate since including top two predictions without thresholding the confidences performs bad, indicating that a probabilistic sentence-level extractor is more suitable for our framework
p4085
aVSince there are almost no clues in the Riedel u'\u005cu2019' s dataset, we only investigate the other two datasets
p4086
aVWe add clues according to their related relations u'\u005cu2019' proportions in the local predictions
p4087
aVFor example, Country and birthPlace take up about 30% in the local predictions, we thus add clues that are related to these two relations, and then move on with new clues related to other relations according to those relations u'\u005cu2019' proportions in the local predictions
p4088
aVAs is shown in Figure 4 , in both datasets, the clues related to more local predictions will solve more inconsistencies, thus are more effective
p4089
aVAdding the first two relations improves the model significantly, and as more relations are added, the performances keep increasing until approaching the still state
p4090
aVIt is worth mentioning that when sufficient learnt clues are added into the model, the results are comparable to those based on the clues refined manually, as shown in Figure 5
p4091
aVThis indicates that the clues can be collected automatically, and further used to examine whether predicted relations are consistent with the existing ones in the KB, which can be considered as a form of quality control
p4092
aVIn this paper, we make use of the global clues derived from KB to help resolve the disagreements among local relation predictions, thus reduce the incorrect predictions and improve the performance of relation extraction
p4093
aVOur framework outperforms the state-of-the-art models if we can find such clues in the KB
p4094
aVWe will also adopt selection preference between entities and relations since sometimes we may not find useful clues
p4095
asg88
(lp4096
sg90
(lp4097
sg92
(lp4098
VIn this paper, we make use of the global clues derived from KB to help resolve the disagreements among local relation predictions, thus reduce the incorrect predictions and improve the performance of relation extraction.
p4099
aVTwo kinds of clues, including implicit argument type information and argument cardinality information of relations are investigated.
p4100
aVOur framework outperforms the state-of-the-art models if we can find such clues in the KB.
p4101
aVFurthermore, our framework is scalable for other local sentence level extractors in addition to the MaxEnt model.
p4102
aVFinally, we show that the clues can be learnt automatically from the KB, and lead to comparable performance to manually refined ones.
p4103
aVFor future work, we will investigate other kinds of clues and attempt a joint optimization framework that could host entity disambiguation, relation extraction and entity linking together.
p4104
aVWe will also adopt selection preference between entities and relations since sometimes we may not find useful clues.
p4105
ag106
asg107
S'P14-1077'
p4106
sg109
(lp4107
VMost existing relation extraction models make predictions for each entity pair locally and individually, while ignoring implicit global clues available in the knowledge base, sometimes leading to conflicts among local predictions from different entity pairs.
p4108
aVIn this paper, we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions.
p4109
aVWe exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation.
p4110
aVExperimental results on three datasets, in both English and Chinese, show that our framework outperforms the state-of-the-art relation extraction models when such clues are applicable to the datasets.
p4111
aVAnd, we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human.
p4112
ag106
asba(icmyPackage
FText
p4113
(dp4114
g3
(lp4115
VOur algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low
p4116
aVTraditional supervised methods [ 28 , 1 ] on small hand-labeled corpora, such as MUC 1 1 http://www.itl.nist.gov/iaui/894.02/related projects/muc/ and ACE 2 2 http://www.itl.nist.gov/iad/mig/tests/ace/ , can achieve high precision and recall
p4117
aVHowever, as producing hand-labeled corpora is laborius and expensive, the supervised approach can not satisfy the increasing demand of building large-scale knowledge repositories with the explosion of Web texts
p4118
aVThe intuition of the paradigm is that one can take advantage of several knowledge bases, such as WordNet 3 3 http://wordnet.princeton.edu , Freebase 4 4 http://www.freebase.com and YAGO 5 5 http://www.mpi-inf.mpg.de/yago-naga/yago , to automatically label free texts, like Wikipedia 6 6 http://www.wikipedia.org and New York Times corpora 7 7 http://catalog.ldc.upenn.edu/LDC2008T19 , based on some heuristic alignment assumptions
p4119
aVAn example accounting for the basic but practical assumption is illustrated in Figure 1, in which we know that the two entities ( Barack Obama, U.S are not only involved in the relation instances 8 8 According to convention, we regard a structured triple r u'\u005cu2062' ( e i , e j ) as a relation instance which is composed of a pair of entities e i , e j and a relation name r with respect to them coming from knowledge bases ( President-of(Barack Obama, U.S.) and Born-in(Barack Obama, U.S.) ), but also co-occur in several relation mentions 9 9 The sentences that contain the given entity pair are called relation mentions appearing in free texts ( Barack Obama is the 44th and current President of the U.S and Barack Obama was born in Honolulu, Hawaii, U.S etc
p4120
aVFor example, the second relation mention in Figure 1 does not explicitly describe any relation instance, so features extracted from this sentence can be noisy
p4121
aVSimilar to noisy features, the generated labels can be incomplete
p4122
aVHowever, the incomplete knowledge base does not contain the corresponding relation instance ( Senate-of(Barack Obama, U.S.)
p4123
aVTherefore, the distant supervision paradigm may generate incomplete labeling corpora
p4124
aVTo the best of our knowledge, we are the first to apply this technique on relation extraction with distant supervision
p4125
aVMore specifically, as shown in Figure 2, we model the task with a sparse matrix whose rows present items (entity pairs) and columns contain noisy textual features and incomplete relation labels
p4126
aVIn such a way, relation classification is transformed into a problem of completing the unknown labels for testing items in the sparse matrix that concatenates training and testing textual features with training labels, based on the assumption that the item-by-feature and item-by-label joint matrix is of low rank
p4127
aV[ 23 ] used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles
p4128
aVHowever, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date
p4129
aVAs we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without hand-labeled annotation
p4130
aVThe basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name
p4131
aVOur work is more relevant to Riedel et al u'\u005cu2019' s [ 21 ] which considered the task as a matrix factorization problem
p4132
aVTheir approach is composed of several models, such as PCA [ 7 ] and collaborative filtering [ 15 ]
p4133
aVOur models for relation extraction are based on the theoretic framework proposed by Goldberg et al
p4134
aVThe new framework for classification enhances the robustness to data noise by penalizing different cost functions for features and labels
p4135
aVSuppose that we have built a training corpus for relation classification with n items (entity pairs), d -dimensional textual features, and t labels (relations), based on the basic alignment assumption proposed by Mintz et al
p4136
aVThis linear classification problem can be transformed into completing the unobservable entries in Y t u'\u005cu2062' e u'\u005cu2062' s u'\u005cu2062' t by means of the observable entries in X t u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n , Y t u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n and X t u'\u005cu2062' e u'\u005cu2062' s u'\u005cu2062' t , based on the assumption that the rank of matrix u'\u005cud835' u'\u005cudc19' u'\u005cu2208' u'\u005cu211d' ( n + m ) × ( d + t ) is low
p4137
aVFormula (2) is usually impractical for real problems as the entries in the matrix u'\u005cud835' u'\u005cudc19' are corrupted by noise
p4138
aVwhere u'\u005cud835' u'\u005cudc19' * as the underlying low-rank matrix
p4139
aVand E is the error matrix
p4140
aVAccording to Formula (1), u'\u005cud835' u'\u005cudc19' * u'\u005cu2208' u'\u005cu211d' ( n + m ) × ( d + t ) can be represented as [ X * , u'\u005cud835' u'\u005cudc16' u'\u005cu2062' X * ] instead of [ X * , Y * ] , by explicitly modeling the bias vector u'\u005cud835' u'\u005cudc1b'
p4141
aVTherefore, this convex optimization model is called DRMC-b
p4142
aVIf we implicitly model the bias vector u'\u005cud835' u'\u005cudc1b' , u'\u005cud835' u'\u005cudc19' * u'\u005cu2208' u'\u005cu211d' ( n + m ) × ( 1 + d + t ) can be denoted by [ u'\u005cud835' u'\u005cudfcf' , X * , u'\u005cud835' u'\u005cudc16' u'\u005cu2032' u'\u005cu2062' X * ] instead of [ X * , Y * ] , in which u'\u005cud835' u'\u005cudc16' u'\u005cu2032' takes the role of [ u'\u005cud835' u'\u005cudc1b' T ; u'\u005cud835' u'\u005cudc16' ] in DRMC-b
p4143
aVThe matrix rank minimization problem is NP-hard
p4144
aVTherefore, Candés and Recht [ 5 ] suggested to use a convex relaxation, the nuclear norm minimization instead
p4145
aVMoreover, Goldfrab and Ma [ 11 ] proved the convergence of the FPC algorithm for solving the nuclear norm minimization problem
p4146
aVWe thus adopt and modify the algorithm aiming to find the optima for our noise-tolerant models, i.e.,, Formulae (3) and (4
p4147
aVAlgorithm 1 describes the modified FPC algorithm for solving DRMC-b, which contains two steps for each iteration
p4148
aVFPC algorithm for solving DRMC-b {algorithmic} \u005cREQUIRE Initial matrix u'\u005cud835' u'\u005cudc19' u'\u005cud835' u'\u005cudfce' , bias u'\u005cud835' u'\u005cudc1b' u'\u005cud835' u'\u005cudfce' ; Parameters u'\u005cu039c' , u'\u005cu039b' ; Step sizes u'\u005cu03a4' z , u'\u005cu03a4' b
p4149
aV\u005cFOR u'\u005cu039c' = u'\u005cu039c' 1 u'\u005cu039c' 2 u'\u005cu2026' u'\u005cu039c' F \u005cWHILE relative error u'\u005cu0395' \u005cSTATE Gradient step u'\u005cud835' u'\u005cudc00' = u'\u005cud835' u'\u005cudc19' - u'\u005cu03a4' z u'\u005cu2062' g u'\u005cu2062' ( u'\u005cud835' u'\u005cudc19' ) , u'\u005cud835' u'\u005cudc1b' = u'\u005cud835' u'\u005cudc1b' - u'\u005cu03a4' b u'\u005cu2062' g u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1b'
p4150
aVTo accelerate the convergence, we use a continuation method to improve the speed u'\u005cu039c' is initialized by a large value u'\u005cu039c' 1 , thus resulting in the fast reduction of the rank at first
p4151
aVThen the convergence slows down as u'\u005cu039c' decreases while obeying u'\u005cu039c' k + 1 = m u'\u005cu2062' a u'\u005cu2062' x u'\u005cu2062' ( u'\u005cu039c' k u'\u005cu2062' u'\u005cu0397' u'\u005cu039c' , u'\u005cu039c' F u'\u005cu039c' F is the final value of u'\u005cu039c' , and u'\u005cu0397' u'\u005cu039c' is the decay parameter
p4152
aVFPC algorithm for solving DRMC-1 {algorithmic} \u005cREQUIRE Initial matrix u'\u005cud835' u'\u005cudc19' u'\u005cud835' u'\u005cudfce' ; Parameters u'\u005cu039c' , u'\u005cu039b' ; Step sizes u'\u005cu03a4' z
p4153
aV\u005cFOR u'\u005cu039c' = u'\u005cu039c' 1 u'\u005cu039c' 2 u'\u005cu2026' u'\u005cu039c' F \u005cWHILE relative error u'\u005cu0395' \u005cSTATE Gradient step u'\u005cud835' u'\u005cudc00' = u'\u005cud835' u'\u005cudc19' - u'\u005cu03a4' z u'\u005cu2062' g u'\u005cu2062' ( u'\u005cud835' u'\u005cudc19'
p4154
aV\u005cENSURE Completed Matrix Z
p4155
aVThe two widely used datasets that we adopt are both automatically generated by aligning Freebase to New York Times corpora
p4156
aV[ 16 ] revealed that as long as the non-negative step sizes satisfy u'\u005cu03a4' z m u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' ( 4 u'\u005cu2062' u'\u005cu03a9' Y u'\u005cu039b' u'\u005cu03a9' X and u'\u005cu03a4' b 4 u'\u005cu2062' u'\u005cu03a9' Y u'\u005cu039b' u'\u005cu2062' ( n + m ) , the FPC algorithm will guarantee to converge to a global optimum
p4157
aVTherefore, we set u'\u005cu03a4' z = u'\u005cu03a4' b = 0.5 to satisfy the above constraints on both two datasets
p4158
aVIn practice, we record the rank of matrix Z at each round of iteration until it converges at a rather small threshold u'\u005cu0395' = 10 - 4
p4159
aVThe reason is that we suppose the optimal low-rank representation of the matrix Z conveys the truly effective information about underlying semantic correlation between the features and the corresponding labels
p4160
aVOn both two datasets, we observe an identical phenomenon that the performance gradually increases as the rank of the matrix declines before reaching the optimum
p4161
aVHowever, it sharply decreases if we continue reducing the optimal rank
p4162
aVAn intuitive explanation is that the high-rank matrix contains much noise and the model tends to be overfitting, whereas the matrix of excessively low rank is more likely to lose principal information and the model tends to be underfitting
p4163
aVThey set u'\u005cu0398' = 5 in the original code by default
p4164
aVTherefore, we follow their settings and adopt the same way to filter the features
p4165
aVWe bypass the description due to the limitation of space
p4166
aVIn practical applications, we also concern about the precision on Top-N predicted relation instances
p4167
aVTherefore, We compare the precision of Top-100s, Top-200s and Top-500s for DRMC-1, DRMC-b and the state-of-the-art method NFE-13 [ 21 ]
p4168
aVDue to the noisy features and incomplete labels, the underlying low-rank data matrix with truly effective information tends to be corrupted and the rank of observed data matrix can be extremely high
p4169
aVHowever, those high ranks result in poor performance
p4170
aVAs the ranks decline before approaching the optimum, the performance gradually improves, implying that our approaches filter the noise in data and keep the principal information for classification via recovering the underlying low-rank data matrix
p4171
aVIn other words, for each approach, the amount of truly effective information about underlying semantic correlation keeps constant for the same dataset, which, to some extent, explains the reason why our approaches are robust to sparse features
p4172
aVOur models are based on matrix completion with low-rank criterion
p4173
aVExperiments demonstrated that the low-rank representation of the feature-label matrix can exploit the underlying semantic correlated information for relation classification and is effective to overcome the difficulties incurred by sparse and noisy features and incomplete labels, so that we achieved significant improvements on performance
p4174
aVFirst, they can not process new coming testing items efficiently, as we have to reconstruct the data matrix containing not only the testing items but also all the training items for relation classification, and compute in iterative fashion again
p4175
aVFor the future work, we plan to improve our models so that they will be capable of incremental learning on large-scale datasets [ 6 ]
p4176
asg88
(lp4177
sg90
(lp4178
sg92
(lp4179
VIn this paper, we contributed two noise-tolerant optimization models 17 17 The source code can be downloaded from https://github.com/nlpgeek/DRMC/tree/master , DRMC-b and DRMC-1, for distantly supervised relation extraction task from a novel perspective.
p4180
aVOur models are based on matrix completion with low-rank criterion.
p4181
aVExperiments demonstrated that the low-rank representation of the feature-label matrix can exploit the underlying semantic correlated information for relation classification and is effective to overcome the difficulties incurred by sparse and noisy features and incomplete labels, so that we achieved significant improvements on performance.
p4182
aVOur proposed models also leave open questions for distantly supervised relation extraction task.
p4183
aVFirst, they can not process new coming testing items efficiently, as we have to reconstruct the data matrix containing not only the testing items but also all the training items for relation classification, and compute in iterative fashion again.
p4184
aVSecond, the volume of the datasets we adopt are relatively small.
p4185
aVFor the future work, we plan to improve our models so that they will be capable of incremental learning on large-scale datasets [ 6 ].
p4186
ag106
asg107
S'P14-1079'
p4187
sg109
(lp4188
VThe essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features.
p4189
aVTo tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank.
p4190
aVWe formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels.
p4191
aVOur algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low.
p4192
aVWe apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix.
p4193
aVThe matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum.
p4194
aVExperiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.
p4195
ag106
asba(icmyPackage
FText
p4196
(dp4197
g3
(lp4198
VA classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words
p4199
aVThe proposed application allows code switching and produces context-sensitive suggestions as writing progresses
p4200
aVInput (L1=French,L2=English u'\u005cu201c' I rentre à la maison because I am tired u'\u005cu201d' Desired output u'\u005cu201c' I return home because I am tired u'\u005cu201d'
p4201
aVThe main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models
p4202
aVPreparing the data to build training and test data for our intended translation assistance system is not trivial, as the type of interactive translation assistant we aim to develop does not exist yet
p4203
aVIt invokes GIZA++ [] to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm []
p4204
aVStep 4 is effectively a filter two thresholds can be configured to discard weak alignments, i.e., those with low probabilities, from the phrase-translation table so that only strong couplings make it into the generated set
p4205
aVThe parameter u'\u005cu039b' 1 adds a constraint based on the product of the two conditional probabilities ( P ( f t f s ) u'\u005cu22c5' P ( f s f t ) ) , and sets a threshold that has to be surpassed
p4206
aVA second parameter u'\u005cu039b' 2 further limits the considered phrase pairs ( f s , f t ) to have the product of their conditional probabilities not not deviate more than a fraction u'\u005cu039b' 2 from the joint probability for the strongest possible pairing for f s , the source fragment f s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2062' g u'\u005cu2062' e u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' _ u'\u005cu2062' t in Figure 1 corresponds to the best scoring translation for a given source fragment f s
p4207
aVThis metric thus effectively prunes weaker alternative translations in the phrase-translation table from being considered if there is a much stronger candidate
p4208
aVNevertheless, it has to be noted that even with u'\u005cu039b' 1 and u'\u005cu039b' 2 , the test set will include a certain amount of errors
p4209
aVThis is due to the nature of the unsupervised method with which the phrase-translation table is constructed
p4210
aVWhilst other thresholds may possibly produce cleaner sets, this is hard to evaluate as finding optimal values causes a prohibitive increase in complexity of the search space, and again this is not necessary to test our hypothesis
p4211
aVThis ensures complete independence of training and test data
p4212
aVThe fact that a phrase-translation table needs to be constructed for the test data is also the reason that the parallel corpus split from which the test data is derived has to be large enough, ensuring better quality
p4213
aVAn ideal test corpus would consist of L2 sentences with L1 fallback as crafted by L2 language learners with an L1 background
p4214
aVHowever, such corpora do not exist as yet
p4215
aVNumerous classifiers are trained and each is an expert in translating a single word or phrase
p4216
aVIn other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained
p4217
aVWords or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases
p4218
aVThe classifiers use the IB1 algorithm [] as implemented in TiMBL []
p4219
aV1 1 http://ilk.uvt.nl/timbl IB1 implements k -nearest neighbour classification
p4220
aVThe choice for this algorithm is motivated by the fact that it handles multiple classes with ease, but first and foremost because it has been successfully employed for word sense disambiguation in other studies [] , in particular in cross-lingual word sense disambiguation, a task closely resembling our current task []
p4221
aVWhen presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table
p4222
aVIf so, we are done quickly and need not rely on context information
p4223
aVIf not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector
p4224
aVThe experiments however showed that inclusion of such keywords did not make any noticeable impact on any of the results, so we restrict ourselves to mentioning this negative result
p4225
aVOur full system, including the scripts for data preparation, training, and evaluation, is implemented in Python and freely available as open-source from http://github.com/proycon/colibrita/
p4226
aVVersion tag v0.2.1 is representative for the version used in this research
p4227
aVThe language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model
p4228
aVNo additional external data was brought in, to keep the comparison fair
p4229
aVWe do so by normalising the class probability from the classifier ( s u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e T u'\u005cu2062' ( H ) ), which is our translation model, and the language model ( s u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e l u'\u005cu2062' m u'\u005cu2062' ( H ) ), in such a way that the highest classifier score for the alternatives under consideration is always 1.0 , and the highest language model score of the sentence is always 1.0
p4230
aVWe first measure absolute accuracy by simply counting all output fragments that exactly match the reference fragments, as a fraction of the total amount of fragments
p4231
aVThis measure may be too strict, so we add a more flexible word accuracy measure which takes into account partial matches at the word level
p4232
aVIf output o is a subset of reference r then a score of o r is assigned for that sentence pair
p4233
aVIf instead, r is a subset of o , then a score of r o will be assigned
p4234
aVThe word accuracy for the entire set is then computed by taking the sum of the word accuracies per sentence pair, divided by the total number of sentence pairs
p4235
aVThe baseline selects the most probable L1 fragment per L2 fragment according to the phrase-translation table
p4236
aVThis baseline, henceforth referred to as the u'\u005cu2019' most likely fragment u'\u005cu2019' baseline (MLF) is analogous to the u'\u005cu2019' most frequent sense u'\u005cu2019' -baseline common in evaluating WSD systems
p4237
aVA second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier
p4238
aVThis number is much larger than the 200 , 000 we mentioned before because single sentence pairs may be reused multiple times with different marked fragments
p4239
aVAmong the classifier experts are only words and phrases that are ambiguous and may thus map to multiple translations
p4240
aVThis implies that such words and phrases must have occurred at least twice in the corpus, though this threshold is made configurable and could have been set higher to limit the number of classifiers
p4241
aVThis shall be further discussed in Section 6.1
p4242
aVAs expected, the LM baseline substantially outperforms the context-insensitive MLF baseline
p4243
aVThird, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2
p4244
aVConclusions with regard to context width may have to be tempered somewhat, as the performance of the l1r1 configuration was found to not be significantly better than that of the l2r2 configuration
p4245
aVThis intuitively makes sense; a context of one may seem to be better than any other when uniformly applied to all classifier experts, but it may well be that certain classifiers benefit from different feature selections
p4246
aVWe therefore proceed with this line of investigation as well
p4247
aVAutomatic configuration selection was done by performing leave-one-out testing (for small number of instances) or 10-fold-cross validation (for larger number of instances, n u'\u005cu2265' 20 ) on the training data per classifier expert
p4248
aVPer classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2
p4249
aVThe auto configuration improves results over the uniformly applied feature selection
p4250
aVHowever, if we enable the language model as we do in the auto + LM configuration we do not notice an improvement over l1r1 + LM , surprisingly
p4251
aVA context size of one prevails in the vast majority of cases, which is not surprising considering the good results we have already seen with this configuration
p4252
aVIn earlier work , we reported a decrease in performance due to overfitting when this is done, so we do not expect it to make a positive impact
p4253
aVIn order to draw accurate conclusions, experiments on a single data set and language pair are not sufficient
p4254
aVWe therefore conducted a number of experiments with other language pairs, and present the abridged results in Table 6
p4255
aVWe see that the language model baseline for English u'\u005cu2192' French shows the same substantial improvement over the baseline as our English u'\u005cu2192' Spanish results
p4256
aVThe same holds for the Chinese u'\u005cu2192' English experiment
p4257
aVThe error rate metrics show improvement as well
p4258
aVWe therefore attach low importance to this deviation in BLEU here
p4259
aVIn all of the aforementioned experiments, the system produced a single solution for each of the fragments, the one it deemed best, or no solution at all if it could not find any
p4260
aVIn all of our experiments recall is high (well above 90 u'\u005cu2062' % ), mostly because train and test data lie in the same domain and have been generated in the same fashion, lower recall is expected with more real-world data
p4261
aVThere are more NLP components that might play a role if such a system were to find practical application
p4262
aVOur classification-based approach may be able to resolve some of these cases operating as an add-on to a regular MT system u'\u005cu2013' or as a independent post-correction system
p4263
aVOur system allows L1 fragments to be of arbitrary length
p4264
aVIf a fragment was not seen during training stage, and is therefore not covered by a classifier expert, then the system will be unable to translate it
p4265
aVNevertheless, if a longer L1 fragment can be decomposed into subfragments that are known, then some recombination of the translations of said sub-fragments may be a good translation for the whole
p4266
asg88
(lp4267
sg90
(lp4268
sg92
(lp4269
VIn this study we have shown the feasibility of a classifier-based translation assistance system in which L1 fragments are translated in an L2 context, in which the classifier experts are built individually per word or phrase.
p4270
aVWe have shown that such a translation assistance system scores both above a context-insensitive baseline, as well as an L2 language model baseline.
p4271
aVFurthermore, we found that combining this cross-language context-sensitive technique with an L2 language model boosts results further.
p4272
aVThe presence of a one-word right-hand side context proves crucial for good results, which has implications for practical translation assistance application that translate as soon as the user finishes an L1 fragment.
p4273
aVRevisiting the translation when right context becomes available would be advisable.
p4274
aVWe tested various configurations and conclude that small context sizes work better than larger ones.
p4275
aVAutomated configuration selection had positive results, yet the system with context size one and an L2 language model component often produces the best results.
p4276
aVIn static configurations, the failure of a wider context window to be more succesful may be attributed to the increased sparsity that comes from such an expansion.
p4277
aVThe idea of a comprehensive translation assistance system may extend beyond the translation of L1 fragments in an L2 context.
p4278
aVThere are more NLP components that might play a role if such a system were to find practical application.
p4279
aVWord completion or predictive editing (in combination with error correction) would for instance seem an indispensable part of such a system, and can be implemented alongside the technique proposed in this study.
p4280
aVA point of more practically-oriented future research is to see how feasible such combinations are and what techniques can be used.
p4281
aVAn application of our idea outside the area of translation assistance is post-correction of the output of some MT systems that, as a last-resort heuristic, copy source words or phrases into their output, producing precisely the kind of input our system is trained on.
p4282
aVOur classification-based approach may be able to resolve some of these cases operating as an add-on to a regular MT system or as a independent post-correction system.
p4283
aVOur system allows L1 fragments to be of arbitrary length.
p4284
aVIf a fragment was not seen during training stage, and is therefore not covered by a classifier expert, then the system will be unable to translate it.
p4285
aVNevertheless, if a longer L1 fragment can be decomposed into subfragments that are known, then some recombination of the translations of said sub-fragments may be a good translation for the whole.
p4286
aVWe are currently exploring this line of investigation, in which the gap with MT narrows further.
p4287
aVFinally, an important line of future research is the creation of a more representative test set.
p4288
aVLacking an interactive system that actually does what we emulate, we hypothesise that good approximations would be to use gap exercises, or cloze tests, that test specific aspects difficulties in language learning.
p4289
aVSimilarly, we may use L2 learner corpora with annotations of code-switching points or errors.
p4290
aVHere we then assume that places where L2 errors occur may be indicative of places where L2 learners are in some trouble, and might want to fall back to generating L1.
p4291
aVBy then manually translating gaps or such problematic fragments into L1 we hope to establish a more realistic test set.
p4292
ag106
asg107
S'P14-1082'
p4293
sg109
(lp4294
VIn this paper we present new research in translation assistance.
p4295
aVWe describe a system capable of translating native language (L1) fragments to foreign language (L2) fragments in an L2 context.
p4296
aVPractical applications of this research can be framed in the context of second language learning.
p4297
aVThe type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known.
p4298
aVThese code switches are subsequently translated to L2 given the L2 context.
p4299
aVWe study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense disambiguation baselines.
p4300
aVA classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words.
p4301
ag106
asba(icmyPackage
FText
p4302
(dp4303
g3
(lp4304
VWe show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database
p4305
aVSuccessful communication of meaning is measured by a successful interaction in this task, and feedback from this interaction is used for learning
p4306
aVFor example, in the context of a game, a description of a game rule is translated successfully if correct game moves can be performed based only on the translation
p4307
aVIn the context of a question-answering scenario, a question is translated successfully if the correct answer is returned based only on the translation of the query
p4308
aVHere, learning proceeds by u'\u005cu201c' trying out u'\u005cu201d' translation hypotheses, receiving a response from interacting in the task, and converting this response into a supervision signal for updating model parameters
p4309
aVIn case of positive feedback, the predicted translation can be treated as reference translation for a structured learning update
p4310
aVIn case of negative feedback, a structural update can be performed against translations that have been approved previously by positive task feedback
p4311
aVResponse-based learning can repeatedly try out system predictions by interacting in the extrinsic task
p4312
aVBuilding on prior work in grounded semantic parsing, we generate translations of queries, and receive feedback by executing semantic parses of translated queries against the database
p4313
aVWe show in an error analysis that this improvement can be attributed to using structural and lexical variants of reference translations as positive examples in response-based learning
p4314
aVFurthermore, translations produced by response-based learning are found to be grammatical
p4315
aVThis is due to the possibility to boost similarity to human reference translations by the additional use of a cost function in our approach
p4316
aVSince there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment
p4317
aVA recent important research direction in SMT has focused on employing automated translation as an aid to human translators
p4318
aVComputer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction [ Saluja et al.2012 ] , to human post-editing operations on a system prediction resulting in a reference translation [ Cesa-Bianchi et al.2008 ] , to human acceptance or overriding of sentence completion predictions [ Langlais et al.2000 , Barrachina et al.2008 , Koehn and Haddow2009 ]
p4319
aVSince retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT
p4320
aVOur work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task
p4321
aVHowever, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work
p4322
aVLastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the CLEF initiative
p4323
aV1 1 http://www.clef-initiative.eu While these approaches focus on improvements of the respective natural language processing task, our goal is to improve SMT by gathering feedback from the task
p4324
aVThe diagram in Figure 1 gives a sketch of response-based learning from semantic parsing in the geographical domain
p4325
aVGiven a manual German translation of the English query as source sentence, the SMT system produces an English target translation
p4326
aVFeedback is generated by executing the parse against the database of geographical facts
p4327
aVDespite a large difference to the original English string, key terms such as elevations and heights , or USA and US , can be mapped into the same predicate in the semantic parse, thus allowing to receive positive feedback from parse execution against the geographical database
p4328
aVRecent approaches to machine learning for SMT formalize the task of discriminating good from bad translations as a structured prediction problem
p4329
aVAssume a joint feature representation u'\u005cu03a6' u'\u005cu2062' ( x , y ) of input sentences x and output translations y u'\u005cu2208' Y u'\u005cu2062' ( x ) , and a linear scoring function s u'\u005cu2062' ( x , y ; w ) for predicting a translation y ^ (where u'\u005cu27e8' u'\u005cu22c5' , u'\u005cu22c5' u'\u005cu27e9' denotes the standard vector dot product) s.t
p4330
aVThe structured perceptron algorithm [ Collins2002 ] learns an optimal weight vector w by updating w on input x ( i ) by the following rule, in case the predicted translation y ^ is different from and scored higher than the reference translation y ( i )
p4331
aVFirstly, update rules that require to compute a feature representation for the reference translation are suboptimal in SMT, because often human-generated reference translations cannot be generated by the SMT system
p4332
aVComputation of distance to the reference translation usually involves cost functions based on sentence-level BLEU ( Nakov et al.2012 , inter alia ) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith2012
p4333
aVHere a meaning representation is u'\u005cu201c' tried out u'\u005cu201d' by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters
p4334
aVWe need to ensure that gold-standard translations lead to positive task-based feedback, that means they can be parsed and executed successfully against the database
p4335
aVIn addition, we can use translation-specific cost functions based on sentence-level BLEU in order to boost similarity of translations to human reference translations
p4336
aVWe denote feedback by a binary execution function e u'\u005cu2062' ( y ) u'\u005cu2208' { 1 , 0 } that tests whether executing the semantic parse for the prediction against the database receives the same answer as the parse for the gold standard reference
p4337
aVOur cost function c u'\u005cu2062' ( y ( i ) , y ) = ( 1 - BLEU u'\u005cu2062' ( y ( i ) , y ) ) is based on a version of sentence-level BLEU Nakov et al.2012
p4338
aVDefine y + as a surrogate gold-standard translation that receives positive feedback, has a high model score, and a low cost of predicting y instead of y ( i )
p4339
aVThe opposite of y + is the translation y - that leads to negative feedback, has a high model score, and a high cost
p4340
aVThe intuition behind this update rule is to discriminate the translation y + that leads to positive feedback and best approximates (or is identical to) the reference within the means of the model from a translation y - which is favored by the model but does not execute and has high cost
p4341
aVThis is done by putting all the weight on the former
p4342
aVUpon predicting translation y ^ , in case of positive feedback from the task, we treat the prediction as surrogate reference by setting y + u'\u005cu2190' y ^ , and by adding it to the set of reference translations for future use
p4343
aVThen we need to compute y - , and update by the difference in feature representations of y + and y - , at a learning rate u'\u005cu0397'
p4344
aVIf the feedback is negative, we want to move the weights away from the prediction, thus we treat it as y -
p4345
aVIf either y + or y - cannot be computed, the example is skipped
p4346
aVThe cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing [ Goldwasser and Roth2013 , Kwiatowski et al.2013 , Berant et al.2013 ]
p4347
aVLastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines [ Collobert and Bengio2004 , Chapelle2007 , Shalev-Shwartz et al.2007 ]
p4348
aVThis parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a language model trained on linearized logical forms
p4349
aVVariants of the response-based learning algorithm described above are implemented as a stand-alone tool that operates on cdec n -best lists of 10,000 translations of the Geoquery training data
p4350
aVIn addition to the model score s , it uses a cost function c based on sentence-level BLEU [ Nakov et al.2012 ] and tests translation hypotheses for task-based feedback using a binary execution function e
p4351
aVThis algorithm can convert predicted translations into references by task-feedback, and additionally use the given original English queries as references
p4352
aVMethod 2, named Exec , relies on task-execution by function e and searches for executable or non-executable translations with highest score s to distinguish positive from negative training examples
p4353
aVIt does not use a cost function and thus cannot make use of the original English queries
p4354
aVThis algorithm can be seen as a stochastic (sub)gradient descent variant of Rampion [ Gimpel and Smith2012 ]
p4355
aVIt does not make use of the semantic parser, but defines positive and negative examples based on score s and cost c with respect to human reference translations
p4356
aVFurthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the Geoquery database
p4357
aVPrecision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both
p4358
aVWe present an experimental comparison of the four different systems according to BLEU and F1, using an extended semantic parser (trained on 880 Geoquery examples) and the original parser (trained on 600 Geoquery training examples
p4359
aVA system ranking according to F1-score shows about 6 points difference between the respective methods, ranking Rebol over Rampion , Exec and cdec
p4360
aVRebol u'\u005cu2019' s combination of task feedback with a cost function achieves the best results since positively executable hypotheses and reference translations can both be exploited to guide the learning process
p4361
aVSince all English reference queries lead to positively executable parses in the setup that uses the extended semantic parser, Rampion implicitly also has access to task feedback
p4362
aVRebol performs worse since BLEU performance is optimized only implicitly in cases where original English queries function as positive examples
p4363
aVWe conjecture that this is due to a higher number of empty parses on the test set which makes this comparison unstable
p4364
aVThe examples show structural and lexical variation that leads to differences on the string level at equivalent positive feedback from the extrinsic task
p4365
aVLexical and structural variants of reference translations can be used to boost model parameters towards translations with positive feedback, while the same translations might be considered as negative examples in standard structured learning
p4366
aVTable 5 shows examples where translations from Rebol and Rampion differ from the gold standard reference, and predictions by Rebol lead to positive feedback, while predictions by Rampion lead to negative feedback
p4367
aVThis can be attributed to the use of sentence-level BLEU as cost function in Rampion and Rebol
p4368
aVTranslation errors of Rampion can be traced back to mistranslations of key terms ( city versus capital , limits or boundaries versus border
p4369
aVOur error analysis shows that response-based learning generates grammatical translations which is due to the additional use of a cost function that boosts similarity of translations to human reference translations
p4370
asg88
(lp4371
sg90
(lp4372
sg92
(lp4373
VWe presented a proposal for a new learning and evaluation framework for SMT.
p4374
aVThe central idea is to ground meaning transfer in successful interaction in an extrinsic task, and use task-based feedback for structured learning.
p4375
aVWe presented a proof-of-concept experiment that defines the extrinsic task as executing semantic parses of translated queries against the Geoquery database.
p4376
aVOur experiments show an improvement of about 6 points in F1-score for response-based learning over structured learning from reference translations.
p4377
aVOur error analysis shows that response-based learning generates grammatical translations which is due to the additional use of a cost function that boosts similarity of translations to human reference translations.
p4378
aVIn future work, we would like to extend our work on embedding SMT in virtual gameplay to larger and more diverse datasets, and involve human feedback in the response-based learning loop.
p4379
ag106
asg107
S'P14-1083'
p4380
sg109
(lp4381
VWe propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input.
p4382
aVWe show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database.
p4383
aVExperiments on the Geoquery database show an improvement of about 6 points in F1-score for response-based learning over learning from references only on returning the correct answer from a semantic parse of a translated query.
p4384
aVIn general, our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT.
p4385
ag106
asba(icmyPackage
FText
p4386
(dp4387
g3
(lp4388
VA user can navigate within the hierarchical summary by clicking on an element of a parent summary to view the associated child summary
p4389
aVFor example, given the topic, u'\u005cu201c' 1998 embassy bombings, u'\u005cu201d' the first summary (Figure 1 ) might mention that the US retaliated by striking Afghanistan and Sudan
p4390
aVWe then describe our methodology to implement the Summa hierarchical summarization system hierarchical clustering in Section 3 and creating summaries based on that clustering in Section 4
p4391
aVWe discuss our experiments in Section 5 , related work in Section 6 , and conclusions in Section 7
p4392
aVFirst, the information presented at the start is small and grows only as the user directs it, so as not to overwhelm the user
p4393
aVSecond, each user directs his or her own experience, so a user interested in one aspect need only explore that section of the data without having to view or understand the entire summary
p4394
aVHaving defined the task, we now describe the methodology behind our implementation, Summa
p4395
aVIn this first implementation, we have opted for temporal organization, since this is generally the most appropriate for news events
p4396
aVThe problem of hierarchical summarization as described in Section 2 has all of the requirements of MDS, and additional complexities of inducing a hierarchical structure, processing an order of magnitude bigger input, generating a much larger output, and enforcing coherence between parent and child summaries
p4397
aVWe simplify the problem by decomposing it into two steps hierarchical clustering and summarizing over the clustering (see Figure 2 for an example
p4398
aVA hierarchical clustering is a tree in which if a cluster g p is the parent of cluster g c , then each sentence in g c is also in g p
p4399
aVThe hierarchical clustering serves as input to the second step u'\u005cu2013' summarizing given the hierarchy
p4400
aVThe hierarchical summary follows the hierarchical structure of the clustering
p4401
aVMoreover, the number of sentences in a flat summary is exactly equal to the number of child clusters of the node, since the user will click a sentence to get to the child summary
p4402
aVBecause we are interested in temporal hierarchical summarization, we hierarchically cluster all the sentences in the input documents by time
p4403
aVUnfortunately, neither agglomerative nor divisive clustering is suitable, since both assume a binary split at each node [ 2 ]
p4404
aVWe use SUTime [ 6 ] to normalize temporal references, and we parse the sentences with the Stanford parser [ 13 ] and use a set of simple heuristics to determine if the timestamps in the sentence refer to the root verb
p4405
aVIf no timestamp is given, we use the article date
p4406
aVSince we wish to partition along the temporal dimension, our problem reduces to identifying the best dates at which to split a cluster into subclusters
p4407
aVWe identify these dates by looking for bursts of activity
p4408
aVHowever, when there is little differentiation in news coverage, we prefer clusters evenly spaced across time
p4409
aVWe thus choose clusters C = { c 1 , u'\u005cu2026' , c k } as follows
p4410
aVAt each level, we cluster the sentences by the method described above and choose the number of clusters k according to the gap statistic [ 30 ]
p4411
aVA hierarchical summary that is only salient and nonredundant may still not be suitable if the sentences within a cluster summary are disconnected or if the parent sentence for a summary does not relate to the child summary
p4412
aVThus, a hierarchical summary must also have intra-cluster coherence and parent-to-child coherence
p4413
aVSalience is the value of each sentence to the topic from which the documents are drawn
p4414
aVWe measure salience of a summary ( S u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' ( X ) ) as the sum of the saliences of individual sentences ( u'\u005cu2211' i S u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' ( x i )
p4415
aVFor example, the reaction sentence, u'\u005cu201c' President Clinton vowed to track down the perpetrators behind the bombs that exploded outside the embassies in Tanzania and Kenya on Friday, u'\u005cu201d' would have a higher score than the action sentence, u'\u005cu201c' Bombs exploded outside the embassies in Tanzania and Kenya on Friday u'\u005cu201d' This problem occurs because the first sentence has a higher ROUGE score (it covers more important words than the second sentence
p4416
aVWe rely on the approximate discourse graph (ADG) that was proposed in [ 8 ] as the basis for measuring coherence
p4417
aVEach node in the ADG is a sentence from the dataset
p4418
aVA negative edge indicates an unfulfilled discourse cue or co-reference mention
p4419
aVUsers navigate the hierarchical summary from parent sentence to child summary, so if the parent sentence bears no relation to the child summary, the user will be understandably confused
p4420
aVIn hierarchical summarization, however, a cluster summary may span hundreds of documents and a wide range of information
p4421
aVFor this reason, we may consider a summary acceptable even if it has limited positive evidence of coherence in the ADG, as long as there is no negative evidence in the form of negative edges
p4422
aVHaving estimated salience, redundancy, and two forms of coherence, we can now put this information together into a single objective function that measures the quality of a candidate hierarchical summary
p4423
aVWe treat redundancy and budget as hard constraints and coherence and salience as soft constraints
p4424
aVLastly, we require that sentences are drawn from the cluster that they represent and that the number of sentences in the summary corresponding to each non-leaf cluster c is equivalent to the number of child clusters of c
p4425
aVThe tradeoff parameters u'\u005cu0392' and u'\u005cu0393' were set based on a development set
p4426
aVOptimizing this objective function is NP-hard, so we approximate a solution by using beam search over the space of partial hierarchical summaries
p4427
aVNotice the contribution from a sentence depends on individual salience, coherence ( C u'\u005cu2062' C u'\u005cu2062' o u'\u005cu2062' h ) based on sentences visible on the user path down the hierarchy to this sentence, and coherence ( P u'\u005cu2062' C u'\u005cu2062' o u'\u005cu2062' h ) based on its parent sentence and its child summary
p4428
aVSince most of the sentence contributions depend on the path from the root to the sentence, we build our partial summary by incrementally adding a sentence top-down in the hierarchy and from first sentence to last within a cluster summary
p4429
aVHowever, we do not fix the child summary at this time u'\u005cu2013' we simply use it to estimate P u'\u005cu2062' C u'\u005cu2062' o u'\u005cu2062' h when using that sentence
p4430
aVSince computing the best child summary is also intractable we approximate a solution by a local search algorithm over the child cluster
p4431
aVAre hierarchical summaries more effective than other methods for learning about complex events
p4432
aVWe selected topics which were between five and fifteen years old so that evaluators would have relatively less pre-existing knowledge about the topic
p4433
aVEvaluating how much a user learned is inherently difficult, more so when the goal is to allow the user the freedom to explore information based on individual interest
p4434
aVFor this reason, instead of asking a set of predefined questions, we assess the knowledge gain by following the methodology of [ 26 ] u'\u005cu2013' asking users to write a paragraph summarizing the information learned
p4435
aVUsing the same setup as in the previous experiment, for each topic, five AMT workers spent three minutes reading through a timeline or summary and were then asked to write a description of what they had learned
p4436
aVStandard checks such as approval rating, location filtering, etc were used for removing spam
p4437
aVThe results of this experiment are as follows
p4438
aVIn latter cases, the hierarchical summaries provided little advantage over the timelines because it was more difficult to arrange the sentences hierarchically
p4439
aVSince Summa was judged to be so much superior to flat MDS systems in Section 5.1 , it is surprising that users descriptions from flat MDS were preferred nearly as often as those from Summa
p4440
aVWhile the flat summaries were disjointed, they were good at including salient information, with the most salient tending to be near the start of the summary
p4441
aVThus, descriptions from both Summa and flat MDS generally covered the most salient information
p4442
aVIn this experiment, we assess the salience of the information captured by the different systems, and the ability of Summa to organize the information so that more important information is placed at higher levels
p4443
aVWe first automatically assessed informativeness by calculating the ROUGE-1 scores of the output of each of the systems
p4444
aV5 5 We excluded one topic (the handover of the Lockerbie bombing suspects) because the corresponding Wikipedia article had insufficient information
p4445
aVNote that there is no good translation of ROUGE for hierarchical summarization
p4446
aVThus, we simply use the traditional ROUGE metric, which will not capture any of the hierarchical format
p4447
aVThis score will essentially serve as a rough measure of coverage of the entire summary to the Wikipedia article
p4448
aVThe scores for each of the systems are as follows
p4449
aVWhile ROUGE serves as a rough measure of coverage, we were interested in gathering more fine-grained information on the informativeness of each system
p4450
aVBecause reading 300 articles per topic is impractical, we asked AMT workers to read a Wikipedia article on the same topic and then identify the three most important events and the five most important secondary events
p4451
aVWhile recognizing primary events is relatively simple because they are repeated frequently, identification of important secondary events often requires external knowledge
p4452
aVWhile the facts of the sentences made sense together, the summaries sometimes did not read as if they were written by a human, but as a series of disparate sentences
p4453
aVFor example, suppose the parent sentence is, u'\u005cu201c' A Swissair plane Wednesday night crashed off Nova Scotia, Canada u'\u005cu201d' A very good child sentence is, u'\u005cu201c' The airline confirmed that all passengers died u'\u005cu201d' However, based on their surface features, the sentence, u'\u005cu201c' A plane made an unscheduled landing after a Swissair plane crashed off the coast of Canada, u'\u005cu201d' appears to be a better choice
p4454
aVWhile timelines can be useful for understanding events, they do not generalize to other domains
p4455
aV[ 9 ] proposed a probabilistic technique for extracting a diverse set of threads from a given collection
p4456
aVOther research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure [ 22 , 5 ] , or gains coverage by drawing sentences from different parts of the hierarchy [ 34 , 31 ]
p4457
aVWhen compared to timelines, users learned more with Summa in twice as many cases, and Summa was preferred more than three times as often
p4458
aVWe thank Hui Lin and Jeff Bilmes for providing us with their code
p4459
asg88
(lp4460
sg90
(lp4461
sg92
(lp4462
VWe have introduced a new paradigm for large-scale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries.
p4463
aVWe present Summa , an implemented hierarchical news summarization system, 7 7 http://knowitall.cs.washington.edu/summa/ and demonstrate its effectiveness in a user study that compares Summa with a timeline system and a flat MDS system.
p4464
aVWhen compared to timelines, users learned more with Summa in twice as many cases, and Summa was preferred more than three times as often.
p4465
aVWhen compared to flat summaries, users overwhelming preferred Summa and learned just as much.
p4466
aVThis first implementation performs temporal clustering in future work, we will investigate dynamically selecting an organizing principle that is best suited to the data at each level of the hierarchy by entity, by location, by event, or by date.
p4467
aVWe also intend to scale the system to even larger document collections, and explore joint clustering and summarization.
p4468
aVLastly, we plan to research hierarchical summarization in other domains.
p4469
ag106
asg107
S'P14-1085'
p4470
sg109
(lp4471
VMulti-document summarization (MDS) systems have been designed for short, unstructured summaries of 10-15 documents, and are inadequate for larger document collections.
p4472
aVWe propose a new approach to scaling up summarization called hierarchical summarization , and present the first implemented system, Summa.
p4473
aVSumma produces a hierarchy of relatively short summaries, in which the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest.
p4474
aVSumma optimizes for coherence as well as coverage of salient information.
p4475
aVIn an Amazon Mechanical Turk evaluation, users prefered Summa ten times as often as flat MDS and three times as often as timelines.
p4476
ag106
asba(icmyPackage
FText
p4477
(dp4478
g3
(lp4479
VWe derive three features from these timelines, and show that their use in supervised summarization lead to a significant 4.1% improvement in ROUGE performance over a state-of-the-art baseline
p4480
aVBesides the increasing availability of annotation standards (e.g.,, TimeML [ 21 ] ) and corpora (e.g.,, TIDES [ 9 ] , TimeBank [ 22 ] ), the community has also organized three successful evaluation workshops u'\u005cu2014' TempEval-1 [ 25 ] , -2 [ 26 ] , and -3 [ 24 ]
p4481
aVAs the state-of-the-art improves, these workshops have moved away from the piecemeal evaluation of individual temporal processing tasks and towards the evaluation of complete end-to-end systems in TempEval-3
p4482
aVWe hope to improve the quality of the summaries that are generated by considering temporal information found in the input text
p4483
aVRecognizing that sentence (3) is about a storm that had happened in the past is important when writing a summary about the recent storm, as it is not relevant and can likely be excluded
p4484
aV1) A fierce cyclone packing extreme winds and torrential rain smashed into Bangladesh u'\u005cu2019' s southwestern coast Thursday, wiping out homes and trees in what officials described as the worst storm in years
p4485
aV2) More than 100,000 coastal villagers have been evacuated before the cyclone made landfall
p4486
aVThere are fewer events which talk about the previous storm
p4487
aVThus, temporal information does assist in identifying which sentences are more relevant to the final summary
p4488
aVOur work is significant as it addresses an important gap in the exploitation of temporal information
p4489
aVIn this work we construct timelines (as a representation of temporal information) automatically and incorporate them into a state-of-the-art multi-document summarization system
p4490
aVThis is achieved with 1) three novel features derived from timelines to help measure the saliency of sentences, as well as 2) TimeMMR , a modification to the traditional Maximal Marginal Relevance (MMR) [ 3 ]
p4491
aVThis work additionally proposes the use of the lengths of timelines as a metric to gauge the usefulness of timelines
p4492
aVTogether with the earlier described contributions, this metric further improves summarization, yielding an overall 5.9% performance increase
p4493
aVAfter laying out these events onto a timeline by making use of these timestamps, the number of events that happen within the same day is used to influence sentence scoring
p4494
aVIn sentence re-ordering, final summaries are re-arranged so that the extracted sentences that form the summary are in a chronological order
p4495
aVDepending on the style of writing or journalistic guidelines, a summary can arguably be written in a number of ways
p4496
aVThe use of recency as an indicator of saliency is useful, yet disregards other accessible temporal information
p4497
aVIf a summary of a whole sequence of events is desired, recency becomes less useful
p4498
aVIn this work, we have simplified this idea by dropping the need for event co-referencing (removing a source of propagated error), and augmented it with two additional features derived from timelines
p4499
aVBy doing so, we are able to make better use of the available temporal information, taking into account all known events and the time in which they occur
p4500
aVThey indicate the temporal relationships between two basic temporal units
p4501
aVSWING is a supervised, extractive summarization system which ranks sentences based on scores computed using a set of features in the Sentence Scoring phase
p4502
aVThe timelines built in the earlier temporal processing can be incorporated into this pipeline by deriving a set of features used to score sentences in Sentence Scoring , and as input to the MMR algorithm when computing similarity in Sentence Re-ordering
p4503
aVGeneralizing, we refer to the time period an event takes place in as its time span (vertical dotted lines
p4504
aVAs a simplifying assumption, events are laid out on the timeline based on the starting time of their time span
p4505
aVReferring to Figure 1 , whose timeline is shown in Figure 2 , we see that the time span with the most number of events is when the latest cyclone made landfall
p4506
aVAssigning higher scores for sentences which contain events in this time span will help us to select more relevant sentences if we want a summary about the cyclone
p4507
aVThe importance of a time span T u'\u005cu2062' S i is computed by normalizing the number of events in T u'\u005cu2062' S i against the number of events in T u'\u005cu2062' S L
p4508
aVThen the temporal coverage of a sentence is defined as the number of time spans between the earliest time span T u'\u005cu2062' S a and the latest time span T u'\u005cu2062' S c
p4509
aVThe constraint on the number of sentences that can be included in a summary requires us to select compact sentences which contain as many relevant facts as possible
p4510
aVTraditional lexical measures may attempt to achieve this by computing the ratio of keyphrases to the number of words in a sentence [ 11 ]
p4511
aVStated equivalently, when two sentences are of the same length, if one contains more keyphrases, it should contain more useful facts
p4512
aVTCD parallels this idea with the use of temporal information, i.e., if two sentences are of the same temporal coverage, then the one with more events should carry more useful facts
p4513
aVFormally, if a sentence s contains events u'\u005cud835' u'\u005cudd3c' s = { e 1 , u'\u005cu2026' , e n } , where each event is associated with a time span T u'\u005cu2062' S i , then T u'\u005cu2062' C u'\u005cu2062' D is computed using
p4514
aV1) An official in Barisal, 120 kilometres south of Dhaka, spoke of severe destruction as the 500 kilometre-wide mass of cloud passed overhead
p4515
aVIn each iteration, s is penalized if it is lexically similar to other sentences that have already been selected to form the eventual summary S = { s 1 , s 2 , u'\u005cu2026' }
p4516
aVThe motivating idea is to reduce repeated information by preferring sentences which bring in new facts
p4517
aVA summary about the hurricane need not contain all of these sentences as they are all describing the same thing
p4518
aVHowever it is not trivial for the lexically-motivated MMR algorithm to detect that events like u'\u005cu201c' passed u'\u005cu201d' , u'\u005cu201c' uprooted u'\u005cu201d' or u'\u005cu201c' damaged u'\u005cu201d' are in fact repetitive
p4519
aVThus, we propose further penalizing the score of s if it contains events that happen in similar time spans as those contained in sentences within S
p4520
aVwhere S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' ( s ) is the score of s , S is the set of sentences already selected to be in the summary from previous iterations, and R u'\u005cu2062' 2 is the predicted ROUGE-2 score of s with respect to the already selected sentences ( S u'\u005cu0393' is a weighting parameter which is empirically set to 0.9 after tuning over a development dataset u'\u005cud835' u'\u005cudcaf' is the proportion of events in s which happen in the same time span as another event in any other sentence in S
p4521
aVTwo events are said to be in the same time span if one happens within the time period the other happens in
p4522
aVWhile TimeMMR is proposed here as an improvement over MMR, the premise is that incorporating temporal information can be helpful to minimize redundancy in summaries
p4523
aVWe chose to use MMR here as a proof-of-concept to demonstrate the viability of such a technique, and to easily integrate our work into SWING
p4524
aVTemporal processing is imperfect
p4525
aVThis can be done by computing a metric which can be used to decide whether or not timelines should be used for a particular input document collection
p4526
aVWe postulate that the length of a timeline can serve as a simple reliability filtering metric
p4527
aVThe intuition for this is that for longer timelines (which contain more events), possible errors are spread over the entire timeline, and do not overpower any useful signal that can be obtained from the timeline features outlined earlier
p4528
aVSince the focus of this paper is on multi-document summarization, we employ only the three generic features, i.e.,, 1) sentence position, 2) sentence length, and 3) interpolated n-gram document frequency in our experiments below
p4529
aVSummarization evaluation is done using ROUGE-2 (R-2) [ 13 ] , as it has previously been shown to correlate well with human assessment [ 14 ] and is often used to evaluate automatic text summarization
p4530
aVWe also show the results of two reference systems, CLASSY [ 5 ] and POLYCOM [ 30 ] , as benchmarks
p4531
aVCLASSY and POLYCOM are top performing systems at TAC-2011 (ranked 2nd and 3rd by R-2 in TAC 2011, respectively; the full version of SWING was ranked 1st with a R-2 score of 0.1380
p4532
aVWe assume that the various input document sets to be summarized are available at the time of processing
p4533
aVHence in these experiments, the threshold for filtering is set to be the average of all the timeline sizes over the whole input dataset
p4534
aVThe same drop occurs even when reliability filtering is used (Rows 9 to 12
p4535
aVThese indicate that all the proposed features are important and need to be used together to be effective
p4536
aVLooking at Rows 1 to 8, and Rows 9 to 16, we see the importance of reliability filtering
p4537
aVTo help visualize what the differences in these ROUGE scores mean, Figure 7 shows two summaries 1 1 The produced summaries are truncated to fit within a 100-word limit imposed by the TAC-2011 guidelines generated for document set D1117C of the TAC-2011 dataset
p4538
aVu'\u005cu211d' 2) A top Army general vowed to personally oversee the upgrading of Walter Reed Army Medical Center u'\u005cu2019' s Building 18, a dilapidated former hotel that houses wounded soldiers as outpatients
p4539
aV[2]
p4540
aVFor the analysis on timeline features, we only present an analysis for TSI and CTSI due to space constraints
p4541
aVIt is easy to see why ( u'\u005cud835' u'\u005cudd43' 1) scores higher for R-2 u'\u005cu2014' it describes the cause of the accident just as it occurred u'\u005cu211d' 1) however talks about events which happened before the accident itself (e.g.,, how much of the tower had already been erected
p4542
aVIn this case time span importance is able to correctly guide summary generation by favoring time spans containing events related to the actual toppling
p4543
aVHowever we believe it is because the ROUGE measures that are used for evaluation are not suited for this purpose
p4544
aVRecall that TimeMMR seeks to eliminate redundancy based on time span similarities and not lexical likeness
p4545
aVTimeMMR penalizes ( u'\u005cu211d' 3 u'\u005cu211d' 3) reports that the shoe-throwing incident happened as the U.S
p4546
aVPresident Bush appeared together with the Iraqi Prime Minister Nouri al-Maliki
p4547
aVSince ( u'\u005cu211d' 1) and ( u'\u005cu211d' 3) talk about the same time span, TimeMMR down-weights ( u'\u005cu211d' 3
p4548
aVu'\u005cu211d' 3) The incident occurred as Bush was appearing with Iraqi Prime Minister Nouri al-Maliki
p4549
aVT =0 means that timelines are used for all input document sets, whereas T =100 means that no timelines are used, as the length of the longest timeline is less than 100
p4550
aVAs the threshold increases from 0 to 40 u'\u005cu2013' 50, summarization performance improves while the number of document sets where temporal information is used is reduced
p4551
aVThis affirms our use of the average length of timelines as the threshold value in our earlier experiments
p4552
aVBeyond 60, the R-2 scores are still higher than that obtained by SWING , but no longer significantly different
p4553
aVAt these higher thresholds, temporal information is still able to help get an improvement in R-2
p4554
aVHowever as this affects only very few out of the 44 document sets, statistical variances mean that these R-2 scores are no longer significant from that produced by SWING
p4555
aVTo overcome errors propagated from the underlying temporal processing systems, we proposed a reliability filtering metric which can be used to help decide when temporal information should be used for summarization
p4556
aVThe use of this metric leads to an overall 5.9% gain in R-2 over the competitive SWING baseline
p4557
aVThis can help us better understand their value in improving content selection
p4558
aVAs noted earlier, it will be also be useful to repeat our experiments with less lexicon-influenced measures like the Pyramid method [ 20 ]
p4559
aVManual assessment of the generated summaries can also be done to give a better picture of the quality of the summaries generated with the use of timelines.Â  Finally, given the importance of reliability filtering, a natural question is if there are other metrics that can be used to get better results
p4560
asg88
(lp4561
sg90
(lp4562
sg92
(lp4563
VWe have shown in this work how temporal information in the form of timelines can be incorporated into multi-document summarization.
p4564
aVWe achieve this through two means, using.
p4565
aV1) three novel features derived from timelines to measure the saliency of sentences, and 2) TimeMMR which considers time span similarity to enhance the traditional MMR s lexical diversity measure.
p4566
aVTo overcome errors propagated from the underlying temporal processing systems, we proposed a reliability filtering metric which can be used to help decide when temporal information should be used for summarization.
p4567
aVThe use of this metric leads to an overall 5.9% gain in R-2 over the competitive SWING baseline.
p4568
aVIn future work, we are keen to study our proposed timeline-related features more intrinsically in the context of human-generated summaries.
p4569
aVThis can help us better understand their value in improving content selection.
p4570
aVAs noted earlier, it will be also be useful to repeat our experiments with less lexicon-influenced measures like the Pyramid method [ 20 ].
p4571
aVManual assessment of the generated summaries can also be done to give a better picture of the quality of the summaries generated with the use of timelines.Â  Finally, given the importance of reliability filtering, a natural question is if there are other metrics that can be used to get better results.
p4572
ag106
asg107
S'P14-1087'
p4573
sg109
(lp4574
VWe study the use of temporal information in the form of timelines to enhance multi-document summarization.
p4575
aVWe employ a fully automated temporal processing system to generate a timeline for each input document.
p4576
aVWe derive three features from these timelines, and show that their use in supervised summarization lead to a significant 4.1% improvement in ROUGE performance over a state-of-the-art baseline.
p4577
aVIn addition, we propose TimeMMR , a modification to Maximal Marginal Relevance that promotes temporal diversity by way of computing time span similarity, and show its utility in summarizing certain document sets.
p4578
aVWe also propose a filtering metric to discard noisy timelines generated by our automatic processes, to purify the timeline input for summarization.
p4579
aVBy selectively using timelines guided by filtering, overall summarization performance is increased by a significant 5.9%.
p4580
ag106
asba(icmyPackage
FText
p4581
(dp4582
g3
(lp4583
VIn this work we present a chance-corrected metric based on Krippendorff u'\u005cu2019' s u'\u005cu0391' , adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications
p4584
aVHowever, no such measure is in widespread use for the task of syntactic annotation
p4585
aVThis is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation
p4586
aVFor this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures
p4587
aVAs shown in \u005cciteN Art:Poe08, such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent
p4588
aVIn this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff u'\u005cu2019' s u'\u005cu0391' and tree edit distance
p4589
aVNext, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work
p4590
aVThe definitive reference for agreement measures in computational linguistics is \u005cciteN Art:Poe08, who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures
p4591
aVInstead, the grammar parses the input sentences, and the annotator selects the correct parse (or rejects all the candidates) based on discriminants 2 2 A discriminant is an attribute of the analyses produced by the grammar where some of the analyses differ, e.g., is the word jump a noun or a verb, or does a PP attach to a VP or the VP u'\u005cu2019' s object NP of the parse forest
p4592
aVThis is different from our approach in that agreement is computed on annotator decisions rather than on the treebanked analyses, and is only applicable to grammar-based approaches such as HPSG and LFG treebanking
p4593
aVThe idea of using edit distance as the basis for an inter-annotator agreement metric has previously been explored by \u005cciteN Fournier13
p4594
aVIf not all coders annotate all items, the different X i will be of different sizes
p4595
aVThese metrics express agreement on a nominal coding task as the ratio u'\u005cu039a' , u'\u005cu03a0' = A o - A e / 1 - A e where A o is the observed agreement and A e the expected agreement according to some model of u'\u005cu201c' random u'\u005cu201d' annotation
p4596
aVdiffering only in how they estimate the probabilities u'\u005cu039a' assigns separate probability distributions to each coder based on their observed behaviour, while u'\u005cu03a0' uses the same distribution for both coders based on their aggregate behaviour
p4597
aVIn the case of dependency-based syntax we could conceivably use a variant of these metrics by considering the ID of a token u'\u005cu2019' s head as a categorical variable (the approach taken in [] ), but we argue that this is not satisfactory
p4598
aVThis use of the metrics would consider agreement on categories such as u'\u005cu201c' tokens whose head is token number 24 u'\u005cu201d' , which is obviously not a linguistically informative category
p4599
aVThus we have to reject this way of assessing the reliability of dependency syntax annotation
p4600
aVAs shown by the existence of three different metrics ( u'\u005cu039a' , u'\u005cu03a0' and S [] ) for the relatively simple task of nominal coding, the choice of model for P ( t c ) will not be obvious, and thus differing choices of generative model as well as different choices for parameters such as smoothing will result in subtly different agreement metrics
p4601
aVInstead, we propose to use an agreement measure based on Krippendorff u'\u005cu2019' s u'\u005cu0391' [] and tree edit distance
p4602
aVNote that in the expression for D e , we are computing the difference between annotations for different items; thus, our distance function for syntactic trees needs to be able to compute the difference between arbitrary trees for completely unrelated sentences
p4603
aVThe function u'\u005cu0394' can be any function as long as it is a metric; that is, it must be (1) non-negative, (2) symmetric, (3) zero only for identical inputs, and (4) it must obey the triangle inequality
p4604
aVThis immediately excludes metrics like ParsEval [] and Leaf-Ancestor [] , since they assume that the trees being compared are parses of the same sentence
p4605
aVTree edit distance has previously been used in the TedEval software [] for parser evaluation agnostic to both annotation scheme and theoretical framework, but this by itself is still an uncorrected accuracy measure and thus unsuitable for our purposes
p4606
aV3 3 While it is quite different from other parser evaluation schemes, TedEval does not correct for chance agreement and is thus an uncorrected metric
p4607
aVIt could of course form the basis for a corrected metric, given a suitable measure of expected agreement
p4608
aVWhen comparing syntactic trees, we only want to compare dependency relations or non-terminal categories
p4609
aVTherefore we remove the leaf nodes in the case of phrase structure trees, and in the case of dependency trees we compare trees whose edges are unlabelled and nodes are labelled with the dependency relation between that word and its head; the root node receives the label u'\u005cu0395'
p4610
aVThe different functions have different properties, and different advantages and drawbacks, and the nature of their strengths and weaknesses differ
p4611
aVWe will therefore perform a number of synthetic experiments to investigate their properties in a controlled environment, before applying them to real-world data
p4612
aVWe could at this point apply our metrics to various real corpora and compare the results, but since the consistency of the corpora is unknown, it u'\u005cu2019' s impossible to say whether the best metric is the one resulting in the highest scores, the lowest scores or somewhere in the middle
p4613
aVThe general approach we take is based on that used by \u005cciteN Mathet:etal12, adapted to dependency trees
p4614
aVThis way tokens close to the root have a fair chance of having candidate heads if they are selected
p4615
aVA pre-order traversal would result in tokens close to the root having few options, and in particular if the root has a single child, that node has no possible new heads unless one of its children has been assigned the root as its new head first
p4616
aVFor example in the trees in figure 2 , assigning any other head than the root to the Pred nodes directly dominated by the root will result in invalid (cyclic and unconnected) dependency trees
p4617
aVInitial exploration of the data showed that the mean follows the median very closely regardless of metric and perturbation level, and therefore we only report the mean scores across runs in this paper
p4618
aVMean LAS at p r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' a u'\u005cu2062' c u'\u005cu2062' h = 1 of Figure 5 is 23.9%, clearly much higher than we would expect if the trees were completely random
p4619
aVThe u'\u005cu0394' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f function causes an extreme shift of the distances towards 0; more than 30% of the sentence pairs have distance 0, 1, or 2, which causes D e d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f to be extremely low and thus gives disproportionally large weight to non-zero distances in D o d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f
p4620
aVOn the other hand u'\u005cu0394' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m causes a rightward shift of the distances, which results in a high D e n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m and thus individual disagreements having less weight
p4621
aVSynthetic experiments do not always fully reflect real-world behaviour, however
p4622
aVTherefore we will also evaluate our metrics on real-world inter-annotator agreement data sets
p4623
aVThe data studied in this work has previously been used by \u005cciteN Skjaerholt13 to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures
p4624
aVA distinguishing feature of the tectogrammatical analyses, vis a vis the other treebanks we are using, is that semantically empty words only take part in the analytical annotation layer and nodes are inserted at the tectogrammatical layer to represent covert elements of the sentence not present in the surface syntax of the analytical layer
p4625
aVThus, inserting and deleting nodes is a central part of the task of tectogrammatical annotation, unlike the more surface-oriented annotation of our other treebanks, where the tokenisation is fixed before the text is annotated
p4626
aVThe Star-Sem Data is a portion of the dataset released for the *SEM 2012 shared task [] , parsed using the LinGO English Resource Grammar (ERG, [] ) and the resulting parse forest disambiguated based on discriminants
p4627
aVThe ERG is an HPSG-based grammar, and as such its analyses are attribute-value matrices (AVMs); an AVM is not a tree but a directed acyclic graph however, and for this reason we compute agreement not on the AVM but the so-called derivation tree
p4628
aVA u'\u005cu222a' B and we use the Jaccard similarity of the sets of labelled bracketings of two trees as our uncorrected measure
p4629
aVTo compute the similarity for a complete set of annotations we use the mean pairwise Jaccard similarity weighted by sentence length; that is, the same procedure as in 3 , but using Jaccard similarity rather than LAS
p4630
aVSince LAS assumes that both of the sentences compared have identical sets of tokens, we had to exclude a number of sentences from the LAS computation in the cases of the English and Italian CDT corpora, and especially the PCEDT
p4631
aVThe large number of sentences excluded in the PCEDT is due to the fact that in the tectogrammatical analysis of the PCEDT, inserting and deleting nodes is an important part of the annotation task
p4632
aVLooking at the results in Table LABEL:tbl:alpha-real , we observe two things
p4633
aVLAS order the corpora NDT 3, 2, 1, CDT da, en, it, es, PCEDT, whereas u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f and u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m gives the order NDT 2, 1, 3, PCEDT, CDT da, en, it, es, and u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n gives the same order as the other alphas but with CDT es and it changing places
p4634
aVFurthermore, as the scatterplot in Figure 6 shows, there is a clear correlation between the u'\u005cu0391' metrics and LAS, if we disregard the PCEDT results
p4635
aVThe reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS
p4636
aVThe corpus that scores the highest for all three metrics is the SSD corpus; the reason for this is uncertain, as our corpora differ along many dimensions, but the fact that the annotation was done by professional linguists who are very familiar with the grammar used to parse the data is likely a contributing factor
p4637
aVThe difference between the u'\u005cu0391' metrics and the Jaccard similarity is larger than the difference between u'\u005cu0391' and LAS for our dependency corpora, however the two similarity metrics are not comparable, and it is well known that for phrase structures single disagreements such as a PP-attachment disagreement can result in multiple disagreeing bracketings
p4638
aVFirst of all, we disqualify the LAS metric, primarily due to the methodological inadequacies of using an uncorrected measure
p4639
aVWhile our experiments did not reveal any serious shortcomings (unlike those of [] who in the case of categorisation showed that for large p the uncorrected measure can be increasing ), the methodological problems of uncorrected metrics makes us wary of LAS as an agreement metric
p4640
aVNext, of the three u'\u005cu0391' metrics, u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n is clearly the best; u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f is extremely sensitive to even moderate amounts of disagreement, while u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m is overly lenient
p4641
aVLooking solely at Figure 3 , one might be led to believe that LAS and u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n are interchangeable, but this is not the case
p4642
aVAs shown by Figures 4 and 5 , the paraboloid shape of the LAS curve in Figure 3 is simply the combination of the metric u'\u005cu2019' s linear responses to both label and structural perturbations
p4643
aVThe behaviour of u'\u005cu0391' on the other hand is more complex, with structural noise being penalised harder than perturbations of the labels
p4644
aVThus, the similarity of LAS and u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n is not at all assured when the amounts of structural and labelling disagreements differ
p4645
aVAdditionally, we consider this imbalanced weighting of structural and labelling disagreements a benefit, as structure is the larger part of syntactic annotation compared to the labelling of the dependencies/bracketings
p4646
aVThe use of a distance function to define the metric means that more fine-grained distinctions can be made; for example, if the set of labels on the structures is highly structured, partial credit can be given for differing annotations that overlap
p4647
aVFor example, if different types of adverbials (temporal, negation, etc.) receive different relations, as is the case in the Swedish Talbanken05 [] corpus, confusion of different adverbial types can be given less weight than confusion between subject and object
p4648
aVThe u'\u005cu0391' -based metrics are also far easier to apply to a more complex annotation task such as the tectogrammatical annotation of the PCEDT
p4649
aVIn this task inserting and deleting nodes is an integral part of the annotation, and if two annotators insert or delete different nodes the all-or-nothing requirement of identical yield of the LAS metric makes it impossible as an evaluation metric in this setting
p4650
aVIn future work, we would like to investigate the use of other distance functions, in particular the use of approximate tree edit distance functions such as the p u'\u005cu2062' q -gram algorithm []
p4651
aVThis is due to the fact that u'\u005cu0391' requires O u'\u005cu2062' ( n 2 ) comparisons to be made, each of which is O u'\u005cu2062' ( n 2 ) using our current approach
p4652
aVAnother avenue for future work is improved synthetic experiments
p4653
aVAs we saw, our implementation of tree perturbations was biased towards trees similar in shape to the source tree, and an improved permutation algorithm may reveal interesting edge-case behaviour in the metrics
p4654
aVA method for perturbing phrase structure trees would also be interesting, as this would allow us to repeat the synthetic experiments performed here using phrase structure corpora to compare the behaviour of the metrics on the two types of corpus
p4655
asg88
(lp4656
sg90
(lp4657
sg92
(lp4658
VThe most important conclusion we draw from this work is the most appropriate agreement metric for syntactic annotation.
p4659
aVFirst of all, we disqualify the LAS metric, primarily due to the methodological inadequacies of using an uncorrected measure.
p4660
aVWhile our experiments did not reveal any serious shortcomings (unlike those of [] who in the case of categorisation showed that for large p the uncorrected measure can be increasing ), the methodological problems of uncorrected metrics makes us wary of LAS as an agreement metric.
p4661
aVNext, of the three metrics, p l a i n is clearly the best; d i f f is extremely sensitive to even moderate amounts of disagreement, while n o r m is overly lenient.
p4662
aVLooking solely at Figure 3 , one might be led to believe that LAS and p l a i n are interchangeable, but this is not the case.
p4663
aVAs shown by Figures 4 and 5 , the paraboloid shape of the LAS curve in Figure 3 is simply the combination of the metric s linear responses to both label and structural perturbations.
p4664
aVThe behaviour of on the other hand is more complex, with structural noise being penalised harder than perturbations of the labels.
p4665
aVThus, the similarity of LAS and p l a i n is not at all assured when the amounts of structural and labelling disagreements differ.
p4666
aVAdditionally, we consider this imbalanced weighting of structural and labelling disagreements a benefit, as structure is the larger part of syntactic annotation compared to the labelling of the dependencies/bracketings.
p4667
aVFinally our experiments show that is a single metric that is applicable to both dependencies and phrase structure trees.
p4668
aVFurthermore, metrics are far more flexible than simple accuracy metrics.
p4669
aVThe use of a distance function to define the metric means that more fine-grained distinctions can be made; for example, if the set of labels on the structures is highly structured, partial credit can be given for differing annotations that overlap.
p4670
aVFor example, if different types of adverbials (temporal, negation, etc.) receive different relations, as is the case in the Swedish Talbanken05 [] corpus, confusion of different adverbial types can be given less weight than confusion between subject and object.
p4671
aVThe -based metrics are also far easier to apply to a more complex annotation task such as the tectogrammatical annotation of the PCEDT.
p4672
aVIn this task inserting and deleting nodes is an integral part of the annotation, and if two annotators insert or delete different nodes the all-or-nothing requirement of identical yield of the LAS metric makes it impossible as an evaluation metric in this setting.
p4673
ag106
asg107
S'P14-1088'
p4674
sg109
(lp4675
VFollowing the works of \u005cciteN Carletta96 and \u005cciteN Art:Poe08, there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.
p4676
aVWith this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F 1 (for phrase structure) and accuracy scores (for dependencies.
p4677
aVIn this work we present a chance-corrected metric based on Krippendorff s , adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications.
p4678
aVTo evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric s responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.
p4679
aV1 1 The code used to produce the data in this paper, and some of the datasets used, are available to download at https://github.com/arnsholt/syn-agreement/.
p4680
aVedgefromparent/.style=-¿,draw,font.
p4681
ag106
asba(icmyPackage
FText
p4682
(dp4683
g3
(lp4684
VAnswering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing
p4685
aVThose efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base
p4686
aV[0]leftmargin=*,itemindent=0em,itemsep=-2pt,topsep=0pt
p4687
aVThese systems were limited to closed-domains due to a lack of knowledge resources, computing power, and ability to robustly understand natural language
p4688
aVWith the recent growth in KBs such as DBPedia [ 1 ] , Freebase [ 4 ] and Yago2 [ 18 ] , it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now , based on Google u'\u005cu2019' s Knowledge Graph , and Facebook Graph Search , based on social network connections
p4689
aVPerformance is thus bounded by the accuracy of the original semantic parsing, and the well-formedness of resultant database queries
p4690
aVThe Information Extraction (IE) community approaches QA differently first performing relatively coarse information retrieval as a way to triage the set of possible answer candidates, and only then attempting to perform deeper analysis
p4691
aVResearchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery [ 35 ]
p4692
aVWhile making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared we show that u'\u005cu201c' traditional u'\u005cu201d' IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of  34% F 1 as compared to Berant et al
p4693
aVWe will view a KB as an interlinked collection of u'\u005cu201c' topics u'\u005cu201d'
p4694
aVOne challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB
p4695
aVFor example, for the question who cheated on celebrity A , answers can be retrieved via the Freebase relation celebrity.infidelity.participant , but the connection between the phrase cheated on and the formal KB relation is not explicit
p4696
aV2013 ) , who collected thousands of commonly asked questions by crawling the Google Suggest service
p4697
aVMore recent research started to minimize this direct supervision by using latent meaning representations [ 2 , 24 ] or distant supervision [ 23 ]
p4698
aVOur work pushes the data challenge to the limit by mining directly from ClueWeb , a 5TB collection of web data
p4699
aVIf you asked someone what is the name of justin bieber brother , 3 3 All examples used in this paper come from the training data crawled from Google Suggest
p4700
aVUnfortunately Freebase does not contain an exact relation called brother , but instead sibling
p4701
aVThus further inference (i.e.,, brother u'\u005cu2194' male sibling) has to be made
p4702
aVWith regards to the question, we know we are looking for the name of a person based on the following
p4703
aVif a node was tagged with a question feature, then replace this node with its question feature, e.g.,, what u'\u005cu2192' qword=what;
p4704
aVspecial case) if a qtopic node was tagged as a named entity, then replace this node with its its named entity form, e.g.,, bieber u'\u005cu2192' qtopic=person;
p4705
aVThen features are extracted in the following form with s the source and t the target node, for every edge e u'\u005cu2062' ( s , t ) in the graph, extract s , t , s u'\u005cu2223' t and s u'\u005cu2062' u'\u005cu2223' e u'\u005cu2223' u'\u005cu2062' t as features
p4706
aVFor the edge, prep_of(qfocus=name, brother) , this would mean the following features qfocus=name , brother , qfocus=name u'\u005cu2223' brother , and qfocus=name u'\u005cu2223' prep_of u'\u005cu2223' brother
p4707
aVFurthermore, the reason that we have kept some lexical features, such as brother , is that we hope to learn from training a high correlation between brother and some Freebase relations and properties (such as sibling and male ) if we do not possess an external resource to help us identify such a correlation
p4708
aVGiven a topic, we selectively roll out the Freebase graph by choosing those nodes within a few hops of relationship to the topic node , and form a topic graph
p4709
aVThese properties, along with the sibling relationship to the topic node, are important cues for answering the question
p4710
aVThus for the Freebase graph, we use relations (with directions) and properties as features for each node
p4711
aVSome of the mapping can be simply detected as paraphrasing or lexical overlap
p4712
aVFor example, the person.parents relationship helps answering questions about parenthood
p4713
aVFor instance, for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training
p4714
aVThus assuming there is an alignment model that is able to tell how likely one relation maps to the original question, we add extra alignment-based features for the incoming and outgoing relation of each node
p4715
aVWe combine question features and Freebase features (per node) by doing a pairwise concatenation
p4716
aVThis simple example points out that every part of the question could change what the question inquires eventually
p4717
aVThus we need to count for each word w in Q
p4718
aVDue to the bias and incompleteness of any data source, we approximate the true probability of P with P ~ under our specific model
p4719
aVFor instance, both people.person.parents and fictional_universe.fictional_character.parents indicate the parent relationship but the latter is much less commonly annotated
p4720
aVBy counting how many times each relation R was annotated, we can estimate P ~ u'\u005cu2062' ( R ) and P ~ u'\u005cu2062' ( r
p4721
aVWe split each html document by sentences [ 21 ] using NLTK [ 3 ] and extracted those with at least two Freebase entities which has at least one direct established relation according to Freebase
p4722
aVThe extraction formed two parallel corpora, one with u'\u005cu201c' relation - sentence u'\u005cu201d' pairs (for estimating P ~ ( w u'\u005cu2223' R ) and P ~ u'\u005cu2062' ( R ) ) and the other with u'\u005cu201c' subrelations - sentence u'\u005cu201d' pairs (for P ~ ( w u'\u005cu2223' r ) and P ~ u'\u005cu2062' ( r )
p4723
aVSince the relations on one side of these pairs are not natural sentences, we ran the most simple IBM alignment Model 1 [ 5 ] to estimate the translation probability with GIZA++ [ 30 ]
p4724
aVTreating the aligned pairs as observation , the co-occurrence matrix between aligning relations and words was computed
p4725
aVFor instance, for the film.actor.film relation (mapping from film names to actor names), the top words given by P ~ ( w u'\u005cu2223' R ) are won , star , among , show
p4726
aVFor the film.film.directed_by relation, some important stop words that could indicate this relation, such as by and with , rank directly after director and direct
p4727
aVBoth ClueWeb and its Freebase annotation has a bias
p4728
aVThus we were firstly interested in the coverage of mined relation mappings
p4729
aVWe evaluated on the training set in two aspects coverage and prediction performance
p4730
aVWe define answer node as the node that is the answer and answer relation as the relation from the answer node to its direct parent
p4731
aVThen we computed how much and how well the answer relation was triggered by ReverbMapping and CluewebMapping
p4732
aVThus for the question, who is the father of King George VI , we ask two questions does the mapping, 1 coverage) contain the answer relation people.person.parents
p4733
aVWe computed standard MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank), shown in Table 2 (a
p4734
aVAs a simple baseline, u'\u005cu201c' word overlap u'\u005cu201d' counts the overlap between relations and the question
p4735
aVReverbMapping does the same, except that we took a uniform distribution on P ~ ( w u'\u005cu2223' R ) and P ~ u'\u005cu2062' ( R ) since the contributed dataset did not include co-occurrence counts to estimate these probabilities
p4736
aV2013 ) originally used it they employed a discriminative log-linear model to judge relations and that might yield better performance
p4737
aVAs a fair comparison, ranking of CluewebMapping under uniform distribution is also included in Table 2 (a
p4738
aVThese percentage numbers are good clue for feature design for instance, we may be confident in a relation if it is ranked top 5 or 10 by CluewebMapping
p4739
aV2013 ) ) if a predicted answer list does not have a perfect match with all gold answers, as a lot of questions in W eb Q uestions contain more than one answer
p4740
aVW eb Q uestions not only has answers annotated, but also which Freebase topic nodes the answers come from
p4741
aVThus we evaluated the ranking of retrieval with the gold standard annotation on train-all , shown in Table 3
p4742
aVWe took this as a u'\u005cu201c' good enough u'\u005cu201d' IR front-end and used it on test
p4743
aVThe API returns almost identical information as displayed via a web browser to a user viewing this topic
p4744
aVGiven that turkers annotated answers based on the topic page via a browser, this supports the assumption that the same answer would be located in the topic graph, which is then passed to the QA engine for feature extraction and classification
p4745
aVWe treat QA on Freebase as a binary classification task for each node in the topic graph, we extract features and judge whether it is the answer node
p4746
aVThe L1 regularization encourages sparse features by driving feature weights towards zero, which was ideal for the over-generated feature space
p4747
aV1) u'\u005cu201c' basic u'\u005cu201d' features include feature productions read off from the feature graph (Figure 1 ); 2) u'\u005cu201c' + word overlap u'\u005cu201d' adds additional features on whether sub-relations have overlap with the question; and 3) u'\u005cu201c' + CluewebMapping u'\u005cu201d' adds the ranking of relation prediction given the question according to CluewebMapping
p4748
aVTable 3 ), thus we also tested on the top 10 results returned by the Search API
p4749
aV2013 ) also used ClueWeb indirectly through ReVerb
p4750
aVThus we took out the word overlapping and CluewebMapping based features, and the new F 1 on test was 36.9 u'\u005cu2062' %
p4751
asg88
(lp4752
sg90
(lp4753
sg92
(lp4754
VWe proposed an automatic method for Question Answering from structured data source (Freebase.
p4755
aVOur approach associates question features with answer patterns described by Freebase and has achieved state-of-the-art results on a balanced and realistic QA corpus.
p4756
aVTo compensate for the problem of domain mismatch or overfitting, we exploited ClueWeb, mined mappings between KB relations and natural language text, and showed that it helped both relation prediction and answer extraction.
p4757
aVOur method employs relatively lightweight machinery but has good performance.
p4758
aVWe hope that this result establishes a new baseline against which semantic parsing researchers can measure their progress towards deeper language understanding and answering of human questions.
p4759
aVWe thank the Allen Institute for Artificial Intelligence for funding this work.
p4760
aVWe are also grateful to Jonathan Berant, Tom Kwiatkowski, Qingqing Cai, Adam Lopez, Chris Callison-Burch and Peter Clark for helpful discussion and to the reviewers for insightful comments.
p4761
ag106
asg107
S'P14-1090'
p4762
sg109
(lp4763
VAnswering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing.
p4764
aVThose efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base.
p4765
aVHere we show that relatively modest information extraction techniques, when paired with a web-scale corpus, can outperform these sophisticated approaches by roughly 34% relative gain.
p4766
aV[0]leftmargin=*,itemindent=0em,itemsep=-2pt,topsep=0pt.
p4767
ag106
asba(icmyPackage
FText
p4768
(dp4769
g3
(lp4770
VWe translate questions to answers based on CYK parsing
p4771
aVAnswers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated
p4772
aVA linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs
p4773
aVKnowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs
p4774
aVFirst, the input question is transformed into its meaning representation (MR) by an independent semantic parser [ 26 , 20 , 2 , 17 , 4 , 22 , 1 , 14 , 3 ] ; Then, the answers are retrieved from existing KBs using generated MRs as queries
p4775
aVUnlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly
p4776
aVBorrowing ideas from machine translation (MT), we treat the QA task as a translation procedure
p4777
aVLike MT, CYK parsing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of that cell; unlike MT, which uses offline-generated translation tables to translate source phrases into target translations, a semantic parsing-based question translation method is used to translate each span into its answers on-the-fly, based on question patterns and relation expressions
p4778
aVDerivations generated during such a translation procedure are modeled by a linear model, and minimum error rate training (MERT) [ 21 ] is used to tune feature weights based on a set of question-answer pairs
p4779
aV2) We propose a robust method to transform single-relation questions into formal triple queries as their MRs, which trades off between transformation accuracy and recall using question patterns and relation expressions respectively
p4780
aVFormally, given a knowledge base u'\u005cud835' u'\u005cudca6' u'\u005cu2062' u'\u005cu212c' and an NL question u'\u005cud835' u'\u005cudcac' , our KB-QA method generates a set of formal triples-answer pairs { u'\u005cu27e8' u'\u005cud835' u'\u005cudc9f' , u'\u005cud835' u'\u005cudc9c' u'\u005cu27e9' } as derivations, which are scored and ranked by the distribution P ( u'\u005cu27e8' u'\u005cud835' u'\u005cudc9f' , u'\u005cud835' u'\u005cudc9c' u'\u005cu27e9' u'\u005cud835' u'\u005cudca6' u'\u005cu212c' , u'\u005cud835' u'\u005cudcac' ) defined as follows
p4781
aVFor the sake of convenience, we omit the I u'\u005cu2062' D information in the rest of the paper
p4782
aVAccording to the above description, our KB-QA method can be decomposed into four tasks as
p4783
aVThe above operations are equivalent to answering a simplified question, which is obtained by replacing the answerable spans in the original question with their corresponding answers
p4784
aVNote that if no predicate p or answer e o u'\u005cu2062' b u'\u005cu2062' j can be generated, { u'\u005cud835' u'\u005cudcac' , N u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' l , u'\u005cud835' u'\u005cudcac' } will be returned as a special triple, which sets e o u'\u005cu2062' b u'\u005cu2062' j to be u'\u005cud835' u'\u005cudcac' itself, and p to be N u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' l
p4785
aVTwo question translation methods are presented in the rest of this subsection, which are based on question patterns and relation expressions respectively
p4786
aVAlgorithm 2 shows how to generate formal triples for a span u'\u005cud835' u'\u005cudcac' based on question patterns ( u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' -based question translation
p4787
aVFirst, 5W queries, which begin with What, Where, Who, When, or Which, are selected from a large scale query log of a commercial search engine; Then, a cleaned entity dictionary is used to annotate each query by replacing all entity mentions it contains with the symbol [ S u'\u005cu2062' l u'\u005cu2062' o u'\u005cu2062' t ]
p4788
aVFrom experiments (Table 3 in Section 4.3) we can see that, question pattern based question translation can achieve high end-to-end accuracy
p4789
aVBut as human efforts are needed in the mining procedure, this method cannot be extended to large scale very easily
p4790
aVAiming to alleviate the coverage issue occurring in u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' -based method, an alternative relation expression ( u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' ) -based method is proposed, and will be used when the u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' -based method fails
p4791
aVWe define u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' p as a relation expression set for a given KB predicate p u'\u005cu2208' u'\u005cud835' u'\u005cudca6' u'\u005cu2062' u'\u005cu212c'
p4792
aVAlgorithm 3 shows how to generate triples for a question u'\u005cud835' u'\u005cudcac' based on relation expressions
p4793
aVIf this score is larger than 0 , which means there are overlaps between u'\u005cud835' u'\u005cudcac' u'\u005cu2019' s context and u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' p , then q will be used as the triple query of u'\u005cud835' u'\u005cudcac' , and a set of formal triples will be generated based on q and u'\u005cud835' u'\u005cudca6' u'\u005cu2062' u'\u005cu212c' (from Line 7 to Line 15
p4794
aVThen, we extract the shortest path between paired entities in the dependency tree of each sentence as an u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' candidate for the given predicate
p4795
aVThe intuition is that any sentence containing such entity pairs occur in an assertion is likely to express the predicate of that assertion in some way
p4796
aVSometimes, a question may provide multiple constraints to its answers movie starred by Tom Hanks in 1994 is one such question
p4797
aVAll the films as the answers of this question should satisfy the following two constraints
p4798
aVWe propose a dependency tree-based method to handle such multiple-constraint questions by (i) decomposing the original question into a set of sub-questions using syntax-based patterns; and (ii) intersecting the answers of all sub-questions as the final answers of the original question
p4799
aVNote, question decomposition only operates on the original question and question spans covered by complete dependency subtrees
p4800
aVIf a question matches any one of these patterns, then sub-questions are generated by collecting the paths between n 0 and each n i ( i 0 ) in the pattern, where each n denotes a complete subtree with a noun, number, or question word as its root node, the symbol * above p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' p * denotes this preposition can be skipped in matching
p4801
aVFor the question mentioned at the beginning, its two sub-questions generated are movie starred by Tom Hanks and movie starred in 1994 , as its dependency form matches pattern (a
p4802
aVSimilar ideas are used in IBM Watson [ 13 ] as well
p4803
aVAs dependency parsing is not perfect, we generate single triples for such questions without considering constraints as well, and add them to the search space for competition h s u'\u005cu2062' y u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' a u'\u005cu2062' x u'\u005cu2062' _ u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' ( u'\u005cu22c5' ) is used to boost triples that are converted from sub-questions generated by question decomposition
p4804
aVThese three scores are used as features to rank answers generated in QA procedure
p4805
aVGiven a set of question-answer pairs { u'\u005cud835' u'\u005cudcac' i , u'\u005cud835' u'\u005cudc9c' i r u'\u005cu2062' e u'\u005cu2062' f } as the development (dev) set, we use the minimum error rate training (MERT) [ 21 ] algorithm to tune the feature weights u'\u005cu039b' i M in our proposed model
p4806
aVN is the number of questions in the dev set, u'\u005cud835' u'\u005cudc9c' i r u'\u005cu2062' e u'\u005cu2062' f is the correct answers as references of the i t u'\u005cu2062' h question in the dev set, u'\u005cud835' u'\u005cudc9c' i ^ is the top-1 answer candidate of the i t u'\u005cu2062' h question in the dev set based on feature weights u'\u005cu039b' 1 M , E u'\u005cu2062' r u'\u005cu2062' r u'\u005cu2062' ( u'\u005cu22c5' ) is the error function which is defined as
p4807
aVPoon ( 2013 ) has proposed an unsupervised method by adopting grounded-learning to leverage the database for indirect supervision
p4808
aV2013 ) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data
p4809
aV1) Question answering and semantic parsing are performed in an joint way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation expressions in a cascaded way; and (3) We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters
p4810
aV1) Instead of using facts extracted using the open IE method, we leverage a large scale, high-quality knowledge base; (2) We can handle multiple-relation questions, instead of single-relation queries only, based on our translation based KB-QA framework
p4811
aVBut MT in there work means to translate questions into n -best translations, which are used for finding similar sentences in the document collection that probably contain answers
p4812
aV2013 ) , we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test
p4813
aVTable 2 shows the statistics of question patterns and relation expressions used in our KB-QA system
p4814
aVAs all question patterns are collected with human involvement as we discussed in Section 2.3.1, the quality is very high ( 98 u'\u005cu2062' %
p4815
aV2013 ) is one of the latest work which has reported QA results based on a large scale, general domain knowledge base (Freebase), we consider their evaluation result on WEBQUESTIONS as our baseline
p4816
aVOur KB-QA system generates the k -best derivations for each question span, where k is set to 20
p4817
aVSince Freebase is completely contained by our KB, we disallow all entities which are not included by Freebase
p4818
aVBy doing so, our KB provides the same knowledge as Freebase does, which means we do not gain any extra advantage by using a larger KB
p4819
aVBut we still allow ourselves to use the static rank scores and confidence scores of entities as features, as we described in Section 2.4
p4820
aVWe first show the overall evaluation results of our KB-QA system and compare them with baseline u'\u005cu2019' s results on Dev and Test
p4821
aV2013 ) have used a lexicon extracted from a subset of ReVerb triples [ 19 ] , which is similar to the relation expression set used in question translation
p4822
aVBut as our relation expressions are extracted by an in-house extractor, we can record their extraction-related statistics as extra information, and use them as features to measure the mapping quality
p4823
aVBesides, as a portion of entities in our KB are extracted from Wiki, we know the one-to-one correspondence between such entities and Wiki pages, and use this information in relation expression extraction for entity disambiguation
p4824
aVThe underlying intuition of using patterns is that those high-frequent questions/queries should and can be treated and solved in the QA task, by involving human effort at a relative small price but with very impressive accuracy
p4825
aV1) The quality of the relation expressions is better than the quality of the lexicon entries used in the baseline; and (2) We use the extraction-related statistics of relation expressions as features, which brings more information to measure the confidence of mapping between NL phrases and KB predicates, and makes the model to be more flexible
p4826
aVMeanwhile, u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' o u'\u005cu2062' n u'\u005cu2062' l u'\u005cu2062' y perform worse ( 11.8 u'\u005cu2062' % ) than u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' o u'\u005cu2062' n u'\u005cu2062' l u'\u005cu2062' y , due to coverage issue
p4827
aVBut by comparing the precisions of these two settings, we find u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' o u'\u005cu2062' n u'\u005cu2062' l u'\u005cu2062' y (97.5%) outperforms u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' o u'\u005cu2062' n u'\u005cu2062' l u'\u005cu2062' y (73.2%) significantly, due to its high quality
p4828
aVThis means how to extract high-quality question patterns is worth to be studied for the question answering task
p4829
aVAs the performance of our KB-QA system relies heavily on the k -best beam approximation, we evaluate the impact of the beam size and list the comparison results in Figure 6
p4830
aVWe can see that as we increase k incrementally, the accuracy increase at the same time
p4831
aVHowever, a larger k (e.g., 200) cannot bring significant improvements comparing to a smaller one (e.g.,, 20), but using a large k has a tremendous impact on system efficiency
p4832
aVSo we choose k = 20 as the optimal value in above experiments, which trades off between accuracy and efficiency
p4833
aVActually, the size of our system u'\u005cu2019' s search space is much smaller than the one of the semantic parser used in the baseline.This is due to the fact that, if triple queries generated by the question translation component cannot derive any answer from KB, we will discard such triple queries directly during the QA procedure
p4834
aVSince named entity recognizers trained on Penn TreeBank usually perform poorly on web queries, We instead use a simple string-match method to detect entity mentions in the question using a cleaned entity dictionary dumped from our KB
p4835
aVOne problem of doing so is the entity detection issue
p4836
aVFor example, in the question who was Esther u'\u005cu2019' s husband we cannot detect Esther as an entity, as it is just part of an entity name
p4837
aVWe need an ad-hoc entity detection component to handle such issues, especially for a web scenario, where users often type entity names in their partial or abbreviation forms
p4838
aVSince each relation expression must contain at least one content word, this question cannot match any relation expression
p4839
aVFor the following question who did Steve Spurrier play pro football for as an example, since the unigram play exists in both Film.Film.Actor and American_Football.Player.Current_Team u'\u005cu2019' s relation expression sets, we made a wrong prediction, which led to wrong answers
p4840
aVFor this example, we can give all book names where Sherlock Holmes appeared in, but we cannot rank them based on their publication date , as we cannot learn the alignment between the constraint word first occurred in the question and the predicate Book.Written_Work.Date_Of_First_Publication from training data automatically
p4841
aVAlthough we have followed some work [ 22 , 18 ] to handle such special linguistic phenomena by defining some specific operators, it is still hard to cover all unseen cases
p4842
aVWe leave this to future work as an independent topic
p4843
aVThis paper presents a translation-based KB-QA method that integrates semantic parsing and QA in one unified framework
p4844
aVComparing to the baseline system using an independent semantic parser with state-of-the-art performance, we achieve better results on a general domain evaluation set
p4845
aVSeveral directions can be further explored in the future i) We plan to design a method that can extract question patterns automatically, using existing labeled question patterns and KB as weak supervision
p4846
aVAs we discussed in the experiment part, how to mine high-quality question patterns is worth further study for the QA task; (ii) We plan to integrate an ad-hoc NER into our KB-QA system to alleviate the entity detection issue; (iii) In fact, our proposed QA framework can be generalized to other intelligence besides knowledge bases as well
p4847
aVAny method that can generate answers to questions, such as the Web-based QA approach, can be integrated into this framework, by using them in the question translation component
p4848
asg88
(lp4849
sg90
(lp4850
sg92
(lp4851
VThis paper presents a translation-based KB-QA method that integrates semantic parsing and QA in one unified framework.
p4852
aVComparing to the baseline system using an independent semantic parser with state-of-the-art performance, we achieve better results on a general domain evaluation set.
p4853
aVSeveral directions can be further explored in the future i) We plan to design a method that can extract question patterns automatically, using existing labeled question patterns and KB as weak supervision.
p4854
aVAs we discussed in the experiment part, how to mine high-quality question patterns is worth further study for the QA task; (ii) We plan to integrate an ad-hoc NER into our KB-QA system to alleviate the entity detection issue; (iii) In fact, our proposed QA framework can be generalized to other intelligence besides knowledge bases as well.
p4855
aVAny method that can generate answers to questions, such as the Web-based QA approach, can be integrated into this framework, by using them in the question translation component.
p4856
ag106
asg107
S'P14-1091'
p4857
sg109
(lp4858
VA typical knowledge-based question answering (KB-QA) system faces two challenges one is to transform natural language questions into their meaning representations (MRs); the other is to retrieve answers from knowledge bases (KBs) using generated MRs.
p4859
aVUnlike previous methods which treat them in a cascaded manner, we present a translation-based approach to solve these two tasks in one unified framework.
p4860
aVWe translate questions to answers based on CYK parsing.
p4861
aVAnswers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated.
p4862
aVA linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs.
p4863
aVCompared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results.
p4864
ag106
asba(icmyPackage
FText
p4865
(dp4866
g3
(lp4867
VWe propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information, driven by two representations of discourse a shallow representation centered around discourse markers, and a deep one based on Rhetorical Structure Theory
p4868
aVThis is problematic for NF QA, where questions are answered not by atomic facts, but by larger cross-sentence conceptual structures that convey the desired answers
p4869
aVThus, to answer NF questions, one needs a model of what these answer structures look like
p4870
aVWe propose a novel answer reranking (AR) model that combines lexical semantics (LS) with discourse information, driven by two representations of discourse a shallow representation centered around discourse markers and surface text information, and a deep one based on the Rhetorical Structure Theory (RST) discourse framework [ 7 ]
p4871
aVWe demonstrate that modeling discourse is greatly beneficial for NF AR for two types of NF questions, manner ( u'\u005cu201c' how u'\u005cu201d' ) and reason ( u'\u005cu201c' why u'\u005cu201d' ), across two large datasets from different genres and domains u'\u005cu2013' one from the community question-answering (CQA) site of Yahoo
p4872
aVWe demonstrate good domain transfer performance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly applicable to NF QA
p4873
aVThe body of work on factoid QA is too broad to be discussed here (see, e.g.,, the TREC workshops for an overview
p4874
aVWe extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR
p4875
aV[ 20 ] extracted 47 cue phrases such as because from a small collection of web documents, and used the cosine similarity between an answer candidate and a bag of words containing these cue phrases as a single feature in their reranking model for non-factoid why QA
p4876
aV[ 12 ] built a classifier to identify causal relations using a small set of cue phrases (e.g.,, because and is caused by
p4877
aV[ 18 ] conducted an initial evaluation of the utility of RST structures to why QA by evaluating performance on a small sample of seven WSJ articles drawn from the RST Treebank [ 1 ]
p4878
aVHere we use no meta data and rely solely on linguistic features
p4879
aVThus, for a given question, all its answers are fetched from the answer collection, and an initial ranking is constructed based on the cosine similarity between theirs and the question u'\u005cu2019' s lemma vector representations, with lemmas weighted using tf.idf (Ch
p4880
aVWe use this setup to answer questions from a biology textbook, where each section is indexed as a standalone document, and each paragraph in a given document is considered as a candidate answer
p4881
aVBecause the number of answer candidates is typically large (e.g.,, equal to the number of paragraphs in the textbook), we return the N top candidates with the highest scores
p4882
aV7 7 http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html In addition to these features, each reranker also includes a single feature containing the score of each candidate, as computed by the above candidate retrieval (CR) component
p4883
aV8 8 Including these scores as features in the reranker model is a common strategy that ensures that the reranker takes advantage of the analysis already performed by the CR model
p4884
aVWe propose two separate discourse representation schemes u'\u005cu2013' one shallow, centered around discourse markers, and one deep, based on RST
p4885
aV1) A discourse marker from Daniel Marcu u'\u005cu2019' s list (see Appendix B in Marcu [ 9 ] ), that serves as a divisive boundary between sentences
p4886
aVExamples of these markers include and, in, that, for, if, as, not, by, and but ; (2) two marker arguments , i.e.,, text segments before and after the marker, labeled to indicate if they are related to the question text or not; and (3) a sentence range around the marker, which defines the length of these segments (e.g.,, ± 2 sentences
p4887
aVFurther, the text preceeding by matches text from the question (and is therefore labeled QSEG ), while the text after by differs considerably from the question text, and is labeled OTHER
p4888
aVWe label marker arguments based on their similarity to question content
p4889
aVIf text before or after a marker out to a given sentence range matches the entire text of the question (with a cosine similarity score larger than a threshold), that argument takes on the label QSEG , or OTHER otherwise
p4890
aVArgument labels indicate only if lemmas from the question were found in a discourse structure present in an answer candidate, and do not speak to the specific lemmas that were found
p4891
aVIt is important to note that these discourse features are more expressive than features based on discourse markers alone [ 5 , 19 ]
p4892
aVThe discourse parser model (DPM) is based on the RST discourse framework [ 7 ]
p4893
aVIn the bottom part of Figure 2 , we show hypotactic relations as directed arrows, from the nucleus to the satellite
p4894
aVHowever, this also introduces noise because discourse analysis is a complex task and discourse parsers are not perfect
p4895
aVTo mitigate this, we used a simple feature generation strategy, which creates one feature for each individual discourse relation by concatenating the relation type with the labels of the discourse units participating in it
p4896
aVTo this end, for every relation, we extract the entire text dominated by each of its arguments, and we generate labels for the two participants in the relation using the same strategy as the DMM (based on the similarity with the question content
p4897
aVSimilar to the DMM, these features take real values obtained by averaging the cosine similarity of the arguments with the question content
p4898
aV9 9 We investigated more complex features, e.g.,, by exploring depths of two and three in the discourse tree, and also models that relied on tree kernels over these trees, but none improved upon this simple representation
p4899
aV[ 22 ] , we include lexical semantics in our reranking model
p4900
aVLike any language model, a RNNLM estimates the probability of observing a word given the preceding context, but, in this process, it learns word embeddings into a latent, conceptual space with a fixed number of dimensions
p4901
aVConsequently, related words tend to have vectors that are close to each other in this space
p4902
aVWe derive two LS measures from these vectors, which are then are included as features in the reranker
p4903
aVThe first is a measure of the overall LS similarity of the question and answer candidate, which is computed as the cosine similarity between the two composite vectors of the question and the answer candidate
p4904
aVThese composite vectors are assembled by summing the vectors for individual question (or answer candidate) words, and re-normalizing this composite vector to unit length
p4905
aVDue to the speed limitations of the discourse parser, we randomly drew 10,000 QA pairs from the corpus of how questions described by Surdeanu et al
p4906
aV[ 17 ] using their filtering criteria, with the additional criterion that answers had to contain at least four community-generated answers, one of which was voted as the top answer
p4907
aVThe number of answers to each question ranged from 4 to over 50, with the average 9
p4908
aVThe setup in this paper, commonly used in the CQA community [ 21 ] , is more relevant here because it includes both high and low quality answers
p4909
aVThe entire biology text (at paragraph granularity) serves as the possible set of answers
p4910
aVNote that while our system retrieves answers at paragraph granularity, the expert was not constrained in any way during the annotation process, so gold answers might be smaller than a paragraph or span multiple paragraphs
p4911
aVFor the YA CQA corpora, 50% of QA pairs were used for training, 25% for development, and 25% for test
p4912
aVBecause of the small size of the Bio corpus, it was evaluated using 5-fold cross-validation, with three folds for training, one for development, and one for test
p4913
aVNote that, because these domains are considerably different from the RST Treebank, the parser fails to produce a tree on a large number of answer candidates
p4914
aVThe latter was created by extracting a) pages matching a word/phrase in a glossary of biology (derived from the textbook); plus (b) pages hyperlinked from (a) that are also tagged as being in a small set of (hand-selected) biology-related categories
p4915
aVIn the Bio corpus, because answer candidates are not guaranteed to match gold annotations exactly, these metrics do not immediately apply
p4916
aVWe adapted them to this dataset by weighing each answer by its overlap with gold answers, where overlap is measured as the highest F1 score between the candidate and a gold answer
p4917
aVThus, P@1 reduces to this F1 score for the top answer
p4918
aVFor example, if the best answer for a question appears at rank 2 with an F1 score of 0.3 , the corresponding MRR score is 0.3 / 2
p4919
aV15 15 The performance of all models can ultimately be increased by using more sophisticated learning frameworks, and considering more answer candidates in CR (for Bio
p4920
aVExamining Table 1 , several trends are clear
p4921
aVThis disparity is likely due to the difficulty in assembling LS training data at an appropriate level for the biology corpus, contrasted with the relative abundance of large scale open-domain lexical semantic resources
p4922
aVFinally, while the discourse models perform well for HOW or manner questions, performance on Bio WHY corpus suggests that reason questions are particularly amenable to discourse analysis
p4923
aVTo tease apart the relative contribution of discourse features that occur only within a single sentence versus features that span multiple sentences, we examined the performance of the full model when using only intra-sentence features, i.e.,, SR0 features for DMM, and features based on discourse relations where both EDUs appear in the same sentence for DPM, versus the full intersentence models
p4924
aVBecause these discourse models appear to capture high-level information about answer structures, we hypothesize that the models should make use of many of the same discourse features, even when training on data from different domains
p4925
aVThe in-domain performance of the ensemble model is similar to that of the single classifier in both YA and Bio HOW so we omit these results here for simplicity
p4926
aVThis confirms existing evidence that ensemble models perform better cross-domain because they overfit less [ 2 , 4 ]
p4927
aVThe ensemble model without LS (third line) has a nearly identical P@1 score as the equivalent in-domain model (line 13 in Table 1), while slightly surpassing in-domain MRR performance
p4928
aVTo the best of our knowledge, this is one of the most striking demonstrations of domain transfer in answer ranking for non-factoid QA, and highlights the generality of these discourse features in identifying answer structures across domains and genres
p4929
aVWe hypothesize that the limited transfer observed for models with LS compared to their counterparts without LS is due to the disparity in the size and utility of the biology LS training data compared to the open-domain LS resources
p4930
aVSo far, we have treated LS and discourse as distinct features in the reranking model, However, given that LS features greatly improve the CR baseline, we hypothesize that a natural extension to the discourse models would be to make use of LS similarity (in addition to the traditional information retrieval similarity) to label discourse segments
p4931
aVWe implemented two such models, denoted DMM L u'\u005cu2062' S and DPM L u'\u005cu2062' S , by replacing the component that assigns argument labels with one that relies on LS
p4932
aVSpecifically, as in § 4.3 , we compute the cosine similarity between the composite LS vectors of the question text and each marker argument (in DMM) or EDU (in DPM), and label the corresponding answer segment QSEG if this score is higher than a threshold, or OTHER otherwise
p4933
aVBecause we are adding two new discourse models, we now tune four segment matching thresholds, one for each of the DMM, DPM, DMM L u'\u005cu2062' S , and DPM L u'\u005cu2062' S models
p4934
aVWe chose the Bio setup for this analysis because it is more complex than the CQA one here gold answers may have a granularity completely different from what the system choses as best answers (in our particular case, the QA system is currently limited to answers consisting of single paragraphs, whereas gold answers may be of any size
p4935
aVIn these cases, the model selected an on-topic answer paragraph in the same subsection of the textbook as a gold answer
p4936
aVOften times this paragraph directly preceded or followed the gold answer
p4937
aVHere, both the CR and full model chose a paragraph containing a different gold answer
p4938
aVHowever, as discussed, gold answers may unevenly straddle paragraph boundaries, and the paragraph chosen by the model happened to have a somewhat lower overlap with its gold answer than the one chosen by the baseline
p4939
aVThe model chose a paragraph that had many of the same words as the question, but is on a different topic
p4940
aVFor example, for the question u'\u005cu201d' How are fossil fuels formed, and why do they contain so much energy u'\u005cu201d' , the model selected an answer that mentions fossil fuels in a larger discussion of human ecological footprints
p4941
asg88
(lp4942
sg90
(lp4943
sg92
(lp4944
VThis work focuses on two important aspects of answer reranking for non-factoid QA similarity between question and answer content, and answer structure.
p4945
aVWhile the former has been addressed with a variety of lexical-semantic models, the latter has received little attention.
p4946
aVHere we show how to model answer structures using discourse and how to integrate the two aspects into a holistic framework.
p4947
aVEmpirically we show that modeling answer discourse structures is complementary to modeling lexical semantic similarity and that the best performance is obtained when they are tightly integrated.
p4948
aVWe evaluate the proposed approach on multiple genres and question types and obtain benefits of up to 24% relative improvement over a strong baseline that combines information retrieval and lexical semantics.
p4949
aVWe further demonstrate that answer discourse structures are largely independent of domain and transfer well, even between radically different datasets.
p4950
aVThis work is open source and available at http://nlp.sista.arizona.edu/releases/acl2014.
p4951
ag106
asg107
S'P14-1092'
p4952
sg109
(lp4953
VWe propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information, driven by two representations of discourse a shallow representation centered around discourse markers, and a deep one based on Rhetorical Structure Theory.
p4954
aVWe evaluate the proposed model on two corpora from different genres and domains one from Yahoo.
p4955
aVAnswers and one from the biology domain, and two types of non-factoid questions manner and reason.
p4956
aVWe experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer, improving performance up to 24% (relative) over a state-of-the-art model that exploits lexical semantic similarity alone.
p4957
aVWe further demonstrate excellent domain transfer of discourse information, suggesting these discourse features have general utility to non-factoid question answering.
p4958
ag106
asba(icmyPackage
FText
p4959
(dp4960
g3
(lp4961
VExperiments show that our method outperforms baselines that are based on state-of-the-art methods
p4962
aVBut the vibrio risk due to global warming was observed in Baker-Austin et al
p4963
aVThe world can be seen as a network of causality where people, organizations, and other kinds of entities causally depend on each other
p4964
aVThis network is so huge and complex that it is almost impossible for humans to exhaustively predict the consequences of a given event
p4965
aVIndeed, after the Great East Japan Earthquake in 2011, few expected that it would lead to an enormous trade deficit in Japan due to a sharp increase in energy imports
p4966
aVOur ultimate goal is to develop a system that supports scenario planning through generating possible future events using big data, which would contain what Donald Rumsfeld called u'\u005cu201c' unknown unknowns u'\u005cu201d' 1 1 http://youtu.be/GiPe1OiKQuk [ 27 ]
p4967
aVNote that, in this paper, A u'\u005cu2192' B denotes that A causes B , which means that u'\u005cu201c' if A happens, the probability of B increases u'\u005cu201d' Our notion of causality should be interpreted probabilistically rather than logically
p4968
aVOur method extracts event causality based on three assumptions that are embodied as features of our classifier
p4969
aVFirst, we assume that two nouns (e.g., slash-and-burn agriculture and desertification ) that take some specific binary semantic relations (e.g., A causes B ) tend to constitute event causality if combined with two predicates (e.g., conduct and exacerbate
p4970
aVSuch semantic relations can be expressed by (otherwise unintuitive) patterns like A is an ingredient for B
p4971
aVAs such, semantic relations like the Material relation can also be useful
p4972
aV\u005cUnderline CO2 levels rose, so \u005cUnderline climatic anomalies were observed , while an unlikely context would be
p4973
aVIt remains uncertain whether if \u005cUnderline the recession is bottomed \u005cUnderline the declining birth rate is halted
p4974
aVExperiments using 600 million web pages [ 2 ] show that our method outperforms baselines based on state-of-the-art methods [ 8 , 12 ] by more than 19% of average precision
p4975
aVWe require that event causality be self-contained , i.e.,, intelligible as causality without the sentences from which it was extracted
p4976
aVFor example, omit toothbrushing u'\u005cu2192' get a cavity is self-contained, but omit toothbrushing u'\u005cu2192' get a girlfriend is not since this is not intelligible without a context
p4977
aVThis is important since future scenarios, which are generated by chaining event causality as described below, must be self-contained, unlike Hashimoto et al
p4978
aVOur scenario generation method generates scenarios by chaining extracted event causality; generating A u'\u005cu2192' B u'\u005cu2192' C from A u'\u005cu2192' B and B u'\u005cu2192' C
p4979
aVThe challenge is that many acceptable scenarios are overlooked if we require the joint part of the chain ( B above) to be an exact match
p4980
aVFor example, our method can identify the compatibility between sea temperatures are high and sea temperatures rise to chain global warming worsens u'\u005cu2192' sea temperatures are high and sea temperatures rise u'\u005cu2192' vibrio parahaemolyticus 2 2 A bacterium in the sea causing food-poisoning fouls (water
p4981
aVAccordingly, we generated a scenario deforestation continues u'\u005cu2192' global warming worsens u'\u005cu2192' sea temperatures rise u'\u005cu2192' vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus that was crawled in 2007, but the vibrio risk due to global warming has actually been observed in the Baltic sea and reported in Baker-Austin et al
p4982
aVFor event causality extraction , clues used by previous methods can roughly be categorized as lexico-syntactic patterns [ 1 , 20 ] , words in context [ 19 ] , associations among words [ 28 , 22 , 8 ] , and predicate semantics [ 12 ]
p4983
aVBesides features similar to those described above, we propose semantic relation features 3 3 Radinsky et al
p4984
aVWe show that such thorough exploitation of new and existing features leads to high performance
p4985
aVOther clues include shared arguments [ 28 , 4 , 5 ] , which we ignore since we target event causality about two distinct entities
p4986
aV2013 ) u'\u005cu2019' s web information analysis system provides a what-happens-if QA service, which is based on our scenario generation method
p4987
aVWe also require the predicate of the cause phrase to syntactically depend on the effect phrase in the sentence from which the event causality was extracted; we guarantee this by verifying the dependencies of the original sentence
p4988
aVIn Japanese, since the temporal order between events is usually determined by precedence in a sentence, we require the cause phrase to precede the effect phrase
p4989
aV5 5 Hashimoto et al u'\u005cu2019' s method constructs a network of templates based on their co-occurrence in web sentences with a small number of polarity-assigned seed templates and infers the polarity of all the templates in the network by a constraint solver based on the spin model [ 24 ]
p4990
aVAfter applying additional filters (see Section LABEL:A-sec:filtering-conditions in the supplementary notes) including those based on a stop-word list and a causal connective list to remove unlikely event causality candidates that are not removed by the above filter, we finally acquired 2,451,254 event causality candidates
p4991
aVIts relation to event causality might seem unclear, but a material can be seen as a u'\u005cu201c' cause u'\u005cu201d' of a product
p4992
aVIndeed materials can participate in event causality with the help of such template pairs as A is stolen u'\u005cu2192' B is made as in plutonium is stolen u'\u005cu2192' atomic bomb is made
p4993
aVUse is the relation between means (or instruments) and the purpose for using them
p4994
aVNote that means can be seen as u'\u005cu201c' causing u'\u005cu201d' or u'\u005cu201c' realizing u'\u005cu201d' the purpose of using the means in this relation, and actually event causality can be obtained by incorporating noun pairs of this relation into template pairs like activate A u'\u005cu2192' conduct B
p4995
aVThis relation is, so to speak, u'\u005cu201c' negative Causation u'\u005cu201d' since the entity denoted by the noun completing the A slot makes the entity denoted by the B noun NOT realized
p4996
aVSuch noun pairs mean event causality by substituting them into template pairs like omit A u'\u005cu2192' get B
p4997
aVThe semantic classes were obtained from our web corpus based on Kazama and Torisawa ( 2008
p4998
aVExcitation is divided into six sub types based on the excitation polarity of the binary patterns, the argument positions, and the existence of causative markers
p4999
aVWe believe that contexts exist where event causality candidates are more likely to appear, as described in Section 1
p5000
aVWe developed features that capture the characteristics of likely contexts for Japanese event causality (See Section LABEL:A-sec:context-features in the supplementary notes
p5001
aVCEA-based features are based on the Cause Effect Association (CEA) measure of Do et al
p5002
aVWe omit Do et al u'\u005cu2019' s D u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' t , which is a constant since we set our window size to one
p5003
aVWikipedia-based features are the co-occurrence counts and the PMI values between cause and effect nouns calculated using Wikipedia (as of 2013-Sep-19
p5004
aVWe also checked whether an Wikipedia article whose title is a cause (effect) noun contains its effect (cause) noun, as detailed in Section LABEL:A-sec:wikipedia-based-association-features in the supplementary notes
p5005
aVDefinition-based features , as detailed in Section LABEL:A-sec:definition-based-association-features in the supplementary notes, resemble the Wikipedia-based features except that the information source is the definition sentences automatically acquired from our 600 million web pages using the method of Hashimoto et al
p5006
aVWeb-based association measures were obtained from the same database as AC4 in Table 2
p5007
aVBase features represent the basic properties of event causality like nouns, templates, and their excitation polarities (See Section LABEL:A-sec:base-features in the supplementary notes
p5008
aVUsing the above features, a classifier 6 6 We used SVM l u'\u005cu2062' i u'\u005cu2062' g u'\u005cu2062' h u'\u005cu2062' t with the polynominal kernel ( d = 2 ), available at http://svmlight.joachims.org classifies each event causality candidate into causality and non-causality
p5009
aVEach event causality candidate may be given multiple original sentences, since a phrase pair can appear in multiple sentences, in which case it is given more than one SVM score
p5010
aVOur future scenario generation method creates scenarios by chaining event causalities
p5011
aVHowever, this approach would overlook many acceptable scenarios as discussed in Section 1
p5012
aVFor example, global warming worsens u'\u005cu2192' sea temperatures are high and sea temperatures rise u'\u005cu2192' vibrio parahaemolyticus fouls (water) can be chained to constitute an acceptable scenario, but the joint part is not the same string
p5013
aVAlthough we have no definite answer yet, we name it the causal-compatibility of two phrases and provide its preliminary characterization based on the excitation polarity
p5014
aVTwo phrases are causally-compatible if they mention the same entity (typically described by a noun) that is predicated by the templates of the same excitation polarity
p5015
aVIndeed, both X rise and X are high are excitatory and hence sea temperatures are high and sea temperatures rise are causally-compatible
p5016
aVScenarios ( s u'\u005cu2062' c s) generated by chaining causally-compatible phrase pairs are scored by S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' ( s u'\u005cu2062' c ) , which embodies our assumption that an acceptable scenario consists of plausible event causality pairs
p5017
aVAn original sentence filter removes a scenario if two event causality pairs that are chained in it are extracted from original sentences between which no word overlap exists other than words constituting causality pairs
p5018
aVA common argument filter removes a scenario if a joint part consists of two templates that share no argument in our u'\u005cu27e8' argument, template u'\u005cu27e9' database, which is compiled from the syntactic dependency data between arguments and templates extracted from our web corpus
p5019
aVNext we describe our experiments on event causality extraction and show (a) that most of our features are effective and (b) that our method outperforms the baselines based on state-of-the-art methods [ 8 , 12 ]
p5020
aVOur method achieved 70% precision at 13% recall; we can extract about 69,700 event causality pairs with 70% precision, as described below
p5021
aVFor the test data , we randomly sampled 23,650 examples of u'\u005cu27e8' event causality candidate, original sentence u'\u005cu27e9' among which 3,645 were positive from 2,451,254 event causality candidates extracted from our web corpus (Section 3.1
p5022
aVOur evaluation is based on average precision ; 9 9 It is obtained by computing the precision for each point in the ranked list where we find a positive sample and averaging all the precision figures [ 18 ] we believe that it is important to rank the plausible event causality candidates higher
p5023
aVThe performance was even worse when using no semantic relation ( u'\u005cu201c' None u'\u005cu201d' in Table 4
p5024
aVConsequently we conclude that not only semantic relations directly relevant to causality like Causation but also those that seem to lack direct relevance to causality like Material are somewhat effective
p5025
aVFinally, Table 5 shows the performance drop by removing the Wikipedia-, definition-, web-, and CEA-based features
p5026
aVWe compared our method and two baselines based on Do et al
p5027
aVCEA u u'\u005cu2062' n u'\u005cu2062' s is an unsupervised method that uses CEA to rank event causality candidates, and CEA s u'\u005cu2062' u u'\u005cu2062' p is a supervised method using SVM and the CEA features, whose ranking is based on the SVM scores
p5028
aVThe baselines are not complete implementations of Do et al u'\u005cu2019' s method which uses discourse relations identified based on Lin et al
p5029
aVNonetheless, we believe that this comparison is informative since CEA can be seen as the main component; they achieved a F1 of 41.7% for extracting causal event relations, but with only CEA they still achieved 38.6%
p5030
aVProposed is the best and the CEA features slightly contribute to the performance, as Proposed-CEA indicates
p5031
aVWe observed that CEA s u'\u005cu2062' u u'\u005cu2062' p and CEA u u'\u005cu2062' n u'\u005cu2062' s performed poorly and tended to favor event causality candidates whose phrase pairs were highly relevant to each other but described the contrasts of events rather than event causality (e.g., build a slow muscle and build a fast muscle ) probably because their main components are PMI values
p5032
aVNext we compared our method with the baselines based on Hashimoto et al
p5033
aVThey developed an automatic excitation template acquisition method that assigns each template an excitation value in range [ - 1 , 1 ] that is positive if the template is excitatory and negative if it is inhibitory
p5034
aVCs u u'\u005cu2062' n u'\u005cu2062' s is an unsupervised method that uses C u'\u005cu2062' s for ranking, and Cs s u'\u005cu2062' u u'\u005cu2062' p is a supervised method using SVM with C u'\u005cu2062' s as the only feature that uses SVM scores for ranking
p5035
aVNote that some event causality candidates were not given excitation values for their templates, since some templates were acquired by manual annotation without Hashimoto et al u'\u005cu2019' s method
p5036
aVSince Cs s u'\u005cu2062' u u'\u005cu2062' p performed slightly better when using all of the training data in our preliminary experiments, we used all of it
p5037
aVIts average precision is different from that in Table 6 due to the difference in test data described above
p5038
aVHowever, as described in Section 1 , our event causality criteria are different; since they regarded phrase pairs that were not self-contained as event causality (their annotators checked the original sentences of phrase pairs to see if they were event causality), their judgments tended to be more lenient than ours, which explains the performance difference
p5039
aVIn preliminary experiments, since our proposed method u'\u005cu2019' s performance degraded when C u'\u005cu2062' s was incorporated, we did not use it in our method
p5040
aVWe extracted 557 social problem nouns and used the cause phrases of the event causality candidates that consisted of one of the social problem nouns as the scenario u'\u005cu2019' s beginning event
p5041
aVWe applied our event causality extraction method to 2,451,254 candidates (Section 3.1 ) and culled the top 1,200,000 phrase pairs from them (See Section LABEL:A-sec:examples-of-event-causality in the supplementary notes for examples
p5042
aVThe samples were annotated by three annotators (not the authors), who were instructed to regard a sample as acceptable if each event causality that constitutes it is plausible and the sample as a whole constitutes a single coherent story
p5043
aVFinal judgment was made by majority vote
p5044
aVThis is probably because scenario judgment requires careful consideration about various possible futures for which individual annotators tend to draw different conclusions
p5045
aVThe estimated number is calculated as the product of the recall at 70% precision and the number of acceptable scenarios in all the generated scenarios, which is estimated by the annotated samples
p5046
aVThe curve is drawn in the same way as the precision-recall curve except that the X-axis indicates the estimated number of acceptable scenarios
p5047
aV22 among the 341 samples were non-trivial
p5048
aVAccordingly, we estimate that we can generate 2,200 ( 50 , 000 × 22 500 ) acceptable and non-trivial scenarios from the top 50,000
p5049
aV2013 ) that observed the emerging vibrio risk in the Baltic sea due to global warming
p5050
aVIn a sense, we u'\u005cu201c' predicted u'\u005cu201d' an event observed in 2013 from documents written in 2007, although the scenario was ranked as low as 240,738th
p5051
aVWe proposed a supervised method for event causality extraction that exploits semantic relation, context, and association features
p5052
asg88
(lp5053
sg90
(lp5054
sg92
(lp5055
VWe proposed a supervised method for event causality extraction that exploits semantic relation, context, and association features.
p5056
aVWe also proposed methods for our new task, future scenario generation.
p5057
aVThe methods chain event causality by causal-compatibility.
p5058
aVWe generated non-trivial scenarios with reasonable precision, and predicted future events from web documents.
p5059
aVIncreasing their rank is future work.
p5060
ag106
asg107
S'P14-1093'
p5061
sg109
(lp5062
VWe propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture exacerbate desertification from the web using semantic relation (between nouns), context, and association features.
p5063
aVExperiments show that our method outperforms baselines that are based on state-of-the-art methods.
p5064
aVWe also propose methods of generating future scenarios like conduct slash-and-burn agriculture exacerbate desertification increase Asian dust (from China) asthma gets worse.
p5065
aVExperiments show that we can generate 50,000 scenarios with 68% precision.
p5066
aVWe also generated a scenario deforestation continues global warming worsens sea temperatures rise vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus crawled in 2007.
p5067
aVBut the vibrio risk due to global warming was observed in Baker-Austin et al.
p5068
aV2013.
p5069
aVThus, we predicted the future event sequence in a sense.
p5070
aV[A-]acl2014-supplementary-cameraready.
p5071
ag106
asba(icmyPackage
FText
p5072
(dp5073
g3
(lp5074
VIn this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books
p5075
aVSubsequently, we compare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a particular sense has died
p5076
aVHowever, another equally important aspect that has not been so far well investigated corresponds to one or more changes that a word might undergo in its sense
p5077
aVThis particular aspect is getting increasingly attainable as more and more time-varying text data become available in the form of millions of digitized books [ Goldberg and Orwant2013 ] gathered over the last centuries
p5078
aVAs a motivating example one could consider the word u'\u005cu201c' sick u'\u005cu201d' u'\u005cu2013' while according to the standard English dictionaries the word is normally used to refer to some sort of illness, a new meaning of u'\u005cu201c' sick u'\u005cu201d' referring to something that is u'\u005cu201c' crazy u'\u005cu201d' or u'\u005cu201c' cool u'\u005cu201d' is currently getting popular in the English vernacular
p5079
aVThis change is further interesting because while traditionally u'\u005cu201c' sick u'\u005cu201d' has been associated to something negative in general, the current meaning associates positivity with it
p5080
aVToward this objective we make the following contributions a) devise a time-varying graph clustering based sense induction algorithm, (b) use the time-varying sense clusters to develop a split-join based approach for identifying new senses of a word, and (c) evaluate the performance of the algorithms on various datasets using different suitable approaches along with a detailed error analysis
p5081
aVRemarkably, comparison with the English WordNet indicates that in 44% cases, as identified by our algorithm, there has been a birth of a completely novel sense, in 46% cases a new sense has split off from an older sense and in 43% cases two or more older senses have merged in to form a new sense
p5082
aVIn Section 4 we present an approach based on graph clustering to identify the time-varying sense clusters and in Section 5 we present the split-merge based approach for tracking word sense changes
p5083
aVSome of the first attempts to automatic word sense discovery were made by Karen Spärck Jones [ Jones1986 ] ; later in lexicography, it has been extensively used as a pre-processing step for preparing mono- and multi-lingual dictionaries [ Kilgarriff and Tugwell2001 , Kilgarriff2004 ]
p5084
aVHowever, as we have already pointed out that none of these works consider the temporal aspect of the problem
p5085
aVOpinion formation deals with the self-organisation and emergence of shared vocabularies whereas our work focuses on how the different senses of these vocabulary words change over time and thus become u'\u005cu201c' out-of-vocabulary u'\u005cu201d'
p5086
aVTopic detection involves detecting the occurrence of a new event such as a plane crash, a murder, a jury trial result, or a political scandal in a stream of news stories from multiple sources and tracking is the process of monitoring a stream of news stories to find those that track (or discuss) the same event
p5087
aVThis is done on shorter timescales (hours, days), whereas our study focuses on larger timescales (decades, centuries) and we are interested in common nouns, verbs and adjectives as opposed to events that are characterized mostly by named entities
p5088
aVOther similar works on dynamic topic modelling can be found in [ Blei and Lafferty2006 , Wang and McCallum2006 ]
p5089
aVIt only reports frequency of word usage over the years, but does not give any correlation among them as e.g.,, in [ Heyer et al.2009 ] , and does not analyze their senses
p5090
aVA few approaches suggested by [ Bond et al.2009 , Pääkkö and Lindén2012 ] attempt to augment WordNet synsets primarily using methods of annotation
p5091
aV[ Cook et al.2013 ] attempts to induce word senses and then identify novel senses by comparing two different corpora the u'\u005cu201c' focus corpora u'\u005cu201d' (i.e.,, a recent version of the corpora) and the u'\u005cu201c' reference corpora u'\u005cu201d' (older version of the corpora
p5092
aVHowever, this method is limited as it only considers two time points to identify sense changes as opposed to our approach which is over a much larger timescale, thereby, effectively allowing us to track the points of change and the underlying causes
p5093
aVIn this approach, we first extract each word and a set of its context features, which are formed by labeled and directed dependency parse edges as provided in the dataset
p5094
aVFollowing this, we compute the frequencies of the word, the context and the words along with their context
p5095
aVIf a word undergoes a sense change, this can be detected by comparing its senses obtained from two different time periods
p5096
aVSince we aim to detect this change automatically, we require distributional representations corresponding to word senses for different time periods
p5097
aVWe, therefore, utilize the basic hypothesis of unsupervised sense induction to induce the sense clusters over various time periods and then compare these clusters to detect sense change
p5098
aVThe algorithm, in particular, produces a set of clusters for each target word by decomposing its open neighborhood
p5099
aVIf a word undergoes sense change, this can be detected by comparing the sense clusters obtained from two different time periods by the algorithm outlined above
p5100
aVWe next describe our algorithm for detecting sense change from these sets of sense clusters
p5101
aVWe hypothesize that word w can undergo sense change from one time interval ( t u'\u005cu2062' v i ) to another ( t u'\u005cu2062' v j ) as per one of the following scenarios
p5102
aVA sense cluster s i u'\u005cu2062' z in t u'\u005cu2062' v i splits into two (or more) sense clusters, s j u'\u005cu2062' p 1 and s j u'\u005cu2062' p 2 in t u'\u005cu2062' v j
p5103
aVWe append an additional row and column to capture the fraction of words, which did not show up in any of the sense clusters in another time interval
p5104
aVSo, an element I k u'\u005cu2062' l of the matrix
p5105
aV1 u'\u005cu2264' k u'\u005cu2264' m , l = n + 1 denotes the fraction of words in the sense cluster s i u'\u005cu2062' k , that did not show up in any of the n clusters in t u'\u005cu2062' v j
p5106
aVThus, the matrix I captures all the four possible scenarios for sense change
p5107
aVSince we can not expect a perfect split, birth etc., we used certain threshold values to detect if a candidate word is undergoing sense change via one of these four cases
p5108
aVIn Figure 1 , as an example, we illustrate the birth of a new sense for the word u'\u005cu2018' compiler u'\u005cu2019'
p5109
aVFrom the above list, we retain only those candidate words, which have a part-of-speech tag u'\u005cu2018' NN u'\u005cu2019' or u'\u005cu2018' NNS u'\u005cu2019' , as we focus on nouns for this work
p5110
aVWe sort the candidate list obtained in Stage 2 as per their occurrence in the first time period
p5111
aVThen, we remove the top 20 u'\u005cu2062' % and the bottom 20 u'\u005cu2062' % words from this list
p5112
aVTherefore, we consider the torso of the frequency distribution which is the most informative part for this type of an analysis
p5113
aVAn element of the table shows the number of candidate words obtained by comparing the corresponding source and target time periods
p5114
aVFor most of the cases, the number of candidate birth senses tends to increase as we go from left to right
p5115
aVSimilarly, this number decreases as we go down in the table
p5116
aVThis is quite intuitive since going from left to right corresponds to increasing the gap between two time periods while going down corresponds to decreasing this gap
p5117
aVAs the gap increases (decreases), one would expect more (less) new senses coming in
p5118
aVEven while moving diagonally, the candidate words tend to decrease as we move downwards
p5119
aVThis corresponds to the fact that the number of years in the time periods decreases as we move downwards, and therefore, the gap also decreases
p5120
aVFormally, we consider a sense change from t u'\u005cu2062' v i to t u'\u005cu2062' v j stable if it was also detected while comparing t u'\u005cu2062' v i with the following time periods t u'\u005cu2062' v k s
p5121
aVSimilarly, for a candidate sense change from t u'\u005cu2062' v i to t u'\u005cu2062' v j , we say that the location of the sense change is t u'\u005cu2062' v j if and only if that sense change does not get detected by comparing t u'\u005cu2062' v i with any time interval t u'\u005cu2062' v k , intermediate between t u'\u005cu2062' v i and t u'\u005cu2062' v j
p5122
aVHowever, not all the candidate words were stable
p5123
aVThus, it was important to prune these results using stability analysis
p5124
aVFor instance, among the 4238 candidate birth sense detected by comparing T 1 and T 6 , many of these new senses might have come up in between T 2 to T 5 as well
p5125
aVWe prune these lists further based on the stability of the sense, as well as to locate the approximate time interval, in which the sense change might have occurred
p5126
aVOnce we were able to locate the senses as well as to find the age of the senses, we attempted to select some representative words and plotted them on a timeline as per the birth period and their age in Figure 2
p5127
aVDuring evaluation, we considered the clusters obtained using the 1909-1953 time-slice as our reference and attempted to track sense change by comparing these with the clusters obtained for 2002-2005
p5128
aVSince it was difficult to go through all the candidate sense changes for all the comparisons manually, we decided to randomly select some candidate words, which were flagged by our algorithm as undergoing sense change, while comparing 1909-1953 and 2002-2005 DT
p5129
aVOne of the authors annotated each of the birth cases identifying whether or not the algorithm signalled a true sense change while another author did the same task for the split/join cases
p5130
aVThe accuracy as per manual evaluation was found to be 60.4% for the birth cases and 57% for the split/join cases
p5131
aVTable 3 shows the evaluation results for a few candidate words, flagged due to birth
p5132
aVColumns correspond to the candidate words, words obtained in the cluster of each candidate word (we will use the term u'\u005cu2018' birth cluster u'\u005cu2019' for these words, henceforth), which indicated a new sense, the results of manual evaluation as well as the possible sense this birth cluster denotes
p5133
aVA further analysis of the words marked due to birth in the random samples indicates that there are 22 technology-related words, 2 slangs, 3 economics related words and 2 general words
p5134
aVFor the split-join case we found that there are 3 technology-related words while the rest of the words are general
p5135
aVTherefore one of the key observations is that most of the technology related words (where the neighborhood is completely new) could be extracted from our birth results
p5136
aVIn contrast, for the split-join instances most of the results are from the general category since the neighborhood did not change much here; it either got split or merged from what it was earlier
p5137
aVWe chose WordNet for automated evaluation because not only does it have a wide coverage of word senses but also it is being maintained and updated regularly to incorporate new senses
p5138
aVWe did this evaluation for the candidate birth, join and split sense clusters obtained by comparing 1909-1953 time period with respect to 2002-2005
p5139
aVThe CW cluster is then aligned to WordNet synsets by comparing the clusters with WordNet graph and the synset with the maximum alignment score is returned as the output
p5140
aVIn summary, the aligner tool takes as input the CW cluster and returns a WordNet synset id that corresponds to the cluster words
p5141
aVFor a candidate word flagged as birth, we first find out the set of all WordNet synset ids for its CW clusters in the source time period (1909-1953 in this case
p5142
aVThen, if s n u'\u005cu2062' e u'\u005cu2062' w u'\u005cu2209' S i u'\u005cu2062' n u'\u005cu2062' i u'\u005cu2062' t , it implies that this is a new sense that was not present in the source clusters and we call it a u'\u005cu2018' success u'\u005cu2019' as per WordNet
p5143
aVFor the join case, we find WordNet synset ids s 1 and s 2 for the clusters obtained in the source time period and s n u'\u005cu2062' e u'\u005cu2062' w for the join cluster in the target time period
p5144
aVWe then manually verified some of the words that were deemed as successes, as well as investigated WordNet sense they were mapped to
p5145
aVNew slang words come up every now and then, and this plays an integral part in the phenomena of sense change
p5146
aVWe therefore decided to perform an evaluation as to how many slang words were being detected by our candidate birth clusters
p5147
aVMuch of our evaluation was focussed on the birth sense clusters, mainly because these are more interesting from a lexicographic perspective
p5148
aVTo detect a true death of a sense, persistence analysis was required, that is, to verify if the sense was persisting earlier and vanished after a certain time period
p5149
aVIn this paper, we presented a completely unsupervised method to detect word sense changes by analyzing millions of digitized books archived spanning several centuries
p5150
aVThese results might have strong lexicographic implications u'\u005cu2013' even if one goes by very moderate estimates almost half of the words would be candidate entries in WordNet if they were not already part of it
p5151
aVThis method can be extremely useful in the construction of lexico-semantic networks for low-resource languages, as well as for keeping lexico-semantic resources up to date in general
p5152
aVFuture research directions based on this work are manifold
p5153
aVOn one hand, our method can be used by lexicographers in designing new dictionaries where candidate new senses can be semi-automatically detected and included, thus greatly reducing the otherwise required manual effort
p5154
aVAM would like to thank DAAD for supporting the faculty exchange programme to TU Darmstadt
p5155
aVPG would like to thank Google India Private Ltd for extending travel support to attend the conference
p5156
asg88
(lp5157
sg90
(lp5158
sg92
(lp5159
VIn this paper, we presented a completely unsupervised method to detect word sense changes by analyzing millions of digitized books archived spanning several centuries.
p5160
aVIn particular, we constructed DT networks over eight different time windows, clustered these networks and compared these clusters to identify the emergence of novel senses.
p5161
aVThe performance of our method has been evaluated manually as well as by comparison with WordNet and a list of slang words.
p5162
aVThrough manual evaluation we found that the algorithm could correctly identify 60.4% birth cases from a set of 48 random samples and 57% split/join cases from a set of 21 randomly picked samples.
p5163
aVQuite strikingly, we observe that (i) in 44% cases the birth of a novel sense is attested by WordNet, (ii) in 46% cases the split of an older sense is signalled on comparison with WordNet and (iii) in 43% cases the join of two senses is attested by WordNet.
p5164
aVThese results might have strong lexicographic implications even if one goes by very moderate estimates almost half of the words would be candidate entries in WordNet if they were not already part of it.
p5165
aVThis method can be extremely useful in the construction of lexico-semantic networks for low-resource languages, as well as for keeping lexico-semantic resources up to date in general.
p5166
aVFuture research directions based on this work are manifold.
p5167
aVOn one hand, our method can be used by lexicographers in designing new dictionaries where candidate new senses can be semi-automatically detected and included, thus greatly reducing the otherwise required manual effort.
p5168
aVOn the other hand, this method can be directly used for various NLP/IR applications like semantic search, automatic word sense discovery as well as disambiguation.
p5169
aVFor semantic search, taking into account the newer senses of the word can increase the relevance of the query result.
p5170
aVSimilarly, a disambiguation engine informed with the newer senses of a word can increase the efficiency of disambiguation, and recognize senses uncovered by the inventory that would otherwise have to be wrongly assigned to covered senses.
p5171
aVIn addition, this method can be also extended to the NNP part-of-speech (i.e.,, named entities) to identify changes in role of a person/place.
p5172
aVFurthermore, it would be interesting to apply this method to languages other than English and to try to align new senses of cognates across languages.
p5173
ag106
asg107
S'P14-1096'
p5174
sg109
(lp5175
VIn this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books.
p5176
aVWe construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points.
p5177
aVSubsequently, we compare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a particular sense has died.
p5178
aVWe conduct a thorough evaluation of the proposed methodology both manually as well as through comparison with WordNet.
p5179
aVManual evaluation indicates that the algorithm could correctly identify 60.4% birth cases from a set of 48 randomly picked samples and 57% split/join cases from a set of 21 randomly picked samples.
p5180
aVRemarkably, in 44% cases the birth of a novel sense is attested by WordNet, while in 46% cases and 43% cases split and join are respectively confirmed by WordNet.
p5181
aVOur approach can be applied for lexicography, as well as for applications like word sense disambiguation or semantic search.
p5182
ag106
asba(icmyPackage
FText
p5183
(dp5184
g3
(lp5185
VWe present an unsupervised method for inducing verb classes from verb uses in giga-word corpora
p5186
aVOur method consists of two clustering steps verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames
p5187
aVBy taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering
p5188
aVThe effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data
p5189
aVCapturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP
p5190
aVVerb classes are one such lexical resource
p5191
aVThis monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses
p5192
aVIn this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy
p5193
aVOur method consists of two clustering steps verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames
p5194
aVBy taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy
p5195
aV2009 ) proposed a Dirichlet process mixture model (DPMM; Neal ( 2000 ) ) to cluster verbs based on subcategorization frame distributions
p5196
aV2006 ) ) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features
p5197
aVParisien and Stevenson ( 2011 ) extended their model by adding semantic features
p5198
aV2012 ) extended the model of Titov and Klementiev ( 2012 ) , which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process [ 1 ]
p5199
aVIn particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method [ 43 ]
p5200
aVHowever, the verb itself is still represented as a single data point
p5201
aVAfter performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering
p5202
aVLapata and Brew ( 2004 ) and Li and Brew ( 2007 ) proposed probabilistic models for calculating prior probabilities of verb classes for a verb
p5203
aVThese models are approximated to condition not on verbs but on subcategorization frames
p5204
aVAs mentioned in Li and Brew ( 2007 ) , it is desirable to extend the model to depend on verbs to further improve accuracy
p5205
aV2008 ) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet
p5206
aVThis model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle
p5207
aVSince they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure
p5208
aVHe used a model based on latent Dirichlet allocation (LDA; Blei et al
p5209
aVBoth of these are represented as a probabilistic distribution of words across verbs
p5210
aVHe applied this method to the BNC and acquired 1,200 frames and 400 roles [ 21 ]
p5211
aVAlthough Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost
p5212
aVIn this paper, we propose a two-step approach for inducing semantic frames and verb classes
p5213
aVFirst, we make multiple data points for each verb to deal with verb polysemy (cf polysemy-aware previous studies still represented a verb as one data point [ 14 , 23 ]
p5214
aVTo do that, we induce verb-specific semantic frames by clustering verb uses
p5215
aVThen, we induce verb classes by clustering these verb-specific semantic frames across verbs
p5216
aVinduce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1 , and
p5217
aVinduce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1
p5218
aVWe induce verb-specific semantic frames from verb uses based on the method of Kawahara et al
p5219
aVmerge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation [ 46 ] to get a set of initial frames, and
p5220
aVapply clustering to the initial frames based on the Chinese Restaurant Process [ 1 ] to produce verb-specific semantic frames
p5221
aVWe select an argument in the following order by considering the degree of effect on the verb sense
p5222
aV3 3 If a predicate-argument structure has multiple prepositional phrases, one of them is randomly selected
p5223
aVP ( f i c j ) is defined based on the Dirichlet-Multinomial distribution as follows
p5224
aVThe original method in Kawahara et al
p5225
aV2014 ) defined w as pairs of slots and words, e.g.,, u'\u005cu201c' nsubj:child u'\u005cu201d' and u'\u005cu201c' dobj:bird, u'\u005cu201d' but does not consider slot-only features, e.g.,, u'\u005cu201c' nsubj u'\u005cu201d' and u'\u005cu201c' dobj, u'\u005cu201d' which ignore lexical information
p5226
aVWe regard each output cluster as a semantic frame, by merging the initial frames in a cluster into a semantic frame
p5227
aVWe can use exactly the same clustering method as described in Section 3.2.3 by using semantic frames for multiple verbs as an input instead of initial frames for a single verb
p5228
aVThis is because an initial frame has the same structure as a semantic frame, which is produced by merging initial frames
p5229
aVWe regard each output cluster as a verb class this time
p5230
aVFor the features, w , in equation ( 2 ), we try the two representations again slot-only features and slot-word pair features
p5231
aVThe other representation using the slot-word pairs means that semantic similarity based on word overlap is naturally considered by looking at lexical information
p5232
aVThese two levels of evaluations are performed by considering the work of Reichart et al
p5233
aVTo prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information
p5234
aVHowever, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes
p5235
aV2003 ) evaluated hard clusterings based on a gold standard with multiple classes per verb
p5236
aVIn addition, to penalize clusters that consist of only one verb, such singleton clusters in K are considered as errors, as is usual with modified purity
p5237
aVThe normalized modified purity (nmPU) can then be written as follows
p5238
aVK i denotes the number of positive components in K i , and c i u'\u005cu2062' v denotes the v -th component of K i u'\u005cu0394' K i u'\u005cu2062' ( K i u'\u005cu2229' G j ) means the total mass of the set of verbs in K i u'\u005cu2229' G j , given by summing up the values in K i
p5239
aVK i u'\u005cu2229' G j because all the values of c i u'\u005cu2062' v are equal to 1
p5240
aVAs usual, the following normalized inverse purity (niPU) is used to measure the recall of a clustering
p5241
aVFinally, we use the harmonic mean (F 1 ) of nmPU and niPU as a single measure of clustering quality
p5242
aVWe first evaluate our induced verb classes on the test set created by Korhonen et al
p5243
aV2003 ) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin u'\u005cu2019' s classes and the LCS database [ 6 ]
p5244
aVAn excerpt from this data is shown in Table 1
p5245
aVAs our baselines, we adopt two previously proposed methods
p5246
aV2003 ) , prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP_PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb
p5247
aVIt is necessary to specify the number of clusters, k , for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies
p5248
aVThe results of the IB baseline and our methods are obtained by averaging five runs
p5249
aVWe can see that u'\u005cu201c' web/SW-S u'\u005cu201d' achieved the best performance and obtained a higher F 1 than the baselines by more than nine points u'\u005cu201c' Web/SW-S u'\u005cu201d' uses the combination of slot-word pair features for clustering verb-specific frames and slot-only features for clustering across verbs
p5250
aVInterestingly, this result indicates that slot distributions are more effective than lexical information in slot-word pairs for inducing verb classes similar to the gold standard
p5251
aVThis result is consistent with expectations, given a gold standard based on Levin u'\u005cu2019' s verb classes, which are organized according to the syntactic behavior of verbs
p5252
aVSince we focus on the handling of verb polysemy, predominant class induction for each verb is not our main objective
p5253
aVTo output a single class for each verb by using our proposed method, we skip the induction of verb-specific semantic frames and instead create a single frame for each verb by merging all predicate-argument structures of the verb
p5254
aVWe evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al
p5255
aV2003 ) , using the gold standard with multiple classes, which we also use for our multi-class evaluations
p5256
aVAs we did with the multi-class evaluations, we adopt modified purity (mPU), inverse purity (iPU) and their harmonic mean (F 1 ) as the metrics for the evaluation with predominant classes
p5257
aVIt is not necessary to normalize these metrics when we treat verbs as monosemous, and evaluate against the predominant sense
p5258
aVWhen we evaluate against the multiple classes in the gold standard, we do normalize the inverse purity
p5259
aVThe clusterings with the NN and IB methods are obtained by using the VALEX subcategorization lexicon
p5260
aVNote that our results of the NN and IB methods are different from those reported in their paper since the data source is different
p5261
aVFrom the result, we can see that the induced verb classes based on slot-only features did not achieve a higher F 1 than those based on slot-word pair features in many cases
p5262
aVWe speculate that slot distributions are not so different among verbs when all uses of a verb are merged into one frame, and thus their discrimination power is lower than that in the intermediate construction of semantic frames
p5263
aVIt is not necessary to normalize these metrics because the clustering of these instances is hard
p5264
aVThe results of these methods are obtained by averaging five runs
p5265
aV9 9 Since FrameNet frames are not assigned to all verbs of SemLink, the number of verbs is different from the evaluations against VerbNet classes
p5266
aVBased on the best results in the above evaluations, we induced semantic frames using slot-word pair features, and then induced verb classes using slot-only features
p5267
aVFor instance, u'\u005cu201c' Class 2 u'\u005cu201d' consists of the semantic frames u'\u005cu201c' need:2 u'\u005cu201d' and u'\u005cu201c' say:2 u'\u005cu201d' These frames were merged due to the high syntactic similarity of constituting slot distributions, which are comprised of a subject and a sentential complement
p5268
aVWe presented a step-wise unsupervised method for inducing verb classes from instances in giga-word corpora
p5269
aVBoth clustering steps are performed with exactly the same method, which is based on the Chinese Restaurant Process
p5270
aVFrom the results, we can see that the combination of the slot-word pair features for clustering verb-specific frames and the slot-only features for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points
p5271
aVThis indicates that slot distributions are more effective than lexical information in slot-word pairs for the induction of verb classes, when Levin-style classes are used for evaluation
p5272
aVThis is consistent with Levin u'\u005cu2019' s principle of organizing verb classes according to the syntactic behavior of verbs
p5273
aVAs applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation
p5274
aVFor instance, Kawahara and Kurohashi ( 2006 ) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus
p5275
asg88
(lp5276
sg90
(lp5277
sg92
(lp5278
VWe presented a step-wise unsupervised method for inducing verb classes from instances in giga-word corpora.
p5279
aVThis method first clusters predicate-argument structures to induce verb-specific semantic frames and then clusters these semantic frames across verbs to induce verb classes.
p5280
aVBoth clustering steps are performed with exactly the same method, which is based on the Chinese Restaurant Process.
p5281
aVThe resulting semantic frames and verb classes are open to the public and also can be searched via our web interface.
p5282
aV10 10 http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/.
p5283
aVFrom the results, we can see that the combination of the slot-word pair features for clustering verb-specific frames and the slot-only features for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points.
p5284
aVThis indicates that slot distributions are more effective than lexical information in slot-word pairs for the induction of verb classes, when Levin-style classes are used for evaluation.
p5285
aVThis is consistent with Levin s principle of organizing verb classes according to the syntactic behavior of verbs.
p5286
aVAs applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation.
p5287
aVFor instance, Kawahara and Kurohashi ( 2006 ) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus.
p5288
aVIt is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification [ 34 ] and argumentative zoning [ 8 ].
p5289
ag106
asg107
S'P14-1097'
p5290
sg109
(lp5291
VWe present an unsupervised method for inducing verb classes from verb uses in giga-word corpora.
p5292
aVOur method consists of two clustering steps verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.
p5293
aVBy taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering.
p5294
aVIn our experiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words.
p5295
aVThe effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data.
p5296
ag106
asba(icmyPackage
FText
p5297
(dp5298
g3
(lp5299
VThe main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples
p5300
aVCognitively, it is more plausible to assume that children obtain only terminal strings of parse trees and not the actual parse trees
p5301
aVThis means the unsupervised setting is a better model for studying language acquisition
p5302
aVMost existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g., probabilistic context free grammars [ Jelinek et al.1992 ] , and the constituent context model [ Klein and Manning2002 ]
p5303
aVLearning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood [ Klein and Manning2002 ] or a variant of it [ Smith and Eisner2005 , Cohen and Smith2009 , Headden et al.2009 , Spitkovsky et al.2010b , Gillenwater et al.2010 , Golland et al.2012 ]
p5304
aVUnfortunately, finding the global maximum for these objective functions is usually intractable [ Cohen and Smith2012 ] which often leads to severe local optima problems (but see Gormley and Eisner, 2013
p5305
aVThus, strong experimental results are often achieved by initialization techniques [ Klein and Manning2002 , Gimpel and Smith2012 ] , incremental dataset use [ Spitkovsky et al.2010a ] and other specialized techniques to avoid local optima such as count transforms [ Spitkovsky et al.2013 ]
p5306
aVHowever, due to the presence of latent variables, structure learning of latent trees is substantially more complicated than in observed models
p5307
aVIntuitively, however, latent tree models encode low rank dependencies among the observed variables permitting the development of u'\u005cu201c' spectral u'\u005cu201d' methods that can lead to provably correct solutions
p5308
aVIn particular we leverage the concept of additive tree metrics [ Buneman1971 , Buneman1974 ] in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies [ Choi et al.2011 , Song et al.2011 , Anandkumar et al.2011 , Ishteva et al.2012 ]
p5309
aVAdditive tree metrics can be leveraged by u'\u005cu201c' meta-algorithms u'\u005cu201d' such as neighbor-joining [ Saitou and Nei1987 ] and recursive grouping [ Choi et al.2011 ] to provide consistent learning algorithms for latent trees
p5310
aVMoreover, we show that it is desirable to learn the u'\u005cu201c' minimal u'\u005cu201d' latent tree based on the tree metric ( u'\u005cu201c' minimum evolution u'\u005cu201d' in phylogenetics
p5311
aVUnlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree
p5312
aVThis leads to a severe data sparsity problem even for moderately long sentences
p5313
aVHowever, the choice of verb ( w 1 ) is mostly independent of the determiner
p5314
aVWe could thus conclude that w 2 and w 3 should be closer in the parse tree than w 1 and w 2 , giving us the correct structure
p5315
aVFollowing this intuition, we propose to model the distribution over the latent bracketing states and words for each tag sequence u'\u005cud835' u'\u005cudc99' as a latent tree graphical model, which encodes conditional independences among the words given the latent states
p5316
aVThe model assumes a factorization according to a latent-variable tree
p5317
aVThe latent variables can incorporate various linguistic properties, such as head information, valence of dependency being generated, and so on
p5318
aVThis information is expected to be learned automatically from data
p5319
aVThe orientation of the tree is determined by a direction mapping h dir u'\u005cu2062' ( u ) , which is fixed during learning and decoding
p5320
aVThis means our decoder first identifies (given a POS sequence) an undirected tree, and then orients it by applying h dir on the resulting tree (see below
p5321
aVDecide on u u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) u'\u005cu2208' u'\u005cud835' u'\u005cudcb0' , the undirected latent tree that u'\u005cud835' u'\u005cudc99' maps to
p5322
aVGenerate a tuple u'\u005cud835' u'\u005cudc97' = ( w 1 , u'\u005cu2026' , w u'\u005cu2113' , z 1 , u'\u005cu2026' , z H ) where w i u'\u005cu2208' u'\u005cu211d' p , z j u'\u005cu2208' u'\u005cu211d' m according to Eq
p5323
aVGenerating a bracketing via an undirected tree enables us to build on existing methods for structure learning of latent-tree graphical models [ Choi et al.2011 , Anandkumar et al.2011 ]
p5324
aVThis undirected tree is converted into a directed tree by applying h dir
p5325
aVIt marks the edge e i , j that splits the tree according to the top bracket as the u'\u005cu201c' root edge u'\u005cu201d' (marked in red in Figure 1 (center
p5326
aVIt then creates t from u by directing the tree outward from e i , j as shown in Figure 1 (center
p5327
aVThe resulting t is a binary bracketing parse tree
p5328
aVAs indicated in the above section, we restrict the set of undirected trees to be those such that after applying h dir the resulting t is projective i.e., there are no crossing brackets
p5329
aVWe can then proceed by learning how to map a POS sequence u'\u005cud835' u'\u005cudc99' to a tree t u'\u005cu2208' u'\u005cud835' u'\u005cudcaf' (through u u'\u005cu2208' u'\u005cud835' u'\u005cudcb0' ) by focusing only on examples in u'\u005cud835' u'\u005cudc9f' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' )
p5330
aVIf all the variables were observed, then the Chow-Liu algorithm [ Chow and Liu1968 ] could be used to find the most likely tree structure u u'\u005cu2208' u'\u005cud835' u'\u005cudcb0'
p5331
aVFor example, as we see in § 3.2 we will define the distance d u'\u005cu2062' ( i , j ) to be a function of the covariance matrix u'\u005cud835' u'\u005cudd3c' [ v i v j u'\u005cu22a4' u ( u'\u005cud835' u'\u005cudc99' ) , u'\u005cu0398' ( u'\u005cud835' u'\u005cudc99' ) ]
p5332
aVThus if v i and v j are both observed variables, the distance can be directly computed from the data
p5333
aV[ M ] × [ M ] u'\u005cu2192' u'\u005cu211d' is an additive tree metric [ Erdõs et al.1999 ] for the undirected tree u u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' ) if it is a distance metric, 2 2 This means that it satisfies d u'\u005cu2062' ( i , j ) = 0 if and only if i = j , the triangle inequality and is also symmetric and furthermore, u'\u005cu2200' i , j u'\u005cu2208' [ M ] the following relation holds
p5334
aVwhere path u u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' ) u'\u005cu2062' ( i , j ) is the set of all the edges in the (undirected) path from i to j in the tree u u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' )
p5335
aVAs we describe below, given the tree structure, the additive tree metric property allows us to compute u'\u005cu201c' backwards u'\u005cu201d' the distances among the latent variables as a function of the distances among the observed variables
p5336
aVIn addition, since u u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) is assumed to be known from context, we denote d u u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) u'\u005cu2062' ( i , j ) just by d u'\u005cu2062' ( i , j )
p5337
aVGiven the fact that the distance between a pair of nodes is a function of the random variables they represent (according to the true model), only u'\u005cud835' u'\u005cudc6b' W u'\u005cu2062' W can be empirically estimated from data
p5338
aVHowever, if the underlying tree structure is known, then Definition 1 can be leveraged to compute u'\u005cud835' u'\u005cudc6b' Z u'\u005cu2062' Z and u'\u005cud835' u'\u005cudc6b' Z u'\u005cu2062' W as we show below
p5339
aVIt then follows that the other elements of the distance matrix can be computed based on Definition 1
p5340
aVThen using path additivity (Definition 1 ), it can be shown that for any a u'\u005cu2217' u'\u005cu2208' A u'\u005cu2217' , b u'\u005cu2217' u'\u005cu2208' B u'\u005cu2217' it holds that
p5341
aVEmpirically, one can obtain a more robust empirical estimate d ^ u'\u005cu2062' ( i , j ) by averaging over all valid choices of a u'\u005cu2217' , b u'\u005cu2217' in Eq
p5342
aVIf w i and z i were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m
p5343
aVFurthermore, Assumption 1 makes it explicit that regardless of the size of p , the relationships among the variables in the latent tree are restricted to be of rank m , and are thus low rank since p m
p5344
aVIf Assumption 1 holds then, d spectral is an additive tree metric (Definition 1
p5345
aVFrom here, we use d to denote d spectral , since that is the metric we use for our learning algorithm
p5346
aVIt has been shown [ Rzhetsky and Nei1993 ] that for any additive tree metric, u u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) can be recovered by solving arg u'\u005cu2062' min u u'\u005cu2208' u'\u005cud835' u'\u005cudcb0' u'\u005cu2061' c u'\u005cu2062' ( u ) for c u'\u005cu2062' ( u )
p5347
aVNote that the metric d we use in defining c u'\u005cu2062' ( u ) is based on the expectations from the true distribution
p5348
aVIn practice, the true distribution is unknown, and therefore we use an approximation for the distance metric d ^
p5349
aVAs we discussed in § 3.1 all elements of the distance matrix are functions of observable quantities if the underlying tree u is known
p5350
aVHowever, if we restrict u to be in u'\u005cud835' u'\u005cudcb0' , as we do in the above, then maximizing c ^ u'\u005cu2062' ( u ) over u'\u005cud835' u'\u005cudcb0' can be solved using the bilexical parsing algorithm from Eisner and Satta1999
p5351
aVThis is because the computation of the other sub-blocks of the distance matrix only depend on the partitions of the nodes shown in Figure 3 into A , B , G , and H , and not on the entire tree structure
p5352
aVTherefore, the procedure to find a bracketing for a given POS tag u'\u005cud835' u'\u005cudc99' is to first estimate the distance matrix sub-block u'\u005cud835' u'\u005cudc6b' ^ W u'\u005cu2062' W from raw text data (see § 3.4 ), and then solve the optimization problem arg u'\u005cu2062' min u u'\u005cu2208' u'\u005cud835' u'\u005cudcb0' u'\u005cu2061' c ^ u'\u005cu2062' ( u ) using a variant of the Eisner-Satta algorithm where c ^ u'\u005cu2062' ( u ) is identical to c u'\u005cu2062' ( u ) in Eq
p5353
aVFirst an undirected u u'\u005cu2208' u'\u005cud835' u'\u005cudcb0' is generated (only as a function of the POS tags), and then u is mapped to a bracketing using a direction mapping h dir
p5354
aVWe then showed that we can define a distance metric between nodes in the undirected tree, such that minimizing it leads to a recovery of u
p5355
aVIf the true distance metric is known, with respect to the true distribution that generates the words in a sentence, then u can be fully recovered by optimizing the cost function c u'\u005cu2062' ( u
p5356
aVWe now address the data sparsity problem, in particular that u'\u005cud835' u'\u005cudc9f' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) can be very small, and therefore estimating d for each POS sequence separately can be problematic
p5357
aVThe local syntactic context acts as an u'\u005cu201c' anchor, u'\u005cu201d' which enhances or replaces a word index in a sentence with local syntactic context
p5358
aVInstead of computing this block by computing the empirical covariance matrix for positions ( j , k ) in the data u'\u005cud835' u'\u005cudc9f' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) , the algorithm uses all of the pairs ( j u'\u005cu2032' , k u'\u005cu2032' ) from all of N training examples
p5359
aVOnce the empirical estimates for the covariance matrices are obtained, a variant of the Eisner-Satta algorithm is used, as mentioned in § 3.3
p5360
aVOur main theoretical guarantee is that Algorithm 1 will recover the correct tree u u'\u005cu2208' u'\u005cud835' u'\u005cudcb0' with high probability, if the given top bracket is correct and if we obtain enough examples ( u'\u005cud835' u'\u005cudc98' ( i ) , u'\u005cud835' u'\u005cudc99' ( i ) ) from the model in §2
p5361
aVDenote u'\u005cu03a3' u'\u005cud835' u'\u005cudc99' u'\u005cu2062' ( j , k ) ( r ) as the r t u'\u005cu2062' h singular value of u'\u005cud835' u'\u005cudeba' u'\u005cud835' u'\u005cudc99' u'\u005cu2062' ( j , k
p5362
aVLet u'\u005cu03a3' u'\u005cu2217' u'\u005cu2062' ( x ) := min j , k u'\u005cu2208' u'\u005cu2113' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) u'\u005cu2061' min u'\u005cu2061' ( u'\u005cu03a3' u'\u005cud835' u'\u005cudc99' u'\u005cu2062' ( j , k ) ( m ) )
p5363
aVDefine u ^ as the estimated tree for tag sequence u'\u005cud835' u'\u005cudc31' and u u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' ) as the correct tree
p5364
aVwhere u'\u005cu039d' u'\u005cud835' u'\u005cudc99' u'\u005cu2062' ( u'\u005cu0393' ) , defined in the supplementary, is a function of the underlying distribution over the tag sequences u'\u005cud835' u'\u005cudc99' and the kernel bandwidth u'\u005cu0393'
p5365
aVThus, the sample complexity of our approach depends on the dimensionality of the latent and observed states ( m and p ), the underlying singular values of the cross-covariance matrices ( u'\u005cu03a3' u'\u005cu2217' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) ) and the difference in the cost of the true tree compared to the cost of the incorrect trees ( u'\u005cu25b3' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' )
p5366
aVIf no verb exists, return ( [ 0 , 1 ] , [ 1 , u'\u005cu2113' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) ] )
p5367
aVAs mentioned earlier, each w i can be an arbitrary feature vector
p5368
aVThe OSCCA embeddings behaved better, so we only report its results
p5369
aVFor CCM, we found that if the full dataset (all sentence lengths) is used in training, then performance degrades when evaluating on sentences of length u'\u005cu2264' 10
p5370
aVWe therefore restrict the data used with CCM to sentences of length u'\u005cu2264' u'\u005cu2113' , where u'\u005cu2113' is the maximal sentence length being evaluated
p5371
aVThis does not happen with our algorithm, which manages to leverage lexical information whenever more data is available
p5372
aVWe therefore use the full data for our method for all lengths
p5373
aVFor CCM, we also experimented with the original parts of speech, universal tags (CCM-U), the cross-product of the original parts of speech with the Brown clusters (CCM-OB), and the cross-product of the universal tags with the Brown clusters (CCM-UB
p5374
aVThe results in Table 1 indicate that the vanilla setting is the best for CCM
p5375
aVThus, for all results, we use universal tags for our method and the original POS tags for CCM
p5376
aVWe also tried letting CCM choose different hyperparameters for different sentence lengths based on dev-set likelihood, but this gave worse results than holding them fixed
p5377
aVNN, CC, and BC indicate the performance of our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for h dir described in § 4.1
p5378
aVFor our method, test set results can be obtained by using Algorithm 3.3 (except the distances are computed using the training data
p5379
aVWe didn u'\u005cu2019' t have neural embeddings for German and Chinese (which worked best for English) and thus only used Brown cluster embeddings
p5380
aVHowever, for German and Chinese note that the u'\u005cu201c' BC-O u'\u005cu201d' performs substantially better, suggesting that if we had a better top bracket heuristic our performance would increase
p5381
aVFigure 4 shows a histogram of the performance level for sentences of length u'\u005cu2264' 10 for different random initializers
p5382
aVAs one can see, for some restarts, CCM obtains accuracies lower than 30 u'\u005cu2062' % due to local optima
p5383
aVOur method does not suffer from local optima and thus does not require careful initialization
p5384
aVOur approach is not directly comparable to Seginer u'\u005cu2019' s because he uses punctuation, while we use POS tags
p5385
asg88
(lp5386
sg90
(lp5387
sg92
(lp5388
VWe described a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery.
p5389
aVEmpirically, our algorithm performs favorably to the CCM of Klein and Manning (2002) without the need for careful initialization.
p5390
aVAcknowledgements.
p5391
aVThis work is supported by NSF IIS1218282, NSF IIS1111142, NIH R01GM093156, and the NSF Graduate Research Fellowship Program under Grant No.
p5392
aV0946825 (NSF Fellowship to APP.
p5393
ag106
asg107
S'P14-1100'
p5394
sg109
(lp5395
VWe propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery.
p5396
aVOur approach is grammarless we directly learn the bracketing structure of a given sentence without using a grammar model.
p5397
aVThe main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples.
p5398
aVAlthough finding the minimal latent tree is NP-hard in general, for the case of projective trees we find that it can be found using bilexical parsing algorithms.
p5399
aVEmpirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization.
p5400
ag106
asba(icmyPackage
FText
p5401
(dp5402
g3
(lp5403
VSemantics could potentially be useful, since words with different meanings have distinct phonetics, but it is unclear how many word meanings are known to infants learning phonetic categories
p5404
aVWe show that attending to a weaker source of semantics, in the form of a distribution over topics in the current context, can lead to improvements in phonetic category learning
p5405
aVIn theory, semantic information could offer a valuable cue for phoneme induction 1 1 The models in this paper do not distinguish between phonetic and phonemic categories, since they do not capture phonological processes (and there are also none present in our synthetic data
p5406
aVWe thus use the terms interchangeably by helping infants distinguish between minimal pairs, as linguists do ( 48
p5407
aVHowever, due to a widespread assumption that infants do not know the meanings of many words at the age when they are learning phonetic categories (see 42 for a review), most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information ( 8 ; 9 ; 11 ; 26 ; 50 )
p5408
aVThe extent of infants u'\u005cu2019' semantic knowledge is not yet known, but existing evidence shows that six-month-olds can associate some words with their referents ( 4 ; 46 ; 47 ) , leverage non-acoustic contexts such as objects or articulations to distinguish similar sounds ( 44 ; 52 ) , and map meaning (in the form of objects or images) to new word-forms in some laboratory settings ( 15 ; 16 ; 39
p5409
aVThese findings indicate that young infants are sensitive to co-occurrences between linguistic stimuli and at least some aspects of the world
p5410
aVIn this paper we explore the potential contribution of semantic information to phonetic learning by formalizing a model in which learners attend to the word-level context in which phones appear (as in the lexical-phonetic learning model of 11 ) and also to the situations in which word-forms are used
p5411
aVEven in the absence of word-meaning mappings, situational information is potentially useful because similar-sounding words uttered in similar situations are more likely to be tokens of the same lexeme (containing the same phones) than similar-sounding words uttered in different situations
p5412
aV11 ) , we show a clear improvement over previous models in both phonetic and lexical (word-form) categorization when situational context is used as an additional source of information
p5413
aVThis improvement is especially noticeable when the word-level context is providing less information, arguably the more realistic setting
p5414
aVThese results demonstrate that relying on situational co-occurrence can improve phonetic learning, even if learners do not yet know the meanings of individual words
p5415
aVInfants attend to distributional characteristics of their input ( 24 ; 23 ) , leading to the hypothesis that phonetic categories could be acquired on the basis of bottom-up distributional learning alone ( 8 ; 50 ; 26
p5416
aVHowever, this would require sound categories to be well separated, which often is not the case u'\u005cu2014' for example, see Figure 1 , which shows the English vowel space that is the focus of this paper
p5417
aVRecent work has investigated whether infants could overcome such distributional ambiguity by incorporating top-down information, in particular, the fact that phones appear within words
p5418
aVThis u'\u005cu201c' protolexicon u'\u005cu201d' can help differentiate phonetic categories by adding word contexts in which certain sound categories appear ( 42 ; 12
p5419
aVIf a word token is assigned to a lexeme, x i = u'\u005cu2113' , the vowels within the word are assigned to that lexeme u'\u005cu2019' s vowel categories, w i u'\u005cu2062' j = v u'\u005cu2113' u'\u005cu2062' j = c
p5420
aVLexical information helps with phonetic categorization because it can disambiguate highly overlapping categories, such as the ae and eh categories in Figure 1
p5421
aVA purely distributional learner who observes a cluster of data points in the ae - eh region is likely to assume all these points belong to a single category because the distributions of the categories are so similar
p5422
aVHowever, a learner who attends to lexical context will notice a difference contexts that only occur with ae will be observed in one part of the ae - eh region, while contexts that only occur with eh will be observed in a different (though partially overlapping) space
p5423
aVWhen two word tokens contain the same consonant frame but different vowels (i.e.,, minimal pairs), the model is more likely to categorize those two vowels together
p5424
aVThus, the model has trouble distinguishing minimal pairs
p5425
aVWe hypothesize that if a learner is able to associate words with the contexts of their use (as children likely are), this could provide a weak source of information for disambiguating minimal pairs even without knowing their exact meanings
p5426
aVThat is, if the learner hears k V 1 t and k V 2 t in different situational contexts, they are likely to be different lexical items (and V 1 and V 2 different phones), despite the lexical similarity between them
p5427
aVTo demonstrate the benefit of situational information, we develop the Topic-Lexical-Distributional (TLD) model, which extends the LD model by assuming that words appear in situations analogous to documents in a topic model
p5428
aVEach situation h is associated with a mixture of topics u'\u005cu0398' h , which is assumed to be observed
p5429
aVThus, for the i th token in situation h , denoted x h u'\u005cu2062' i , the observed data will be its frame f h u'\u005cu2062' i , vowels u'\u005cud835' u'\u005cudc98' h u'\u005cu2062' i , and topic vector u'\u005cu0398' h
p5430
aVWe assume further that as the child learns the language, she will begin to associate specific words with each topic as well
p5431
aVThus, in the TLD model, the words used in a situation are topic-dependent, implying meaning, but without pinpointing specific referents
p5432
aVThe occurrence of similar-sounding words in different situations with mostly non-overlapping topics will provide evidence that those words belong to different topics and that they are therefore different lexemes
p5433
aVConversely, potential minimal pairs that occur in situations with similar topic distributions are more likely to belong to the same topic and thus the same lexeme
p5434
aVAlthough we assume that children infer topic distributions from the non-linguistic environment, we will use transcripts from childes to create the word/phone learning input for our model
p5435
aV37 ) found that topics learned from similar transcript data using a topic model were strongly correlated with immediate activities and contexts
p5436
aVWe therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model ( 5 ) on a superset of the child-directed transcript data we use for lexical-phonetic learning, dividing the transcripts into small sections (the u'\u005cu2018' documents u'\u005cu2019' in LDA) that serve as our distinct situations u'\u005cud835' u'\u005cudc89'
p5437
aVAs noted above, the learned document-topic distributions u'\u005cud835' u'\u005cudf3d' are treated as observed variables in the TLD model to represent the situational context
p5438
aVThe topic-word distributions learned by LDA are discarded, since these are based on the (correct and unambiguous) words in the transcript, whereas the TLD model is presented with phonetically ambiguous versions of these word tokens and must learn to disambiguate them and associate them with topics
p5439
aVA DP is parametrized as D u'\u005cu2062' P u'\u005cu2062' ( u'\u005cu0391' , H ) , where u'\u005cu0391' is a real-valued hyperparameter and H is a base distribution
p5440
aVH may be continuous, as when it generates phonetic categories in formant space, or discrete, as when it generates lexemes as a list of phonetic categories
p5441
aVA draw from a DP, G u'\u005cu223c' D u'\u005cu2062' P u'\u005cu2062' ( u'\u005cu0391' , H ) , returns a distribution over a set of draws from H , i.e.,, a discrete distribution over a set of categories or lexemes generated by H
p5442
aVIf H is infinite, the support of the DP is likewise infinite
p5443
aVThe Infinite Gaussian Mixture Model (IGMM) ( 35 ) includes a DP prior, as described above, in which the base distribution H C generates multivariate Gaussians drawn from a Normal Inverse-Wishart prior
p5444
aV5 5 This compound distribution is equivalent to u'\u005cu03a3' c u'\u005cu223c' u'\u005cud835' u'\u005cudc3c' u'\u005cud835' u'\u005cudc4a' ( u'\u005cu03a3' 0 , u'\u005cu039d' 0 ) , u'\u005cu039c' c u'\u005cu03a3' c u'\u005cu223c' N ( u'\u005cu039c' 0 , u'\u005cu03a3' c u'\u005cu039d' 0 ) Each observation, a formant vector w i u'\u005cu2062' j , is drawn from the Gaussian corresponding to its category assignment c i u'\u005cu2062' j
p5445
aVThis is the baseline IGMM model, which clusters vowel tokens using bottom-up distributional information only; the LD model adds top-down information by assigning categories in the lexicon, rather than on the token level
p5446
aVEach such lexeme is represented as a frame plus a list of vowel categories u'\u005cud835' u'\u005cudc97' u'\u005cu2113'
p5447
aVLexeme assignments for each token are drawn from a DP with a lexicon-generating base distribution H L
p5448
aVThe TLD model retains the IGMM vowel phone component, but extends the lexicon of the LD model by adding topic-specific lexicons, which capture the notion that lexeme probabilities are topic-dependent
p5449
aVIn the HDP lexicon, a top-level global lexicon is generated as in the LD model
p5450
aVTopic-specific lexicons are then drawn from the global lexicon, containing a subset of the global lexicon (but since the size of the global lexicon is unbounded, so are the topic-specific lexicons
p5451
aVMore formally, the global lexicon is generated as a top-level DP
p5452
aVG L u'\u005cu223c' D u'\u005cu2062' P u'\u005cu2062' ( u'\u005cu0391' l , H L ) (see Section 3.2 ; remember H L includes draws from the IGMM over vowel categories
p5453
aVG L is in turn used as the base distribution in the topic-level DPs, G k u'\u005cu223c' D u'\u005cu2062' P u'\u005cu2062' ( u'\u005cu0391' k , G L
p5454
aVIn the Chinese Restaurant Franchise metaphor often used to describe HDPs, G L is a global menu of dishes (lexemes
p5455
aVInference (Section 5 ) is defined in terms of tables rather than lexemes; if multiple tables draw the same dish from G L , tokens at these tables share a lexeme
p5456
aVThe likelihood of the vowels is calculated by marginalizing over all possible means and variances of the Gaussian category parameters, given the NIW prior
p5457
aVThis corpus consists of transcripts of speech directed at infants between the ages of 9 and 15 months, captured in a naturalistic setting as parent and child went about their day
p5458
aVThis ensures variability of situations
p5459
aVWe restrict the corpus to content words by retaining only words tagged as adj, n, part and v (adjectives, nouns, particles, and verbs
p5460
aVThe transcripts do not include phonetic information, so, following Feldman et al
p5461
aV11 ) , we synthesize the formant values using data from Hillenbrand et al
p5462
aVIf there are multiple possible pronunciations, the first one is used
p5463
aVDistinguishing all consonant categories assumes perfect learning of consonants prior to vowel categorization and is thus somewhat unrealistic ( 29 ) , but provides an upper limit on the information that word-contexts can give
p5464
aVDecreasing the number of consonants increases the ambiguity in the corpus bat not only shares a frame ( b_t ) with boat and bite , but also, in the C15 dataset, with put , pad and bad ( b/p_d/t ), and in the C6 dataset, with dog and kite , among many others ( STOP_STOP
p5465
aVEach transcript in the Brent corpus captures about 75 minutes of parent-child interaction, and thus multiple situations will be included in each file
p5466
aVThe transcripts do not delimit situations, so we do this somewhat arbitrarily by splitting each transcript after 50 CDS utterances, resulting in 203 situations for the Brent C1 dataset
p5467
aVWe evaluate against adult categories, i.e.,, the u'\u005cu2018' gold-standard u'\u005cu2019' , since all learners of a language eventually converge on similar categories
p5468
aVSince our model is not a model of the learning process, we do not compare the infant learning process to the learning algorithm.) We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure (VM; 36
p5469
aVWords are evaluated against gold orthography, so homophones, e.g., hole and whole , are distinct gold words
p5470
aVWe compare all three models u'\u005cu2014' TLD, LD, and IGMM u'\u005cu2014' on the vowel categorization task, and TLD and LD on the lexical categorization task (since IGMM does not infer a lexicon
p5471
aVThe TLD supervowels are used much less frequently than the supervowels found by the LD model, containing, on average, only two-thirds as many tokens
p5472
aVFigure 5 shows that TLD also outperforms LD on the lexeme/word categorization task
p5473
aVAgain performance decreases as the consonant categories become coarser, but the additional semantic information in the TLD model compensates for the lack of consonant information
p5474
aVIn the individual components of VM, TLD and LD have similar VC ( u'\u005cu201c' recall u'\u005cu201d' ), but TLD has higher VH ( u'\u005cu201c' precision u'\u005cu201d' ), demonstrating that the semantic information given by the topics can separate potentially ambiguous words, as hypothesized
p5475
aVOverall, the contextual semantic information added in the TLD model leads to both better phonetic categorization and to a better protolexicon, especially when the input is noisier, using degraded consonants
p5476
aVSince infants are not likely to have perfect knowledge of phonetic categories at this stage, semantic information is a potentially rich source of information that could be drawn upon to offset noise from other domains
p5477
aVThe form of the semantic information added in the TLD model is itself quite weak, so the improvements shown here are in line with what infant learners could achieve
p5478
aVRegardless of the specific way in which infants encode semantic information, our method of adding this information by using LDA topics from transcript data was shown to be effective
p5479
aVThis method is practical because it can approximate semantic information without relying on extensive manual annotation
p5480
aVThe LD model extended the phonetic categorization task by adding word contexts; the TLD model presented here goes even further, adding larger situational contexts
p5481
asg88
(lp5482
sg90
(lp5483
sg92
(lp5484
VLanguage acquisition is a complex task, in which many heterogeneous sources of information may be useful.
p5485
aVIn this paper, we investigated whether contextual semantic information could be of help when learning phonetic categories.
p5486
aVWe found that this contextual information can improve phonetic learning performance considerably, especially in situations where there is a high degree of phonetic ambiguity in the word-forms that learners hear.
p5487
aVThis suggests that previous models that have ignored semantic information may have underestimated the information that is available to infants.
p5488
aVOur model illustrates one way in which language learners might harness the rich information that is present in the world without first needing to acquire a full inventory of word meanings.
p5489
aVThe contextual semantic information that the TLD model tracks is similar to that potentially used in other linguistic learning tasks.
p5490
aVTheories of cross-situational word learning ( 40 ; 53 ) assume that sensitivity to situational co-occurrences between words and non-linguistic contexts is a precursor to learning the meanings of individual words.
p5491
aVUnder this view, contextual semantics is available to infants well before they have acquired large numbers of semantic minimal pairs.
p5492
aVHowever, recent experimental evidence indicates that learners do not always retain detailed information about the referents that are present in a scene when they hear a word ( 27 ; 49.
p5493
aVThis evidence poses a direct challenge to theories of cross-situational word learning.
p5494
aVOur account does not necessarily require learners to track co-occurrences between words and individual objects, but instead focuses on more abstract information about salient events and topics in the environment; it will be important to investigate to what extent infants encode this information and use it in phonetic learning.
p5495
aVRegardless of the specific way in which infants encode semantic information, our method of adding this information by using LDA topics from transcript data was shown to be effective.
p5496
aVThis method is practical because it can approximate semantic information without relying on extensive manual annotation.
p5497
aVThe LD model extended the phonetic categorization task by adding word contexts; the TLD model presented here goes even further, adding larger situational contexts.
p5498
aVBoth forms of top-down information help the low-level task of classifying acoustic signals into phonetic categories, furthering a holistic view of language learning with interaction across multiple levels.
p5499
ag106
asg107
S'P14-1101'
p5500
sg109
(lp5501
VLearning phonetic categories is one of the first steps to learning a language, yet is hard to do using only distributional phonetic information.
p5502
aVSemantics could potentially be useful, since words with different meanings have distinct phonetics, but it is unclear how many word meanings are known to infants learning phonetic categories.
p5503
aVWe show that attending to a weaker source of semantics, in the form of a distribution over topics in the current context, can lead to improvements in phonetic category learning.
p5504
aVIn our model, an extension of a previous model of joint word-form and phonetic category inference, the probability of word-forms is topic-dependent, enabling the model to find significantly better phonetic vowel categories and word-forms than a model with no semantic knowledge.
p5505
aVgoodred,blue,teal,green!60!black,orange.
p5506
ag106
asba(icmyPackage
FText
p5507
(dp5508
g3
(lp5509
VAnalyses of filler-gap dependencies usually involve complex syntactic rules or heuristics; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives
p5510
aVTherefore, this work models filler-gap acquisition as a byproduct of learning word orderings (e.g., SVO vs OSV), which must be done at a very young age anyway in order to extract meaning from language
p5511
aVSpecifically, this model, trained on part-of-speech tags, represents the preferred locations of semantic roles relative to a verb as Gaussian mixtures over real numbers
p5512
aVThis approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance
p5513
aVAdditionally, this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B
p5514
aVThe phenomenon of filler-gap, where the argument of a predicate appears outside its canonical position in the phrase structure (e.g., [the apple] i that the boy ate t i or [what] i did the boy eat t i ), has long been an object of study for syntacticians [] due to its apparent processing complexity
p5515
aVRecent studies indicate that comprehension of filler-gap constructions begins around 15 months []
p5516
aVThis finding raises the question of how such a complex phenomenon could be acquired so early since children at that age do not yet have a very advanced grasp of language (e.g., ditransitives do not seem to be generalized until at least 31 months; Goldberg et al
p5517
aVThis work shows that filler-gap comprehension in English may be acquired through learning word orderings rather than relying on hierarchical syntactic knowledge
p5518
aVIn particular, the model described in this paper takes chunked child-directed speech as input and learns orderings over semantic roles
p5519
aVLanguage comprehension precedes production, and the developmental literature on the acquisition of filler-gap constructions is sparsely populated due to difficulties in designing experiments to test filler-gap comprehension in preverbal infants
p5520
aVRecent studies, however, indicate that filler-gap comprehension likely begins earlier than production []
p5521
aVTherefore, studies of verbal children are probably actually testing the acquisition of production mechanisms (planning, motor skills, greater facility with lexical access, etc) rather than the acquisition of filler-gap
p5522
aVNote that these may be related since filler-gap could introduce greater processing load which could overwhelm the child u'\u005cu2019' s fragile production capacity []
p5523
aVshowed that children are able to process wh -extractions from subject position (e.g., [who] i t i ate pie ) as young as 15 months while similar extractions from object position (e.g., [what] i did the boy eat t i ) remain unparseable until around 20 months of age
p5524
aV2 2 Since the wh -phrase is in the same (or a very similar) position as the original subject when the wh -phrase takes subject position, it is not clear that these constructions are true extractions [] , however, this paper will continue to refer to them as such for ease of exposition
p5525
aVBy providing more trials of each condition and controlling for the pragmatic felicity of test statements, provide evidence that 15-month old infants can process wh -extractions from both subject and object positions
p5526
aVObject extractions are more difficult to comprehend than subject extractions, however, perhaps due to additional processing load in object extractions []
p5527
aVSimilarly, show that relativized extractions with a wh -relativizer (e.g., find [the boy] i who t i ate the apple ) are easier to comprehend than relativized extractions with that as the relativizer (e.g., find [the boy] i that t i ate the apple
p5528
aVEven though the infants had no extralinguistic knowledge about the verb, they consistently treated the verb as transitive if two nouns were present and intransitive if only one noun was present
p5529
aVSimilarly, show that intransitive phrases with conjoined subjects (e.g., John and Mary gorped ) are given a transitive interpretation (i.e., John gorped Mary ) at 21 months (henceforth termed u'\u005cu2018' 1-1 role bias u'\u005cu2019' ), though this effect is no longer present at 25 months []
p5530
aVIt is important to note, however, that cross-linguistically children do not seem to generalize beyond two arguments until after at least 31 months of age [] , so a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive
p5531
aVHowever, previous computational models of grammar induction [] , including infant grammar induction [] , have not addressed filler-gap comprehension
p5532
aV4 4 As one reviewer notes, and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms; these have the virtue of scaling up to adult grammar, but due to their complexity, do not seem to have been described as models of early acquisition
p5533
aVThe model presented here learns a single, non-recursive ordering for the semantic roles in each sentence relative to the verb since several studies have suggested that early child grammars may consist of simple linear grammars that are dictated by semantic roles []
p5534
aVThis work assumes learners can already identify nouns and verbs, which is supported by who show that children at an extremely young age can distinguish between content and function words and by who show that children can distinguish between different types of content words
p5535
aVFurther, since demonstrate that, by 14 months, children are able to distinguish nouns from modifiers, this work assumes learners can already chunk nouns and access the nominal head
p5536
aVTo handle recursion, this work assumes that children treat the final verb in each sentence as the main verb (implicitly assuming sentence segmentation), which ideally assigns roles to each of the nouns in the sentence
p5537
aVDue to the findings of , this work adopts a u'\u005cu2018' syntactic bootstrapping u'\u005cu2019' theory of acquisition [] , where structural properties (e.g., number of nouns) inform the learner about semantic properties of a predicate (e.g., how many semantic roles it confers
p5538
aVSince infants infer the number of semantic roles, this work further assumes they already have expectations about where these roles tend to be realized in sentences, if they appear
p5539
aVThe semantic properties of these roles may be learned lexically for each predicate, but that is beyond the scope of this work
p5540
aVTherefore, this work uses syntactic and semantic roles interchangeably (e.g., subject and agent
p5541
aVFinally, following the finding by that children interpret intransitives with conjoined subjects as transitives, this work assumes that semantic roles have a one-to-one correspondence with nouns in a sentence (similarly used as a soft constraint in the semantic role labelling work of Titov and Klementiev, 2012)
p5542
aVThe model represents the preferred locations of semantic roles relative to the verb as distributions over real numbers
p5543
aVThis idea is adapted from who uses it to learn constraint rankings in optimality theory
p5544
aVLearner expectations of where an argument will appear relative to the verb are modelled as two-component Gaussian mixtures one mixture of Gaussians ( G S u'\u005cu2063' u'\u005cu22c5' ) corresponds to the subject argument, another ( G O u'\u005cu2063' u'\u005cu22c5' ) corresponds to the object argument
p5545
aVThere is no mixture for a third argument since children do not generalize beyond two arguments until later in development []
p5546
aVTo reflect the fact that learners have had 15 months of exposure to their language before acquiring filler-gap, the mixture is initialized so that there is a stronger probability associated with the canonical Gaussian than with the non-canonical Gaussian of each mixture
p5547
aV5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age, but the results of the current study are extremely robust to changes in initialization, as discussed in Section 7 of this paper, so this assumption is mostly adopted for ease of exposition
p5548
aVFinally, the one-to-one role bias is explicitly encoded such that the model cannot use a label that has already been used elsewhere in the sentence
p5549
aVThus, the initial model conditions (see Figure 2 ) are most likely to realize an SVO ordering, although it is possible to obtain SOV (by sampling a negative number from the blue curve) or even OSV (by also sampling the red curve very close to 0
p5550
aVIn other words, the model infers that an object extraction may have occurred if there is a u'\u005cu2018' missing u'\u005cu2019' postverbal argument
p5551
aVSince many sentences have more than two nouns, the model is allowed to skip nouns by multiplying a penalty term ( u'\u005cu03a6' ) into the product for each skipped noun; the cost is set at 0.00001 for this study, though see Section 7 for a discussion of the constraints on this parameter
p5552
aVThis formulation achieves the best fit to the training data according to the Bayesian Information Criterion (BIC
p5553
aVThe lack of a canonical subject in English imperatives allows the model to improve the likelihood of the data by using the non-canonical subject Gaussian to capture fictitious postverbal arguments
p5554
aVWhen imperatives are filtered out of the training corpus, the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian
p5555
aVTherefore, if one makes the assumption that imperatives are prosodically-marked for learners (e.g., the learner is the implicit subject), the best model is one that lacks a non-canonical subject
p5556
aV7 7 This finding suggests that a Dirichlet Process or other means of dynamically determining the number of components in each mixture would converge to a model that lacks non-canonical subjects if imperative filtering were employed
p5557
aVThe remainder of this paper assumes a symmetric model to demonstrate what happens if such an assumption is not made; for the evaluations described in this paper, the results are similar in either case
p5558
aVThis model differs from other non-recursive computational models of grammar induction (e.g., Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models
p5559
aVInstead, it determines the best ordering for the sentence as a whole
p5560
aVThis approach bears some similarity to a Generalized Mallows model [] , but the current formulation was chosen due to being independently posited as cognitively plausible []
p5561
aVBased on an initial analysis of chunker performance, yes is hand-corrected to not be a noun
p5562
aVPoor chunker perfomance is likely due to a mismatch in chunker training and testing domains (Wall Street Journal text vs transcribed speech), but chunking noise may be a good estimation of learner uncertainty, so the remaining text is left uncorrected
p5563
aVSince the model is not lexicalized, these roles correspond to the semantic roles most commonly associated with subject and object
p5564
aVThe prior probability of each Gaussian is updated as the ratio of that Gaussian u'\u005cu2019' s labellings to the total number of labellings from that mixture in the corpus
p5565
aVSince the model is unsupervised, it is trained on a given corpus (e.g., Eve) before being tested on the role annotations of that same corpus
p5566
aVThe Eve corpus was used for development purposes, 8 8 This is included for transparency, though the initial parameters have very little bearing on the final results as stated in Section 7 , so the danger of overfitting to development data is very slight and the Adam data was used only for testing
p5567
aVThe BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles
p5568
aVTherefore, overall accuracy results (see Table 3 ) are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript c in all tables
p5569
aVSince children do not generalize above two arguments during the modelled age range [] , the collapsed numbers more closely reflect the performance of a learner at this age than the raw numbers
p5570
aVSince the current work is interested in general filler-gap comprehension at this age, including over unknown verbs, the remaining analyses in this paper consider performance when non-agent arguments are collapsed
p5571
aVThis is unsurprising because, prior to training, subjects have little-to-no competition for preverbal role assignments; after training, there is a preverbal extracted object category, which the model can erroneously use
p5572
aVThe primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make [] , the 1-1 role bias error ( John and Mary gorped interpreted as John gorped Mary
p5573
aVSimilar to the model presented in this paper, BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments
p5574
aVdemonstrate that a supervised perceptron classifier, based on positional features and trained on the silver role label annotations of the BabySRL corpus, manifests 1-1 role bias errors
p5575
aVA comparable evaluation may be run on the current model by generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling (see Table 6
p5576
aV11 11 While Table 6 analyzes erroneous labellings of NNV structure, the u'\u005cu2018' Obj u'\u005cu2019' column of Table 5 (Left) shows model accuracy on NNV structures
p5577
aVThe results of and depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature (there is no combined model
p5578
aVFurther, similar to real children (see Figure 1 ) the model presented in this paper develops beyond this error by the end of its training, 12 12 It is important to note that the unique argument constraint prevents the current model from actually getting the correct, conjoined-subject parse, but it no longer exhibits agent-first bias, an important step for acquiring passives, which occurs between 3 and 4 years [] whereas the BabySRL models still make this error after training
p5579
aVThis evaluation can be replicated for the current study by generating 1,000 sentences with the transitive form of NVN and a further 1,000 sentences with the intransitive form of NV (see Table 7
p5580
aVSince investigate the effects of different initial lexicons, this evaluation compares against the resulting BabySRL from each initializer they initially seed their part-of-speech tagger with either the 10 or 365 most frequent nouns in the corpus or they dispense with the tagger and use gold part-of-speech tags
p5581
aVAs with subject extraction, the model in this paper gets less accurate after training because of the newly minted extracted object category that can be mistakenly used in these canonical settings
p5582
aVWhile the model of outperforms the model presented here when in a transitive setting, their model does much worse in an intransitive setting
p5583
aVThe difference in transitive settings stems from increased lexicalization, as is apparent from their results alone; the model presented here initially performs close to their weakly lexicalized model, though training impedes agent-prediction accuracy due to an increased probability of non-canonical objects
p5584
aVFor the intransitive case, however, whereas the model presented in this paper is generally able to successfully label the lone noun as the subject, the model of chooses to label lone nouns as objects about 40% of the time
p5585
aVThis likely stems from their model u'\u005cu2019' s reliance on argument-argument relative position as a feature; when there is no additional argument to use for reference, the model u'\u005cu2019' s accuracy decreases
p5586
aVThis is borne out by their model (not shown in Table 7 ) that omits the argument-argument relative position feature and solely relies on verb-argument position, which achieves up to 70% accuracy in intransitive settings
p5587
aVThe fact that intransitive sentences are more common than transitive sentences in both the Eve and Adam sections of the BabySRL corpus suggests that learners should be more likely to assign correct roles in an intransitive setting, which is not reflected in the BabySRL results
p5588
aVThe overall reason for the different results between the current work and BabySRL is that BabySRL relies on positional features that measure the relative position of two individual elements (e.g., where a given noun is relative to the verb
p5589
aVSince the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected
p5590
aVThe argument-verb position features impede acquisition of filler-gap by classifying preverbal arguments as agents, and the argument-argument position features inhibit accurate labelling in intransitive settings and result in an agent-first bias which would tend to label extracted objects as agents
p5591
aVAs point out, whereas wh -relatives such as who or which always signify a filler-gap construction, that can occur for many different reasons (demonstrative, determiner, complementizer, etc) and so is a much weaker filler-gap cue
p5592
aVIt is interesting to note that the cuurent model does not make use of that as a cue at all and yet is still slower at acquiring that -relatives than wh -relatives
p5593
aVIn future, it would be interesting to incorporate lexicalization into the model presented in this paper, as this feature seems likely to bridge the gap between this model and BabySRL in transitive settings
p5594
aVSince the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects, it may not work as well in verb-final languages, and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax
p5595
aVFurther, the kind of ordering system proposed in this paper may form an initial basis for learning such grammars []
p5596
asg88
(lp5597
sg90
(lp5598
sg92
(lp5599
g1538
asg107
S'P14-1102'
p5600
sg109
(lp5601
VAnalyses of filler-gap dependencies usually involve complex syntactic rules or heuristics; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives.
p5602
aVTherefore, this work models filler-gap acquisition as a byproduct of learning word orderings (e.g., SVO vs OSV), which must be done at a very young age anyway in order to extract meaning from language.
p5603
aVSpecifically, this model, trained on part-of-speech tags, represents the preferred locations of semantic roles relative to a verb as Gaussian mixtures over real numbers.
p5604
aVThis approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance.
p5605
aVAdditionally, this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B.
p5606
ag106
asba(icmyPackage
FText
p5607
(dp5608
g3
(lp5609
VSpecifically, we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting
p5610
aVThe model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method, and is the first algorithm for learning phonological constraints without presupposing constraint structure
p5611
aVMany aspects of human cognition involve the interaction of constraints that push a decision-maker toward different options, whether in something so trivial as choosing a movie or so important as a fight-or-flight response
p5612
aVThese constraint-driven decisions can be modeled with a log-linear system
p5613
aVWe consider this question by examining the dominant framework in modern phonology, Optimality Theory [, OT] , implemented in a log-linear framework, MaxEnt OT [] , with output forms u'\u005cu2019' probabilities based on a weighted sum of constraint violations
p5614
aVWe propose a new approach to learn constraints with limited innate phonological knowledge by identifying sets of constraint violations that explain the observed distributional data, instead of selecting constraints from an innate set of constraint definitions
p5615
aVBecause the constraints are identified as sets of violations, this also permits constraints specific to a given language to be learned
p5616
aVPrevious OT work has focused on identifying the appropriate formulation of Eval and the values and acquisition of H , while taking Gen and Con as given
p5617
aVHere, we expand the learning task by proposing an acquisition method for Con
p5618
aVAlthough all OT systems share the same core structure, different choices of Eval lead to different behaviors
p5619
aVWeights are always negative in OT; a constraint violation can never make a candidate more likely to win.) For a given input-candidate pair ( x , y ) , f i u'\u005cu2062' ( y , x ) is the number of violations of constraint C i by the pair
p5620
aVAs a maximum entropy model, the probability of y given x is proportional to the exponential of the weighted sum of violations, u'\u005cu2211' i w i u'\u005cu2062' f i u'\u005cu2062' ( y , x
p5621
aVIf u'\u005cud835' u'\u005cudcb4' u'\u005cu2062' ( x ) is the set of all output candidates for the input x , then the probability of y as the winning output is
p5622
aVIf constraints u'\u005cu2019' weights are close together, multiple violations of lower-weighted constraints can reduce a candidate u'\u005cu2019' s probability below that of a competitor with a single high-weight violation
p5623
aVAs the distance between weights in MEOT increases, the probability of a suboptimal candidate being chosen approaches zero; thus the traditional formulation is a limit case of MEOT
p5624
aVA white cell indicates no violation
p5625
aVGrey stripes are overlaid on cells whose value will have a negligible impact on the distribution due to the values of higher-ranked constraint
p5626
aVIn the \u005ctextipa ete tableau at top left, output \u005ctextipa ete has no violations, and therefore a score of zero
p5627
aVOutputs \u005ctextipa Ete and \u005ctextipa etE violate both Harmony (weight 16) and Parse [atr] (weight 8), so their scores are 24
p5628
aVOutput \u005ctextipa EtE violates Parse [atr], and has score 8
p5629
aVThus the log-probability of output \u005ctextipa EtE is 1/8 that of \u005ctextipa ete, and the log-probability of disharmonious \u005ctextipa Ete and \u005ctextipa etE are each 1/24 that of \u005ctextipa ete
p5630
aVAs the ratio between scores increases, the log-probability ratios can become arbitrarily close to zero, approximating the deterministic situation of traditional OT
p5631
aVThis version allows language-particular constraints, but it comes with a computational cost, as the learner must be able to generate and evaluate possible constraints while learning the language u'\u005cu2019' s phonology
p5632
aVDepending on the specific formulation of the constraints, the constraint identification problem may even be NP-hard []
p5633
aVOur approach of casting the learning problem as one of identifying violation profiles is an attempt to determine the amount that can be learned about the active constraints in a paradigm without hypothesizing intensional constraint definitions
p5634
aVEven well-known constraint types, such as generalized alignment, can have disputed structures []
p5635
aVAs such, it is unclear where a universal set of markedness constraints would come from
p5636
aVThe IBPOT model defines a generative process for mappings between input and output forms based on three latent variables the constraint violation matrices F (faithfulness) and M (markedness), and the weight vector w
p5637
aVNote that M is shared across inputs, as M j u'\u005cu2062' l has the same value for all input-output pairs with output y j
p5638
aV2.4 , we assume that F is known as part of the output of Gen []
p5639
aVThe goal of the IBPOT model is to learn the markedness matrix M and weights w for both the markedness and faithfulness constraints
p5640
aVAs for M , we need a non-parametric prior, as there is no inherent limit to the number of markedness constraints a language will use
p5641
aVWe begin by resampling M j u'\u005cu2062' l for all represented constraints M u'\u005cu22c5' l , conditioned on the rest of the violations ( M - ( j u'\u005cu2062' l ) , F ) and the weights w
p5642
aVIdeally, this would draw new constraints from the infinite feature matrix; however, this requires marginalizing the likelihood over possible weights, and we lack an appropriate conjugate prior for doing so
p5643
aVWe consider in each sample at most K * new constraints, with weights based on the auxiliary vector w *
p5644
aVThis approximation retains the unbounded feature set of the IBP, as repeated sampling can add more and more constraints without limit
p5645
aVIf the number of constraints removed is less than K * , w * is filled out with draws from the prior distribution over weights
p5646
aVWe test the model by learning the markedness constraints driving Wolof vowel harmony []
p5647
aVVowel harmony in general refers to a phonological phenomenon wherein the vowels of a word share certain features in the output form even if they do not share them in the input
p5648
aVUnder this ranking, Wolof harmony is achieved by changing a disharmonious ATR to an RTR, unless this creates an \u005ctextipa I vowel
p5649
aVAs in previous MEOT work, all Wolof candidates are faithful with respect to vowel height, either because height changes are not considered by Gen , or because of a high-ranked faithfulness constraint blocking height changes
p5650
aVIf unfaithful vowel heights were allowed by Gen , these unfaithful candidates would incur a violation approximately as strong as * \u005ctextipa I , as neither unfaithful-height candidates nor \u005ctextipa I candidates are attested in the Wolof data
p5651
aVThe Wolof constraints provide an interesting testing ground for the model, because it is a small set of constraints to be learned, but contains the Harmony constraint, which can be violated by non-adjacent segments
p5652
aVNon-adjacent constraints are difficult for string-based approaches because of the exponential number of possible relationships across non-adjacent segments
p5653
aVHowever, the Wolof results show that by learning violations directly, IBPOT does not encounter problems with non-adjacent constraints
p5654
aVThe outputs appear for multiple inputs, as shown in Figure 1
p5655
aVThe candidate outputs are the four combinations of tongue-roots for the given vowel heights; the inputs and candidates are known to the learner
p5656
aVWe generate simulated data by observing 1000 instances of the winning output for each input
p5657
aV6 6 Since data, matrix, and weight likelihoods all shape the learned constraints, there must be enough data for the model to avoid settling for a simple matrix that poorly explains the data
p5658
aVThis is necessary in the current model definition because the IBP produces a prior over binary matrices
p5659
aVThis is done so that the IBPOT weights and phonological standard weights are learned by the same process and can be compared
p5660
aVWe use the same parameters for this baseline as for the IBPOT tests
p5661
aVThe results in this section are based on nine runs each of IBPOT and MEOT; ten MEOT runs were performed but one failed to converge and was removed from analysis
p5662
aVAll eight differences are significant according to t -tests over the nine runs
p5663
aVThe most important differences are those in the data probabilities, as the matrix and weight probabilities are reflective primarily of the choice of prior
p5664
aVTurning to the form of these constraints, Figure 2 shows violation profiles from the last iteration of a representative IBPOT run
p5665
aVBecause vowel heights must be faithful between input and output, the Wolof data is divided into nine separate paradigms , each containing the four candidates (ATR/RTR × ATR/RTR) for the vowel heights in the input
p5666
aVThe violations on a given output form only affect probabilities within its paradigm
p5667
aVAs a result, learned constraints are consistent within paradigms, but across paradigms, the same constraint may serve different purposes
p5668
aVFor instance, the strongest learned markedness constraint, shown as M1 in Figure 2 , has the same violations as the top-ranked constraint that actively distinguishes between candidates in each paradigm
p5669
aVInstead, it learns that M1 has the same violations as Harmony , which is the highest-weighted constraint that distinguishes between candidates in these paradigms
p5670
aVThus in the high-vowel paradigms, M1 serves as * \u005ctextipa I , while in the low/mid-vowel paradigms, it serves as Harmony
p5671
aVThe lower-weighted M2 is defined noisily, as the higher-ranked M1 makes some values of M2 inconsequential
p5672
aVBecause M1 has a much higher weight than M2 , a violation of M2 has a negligible effect on a candidate u'\u005cu2019' s probability
p5673
aV2 , if the losing candidate violates M1 , its probability changes from 10 - 12 when the preferred candidate does not violate M2 to 10 - 8 when it does
p5674
aVOn the non-high paradigms, the meaning of M2 is unclear, as Harmony is handled by M1 and * \u005ctextipa I is unviolated
p5675
aVIn all four paradigms, the model learns that the RTR-RTR candidate violates M2 and the ATR-ATR candidate does not; this appears to be the model u'\u005cu2019' s attempt to reinforce a pattern in the lowest-ranked faithfulness constraint ( Parse [atr]), which the ATR-ATR candidate never violates
p5676
aVThus, while the IBPOT constraints are not identical to the phonologically standard ones, they reflect a version of the standard constraints that is consistent with the IBPOT framework
p5677
aV9 9 In fact, it appears this constraint organization is favored by IBPOT as it allows for lower weights, hence the large difference in w log-probability in Table 1
p5678
aVThe model u'\u005cu2019' s ability to infer constraint violation profiles without theoretical constraint structure provides an alternative solution to the problems of the traditionally innate and universal OT constraint set
p5679
aVAs it jointly learns constraints and weights, the IBPOT model calls to mind Hayes and Wilson u'\u005cu2019' s [] joint phonotactic learner
p5680
aVThis limits their learner in practice by the rapid explosion in the number of constraints as the maximum constraint definition size grows
p5681
aVIBPOT, as proposed here, learns constraints based on binary violation profiles, defined extensionally
p5682
aVNon-binarity can be handled by using the binary matrix M to indicate whether a candidate violates a constraint, with a second distribution determining the number of violations
p5683
aVAlternately, a binary matrix can directly capture non-binary constraints; converted existing non-binary constraints into a binary OT system by representing non-binary constraints as a set of equally-weighted overlapping constraints, each accounting for one violation
p5684
aVThe non-binary harmony constraint, for instance, becomes a set {*(at least one disharmony), *(at least two disharmonies), etc.}
p5685
asg88
(lp5686
sg90
(lp5687
sg92
(lp5688
VA central assumption of Optimality Theory has been the existence of a fixed inventory of universal markedness constraints innately available to the learner, an assumption by arguments regarding the computational complexity of constraint identification.
p5689
aVHowever, our results show for the first time that nonparametric, data-driven learning can identify sparse constraint inventories that both accurately predict the data and are phonologically meaningful, providing a serious alternative to the strong nativist view of the OT constraint inventory.
p5690
ag106
asg107
S'P14-1103'
p5691
sg109
(lp5692
VWe present a method to jointly learn features and weights directly from distributional data in a log-linear framework.
p5693
aVSpecifically, we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting.
p5694
aVThe model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method, and is the first algorithm for learning phonological constraints without presupposing constraint structure.
p5695
aVThe model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis, with a violation structure corresponding to the standard constraints.
p5696
aVThese results suggest an alternative data-driven source for constraints instead of a fully innate constraint set.
p5697
ag106
asba(icmyPackage
FText
p5698
(dp5699
g3
(lp5700
VThe latter option is appealing since it creates a large annotated dataset at low cost
p5701
aVHowever, because annotators that are recruited this way may lack expertise and motivation, the annotations tend to be more noisy and unreliable, which significantly reduces the performance of the classification model
p5702
aVAn intuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels
p5703
aVThe algorithm should be computationally cheap as well as accurate, so it fits well with active learning and other problems that require frequent iterations on large datasets
p5704
aVThe idea is that some effective features may be subdued due to label noise, and the proposed techniques are capable of counteracting such effect, so that the performance of classification algorithms could be less affected by the noise
p5705
aVWith the proposed algorithm, the active learner becomes more accurate and resistant to label noise, thus the mislabeled data points can be more easily and accurately identified
p5706
aVWe consider emotion analysis as an interesting and challenging problem domain of this study, and conduct comprehensive experiments on Twitter data
p5707
aVWe employ Amazon u'\u005cu2019' s Mechanical Turk (AMT) to label the emotions of Twitter data, and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost, as well as learning accurate emotion classifiers
p5708
aVExtensive experiments show that, the proposed techniques are as effective as more computational expensive techniques (e.g, Support Vector Machines) but require significantly less time for training/running, which makes it well-suited for active learning
p5709
aVNoise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant
p5710
aVMingers (1989) explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise
p5711
aVVannoorenberghe and Denoeux (2002) propose a method based on belief decision trees to handle uncertain labels in the training set
p5712
aVA large number of studies have explored noise elimination techniques [ 1 , 22 , 25 , 13 , 5 ] , which identifies and removes mislabeled examples from the dataset as a pre-processing step before building classifiers
p5713
aVOne widely used approach [ 1 , 22 ] is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier
p5714
aV2011) and they further demonstrate that its performance can be significantly improved by utilizing unlabeled data
p5715
aVFor example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms
p5716
aVIn addition, when the noise ratio is high, there may not be adequate amount of data remaining for building an accurate classifier
p5717
aVZeng and Martinez (2001) present an approach based on backpropagation neural networks to automatically correct the mislabeled data
p5718
aVActive learning for data cleaning differs from traditional active learning because the data already has low quality labels
p5719
aVUnlike the work in [ 15 ] , this paper focuses on developing algorithms that can enhance the ability of active learner on identifying labeling errors, which we consider as a key challenge of this approach but ALC has not addressed
p5720
aVThe problem is to obtain a high-quality dataset D by fixing labeling errors in D ^ , and learn an accurate classifier C from it
p5721
aVWe partition T into k subsets, and each time we keep a different subset as testing data and train a classifier using the other k - 1 subsets of data
p5722
aVThis process is repeated k times so that we get a classifier for each of the k subsets
p5723
aVThe top m instances with the highest probabilities belonging to some class but conflicting preliminary labels are selected as the most likely errors for annotators to fix
p5724
aVDuring the re-annotation process we keep the old labels hidden to prevent that information from biasing annotators u'\u005cu2019' decisions
p5725
aV1) accurately predicting the labels of data points and ranking them based on prediction confidence, so that the most likely errors can be effectively identified; (2) requiring less time on training, so that the saved time can be spent on correcting more labeling errors
p5726
aVThus we aim to build a classifier that is both accurate and time efficient
p5727
aVOne possible reason is that some effective features that should be given high weights are inhibited in the training phase due to the labeling errors
p5728
aVFor example, emoticon u'\u005cu201c' :D u'\u005cu201d' is a good indicator for emotion happy , however, if by mistake many instances containing this emoticon are not correctly labeled as happy , this class-specific feature would be underestimated during training
p5729
aVFollowing this idea, we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features, so that they would not be subdued and the instances with such features would have higher chance to be correctly classified
p5730
aVSince we focus on n-gram features, we use the words feature and term interchangeably in this paper
p5731
aVDifferent from the commonly used TF (term frequency) or TF.IDF (term frequency.inverse document frequency) weighting schemes, Delta IDF treats the positive and negative training instances as two separate corpora, and weighs the terms by how biased they are to one corpus
p5732
aVDelta IDF boosts the importance of terms that tend to be class-specific in the dataset, since they are usually effective features in distinguishing one class from another
p5733
aVEach training instance (e.g.,, a document) is represented as a feature vector x i = ( w 1 , i , u'\u005cu2026' , w
p5734
aVFor each instance, we can calculate the TF.Delta-IDF score as its weight
p5735
aVwhere t u'\u005cu2062' f j , i is the number of times term t j occurs in document x i , and u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j is the Delta IDF score of t j
p5736
aVThe model u'\u005cu2019' s ability to discriminate at the feature level can be further enhanced by leveraging the distribution of feature weights across multiple classes, e.g.,, multiple emotion categories funny , happy , sad , exciting , boring , etc
p5737
aVUsing Formula ( 1 ) and dataset D l ^ , we get the Delta IDF weight vector for each class l u'\u005cu0394' l = ( u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f 1 l , u'\u005cu2026' , u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f
p5738
aVFor a class u , we calculate the spreading score s u'\u005cu2062' p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' d j u of each feature t j u'\u005cu2208' V using a non-linear distribution spreading formula as following (where s is the configurable spread parameter
p5739
aVFor any term t j u'\u005cu2208' V , we can get its Delta IDF score on a class l
p5740
aVThe distribution of Delta IDF scores of t j on all classes in L is represented as u'\u005cu0394' j = { u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j 1 , u'\u005cu2026' , u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j
p5741
aVThe mechanism of Formula ( 3 ) is to non-linearly spread out the distribution, so that the importance of class-specific features can be further boosted to counteract the effect of noisy labels
p5742
aVSpecifically, according to Formula ( 3 ), a high (absolute value of) spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes
p5743
aVWhile these feature weighting models can be used to score and rank instances for data cleaning, better classification and regression models can be built by using the feature weights generated by these models as a pre-weight on the data points for other machine learning algorithms
p5744
aVWe conduct experiments on a Twitter dataset that contains tweets about TV shows and movies
p5745
aV2) Emotion expressions could be subtle and ambiguous and thus are easy to miss when labeling quickly
p5746
aVAs minority classes, emotional tweets can be easily missed because the last X tweets are all not emotional, and the annotators do not expect the next one to be either
p5747
aVDue to these reasons, there is a lack of sufficient and high quality labeled data for emotion research
p5748
aVThis AMT annotated dataset was used as the low quality dataset D ^ in our evaluation
p5749
aVAfter that, the same dataset was annotated independently by a group of expert annotators to create the ground truth
p5750
aVNote that some tweets were discarded as mixed examples for each emotion based upon thresholds for how many times they were tagged, and it resulted in different number of tweets in each emotion dataset
p5751
aVSee Table 1 for the statistics of the annotations collected from AMT
p5752
aVIt demonstrates the challenge of annotation by crowdsourcing
p5753
aVThe imbalanced class distribution aggravates the confirmation bias u'\u005cu2013' the minority class examples are especially easy to miss when labeling quickly due to their rare presence in the dataset
p5754
aVAverage Precision (AP) is the average of the algorithm u'\u005cu2019' s precision at every position in the confidence ranked list of results where a true emotional document has been identified
p5755
aVThus, AP places extra emphasis on getting the front of the list correct
p5756
aVSince in real world applications people are primarily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data, we defined a test fold for each TV show or movie in our labeled data set
p5757
aVBased on the dot product or SVM regression scores, we ranked the tweets by how strongly they express the emotion
p5758
aVFor the experimental purpose, the re-annotation was done by assigning the ground truth labels to the selected instances
p5759
aVSince the dataset is highly imbalanced, we applied the under-sampling strategy when training the classifiers
p5760
aVGenerally, Figure 3 shows consistent performance gains as more labels are corrected during active learning
p5761
aVAccording to the figure, SVM-Delta-IDF and SVM-TF are the most advantageous methods, followed by Spread and Delta-IDF
p5762
asg88
(lp5763
sg90
(lp5764
sg92
(lp5765
VIn this paper, we explored an active learning approach to improve data annotation quality for classification tasks.
p5766
aVInstead of training the active learner using computationally expensive techniques (e.g.,, SVM-TF), we used a novel non-linear distribution spreading algorithm.
p5767
aVThis algorithm first weighs the features using the Delta-IDF technique, and then non-linearly spreads out the distribution of the feature scores to enhance the model s ability to discriminate at the feature level.
p5768
aVThe evaluation shows that our algorithm has the following advantages.
p5769
aV1) It intelligently ordered the data points for annotators to annotate the most likely errors first.
p5770
aVThe accuracy was at least comparable with computationally expensive baselines (e.g., SVM-TF.
p5771
aV2) The algorithm trained and ran much faster than SVM-TF, allowing annotators to finish more annotations than competitors.
p5772
aV3) The annotation process improved the dataset quality by positively impacting the accuracy of classifiers that were built upon it.
p5773
ag106
asg107
S'P14-1104'
p5774
sg109
(lp5775
VMany machine learning datasets are noisy with a substantial number of mislabeled instances.
p5776
aVThis noise yields sub-optimal classification performance.
p5777
aVIn this paper we study a large, low quality annotated dataset, created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations.
p5778
aVWe describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost.
p5779
aVEight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computationally expensive techniques.
p5780
aVOur techniques save a considerable amount of time.
p5781
ag106
asba(icmyPackage
FText
p5782
(dp5783
g3
(lp5784
VTaking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence
p5785
aVMany of the issues discussed by politicians and the media are so nuanced that even word choice entails choosing an ideological position
p5786
aVWe say a sentence contains ideological bias if its author u'\u005cu2019' s political position (here liberal or conservative , in the sense of U.S politics) is evident from the text
p5787
aVExisting approaches toward bias detection have not gone far beyond u'\u005cu201c' bag of words u'\u005cu201d' classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents
p5788
aVThis model requires richer data than currently available, so we develop a new political ideology dataset annotated at the phrase level
p5789
aVBy taking into account the hierarchical nature of language, rnn s can model semantic composition , which is the principle that a phrase u'\u005cu2019' s meaning is a combination of the meaning of the words within that phrase and the syntax that combines those words
p5790
aVSince most ideological bias becomes identifiable only at higher levels of sentence trees (as verified by our annotation, Figure 4 ), models relying primarily on word-level distributional statistics are not desirable for our problem
p5791
aVBased on a parse tree, these words form phrases p (Figure 2
p5792
aVEach of these phrases also has an associated vector x p u'\u005cu2208' u'\u005cu211d' d of the same dimension as the word vectors
p5793
aVThese phrase vectors should represent the meaning of the phrases composed of individual words
p5794
aVAs phrases themselves merge into complete sentences, the underlying vector representation is trained to retain the sentence u'\u005cu2019' s whole meaning
p5795
aVIf two words w a and w b merge to form phrase p , we posit that the phrase-level vector is
p5796
aVSupervised rnn s achieve this distinction by applying a regression that takes the node u'\u005cu2019' s vector x p as input and produces a prediction y ^ p
p5797
aVWhen initializing our model, we have two choices we can initialize all of our parameters randomly or provide the model some prior knowledge
p5798
aVAs we see in Section 4 , these choices have a significant effect on final performance
p5799
aVThe word2vec embeddings have linear relationships (e.g.,, the closest vectors to the average of u'\u005cu201c' green u'\u005cu201d' and u'\u005cu201c' energy u'\u005cu201d' include phrases such as u'\u005cu201c' renewable energy u'\u005cu201d' , u'\u005cu201c' eco-friendly u'\u005cu201d' , and u'\u005cu201c' efficient lightbulbs u'\u005cu201d'
p5800
aVTo preserve these relationships as phrases are formed in our sentences, we initialize our left and right composition matrices such that parent vector p is computed by taking the average of children a and b ( W L = W R = 0.5 u'\u005cu2062' u'\u005cud835' u'\u005cudd40' d × d
p5801
aVThis initialization of the composition matrices has previously been effective for parsing [ 25 ]
p5802
aVIn this section we describe our initial dataset (Convote) and explain the procedure we followed for creating our new dataset ( ibc
p5803
aVThis is an expedient choice; in future work we plan to make use of work in political science characterizing candidates u'\u005cu2019' ideological positions empirically based on their behavior [ 3 ]
p5804
aV2 2 Many sentences in Convote are variations on u'\u005cu201c' I think this is a good/bad bill u'\u005cu201d' , and there is also substantial parliamentary boilerplate language
p5805
aVWe therefore use the features in Yano et al
p5806
aVFinally, we balance the resulting dataset so that it contains an equal number of sentences from Democrats and Republicans, leaving us with a total of 7,816 sentences
p5807
aVThere are over a million sentences in the ibc , most of which have no noticeable political bias
p5808
aVTherefore we use the filtering procedure outlined in Section 3.1.1 to obtain a subset of 55,932 sentences
p5809
aVBecause our goal is to distinguish between liberal and conservative bias, instead of the more general task of classifying sentences as u'\u005cu201c' neutral u'\u005cu201d' or u'\u005cu201c' biased u'\u005cu201d' , we filter the dataset further using dualist [ 23 ] , an active learning tool, to reduce the proportion of neutral sentences in our dataset
p5810
aVTo train the dualist classifier, we manually assigned class labels of u'\u005cu201c' neutral u'\u005cu201d' or u'\u005cu201c' biased u'\u005cu201d' to 200 sentences, and selected typical partisan unigrams to represent the u'\u005cu201c' biased u'\u005cu201d' class dualist labels 11,555 sentences as politically biased, 5,434 of which come from conservative authors and 6,121 of which come from liberal authors
p5811
aVFor purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence u'\u005cu2019' s author, where position is either liberal or conservative
p5812
aV5 5 This is a simplification, as the ideological hierarchy in ibc makes clear
p5813
aVFirst, we parse the filtered ibc sentences using the Stanford constituency parser [ 25 ]
p5814
aVBecause of the expense of labeling every node in a sentence, we only label one path in each sentence
p5815
aVThe process for selecting paths is as follows first, if any paths contain one of the top-ten partisan unigrams, 6 6 The words that the multinomial naïve Bayes classifier in dualist marked as highest probability given a polarity market, abortion, economy, rich, liberal, tea, economic, taxes, gun, abortion we select the longest such path; otherwise, we select the path with the most open class constituencies ( np , vp , adjp
p5816
aV60% of contributors passed the initial quiz (the 40% that failed were barred from working on the task), while only 10% of workers who passed the quiz were kicked out for mislabeling subsequent gold paths
p5817
aVIf you feel like the phrase indicates some position to the left or right of the political center, but you u'\u005cu2019' re not sure which direction, please mark Not neutral, but I u'\u005cu2019' m unsure of which direction
p5818
aVSince identifying political bias is a relatively difficult and subjective task, we include all sentences where at least two workers agree on a label for the root node in our final dataset, except when that label is u'\u005cu201c' Not neutral, but I u'\u005cu2019' m unsure of which direction u'\u005cu201d'
p5819
aVSince the root of each sentence is always annotated, this strategy ensures that every node in the tree has a label
p5820
aVDue to this discrepancy, the objective function in Eq. ( 6 ) was minimized by making neutral predictions for almost every node in the dataset
p5821
aVTo account for label imbalance, we subsample the data so that there are an equal number of labels and report accuracy over this balanced dataset
p5822
aVHowever, lr 2 also includes phrase-level annotations as separate training instances
p5823
aV7 7 The Convote dataset was not annotated on the phrase level, so we only provide a result for the IBC dataset
p5824
aV8 8 We do not include phrase-level annotations in the lr 3 feature set because the pseudo-word features can only be computed from full sentence parses
p5825
aVWe generate the final instance representation by concatenating the root vector and the average of all other vectors [ 27 ]
p5826
aVFor this model, we also introduce a hyperparameter u'\u005cu0392' that weights the error at annotated nodes ( 1 - u'\u005cu0392' ) higher than the error at unannotated nodes ( u'\u005cu0392' ); since we have more confidence in the annotated labels, we want them to contribute more towards the objective function
p5827
aVWhile phrase-level annotations do not improve baseline performance, the rnn model significantly benefits from these annotations because the phrases are themselves derived from nodes in the network structure
p5828
aVThis result was unexpected since the Convote labels are noisier than the annotated ibc labels; however, there are three possible explanations for the discrepancy
p5829
aVFirst, Convote has twice as many sentences as ibc , and the extra training data might help the model more than ibc u'\u005cu2019' s better-quality labels
p5830
aVSecond, since the sentences in Convote were originally spoken, they are almost half as short (21.3 words per sentence) as those in the ibc (42.2 words per sentence
p5831
aVFinally, some information is lost at every propagation step, so rnn s are able to model the shorter sentences in Convote more effectively than the longer ibc sentences
p5832
aVAs in previous work [ 27 ] , we visualize the learned vector space by listing the most probable n-grams for each political affiliation in Table 2
p5833
aVIn Figure 5 D, u'\u005cu201c' be used as an instrument to achieve charitable or social ends u'\u005cu201d' reflects a liberal ideology, which the model predicts correctly
p5834
aVSince many different issues are discussed in the ibc , it is likely that our dataset has too few examples of some of these issues for the model to adequately learn the appropriate ideological positions, and more training data would resolve many of these errors
p5835
aVMost previous work on ideology detection ignores the syntactic structure of the language in use in favor of familiar bag-of-words representations for the sake of simplicity
p5836
aVE.g., Gerrish and Blei ( 2011 ) predict the voting patterns of Congress members based on bag-of-words representations of bills and inferred political leanings of those members
p5837
aVWe also want to thank Justin Gross for providing the ibc and Asad Sayeed for help with the Crowdflower task design, as well as Richard Socher and Karl Moritz Hermann for assisting us with our model implementations
p5838
asg88
(lp5839
sg90
(lp5840
sg92
(lp5841
VIn this paper we apply recursive neural networks to political ideology detection, a problem where previous work relies heavily on bag-of-words models and hand-designed lexica.
p5842
aVWe show that our approach detects bias more accurately than existing methods on two different datasets.
p5843
aVIn addition, we describe an approach to crowdsourcing ideological bias annotations.
p5844
aVWe use this approach to create a new dataset from the ibc , which is labeled at both the sentence and phrase level.
p5845
ag106
asg107
S'P14-1105'
p5846
sg109
(lp5847
VAn individual s words often reveal their political ideology.
p5848
aVExisting automated techniques to identify ideology from text focus on bags of words or wordlists, ignoring syntax.
p5849
aVTaking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network ( RNN ) framework to the task of identifying the political position evinced by a sentence.
p5850
aVTo show the importance of modeling subsentential elements, we crowdsource political annotations at a phrase and sentence level.
p5851
aVOur model outperforms existing models on our newly annotated dataset and an existing dataset.
p5852
aVsmalign.
p5853
ag106
asba(icmyPackage
FText
p5854
(dp5855
g3
(lp5856
VThis paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system
p5857
aVWe develop novel features based on both models and use them as soft constraints to guide the translation process
p5858
aVExperiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system
p5859
aVHowever, the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model, and we therefore provide a detailed analysis of the behavior differences between the two
p5860
aVThe popular distortion or lexicalized reordering models in phrase-based SMT make good local predictions by focusing on reordering on word level, while the synchronous context free grammars in hierarchical phrase-based (HPB) translation models are capable of handling non-local reordering on the translation phrase level
p5861
aVHowever, reordering, especially without any help of external knowledge, remains a great challenge because an accurate reordering is usually beyond these word level or translation phrase level reordering models u'\u005cu2019' ability
p5862
aVIn addition, often these translation models fail to respect linguistically-motivated syntax and semantics
p5863
aVAs a result, they tend to produce translations containing both syntactic and semantic reordering confusions
p5864
aVDue to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the target language, there is more than one way to describe the monotonicity or swapping patterns; we therefore design two reordering models one is based on the leftmost aligned target word and the other based on the rightmost target word
p5865
aVPAS) reordering in SMT, it is still an open question whether semantic structure reordering strongly overlaps with syntactic structure reordering, since the semantic structure is closely tied to syntax
p5866
aVThe syntactic reordering model takes a CFG rule (e.g.,, VP u'\u005cu2192' VP u'\u005cu2062' PP u'\u005cu2062' PP ) and models the reordering of the constituents on the left hand side by examining their translation or visit order according to the target language
p5867
aVNote that we refer all core arguments, adjuncts, and predicates as semantic roles; thus we say the PAS in Figure 1 has 4 roles
p5868
aVAccording to the annotation principles in (Chinese) PropBank [ 28 , 42 ] , all the roles in a PAS map to a corresponding constituent in the parse tree, and these constituents (e.g.,, NPs and VBD in Figure 1 ) do not overlap with each other
p5869
aVTreating the two forms of reorderings in a unified way, the semantic reordering model is obtainable by regarding a PAS as a CFG rule and considering a semantic role as a constituent
p5870
aVBecause the translation of a source constituent might result in multiple discontinuous blocks, there can be several ways to describe or group the reordering patterns
p5871
aVTherefore, we design two general constituent reordering sub-models
p5872
aVOne is based on the leftmost aligned word (leftmost reordering model) and the other is based on the rightmost aligned word (rightmost reordering model), as follows
p5873
aVif the first constituent u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' 1 is unaligned, we add a NULL word at the beginning of the target side and link u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' 1 to the NULL word;
p5874
aVif a constituent u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i ( i 1 ) is unaligned, we add a link to the target word which is aligned to u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i - 1 , e.g.,, u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' 4 will be linked to e 3 ; and
p5875
aVif k constituents u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' m 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' m k ( m 1 u'\u005cu2026' m k ) are linked to the same target word, then v m i = v m i + 1 - 1 , e.g.,, since u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' 3 and u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' 4 are both linked to e 3 , then v 3 = v 4 - 1
p5876
aVwhere u'\u005cu03a8' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc50' u'\u005cud835' u'\u005cudc53' u'\u005cud835' u'\u005cudc54' ) indicates the surrounding context of the CFG
p5877
aVBy assuming that any two reordering types in u'\u005cud835' u'\u005cudc3f' u'\u005cud835' u'\u005cudc45' u'\u005cud835' u'\u005cudc47' = { l u'\u005cu2062' r u'\u005cu2062' t 1 , u'\u005cu2026' , l u'\u005cu2062' r u'\u005cu2062' t n - 1 } are independent of each other, we reformulate Eq
p5878
aVSimilarly, the sequence of rightmost reordering types RRT can be decided for a CFG rule u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' u'\u005cu2192' u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' n
p5879
aVAccordingly, for a PAS pas u'\u005cud835' u'\u005cudc43' u'\u005cud835' u'\u005cudc34' u'\u005cud835' u'\u005cudc46' u'\u005cu2192' u'\u005cud835' u'\u005cudc45' 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' u'\u005cud835' u'\u005cudc45' n , we can obtain its sequences of leftmost and rightmost reordering types by using the same way described above
p5880
aVFor u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i and u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i + 1 in cfg , the features are aimed to examine which of them should be translated first
p5881
aVTherefore, most features share two common components the syntactic categories of u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i and u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i + 1
p5882
aVUnlike the conventional phrase and lexical translation features, whose values are phrase pair-determined and thus can be calculated offline, the value of the reordering features can only be obtained during decoding time, and requires word alignment information as well
p5883
aVBefore we present the algorithm integrating the reordering models, we define the following functions by assuming XP i and XP i + 1 are the constituent pair of interest in CFG rule cfg , H is the translation hypothesis and a is its word alignment
p5884
aVu'\u005cu2131' 2 u'\u005cu2062' ( H , u'\u005cu2006' u'\u005cud835' u'\u005cudc50' u'\u005cud835' u'\u005cudc53' u'\u005cud835' u'\u005cudc54' , u'\u005cu2006' u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i , u'\u005cu2006' u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i + 1 ) returns true if the reordering of the pair u'\u005cu27e8' XP i , XP i + 1 u'\u005cu27e9' in rule cfg has not been calculated yet; otherwise returns false
p5885
aVu'\u005cu2131' 3 u'\u005cu2062' ( H , u'\u005cu2006' a , u'\u005cu2006' u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i , u'\u005cu2006' u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i + 1 ) returns the leftmost and rightmost reordering types for the constituent pair u'\u005cu27e8' XP i , XP i + 1 u'\u005cu27e9' , given alignment a , according to Section 3
p5886
aVGiven a hypothesis H with its alignment a , it traverses all CFG rules in the parse tree and sees if two adjacent constituents are conditioned to trigger the reordering models (lines 2-4
p5887
aVNote that Function u'\u005cu2131' 1 returns true if hypothesis H fully covers, or fully contains, constituent X u'\u005cu2062' P i , regardless of the reordering type of X u'\u005cu2062' P i
p5888
aVTo get the two semantic reordering model feature values, we simply use Algorithm 1 and its associated functions from u'\u005cu2131' 1 to u'\u005cu2131' 5 replacing a CFG rule cfg with a PAS pas , and a constituent u'\u005cud835' u'\u005cudc4b' u'\u005cud835' u'\u005cudc43' i with a semantic role u'\u005cud835' u'\u005cudc45' i
p5889
aVAlgorithm 1 therefore permits a unified treatment of syntactic and PAS-based reordering, even though it is expressed in terms of syntactic reordering here for ease of presentation
p5890
aVSince the syntactic parses of the tuning and test data contain 29 types of constituent labels and 35 types of POS tags, we have 29 types of XP + features and 64 types of XP = features
p5891
aVWe first run syntactic parsing and semantic role labeling on the Chinese sentences, then train the models by using MaxEnt toolkit with L1 regularizer [ 34 ]
p5892
aVIn order to understand how well the MR08 system respects their reordering preference, we use the gold alignment dataset LDC2006E86, in which the source sentences are from the Chinese Treebank, and thus both the gold parse trees and gold predicate-argument structures are available
p5893
aVFor example, the first row shows that based on the gold alignment, for u'\u005cu27e8' PP,VP u'\u005cu27e9' , 16% are in monotone and 76% are in swap reordering
p5894
aVHowever, our MR08 system outputs 46% of them in monotone and and 50% in swap reordering
p5895
aVHence, the reordering accuracy for u'\u005cu27e8' PP,VP u'\u005cu27e9' is 54%
p5896
aVThe trend of the results, summarized as performance gain over the baseline and MR08 systems averaged over all test sets, is presented in Table 6
p5897
aVThis is not surprising since the semantic reordering features are exclusively attached to predicates, and the span set of the semantic roles is a strict subset of the span set of the syntactic constituents; only 22% of syntactic constituents are semantic roles
p5898
aVOf all the semantic role pairs, 44% are in the same CFG rules, indicating that this part of semantic reordering has overlap with syntactic reordering
p5899
aVTherefore, the PAS model has fewer opportunities to influence reordering
p5900
aVOnce we have the (semi-)gold alignment, we compute the gold reordering types between two adjacent syntactic constituents or semantic roles
p5901
aVTable 7 shows the accuracy averaged over the four gold reordering sets (the four reference translations
p5902
aVIt shows that 1) as expected, our classifiers do worse on the harder semantic reordering prediction than syntactic reordering prediction; 2) thanks to the high accuracy obtained by the maxent classifiers, integrating either the syntactic or the semantic reordering constraints results in better reordering performance from both syntactic and semantic perspectives; 3) in terms of the mutual impact, the syntactic reordering models help improving semantic reordering more than the semantic reordering models help improving syntactic reordering; and 4) the rightmost models have a learnability advantage over the leftmost models, achieving higher accuracy across the board
p5903
aVWe report the averaged performance by using the gold reordering type extracted from the four reference translations
p5904
aV33.4 on average) and there is still some room for improvement by building a better maximum entropy classifiers (e.g.,, 34.9 vs
p5905
aVSome previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar
p5906
aV[ 44 ] obtained word order by using a reranking approach to reposition nodes in syntactic parse trees
p5907
aV4 4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work
p5908
aVOur work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level
p5909
aVAlthough there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two reordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially
p5910
aVThe authors would like to thank three anonymous reviewers for providing helpful comments, and also acknowledge Ke Wu, Vladimir Eidelman, Hua He, Doug Oard, Yuening Hu, Jordan Boyd-Graber, and Jyothi Vinjumur for useful discussions
p5911
asg88
(lp5912
sg90
(lp5913
sg92
(lp5914
VIn this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model.
p5915
aVThe syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS.
p5916
aVExperiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system.
p5917
aVWe have also discussed the differences between the two linguistic reordering models.
p5918
aVThere are many directions in which this work can be continued.
p5919
aVFirst, the syntactic reordering model can be extended to model reordering among constituents that cross CFG rules.
p5920
aVSecond, although we do not see obvious gain from the semantic reordering model when the syntactic model is adopted, it might be beneficial to further jointly consider the two reordering models, focusing on where each one does well.
p5921
aVThird, to better examine the overlap or synergy between our approach and the non-syntax-based reordering approach, we will conduct direct comparisons and combinations with the latter.
p5922
ag106
asg107
S'P14-1106'
p5923
sg109
(lp5924
VThis paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system.
p5925
aV1) a syntactic reordering model that explores reorderings for context free grammar rules; and 2) a semantic reordering model that focuses on the reordering of predicate-argument structures.
p5926
aVWe develop novel features based on both models and use them as soft constraints to guide the translation process.
p5927
aVExperiments on Chinese-English translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system.
p5928
aVHowever, the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model, and we therefore provide a detailed analysis of the behavior differences between the two.
p5929
ag106
asba(icmyPackage
FText
p5930
(dp5931
g3
(lp5932
VCrowdsourcing is a viable mechanism for creating training data for machine translation
p5933
aVCareful quality control is necessary for crowdsourcing to work well
p5934
aVThis drastically limits which languages SMT can be successfully applied to
p5935
aVBecause of this, collecting parallel corpora for minor languages has become an interesting research challenge
p5936
aVThere are various options for creating training data for new language pairs
p5937
aVUntil relatively recently, little consideration has been given to creating parallel data from scratch
p5938
aVThis is because the cost of hiring professional translators is prohibitively high
p5939
aVThis setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward [ 43 ]
p5940
aVThey also hired US-based Turkers to edit the translations, since the translators were largely based in Pakistan and exhibited errors that are characteristic of speakers of English as a language
p5941
aVWe use translation edit rate (TER) as a measure of translation similarity
p5942
aVTER represents the amount of change necessary to transform one sentence into another, so a low TER means the two sentences are very similar
p5943
aVWe measure aggressiveness by looking at the TER between the pre- and post-edited versions of each editor u'\u005cu2019' s translations; higher TER implies more aggressive editing
p5944
aVTo address this question, we split our translations into 5 bins, based on their TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d
p5945
aVWe also split our editors into 5 bins, based on their effectiveness (i.e., the average amount by which their editing reduces TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d
p5946
aVThis output translation is the result of the combined translation and editing stages
p5947
aVTherefore, our method operates over a heterogeneous network that includes translators and post-editors as well as the translated sentences that they produce
p5948
aVThese two graphs, G T and G C are combined as subgraphs of a third graph ( G T u'\u005cu2062' C
p5949
aVEdges in G T u'\u005cu2062' C connect author pairs (nodes in G T ) to the candidate that they produced (nodes in G C
p5950
aVThe ranking method allows us to obtain a global ranking by taking into account the intra-/inter-component dependencies
p5951
aVA candidate is important if 1) it is similar to many of the other proposed candidates and 2) it is authored by better qualified translators and/or post-editors
p5952
aVBy fusing the above equations, we can have the following iterative calculation in matrix forms
p5953
aVTo this end, we must make the c and t column stochastic [ 20 ] c and t are therefore normalized after each iteration of Equation (4) and (5
p5954
aVWe treat a candidate as a short document and weight each term with tf.idf [ 23 ] , where tf is the term frequency and idf is the inverse document frequency
p5955
aVThe Turker graph, G T , is an undirected graph whose edges represent u'\u005cu201c' collaboration u'\u005cu201d' Formally, let t i and t j be two translator/editor pairs; we say that pair t i u'\u005cu201c' collaborates with u'\u005cu201d' pair t j (and therefore, there is an edge between t i and t j ) if t i and t j share either a translator or an editor (or share both a translator and an editor
p5956
aV1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations
p5957
aVSince we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score [ 27 ] for one professional translator (P1) using the other three (P2,3,4) as a reference set
p5958
aVIn the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations
p5959
aVTherefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets
p5960
aVThis allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators
p5961
aVAs a naive baseline, we choose one candidate translation at random for each input Urdu sentence
p5962
aVTo establish an upper bound for our methods, and to determine if there exist high-quality Turker translations at all, we compute four oracle scores
p5963
aVSince the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations
p5964
aVUsing the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and Callison-Burch ( 2011 ) u'\u005cu2019' s reported score of 28.13, which they achieved using a linear feature-based classification
p5965
aVTheir linear classifier achieved a reported score of 39.06 2 2 Note that the data we used in our experiments are slightly different, by discarding nearly 100 NULL sentences in the raw data
p5966
aVAccording to our experiments, most of the results generated by baselines and oracles are very close to the previously reported values when combining information from both translators and editors
p5967
aVWe first examine the centroid-based ranking on the candidate sub-graph ( G C ) alone to see the effect of voting among translated sentences; we denote this strategy as plain ranking
p5968
aVThen we incorporate the standard random walk on the Turker graph ( G T ) to include the structural information but without yet including any collaboration information; that is, we incorporate information from G T and G C without including edges linking the two together
p5969
aVThis result supports the intuition that a denser collaboration matrix will help propagate saliency to good translators/post-editors and hence provides better predictions for candidate quality
p5970
aVWe have proposed an algorithm for using a two-step collaboration between non-professional translators and post-editors to obtain professional-quality translations
p5971
aVOur method, based on a co-ranking model, selects the best crowdsourced translation from a set of candidates, and is capable of selecting translations which near professional quality
p5972
aVIn future work on crowdsourced translation, further benefits in quality improvement and cost reduction could stem from 1) building ground truth data sets based on high-quality Turkers u'\u005cu2019' translations and 2) identifying when sufficient data has been collected for a given input, to avoid soliciting unnecessary redundant translations
p5973
aVThis material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled u'\u005cu201c' Crowdsourcing Translation u'\u005cu201d' (contract D12PC00368
p5974
asg88
(lp5975
sg90
(lp5976
sg92
(lp5977
VWe have proposed an algorithm for using a two-step collaboration between non-professional translators and post-editors to obtain professional-quality translations.
p5978
aVOur method, based on a co-ranking model, selects the best crowdsourced translation from a set of candidates, and is capable of selecting translations which near professional quality.
p5979
aVCrowdsourcing can play a pivotal role in future efforts to create parallel translation datasets.
p5980
aVIn addition to its benefits of cost and scalability, crowdsourcing provides access to languages that currently fall outside the scope of statistical machine translation research.
p5981
aVIn future work on crowdsourced translation, further benefits in quality improvement and cost reduction could stem from 1) building ground truth data sets based on high-quality Turkers translations and 2) identifying when sufficient data has been collected for a given input, to avoid soliciting unnecessary redundant translations.
p5982
ag106
asg107
S'P14-1107'
p5983
sg109
(lp5984
VCrowdsourcing is a viable mechanism for creating training data for machine translation.
p5985
aVIt provides a low cost, fast turn-around way of processing large volumes of data.
p5986
aVHowever, when compared to professional translation, naive collection of translations from non-professionals yields low-quality results.
p5987
aVCareful quality control is necessary for crowdsourcing to work well.
p5988
aVIn this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals.
p5989
aVWe develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals.
p5990
ag106
asba(icmyPackage
FText
p5991
(dp5992
g3
(lp5993
VWe introduce a novel approach for building language models based on a systematic, recursive exploration of skip n -gram models which are interpolated using modified Kneser-Ney smoothing
p5994
aVOur approach generalizes language models as it contains the classical interpolation with lower order models as a special case
p5995
aVIn an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 u'\u005cu2062' % and 12.7 u'\u005cu2062' % in comparison to traditional language models using modified Kneser-Ney smoothing
p5996
aVLanguage Models are a probabilistic approach for predicting the occurrence of a sequence of words
p5997
aVThe probability P u'\u005cu2062' ( w 1 l ) of this sequence can be broken down into a product of conditional probabilities
p5998
aVBecause of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence
p5999
aVTherefore, by making a Markov assumption the true probability of a word sequence is only approximated by restricting conditional probabilities to depend only on a local context w i - n + 1 i - 1 of n - 1 preceding words rather than the full sequence w 1 i - 1
p6000
aVThe motivation for using lower order models is that shorter contexts may be observed more often and, thus, suffer less from data sparsity
p6001
aVHowever, a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data and hence will lead to an unreliable estimation
p6002
aVBecause of Zipfian word distributions, most words occur very rarely and hence their true probability of occurrence may be estimated only very poorly
p6003
aVOne word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u'\u005cu2014' leading to severe errors even for smoothed language models
p6004
aVThus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning
p6005
aVAmong other techniques, skip n -grams have also been considered as an approach to overcome problems of data sparsity []
p6006
aVHowever, to best of our knowledge, language models making use of skip n -grams models have never been investigated to their full extent and over different levels of lower order models
p6007
aVOur approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways
p6008
aVWe provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n -grams
p6009
aVWe show how our novel approach can indeed easily be interpreted as a generalized version of the current state-of-the-art language models
p6010
aVWe present a large scale empirical analysis of our generalized language models on eight data sets spanning four different languages, namely, a wikipedia-based text corpus and the JRC-Acquis corpus of legislative texts
p6011
aVWe will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model
p6012
aVIntroducing the possibility of gaps between the words in an n -gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space
p6013
aVHowever, with their restriction on a subsequence of words, skip n -grams are also used as a technique to overcome data sparsity []
p6014
aVThe conclusion was that using skip n -grams is often more effective for increasing the number of observations than increasing the corpus size
p6015
aVWe briefly recall modified Kneser-Ney Smoothing as presented in []
p6016
aVModified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models
p6017
aVwhere c u'\u005cu2062' ( w i - n + 1 i ) provides the frequency count that sequence w i - n + 1 i occurs in training data, D is a discount value (which depends on the frequency of the sequence) and u'\u005cu0393' h u'\u005cu2062' i u'\u005cu2062' g u'\u005cu2062' h depends on D and is the interpolation factor to mix in the lower order distribution 1 1 The factors u'\u005cu0393' and D are quite technical and lengthy
p6018
aVAs they do not play a significant role for understanding our novel approach we refer to Appendix A for details
p6019
aVwhere the continuation counts are defined as N 1 + ( u'\u005cu2219' w i - n + 1 i )
p6020
aV{ w i - n c ( w i - n i ) 0 } i.e., the number of different words which precede the sequence w i - n + 1 i
p6021
aVIn particular the equation u'\u005cu2202' 1 u'\u005cu2061' w i - n + 1 i = w i - n + 2 i holds where the right hand side is the subsequence of w i - n + 1 i omitting the first word
p6022
aVWe can thus formulate the interpolation step of modified Kneser-Ney smoothing using our notation as P ^ MKN ( w i w i - n + 2 i - 1 ) = P ^ MKN ( w i u'\u005cu2202' 1 w i - n + 1 i - 1 )
p6023
aVThus, our skip n -grams correspond to n -grams of which we only use k words, after having applied the skip operators u'\u005cu2202' i 1 u'\u005cu2061' u'\u005cu2026' u'\u005cu2062' u'\u005cu2202' i n - k
p6024
aVRegarding the continuation counts we define
p6025
aVAs lowest order model we use u'\u005cu2014' just as done for traditional modified Kneser-Ney [] u'\u005cu2014' a unigram model interpolated with a uniform distribution for unseen words
p6026
aVThe data sets cover different domains and languages
p6027
aVAs languages we considered English ( en ), German ( de ), French ( fr ), and Italian ( it
p6028
aVAs general domain data set we used the full collection of articles from Wikipedia ( wiki ) in the corresponding languages
p6029
aVWe filtered the word tokens by removing all character sequences which did not contain any letter, digit or common punctuation marks
p6030
aVWe learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
p6031
aV2 2 http://west.uni-koblenz.de/Research 3 3 https://github.com/renepickhardt/generalized-language-modeling-toolkit 4 4 http://glm.rene-pickhardt.de We compared the probabilities of our language model implementation (which is a subset of the generalized language model) using KN as well as MKN smoothing with the Kyoto Language Model Toolkit 5 5 http://www.phontron.com/kylm/
p6032
aVSince we got the same results for small n and small data sets we believe that our implementation is correct
p6033
aVThe wikipedia corpus consists of 1.7 bn words
p6034
aVThus, the 80 u'\u005cu2062' % split for training consists of 1.3 bn words
p6035
aVWe have iteratively created smaller training sets by decreasing the split factor by an order of magnitude
p6036
aVSo we created 8 u'\u005cu2062' % / 92 u'\u005cu2062' % and 0.8 u'\u005cu2062' % / 99.2 u'\u005cu2062' % split, and so on
p6037
aVWe have stopped at the 0.008 u'\u005cu2062' % / 99.992 u'\u005cu2062' % split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split
p6038
aVLower perplexity values indicate better results
p6039
aVIn this table we also present the relative reduction of perplexity in comparison to the baseline
p6040
aVAs we can see, the GLM clearly outperforms the baseline for all model lengths and data sets
p6041
aVWe also note that GLMs seem to work better on broad domain text rather than special purpose text as the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC corpora
p6042
aVWe see that the GLM performs particularly well on small training data
p6043
aVAs the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u005cu2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
p6044
aVIt also confirms that our motivation to produce lower order n -grams by omitting not only the first word of the local context but systematically all words has been fruitful
p6045
aVThis feature of the GLM is of particular value, as data sparsity becomes a more and more immanent problem for higher values of n
p6046
aVBeyond the general improvements there is an additional path for benefitting from generalized language models
p6047
aVAs it is possible to better leverage the information in smaller and sparse data sets, we can build smaller models of competitive performance
p6048
aVThis GLM model has a size of 9.5 GB and contains only 427 Mio entries
p6049
aVSo, using a far smaller set of training data we can build a smaller model which still demonstrates a competitive performance
p6050
aVWe have introduced a novel generalized language model as the systematic combination of skip n -grams and modified Kneser-Ney smoothing
p6051
aVThe main strength of our approach is the combination of a simple and elegant idea with an an empirically convincing result
p6052
aVMathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is consequently a real generalization
p6053
aVIn an empirical evaluation, we have demonstrated that for higher orders the GLM outperforms MKN for all test cases
p6054
aVThe value of the weights would have to be chosen according to the probability or counts of the respective skip n -grams
p6055
aVThe discount value D u'\u005cu2062' ( c ) used in formula ( 2.1 ) is defined as []
p6056
aVThe discounting values D 1 , D 2 , and D 3 + are defined as []
p6057
asg88
(lp6058
sg90
(lp6059
sg92
(lp6060
VWe have introduced a novel generalized language model as the systematic combination of skip n -grams and modified Kneser-Ney smoothing.
p6061
aVThe main strength of our approach is the combination of a simple and elegant idea with an an empirically convincing result.
p6062
aVMathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is consequently a real generalization.
p6063
aVIn an empirical evaluation, we have demonstrated that for higher orders the GLM outperforms MKN for all test cases.
p6064
aVThe relative improvement in perplexity is up to 12.7 % for large data sets.
p6065
aVGLMs also performs particularly well on small and sparse sets of training data.
p6066
aVOn a very small training data set we observed a reduction of perplexity by 25.7 %.
p6067
aVOur experiments underline that the generalized language models overcome in particular the weaknesses of modified Kneser-Ney smoothing on sparse training data.
p6068
ag106
asg107
S'P14-1108'
p6069
sg109
(lp6070
VWe introduce a novel approach for building language models based on a systematic, recursive exploration of skip n -gram models which are interpolated using modified Kneser-Ney smoothing.
p6071
aVOur approach generalizes language models as it contains the classical interpolation with lower order models as a special case.
p6072
aVIn this paper we motivate, formalize and present our approach.
p6073
aVIn an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing.
p6074
aVFurthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements.
p6075
aVFinally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data.
p6076
aVUsing a very small training data set of only 736 KB text we yield improvements of even 25.7 % reduction of perplexity.
p6077
ag106
asba(icmyPackage
FText
p6078
(dp6079
g3
(lp6080
VBy performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP, and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula
p6081
aVGiven a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk 1 1 In this work, the risk is defined as the measured volatility of stock prices from the week following the earnings call teleconference
p6082
aVSee details in Section 5
p6083
aVTo do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies
p6084
aVBy varying the number of dimensions of the covariates and the size of the training data, we show that the improvements over the baselines are robust across different parameter settings on three datasets
p6085
aVUsing the same dataset, Tsai and Wang [ 41 ] reformulate the regression problem as a text ranking problem
p6086
aV[ 45 ] have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task
p6087
aVBroadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market [ 3 , 47 ]
p6088
aVFor example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained
p6089
aVOn the other hand, once such assumptions are removed, another problem arises u'\u005cu2014' they might be prone to errors, and suffer from the overfitting issue
p6090
aVTherefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency
p6091
aVOn one hand, copula models [ 31 ] seek to explicitly model the dependency of random variables by separating the marginals and their correlations
p6092
aVFrom an information-theoretic point of view [ 38 ] , various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text
p6093
aVIn NLP, many of the probabilistic text models work in the discrete space [ 9 , 2 ] , but our model is different since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables
p6094
aVBy doing this, we are essentially performing probability integral transform u'\u005cu2014' an important statistical technique that moves beyond the count-based bag-of-words feature space to marginal cumulative density functions space
p6095
aVLast but not least, by using a parametric copula, in our case, the Gaussian copula, we reduce the computational cost from fully nonparametric methods, and explicitly model the correlations among the covariate and the dependent variable
p6096
aVIn the statistics literature, copula is widely known as a family of distribution function
p6097
aVThe idea behind copula theory is that the cumulative distribution function (CDF) of a random vector can be represented in the form of uniform marginal cumulative distribution functions, and a copula that connects these marginal CDFs, which describes the correlations among the input random variables
p6098
aVHowever, in order to have a valid multivariate distribution function regardless of n -dimensional covariates, not every function can be used as a copula function
p6099
aVThe central idea behind copula, therefore, can be summarize by the Sklar u'\u005cu2019' s theorem and the corollary
p6100
aVThen, if the marginal functions are continuous, there exists a unique copula C , such that
p6101
aVFurthermore, if the distributions are continuous, the multivariate dependency structure and the marginals might be separated, and the copula can be considered independent of the marginals [ 21 , 32 ]
p6102
aVTherefore, the copula does not have requirements on the marginal distributions, and any arbitrary marginals can be combined and their dependency structure can be modeled using the copula
p6103
aVThe problem is that text features are sparse, so we need to perform nonparametric kernel density estimation to smooth out the distribution of each variable
p6104
aVHere, K u'\u005cu2062' ( u'\u005cu22c5' ) is the kernel function, where in our case, we use the Box kernel 2 2 It is also known as the original Parzen windows [ 33 ]
p6105
aVComparing to the Gaussian kernel and other kernels, the Box kernel is simple, and computationally inexpensive
p6106
aVThe parameter h is the bandwidth for smoothing 3 3 In our implementation, we use the default h of the Box kernel in the ksdensity function in Matlab
p6107
aVwhere u'\u005cud835' u'\u005cudc08' u'\u005cu2062' { u'\u005cu22c5' } is the indicator function, and u'\u005cu039d' indicates the current value that we are evaluating
p6108
aVNote that the above step is also known as probability integral transform [ 11 ] , which allows us to convert any given continuous distribution to random variables having a uniform distribution
p6109
aVThis is of crucial importance to modeling text data instead of using the classic bag-of-words representation that uses raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity
p6110
aVNow that we have obtained the marginals, and then the joint distribution can be constructed by applying the copula function that models the stochastic dependencies among marginal CDFs
p6111
aVChristensen [ 8 ] shows that sorting and balanced binary trees can be used to calculate the correlation coefficients with complexity of O u'\u005cu2062' ( n u'\u005cu2062' log u'\u005cu2061' n
p6112
aVTherefore, the computational complexity of MLE for the proposed model is O u'\u005cu2062' ( n u'\u005cu2062' log u'\u005cu2061' n )
p6113
aVF x 1 ( x 1 ) , u'\u005cu2026' , F x n ( x n ) ) , one needs to solve the mean response u'\u005cud835' u'\u005cudc04' ^ ( F y ( y
p6114
aVAgain, the reason why we perform approximated inference is that exact inference in the high-dimensional Gaussian copula density is non-trivial, and might not have analytical solutions, but approximate inference using maximum density sampling from the Gaussian copula significantly relaxes the complexity of inference
p6115
aVUsing the stock prices, we can use the equations above to calculate the measured stock volatility after the earnings call, which is the standard measure of risks in finance, and the dependent variable y of our predictive task
p6116
aVWe use the Statistical Toolbox u'\u005cu2019' s linear regression implementation in Matlab, and LibSVM [ 6 ] for training and testing the SVM models
p6117
aVNote that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the u'\u005cu03a3' in the Gaussian copula, there is no hyperparameters that need to be tuned
p6118
aVSpearman u'\u005cu2019' s correlation [ 20 ] and Kendall u'\u005cu2019' s tau [ 23 ] have been widely used in many regression problems in NLP [ 1 , 46 , 42 , 41 ] , and here we use them to measure the quality of predicted values u'\u005cud835' u'\u005cudc32' ^ by comparing to the vector of ground truth u'\u005cud835' u'\u005cudc32'
p6119
aVComparing to second-best approaches, all improvements obtained by the copula model are statistically significant
p6120
aVThis is not surprising at all, because as max-margin models, soft-margin SVM only needs a handful of examples that come with nonvanishing coefficients (support vectors) to find a reasonable margin
p6121
aVFinally, we investigate the robustness of the proposed semiparametric Gaussian copula regression model by varying the amount of features in the covariate space
p6122
aVBoth linear and non-linear SVM models do not have any advantages over the proposed approach
p6123
aVOn the post-2009 dataset that concerns economic growth and recovery, the boundaries among all methods are very clear
p6124
aVWhat are the advantages of copula-based models, and what makes it perform so well
p6125
aVOne advantage we see from the copula model is that it does not require any assumptions on the marginal distributions
p6126
aVThis is rather restricted, because the possible shapes from a K - 1 simplex of Dirichlet is always limited in some sense
p6127
aVThis is extremely practical, because in many natural language processing tasks, we often have to deal with features that are extracted from many different domains and signals
p6128
aVBy applying the Probability Integral Transform to raw features in the copula model, we essentially avoid comparing apples and oranges in the feature space, which is a common problem in bag-of-features models in NLP
p6129
aVIn contrast, by considering the semiparametric case, we not only obtain some expressiveness from the nonparametric models, but also reduce the complexity of the task we are only interested in the finite-dimensional components u'\u005cu03a3' in the Gaussian copula with O u'\u005cu2062' ( n u'\u005cu2062' log u'\u005cu2061' n ) complexity, which is not as computationally difficult as the completely nonparametric cases
p6130
aVAlso, by modeling the marginals and their correlations seperately, our approach is cleaner, easy-to-understand, and allows us to have more flexibility to model the uncertainty of data
p6131
aVOur pilot experiment also aligns with our hypothesis when not performing the kernel density estimation part for smoothing out the marginal distributions, the performances dropped significantly when sparser features are included
p6132
aVHowever, this might not be practical at all in image processing, the u'\u005cu201c' cloud u'\u005cu201d' pixel of a pixel showing the blue sky of a picture are more likelihood to co-occur in the same picture; in natural language processing, the word u'\u005cu201c' mythical u'\u005cu201d' is more likely to co-occur with the word u'\u005cu201c' unicorn u'\u005cu201d' , rather than the word u'\u005cu201c' popcorn u'\u005cu201d'
p6133
aVTherefore, by modeling the correlations among marginal CDFs, the copula model has gained the insights on the dependency structures of the random variables, and thus, the performance of the regression task is boosted
p6134
aVFocusing on the three financial crisis related datasets, the proposed model significantly outperform the standard linear regression method in statistics and strong discriminative support vector regression baselines
p6135
aVBy varying the size of the training data and the dimensionality of the covariates, we have demonstrated that our proposed model is relatively robust across different parameter settings
p6136
asg88
(lp6137
sg90
(lp6138
sg92
(lp6139
VIn this work, we have demonstrated that the more complex quarterly earnings calls can also be used to predict the measured volatility of the stocks in the limited future.
p6140
aVWe propose a novel semiparametric Gausian copula regression approach that models the dependency structure of the language in the earnings calls.
p6141
aVUnlike traditional bag-of-features models that work discrete features from various signals, we perform kernel density estimation to smooth out the distribution, and use probability integral transform to work with CDFs that are uniform.
p6142
aVThe copula model deals with marginal CDFs and the correlation among them separately, in a cleaner manner that is also flexible to parameterize.
p6143
aVFocusing on the three financial crisis related datasets, the proposed model significantly outperform the standard linear regression method in statistics and strong discriminative support vector regression baselines.
p6144
aVBy varying the size of the training data and the dimensionality of the covariates, we have demonstrated that our proposed model is relatively robust across different parameter settings.
p6145
ag106
asg107
S'P14-1109'
p6146
sg109
(lp6147
VEarnings call summarizes the financial performance of a company, and it is an important indicator of the future financial risks of the company.
p6148
aVWe quantitatively study how earnings calls are correlated with the financial risks, with a special focus on the financial crisis of 2009.
p6149
aVIn particular, we perform a text regression task given the transcript of an earnings call, we predict the volatility of stock prices from the week after the call is made.
p6150
aVWe propose the use of copula a powerful statistical framework that separately models the uniform marginals and their complex multivariate stochastic dependencies, while not requiring any prior assumptions on the distributions of the covariate and the dependent variable.
p6151
aVBy performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP, and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula.
p6152
aVIn experiments, we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings.
p6153
ag106
asba(icmyPackage
FText
p6154
(dp6155
g3
(lp6156
VThis paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings, which can be used to measure the semantic relationship between words
p6157
aVWe identify whether a candidate word pair has hypernym u'\u005cu2013' hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms
p6158
aVIn the WordNet hierarchy, senses are organized according to the u'\u005cu201c' is-a u'\u005cu201d' relations
p6159
aVHere, u'\u005cu201c' canine u'\u005cu201d' is called a hypernym of u'\u005cu201c' dog u'\u005cu201d' Conversely, u'\u005cu201c' dog u'\u005cu201d' is a hyponym of u'\u005cu201c' canine u'\u005cu201d' As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications
p6160
aVHowever, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming
p6161
aVTherefore, many researchers have attempted to automatically extract semantic relations or to construct taxonomies
p6162
aVSeveral other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances [ 10 , 23 ]
p6163
aVBesides, distributional similarity methods [ 11 , 12 ] are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used
p6164
aVOur previous method based on web mining [ 8 ] works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics
p6165
aVThis paper proposes a novel approach for semantic hierarchy construction based on word embeddings
p6166
aVFurthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym u'\u005cu2013' hyponym relations (Section 3.3.2
p6167
aVSome have established concept hierarchies based on manually-built semantic resources such as WordNet [ 16 ]
p6168
aVWe have made similar obsevation that about a half of hypernym u'\u005cu2013' hyponym relations are absent in a Chinese semantic thesaurus
p6169
aVTherefore, a broader range of resources is needed to supplement the manually built resources
p6170
aVSeveral other methods are based on lexical patterns
p6171
aVA hierarchy can then be built based on these pairwise relations
p6172
aVGenerally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns
p6173
aVThe distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms
p6174
aVFor distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts
p6175
aV2010 ) design a directional distributional measure to infer hypernym u'\u005cu2013' hyponym relations based on the standard IR Average Precision evaluation measure
p6176
aVu'\u005cu2200' x , y , z u'\u005cu2208' L x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z u'\u005cu2227' z u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y ) u'\u005cu21d2' x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y
p6177
aVActually, x , y and z are unambiguous as the hypernyms of a certain entity
p6178
aVTherefore, G should be a directed acyclic graph (DAG
p6179
aVThese two models can be trained very efficiently on a large-scale corpus because of their low time complexity
p6180
aVAdditionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words
p6181
aVTherefore, we employ the Skip-gram model for estimating word embeddings in this study
p6182
aVThe Skip-gram model adopts log-linear classifiers to predict context words given the current word u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) as input
p6183
aVFirst, u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) is projected to its embedding
p6184
aVThen, log-linear classifiers are employed, taking the embedding as input and predict u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) u'\u005cu2019' s context words within a certain range, e.g., k words in the left and k words in the right
p6185
aVLooking at the well-known example v ( king ) - v ( queen ) u'\u005cu2248' v ( man ) - v ( woman ) , it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs
p6186
aVWe observe that the same property also applies to some hypernym u'\u005cu2013' hyponym relations
p6187
aVAs a preliminary experiment, we compute the embedding offsets between some randomly sampled hypernym u'\u005cu2013' hyponym word pairs and measure their similarities
p6188
aVIntuitively, we assume that all words can be projected to their hypernyms based on a uniform transition matrix
p6189
aVThat is, given a word x and its hypernym y , there exists a matrix u'\u005cu03a6' so that y = u'\u005cu03a6' u'\u005cu2062' x
p6190
aVFor simplicity, we use the same symbols as the words to represent the embedding vectors
p6191
aVA uniform linear projection may still be under-representative for fitting all of the hypernym u'\u005cu2013' hyponym word pairs, because the relations are rather diverse, as shown in Figure 2
p6192
aV3 3 www.ltp-cloud.com/download/ CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym u'\u005cu2013' hyponym relations (right panel, Figure 3
p6193
aVEach word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy
p6194
aVThe senses of words in the first level, such as u'\u005cu201c' ç© ( object ) u'\u005cu201d' and u'\u005cu201c' æ¶é´ ( time ), u'\u005cu201d' are very general
p6195
aVThe fourth level only has sense codes without real words
p6196
aVTherefore, we extract words in the second, third and fifth levels to constitute hypernym u'\u005cu2013' hyponym pairs (left panel, Figure 3
p6197
aVNote that mapping one hyponym to multiple hypernyms with the same projection ( u'\u005cu03a6' u'\u005cu2062' x is unique) is difficult
p6198
aVTherefore, the pairs with the same hyponym but different hypernyms are expected to be clustered into separate groups
p6199
aVFigure 3 shows that the word u'\u005cu201c' dragonfly u'\u005cu201d' in the fifth level has two hypernyms u'\u005cu201c' insect u'\u005cu201d' in the third level and u'\u005cu201c' animal u'\u005cu201d' in the second level
p6200
aVHence the relations dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' insect and dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' animal should fall into different clusters
p6201
aVHypernym-hyponym word pair ( x , y ) is classified into the direct category, only if there doesn u'\u005cu2019' t exist another word z in the training data, which is a hypernym of x and a hyponym of y
p6202
aVBesides, the final hierarchy should be a DAG as discussed in Section 3.1
p6203
aVHowever, the projection method cannot guarantee that theoretically, because the projections are learned from pairwise hypernym u'\u005cu2013' hyponym relations without the whole hierarchy structure
p6204
aVBut this is not the focus of this paper
p6205
aVSo if some conflicts occur, that is, a relation circle exists, we remove or reverse the weakest path heuristically (Figure 5
p6206
aVIn this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 4 Baidubaike ( baike.baidu.com ) is one of the largest Chinese encyclopedias containing more than 7.05 million entries as of September, 2013 which contains about 30 million sentences (about 780 million words
p6207
aVThe Chinese segmentation is provided by the open-source Chinese language processing platform LTP 5 5 www.ltp-cloud.com/demo/ [ 3 ]
p6208
aVThe hierarchies are represented as relations of pairwise words
p6209
aVWe measure the inter-annotator agreement using the kappa coefficient [ 22 ]
p6210
aVWe use precision, recall, and F-score as our metrics to evaluate the performances of the methods
p6211
aVSince hypernym u'\u005cu2013' hyponym relations and its reverse (hyponym u'\u005cu2013' hypernym) have one-to-one correspondence, their performances are equal
p6212
aVWe first evaluate the effect of different number of clusters based on the development data
p6213
aVAs shown in Figure 6 , the performance of clustering is better than non-clustering (when the cluster number is 1), thus providing evidences that learning piecewise projections based on clustering is reasonable
p6214
aVIn this section, we compare the proposed method with previous methods, including manually-built hierarchy extension, pairwise relation extraction based on patterns, word distributions, and web mining (Section 2
p6215
aVTable 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia
p6216
aVM P u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' n refers to the pattern-based method of Hearst ( 1992
p6217
aVThe result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners
p6218
aVThe same training data for projections learning from CilinE (Section 3.3.3 ) is used as seed hypernym u'\u005cu2013' hyponym pairs
p6219
aVLexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
p6220
aVWe then develop a logistic regression classifier based on the patterns to recognize hypernym u'\u005cu2013' hyponym relations
p6221
aVM E u'\u005cu2062' m u'\u005cu2062' b is the proposed method based on word embeddings
p6222
aVThe combination strategy is to simply merge all positive results from the two methods together, and then to infer new relations based on the transitivity of hypernym u'\u005cu2013' hyponym relations
p6223
aVThe reason is that the inference based on the relations identified automatically may lead to error propagation
p6224
aVCombining M E u'\u005cu2062' m u'\u005cu2062' b with M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E achieves a 7% F-score improvement over the best baseline M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p6225
aVTherefore, the proposed method is complementary to the manually-built hierarchy extension method [ 27 ]
p6226
aVActually, we can get different precisions and recalls by adjusting the threshold u'\u005cu0394' (Equation 3
p6227
aVFigure 8 shows that our method loses the relation u'\u005cu201c' ä¹å¤´å± ( Aconitum ) u'\u005cu201d' u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' u'\u005cu201c' æ¯èç§ ( Ranunculaceae u'\u005cu201d' It is because they are very semantically similar (their cosine similarity is 0.9038
p6228
aVTheir representations are so close to each other in the embedding space that we have not find projections suitable for these pairs
p6229
aV2007 ) , and Sang ( 2007 ) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst ( 1992
p6230
aVHowever, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction
p6231
aVFollowing the method for discovering patterns automatically [ 23 ] , McNamee et al
p6232
aV2009 ) propose a method based on patterns to find hypernyms on arbitrary noun phrases
p6233
aVAs our experiments show, pattern-based methods suffer from low recall because of the low coverage of patterns
p6234
aV2006 ) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods
p6235
aV2013b ) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors
p6236
aVIn this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym u'\u005cu2013' hyponym relationships within different clusters
p6237
aVThis paper proposes a novel method for semantic hierarchy construction based on word embeddings, which are trained using a large-scale corpus
p6238
aVUsing the word embeddings, we learn the hypernym u'\u005cu2013' hyponym relationship by estimating projection matrices which map words to their hypernyms
p6239
aVBased on the pairwise hypernym u'\u005cu2013' hyponym relations, we build semantic hierarchies automatically
p6240
aVBy including the hypernym u'\u005cu2013' hyponym relation constraints while training word embeddings, we expect to improve the embeddings such that they become more suitable for this task
p6241
asg88
(lp6242
sg90
(lp6243
sg92
(lp6244
VThis paper proposes a novel method for semantic hierarchy construction based on word embeddings, which are trained using a large-scale corpus.
p6245
aVUsing the word embeddings, we learn the hypernym hyponym relationship by estimating projection matrices which map words to their hypernyms.
p6246
aVFurther improvements are made using a cluster-based approach in order to model the more fine-grained relations.
p6247
aVThen we propose a few simple criteria to identity whether a new word pair is a hypernym hyponym relation.
p6248
aVBased on the pairwise hypernym hyponym relations, we build semantic hierarchies automatically.
p6249
aVIn our experiments, the proposed method significantly outperforms state-of-the-art methods and achieves the best F1-score of 73.74% on a manually labeled test dataset.
p6250
aVFurther experiments show that our method is complementary to the previous manually-built hierarchy extension methods.
p6251
aVFor future work, we aim to improve word embedding learning under the guidance of hypernym hyponym relations.
p6252
aVBy including the hypernym hyponym relation constraints while training word embeddings, we expect to improve the embeddings such that they become more suitable for this task.
p6253
ag106
asg107
S'P14-1113'
p6254
sg109
(lp6255
VSemantic hierarchy construction aims to build structures of concepts linked by hypernym hyponym ( is-a ) relations.
p6256
aVA major challenge for this task is the automatic discovery of such relations.
p6257
aVThis paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings, which can be used to measure the semantic relationship between words.
p6258
aVWe identify whether a candidate word pair has hypernym hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms.
p6259
aVOur result, an F-score of 73.74%, outperforms the state-of-the-art methods on a manually labeled test dataset.
p6260
aVMoreover, combining our method with a previous manually-built hierarchy extension method can further improve F-score to 80.29%.
p6261
aVUTF8gbsn.
p6262
ag106
asba(icmyPackage
FText
p6263
(dp6264
g3
(lp6265
VWe propose a novel abstractive query-based summarization system for conversations, where queries are defined as phrases reflecting a user information needs
p6266
aVWe rank and extract the utterances in a conversation based on the overall content and the phrasal query information
p6267
aVWe cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model
p6268
aVWe propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster
p6269
aVA resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation
p6270
aVAutomatic summarization has been proposed in the past as a way to address this problem (e.g.,, [ 25 ]
p6271
aVHowever, often a good summary cannot be generic and should be a brief and well-organized paragraph that answer a user u'\u005cu2019' s information need
p6272
aVThe Document Understanding Conference (DUC) 1 1 http://www-nlpir.nist.gov/projects/duc/index.html has launched query-focused multidocument summarization as its main task since 2004, by focusing on complex queries with very specific answers
p6273
aVFor example, u'\u005cu201c' How were the bombings of the US embassies in Kenya and Tanzania conducted
p6274
aVTo address these issues, in this work, we tackle the task of conversation summarization based on phrasal queries
p6275
aVWe define a phrasal query as a concatenation of two or more keywords, which is a more realistic representation of a user u'\u005cu2019' s information needs
p6276
aVExample 1 shows two queries and their associated human written summaries based on a single chat log
p6277
aVThis is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling
p6278
aVEven though some works try to address the problem of summarizing multiparty written conversions (e.g.,, [ 20 , 29 , 23 , 32 , 9 ] ), they do so in a generic way (not query-based) and focus on only one conversational domain (e.g.,, meetings
p6279
aVTo address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization
p6280
aV1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on usersâ phrasal queries, instead of well-formed questions
p6281
aVAs a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms [ 17 ]
p6282
aV2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e.,, fluency) of the sentence into consideration
p6283
aV3) Although most of the current summarization approaches use supervised algorithms as a part of their system (e.g.,, [ 30 ] ), our method can be totally unsupervised and does not depend on human annotation
p6284
aVAbstractive summary sentences can be created by aggregating and merging multiple sentences into an abstract sentence
p6285
aVThis task can be considered as content selection
p6286
aVMoreover, this step, stand alone, corresponds to an extractive summarization system
p6287
aVIn order to select and extract the informative summary-worthy utterances, based on the phrasal query and the original text, we consider two criteria i) utterances should carry the essence of the original text; and ii) utterances should be relevant to the query
p6288
aVIn this work, we use log-likelihood ratio to extract the signature terms from chat logs, since log-likelihood ratio leads to better results [ 12 ]
p6289
aVNote that we limit our synsets to the nouns since verb synonyms do not prove to be effective in query expansion [ 13 ]
p6290
aVTo estimate the utterance score, we view both the query terms and the signature terms as the terms that should appear in a human query-based summary
p6291
aVTo achieve this, the most relevant (summary-worthy) utterances that we select are the ones that maximize the coverage of such terms
p6292
aVWe estimate the percentage of the retrieved utterances based on the development set
p6293
aVBy identifying the semantic relations between the sentences, we can discover what information in one sentence is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences
p6294
aVSimilar to earlier work [ 3 , 1 ] , we set this problem as a variant of the Textual Entailment (TE) recognition task [ 5 ]
p6295
aVUsing entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g.,, Maximal Marginal Relevance) and shown to be more effective [ 19 ]
p6296
aVWe follow the same practice as [ 19 ] to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones
p6297
aVIn this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries
p6298
aVWe believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured
p6299
aVMoreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained
p6300
aVWe use the K-mean clustering algorithm by cosine similarity as a distance function between sentence vectors composed of tf.idf scores
p6301
aVAlso notice that the lexical similarity between sentences in one cluster facilitates both the construction of the word graph and finding the best path in the word graph, as described next
p6302
aVIn order to construct a word graph, we adopt the method recently proposed by [ 19 , 7 ] with some optimizations
p6303
aVIn this case the lexical choice for the node is selected based on the tf.idf score of each node;
p6304
aVWe are aiming at generating an informative abstractive sentence for each cluster based on a user query
p6305
aVThe purpose of this function is two-fold i) to generate a grammatical sentence by favoring the links between nodes (words) which appear often; and ii) to generate an informative sentence by increasing the weight of edges connecting salient nodes
p6306
aVThe purpose of such a model is three-fold i) to cover the content of query information optimally; ii) to generate a more readable and grammatical sentence; and iii) to favor strong connections between the concepts
p6307
aVTherefore, the final ranking score of path P is calculated over the normalized scores as
p6308
aVIn order to rank the graph paths, we select all the paths that contain at least one verb and rerank them using our proposed ranking function to find the best path as the summary of the original sentences in each cluster
p6309
aVIn this section, we show the evaluation results of our proposed framework and its comparison to the baselines and a state-of-the-art query-focused extractive summarization system
p6310
aVOne of the challenges of this work is to find suitable conversational datasets that can be used for evaluating our query-based summarization system
p6311
aVIn this way, the title of each summary can be counted as a phrasal query and the corresponding summary is considered as the query-based abstract of the associated chat log including only the information most relevant to the title
p6312
aVTherefore, we can use the human-written query-based abstract as gold standards and evaluate our system automatically
p6313
aVWe use the extracted key-phrases as queries to generate query-based abstracts
p6314
aVSince there is no human-written query-based summary for AMI corpus, we randomly select 10 meetings and evaluate our system manually
p6315
aVIn order to adapt this corpus to our framework, we followed the same query generation process as for the meeting dataset
p6316
aVFinally, we randomly select 10 emails threads and evaluate the results manually
p6317
aV1) Cosine-1st we rank the utterances in the chat log based on the cosine similarity between the utterance and query
p6318
aVThen, we select the first uttrance as the summary;
p6319
aV2) Cosine-all we rank the utterances in the chat log based on the cosine similarity between the utterance and query and then select the utterances with a cosine similarity greater than 0 ;
p6320
aV3) TextRank a widely used graph-based ranking model for single-document sentence extraction that works by building a graph of all sentences in a document and use similarity as edges to compute the salience of sentences in the graph [ 22 ] ;
p6321
aVMoreover, we compare our abstractive system with the first part of our framework (utterance extraction in Figure 1), which can be presented as an extractive query-based summarization system (our extractive system
p6322
aVWe also show the results of the version we use in our pipeline (our pipeline extractive system
p6323
aVIn our pipeline we aim at higher recall, since we later filter sentences and aggregate them to generate new abstract sentences
p6324
aVThen, we asked annotators to give one of three possible ratings for each sentence based on grammaticality perfect (2 pts), only one mistake (1 pt) and not acceptable (0 pts), ignoring capitalization or punctuation
p6325
aVNote that each sentence was evaluated individually, so the human judges were not affected by intra-sentential problems posed by coreference and topic shifts
p6326
aVWe use six randomly selected query-logs from our chat dataset (about 10% of the dataset) for tuning the coefficient parameters
p6327
aVWe set the k parameter in our clustering phase to 10 based on the average number of sentences in the human written summaries
p6328
aVExtractive our full query-based abstractive summariztion system show statistically significant improvements over baselines and other pure extractive summarization systems for ROUGE-1 4 4 The statistical significance tests was calculated by approximate randomization, as described in [ 31 ]
p6329
aVThis means our systems can effectively aggregate the extracted sentences and generate abstract sentences based on the query content
p6330
aVIn other words, using our extractive model described in section 2.1, as a stand alone system, is an effective query-based extractive summarization model
p6331
aVThis can be due to word merging and word replacement choices in the word graph construction, which sometimes change or remove a word in a bigram and consequently may decrease the bigram overlap score
p6332
aVThe LexRank baseline improves the results of the TextRank system by increasing the precision and balancing the precision and recall scores for ROUGE-1 score
p6333
aVWe believe that this is due to the robustness of the LexRank method in dealing with noisy texts (chat conversations) [ 6 ]
p6334
aVThis confirms the validity of the results we obtained by conducting automatic evaluation over the chat dataset
p6335
aVThis is expected since dealing with spoken conversations is more challenging than written ones
p6336
aVFor meeting dataset, the percentage of completely grammatical sentences drops dramatically
p6337
aVThis is due to the nature of spoken conversations which is more error prone and ungrammatical
p6338
aVWe have presented an unsupervised framework for abstractive summarization of spoken and written conversations based on phrasal queries
p6339
aVFor the generation phase, we propose a ranking strategy which selects the best path in the constructed word graph based on fluency, query relevance and content
p6340
aVSecond, we aim at implementing a strategy to order the clusters for generating more coherent abstracts
p6341
asg88
(lp6342
sg90
(lp6343
sg92
(lp6344
VWe have presented an unsupervised framework for abstractive summarization of spoken and written conversations based on phrasal queries.
p6345
aVFor content selection, we propose a sentence extraction model that incorporates query relevance and content importance into the extraction process.
p6346
aVFor the generation phase, we propose a ranking strategy which selects the best path in the constructed word graph based on fluency, query relevance and content.
p6347
aVBoth automatic and manual evaluation of our model show substantial improvement over extraction-based methods, including Biased LexRank, which is considered a state-of-the-art system.
p6348
aVMoreover, our system also yields good grammaticality score for human evaluation and achieves comparable scores with the original sentences.
p6349
aVOur future work is four-fold.
p6350
aVFirst, we are trying to improve our model by incorporating conversational features (e.g.,, speech acts.
p6351
aVSecond, we aim at implementing a strategy to order the clusters for generating more coherent abstracts.
p6352
aVThird, we try to improve our generated summary by resolving coreferences and incorporating speaker information (e.g.,, names) in the clustering and sentence generation phases.
p6353
aVFinally, we plan to take advantage of topic shifts to better segment the relevant parts of conversations in relation to phrasal queries.
p6354
ag106
asg107
S'P14-1115'
p6355
sg109
(lp6356
VWe propose a novel abstractive query-based summarization system for conversations, where queries are defined as phrases reflecting a user information needs.
p6357
aVWe rank and extract the utterances in a conversation based on the overall content and the phrasal query information.
p6358
aVWe cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model.
p6359
aVWe propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster.
p6360
aVA resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation.
p6361
aVAutomatic and manual evaluation results over meeting, chat and email conversations show that our approach significantly outperforms baselines and previous extractive models.
p6362
aVOur phrasal query abstraction framework generates a grammatical abstract from a conversation following three steps, as shown in Figure 1.
p6363
aVIn this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries.
p6364
aVThere are several ways of generating abstract sentences (e.g., [ 2 , 18 , 8 , 23 ] ); however, most of them rely heavily on the sentence structure.
p6365
aVWe believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured.
p6366
aVInstead, we apply an approach that does not rely on syntax, nor on a standard NLG architecture.
p6367
aVMoreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained.
p6368
aVWe perform the task of abstract generation in three steps, as follows.
p6369
ag106
asba(icmyPackage
FText
p6370
(dp6371
g3
(lp6372
VOur proposed methodology treats content selection as a multi-label (ML) classification problem, which takes as input time-series data and outputs a set of templates, while capturing the dependencies between selected templates
p6373
aVWe show that the different methods have different benefits, with ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students
p6374
aVFirstly, they change over time, and secondly they can be dependent on or independent of each other
p6375
aVTherefore, when generating feedback, we need to take into account all variables simultaneously in order to capture potential dependencies and provide more effective and useful feedback that is relevant to the students
p6376
aVContent selection decisions based on trends in time-series data determine the selection of the useful and important variables, which we refer to here as factors , that should be conveyed in a summary
p6377
aVThe decisions of factor selection can be influenced by other factors that their values are correlated with; can be based on the appearance or absence of other factors in the summary; and can be based on the factors u'\u005cu2019' behaviour over time
p6378
aVMoreover, some factors may have to be discussed together in order to achieve some communicative goal, for instance, a teacher might want to refer to student u'\u005cu2019' s marks as a motivation for increasing the number of hours studied
p6379
aVWe frame content selection as a simple classification task given a set of time-series data, decide for each template whether it should be included in a summary or not
p6380
aVHowever, simple classification assumes that the templates are independent of each other, thus the decision for each template is taken in isolation from the others, which is not appropriate for our domain
p6381
aVHere, we propose an alternative method that tackles the challenge of interdependent data by using multi-label (ML) classification, which is efficient in taking data dependencies into account and generating a set of labels (in our case templates) simultaneously []
p6382
aVML classification requires no history, i.e., does not keep track of previous decisions, and thus has a smaller feature space
p6383
aVOur contributions to the field are as follows we present a novel and efficient method for tackling the challenge of content selection using a ML classification approach; we applied this method to the domain of feedback summarisation; we present a comparison with an optimisation technique (Reinforcement Learning), and we discuss the similarities and differences between the two methods
p6384
aVThe difference between the two methods lies in that the collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels
p6385
aVML-kNN identifies for each new instance its k nearest neighbours in the training set and then it predicts the label set by utilising the maximum a posteriori principle according to statistical information derived from the label sets of the k neighbours
p6386
aVEnsemble methods [] are algorithms that use ensembles to perform ML learning and they are based on problem transformation or algorithm adaptation methods
p6387
aVFinally, our domain for feedback generation is motivated by previous studies [] who show that text summaries are more effective in decision making than graphs therefore it is advantageous to provide a summary over showing users the raw data graphically
p6388
aVFor the corpus creation, 11 lecturers selected the content to be conveyed in a summary, given the set of raw data []
p6389
aVAs a result, for the same student there are various summaries provided by the different experts
p6390
aVOur analysis of the dataset showed that there are significant correlations between the factors, for example, the number of lectures attended (LA) correlates with the student u'\u005cu2019' s understanding of the material (Und), see Table 5
p6391
aVAs we will discuss further in Section 5.1 , content decisions are influenced by the previously generated content, for example, if the lecturer has previously mentioned health_issues, mentioning hours_studied has a high probability of also being mentioned
p6392
aVContent is regarded as labels (each template represents a label) and thus the task can be thought of as a classification problem
p6393
aVAs mentioned, there are 4 ways to refer to a factor
p6394
aVOverall, for all factors there are 29 different templates 1 1 There are fewer than 36 templates, because for some factors there are less than 4 possible ways of referring to them
p6395
aVInstead of dealing with this task in a hierarchical way, where the algorithm will first learn whether to talk about a factor and then to decide how to refer to it, we transformed the task in order to reduce the learning steps
p6396
aVTherefore, classification can reduce the decision workload by deciding either in which way to talk about it, or not to talk about a factor at all
p6397
aVTraditional single-label classification is the task of identifying which label one new observation is associated with, by choosing from a set of labels L []
p6398
aVRAkEL is based on Label Powerset (LP), a problem transformation method []
p6399
aVLP benefits from taking into consideration label correlations, but does not perform well when trained with few examples as in our case []
p6400
aVRAkEL overcomes this limitation by constructing a set of LP classifiers, which are trained with different random subsets of the set of labels []
p6401
aVThis algorithm does not perform well when considering a large number of labels, due to the fact that the label space grows exponentially []
p6402
aVRAkEL tackles this problem by constructing an ensemble of LP classifiers and training each one on a different random subset of the set of labels []
p6403
aVThe algorithm was implemented using the MULAN Open Source Java library [] , which is based on WEKA []
p6404
aVRAkEL takes as input the following parameters
p6405
aV1) the numbers of iterations m (which is developer specified and denotes the number of models that the algorithm will produce), (2) the size of labelset k (which is also developer specified), (3) the set of labels L , and (4) the training set D
p6406
aVDuring run time, RAkEL estimates the average decision for each label in L and if the average is greater than a threshold t (determined by the developer) it includes the label in the predicted labelset
p6407
aVIn future, we could perform parameter optimisation by using a technique similar to []
p6408
aVIt was found out that Decision Trees achieved on average 3% higher accuracy
p6409
aVWe, therefore, went on to use Decision Trees that use generation history in three ways
p6410
aVThis method did not take into account other selected templates u'\u005cu2013' it was only based on the time-series data
p6411
aVThe reduced accuracy of the classification with predicted history is due to the error in the predicted values
p6412
aVThe upper-bound accuracy is 78.09% calculated by using the expert previous decisions and not the potentially erroneous predicted decisions
p6413
aVThis result is indicative of the significance of the relations between the factors showing that the predicted decisions are dependent due to existing correlations as discussed in Section 1 , therefore the system should not take these decisions independently
p6414
aVML classification performs better because it does take into account these correlations and dependencies in the data
p6415
aVReinforcement Learning (RL) is a machine learning technique that defines how an agent learns to take optimal actions so as to maximise a cumulative reward []
p6416
aVContent selection is seen as a Markov Decision problem and the goal of the agent is to learn to take the sequence of actions that leads to optimal content selection
p6417
aVIf the agent decides to refer to a factor, the template is selected in a deterministic way, i.e., from the available templates it selects the template that results in higher expected cumulative future reward
p6418
aVWe compared the ML system and the RL system with two baselines described below by measuring the accuracy of their outputs, the reward achieved by the reward function used for the RL system, and finally we also performed evaluation with student users
p6419
aVIn order to reduce the confounding variables, we kept the ordering of content in all systems the same, by adopting the ordering of the rule-based system
p6420
aVRule-based System generates summaries based on Content Selection rules derived by working with a L T expert and a student []
p6421
aVAccuracy was estimated as the proportion of the correctly classified templates to the population of templates
p6422
aVIn order to have a more objective view on the results, the score achieved by each algorithm using the reward function was also calculated
p6423
aVML classification achieved significantly higher accuracy, which was expected as it is a supervised learning method
p6424
aVThere is evidently a mismatch between the rules and the test-set; the content selection rules are based on heuristics provided by a L T Expert rather than by the same pool of lecturers that created the test-set
p6425
aVOn the contrary, the RL is trained to optimise the selected content and not to replicate the existing lecturer summaries, hence there is a difference in accuracy
p6426
aVRL is trained to optimise for this function, and therefore it achieves higher reward, whereas ML is trained to learn by examples, therefore it produces output closer to the gold standard (lecturer u'\u005cu2019' s produced summaries
p6427
aVMoreover, each decision is rewarded with a different value as some combinations of factors and templates have greater or negative regression coefficients
p6428
aVOn the other hand, when mentioning the average difficulty the summary is u'\u005cu201c' punished u'\u005cu201d' with -81 (see description of the reward function in Section 5.2
p6429
aVConsequently, a single poor decision in the ML classification can result in much less reward
p6430
aVThe classification method reduces the generation steps, by making the decision of the factor selection and the template selection jointly
p6431
aVWe have shown that ML classification for summarisation of our time-series data has an accuracy of 76.95% and that this approach significantly outperforms other classification methods as it is able to capture dependencies in the data when making content selection decisions
p6432
aVThis may be due to the fact that the RL optimisation method is able to provide more varied responses over time rather than just emulating the training data as with standard supervised learning approaches
p6433
aVFoster \u005cshortcite Foster2008 found similar results when performing a study on generation of emphatic facial displays
p6434
aVA previous study by Belz and Reiter \u005cshortcite Belz2006 has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality
p6435
aVMulti-label classification generates output closer to gold standard whereas RL can optimise the output according to a reward function
p6436
aVML classification could be used when the goal of the generation is to replicate phenomena seen in the dataset, because it achieves high accuracy, precision and recall
p6437
aVFor this initial experiment, we evaluated with students and not with lecturers, since the students are the recipients of feedback
p6438
aVMoreover, we plan to utilise the results from this student evaluation in order to train an optimisation algorithm to perform summarisation according to students u'\u005cu2019' preferences
p6439
aVIn this case, optimisation would be the preferred method as it would not be appropriate to collect gold standard data from students
p6440
asg88
(lp6441
sg90
(lp6442
sg92
(lp6443
VWe have shown that ML classification for summarisation of our time-series data has an accuracy of 76.95% and that this approach significantly outperforms other classification methods as it is able to capture dependencies in the data when making content selection decisions.
p6444
aVML classification was also directly compared to a RL method.
p6445
aVIt was found that although ML classification is almost 20% more accurate than RL, both methods perform comparably when rated by humans.
p6446
aVThis may be due to the fact that the RL optimisation method is able to provide more varied responses over time rather than just emulating the training data as with standard supervised learning approaches.
p6447
aVFoster \u005cshortcite Foster2008 found similar results when performing a study on generation of emphatic facial displays.
p6448
aVA previous study by Belz and Reiter \u005cshortcite Belz2006 has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality.
p6449
aVIn our study, the human ratings correlate well to the average scores achieved by the reward function.
p6450
aVHowever, the human ratings do not correlate well to the accuracy scores.
p6451
aVIt is interesting that the two methods that score differently on various automatic metrics, such as accuracy, reward, precision, recall and F-score, are evaluated similarly by users.
p6452
aVThe comparison shows that each method can serve different goals.
p6453
aVMulti-label classification generates output closer to gold standard whereas RL can optimise the output according to a reward function.
p6454
aVML classification could be used when the goal of the generation is to replicate phenomena seen in the dataset, because it achieves high accuracy, precision and recall.
p6455
aVHowever, optimisation methods can be more flexible, provide more varied output and can be trained for different goals, e.g., for capturing preferences of different users.
p6456
ag106
asg107
S'P14-1116'
p6457
sg109
(lp6458
VWe present a novel approach for automatic report generation from time-series data, in the context of student feedback generation.
p6459
aVOur proposed methodology treats content selection as a multi-label (ML) classification problem, which takes as input time-series data and outputs a set of templates, while capturing the dependencies between selected templates.
p6460
aVWe show that this method generates output closer to the feedback that lecturers actually generated, achieving 3.5% higher accuracy and 15% higher F-score than multiple simple classifiers that keep a history of selected templates.
p6461
aVFurthermore, we compare a ML classifier with a Reinforcement Learning (RL) approach in simulation and using ratings from real student users.
p6462
aVWe show that the different methods have different benefits, with ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students.
p6463
ag106
asba(icmyPackage
FText
p6464
(dp6465
g3
(lp6466
VThe compression task has received increasing attention in recent years, in part due to the availability of datasets such as the Ziff-Davis corpus [ 24 ] and the Edinburgh compression corpora [ 5 ] , from which the following example is drawn
p6467
aVIn 1967 Chapman, who had cultivated a conventional image, stunned a party by coming out as a homosexual
p6468
aVFollowing an assumption often used in compression systems, the compressed output in this corpus is constructed by dropping tokens from the input sentence without any paraphrasing or reordering
p6469
aVIn this work, we develop approximate inference strategies to the joint approach of Thadani and McKeown ( 2013 ) which trade the optimality guarantees of exact ILP for faster inference by separately solving the n-gram and dependency subproblems and using Lagrange multipliers to enforce consistency between their solutions
p6470
aVMaximum spanning tree algorithms, commonly used in non-projective dependency parsing [ 35 ] , are not easily adaptable to this task since the maximum-weight subtree is not necessarily a part of the maximum spanning tree
p6471
aVWe therefore consider methods to recover approximate solutions for the subproblem of finding the maximum weighted subtree in a graph, common among which is the use of a linear programming relaxation
p6472
aVIn addition, we can recover approximate solutions to this problem by using the Chu-Liu Edmonds algorithm for recovering maximum spanning trees [ 4 , 14 ] over the relatively sparse subgraph defined by a solution to the relaxed LP
p6473
aVMultiple approaches to generate good approximate solutions for joint multi-structure compression, based on Lagrangian relaxation to enforce equality between the sequential and syntactic inference subproblems
p6474
aVEven though compression is typically formulated as a token deletion task, it is evident that dropping tokens independently from an input sentence will likely not result in fluent and meaningful compressive text
p6475
aVTokens in well-formed sentences participate in a number of syntactic and semantic relationships with other tokens, so one might expect that accounting for heterogenous structural relationships between tokens will improve the coherence of the output sentence
p6476
aVFurthermore, much recent work has focused on the challenge of joint sentence extraction and compression, also known as compressive summarization [ 34 , 2 , 1 , 30 , 41 ] , in which questions of efficiency are paramount due to the larger problems involved; however, these approaches largely restrict compression to pruning parse trees, thereby imposing a dependency on parser performance
p6477
aVFollowing this, § 2.3 discusses a dynamic program to find maximum weight bigram subsequences from the input sentence, while § 2.4 covers LP relaxation-based approaches for approximating solutions to the problem of finding a maximum-weight subtree in a graph of potential output dependencies
p6478
aVIn addition, we define bigram indicator variables y i u'\u005cu2062' j u'\u005cu2208' { 0 , 1 } to represent whether a particular order-preserving bigram 2 2 Although Thadani and McKeown ( 2013 ) is not restricted to bigrams or order-preserving n-grams, we limit our discussion to this scenario as it also fits the assumptions of McDonald ( 2006 ) and the datasets of Clarke and Lapata ( 2006 u'\u005cu27e8' t i , t j u'\u005cu27e9' from S is present as a contiguous bigram in C as well as dependency indicator variables z i u'\u005cu2062' j u'\u005cu2208' { 0 , 1 } corresponding to whether the dependency arc t i u'\u005cu2192' t j is present in the dependency parse of C
p6479
aVIn order to recover meaningful compressions by optimizing ( 2 ), the inference step must ensure
p6480
aVDual decomposition [ 26 ] and Lagrangian relaxation in general are often used for solving joint inference problems which are decomposable into individual subproblems linked by equality constraints [ 27 , 44 , 43 , 13 , 32 , 11 , 1 ]
p6481
aVThe multi-structure inference problem described in the previous section seems in many ways to be a natural fit to such an approach since output scores factor over different types of structure that comprise the output compression
p6482
aVEven if ILP-based approaches perform reasonably at the scale of single-sentence compression problems, the exponential worst-case complexity of general-purpose ILPs will inevitably pose challenges when scaling up to (a) handle larger inputs, (b) use higher-order structural fragments, or (c) incorporate additional models
p6483
aVWe can now rewrite the objective in ( 2 ) while enforcing the constraint that the words contained in the sequence u'\u005cud835' u'\u005cudc32' are the same as the words contained in the tree u'\u005cud835' u'\u005cudc33' , i.e.,, u'\u005cud835' u'\u005cudf36' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc32' ) = u'\u005cud835' u'\u005cudf37' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc33' ) , by introducing a vector of Lagrange multipliers u'\u005cud835' u'\u005cudf40' u'\u005cu2208' u'\u005cu211d' n
p6484
aVFinding the u'\u005cud835' u'\u005cudc32' and u'\u005cud835' u'\u005cudc33' that maximize this Lagrangian above yields a dual objective, and the dual problem corresponding to the primal objective specified in ( 2 ) is therefore the minimization of this objective over the Lagrange multipliers u'\u005cud835' u'\u005cudf40'
p6485
aVThis can now be solved with the iterative subgradient algorithm illustrated in Algorithm 2.2
p6486
aVIn each iteration i , the algorithm solves for u'\u005cud835' u'\u005cudc32' ( i ) and u'\u005cud835' u'\u005cudc33' ( i ) under u'\u005cud835' u'\u005cudf40' ( i ) , then generates u'\u005cud835' u'\u005cudf40' ( i + 1 ) to penalize inconsistencies between u'\u005cud835' u'\u005cudf36' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc32' ( i ) ) and u'\u005cud835' u'\u005cudf37' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc33' ( i )
p6487
aVOtherwise, if the algorithm starts oscillating between a few primal solutions, the underlying LP must have a non-integral solution in which case approximation heuristics can be employed
p6488
aVwhere R is the required number of output tokens and the scoring function is defined as
p6489
aVso as to solve f u'\u005cu2062' ( u'\u005cud835' u'\u005cudc32' , u'\u005cud835' u'\u005cudf40' , u'\u005cu03a8' , u'\u005cud835' u'\u005cudf3d' ) from ( 4
p6490
aVThis approach requires O u'\u005cu2062' ( n 2 u'\u005cu2062' R ) time in order to identify the highest scoring sequence u'\u005cud835' u'\u005cudc32' and corresponding token configuration u'\u005cud835' u'\u005cudf36' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc32' )
p6491
aVIn order to produce a solution to this subproblem, we use an LP relaxation of the relevant portion of the ILP from Thadani and McKeown ( 2013 ) by omitting integer constraints over the token and dependency variables in u'\u005cud835' u'\u005cudc31' and u'\u005cud835' u'\u005cudc33' respectively
p6492
aVFirst, tokens in the solution must only be active if they have a single active incoming dependency edge
p6493
aVIn addition, they serve to establish connectivity for the dependency structure u'\u005cud835' u'\u005cudc33' since commodity can only originate in one location u'\u005cu2014' at the pseudo-token root which has no incoming commodity variables
p6494
aVIn the LP relaxation, x i and z i u'\u005cu2062' j are redefined as real-valued variables in [ 0 , 1 ] , potentially resulting in fractional values for dependency and token indicators
p6495
aVAs a result, the commodity flow network is able to establish connectivity but cannot enforce a tree structure, for instance, directed acyclic structures are possible and token indicators x i may be partially be assigned to the solution structure
p6496
aVThe viability of this approximation strategy is due to the following
p6497
aVThe bigram subproblem is guaranteed to return a well-formed integral solution which obeys the imposed compression rate, so we are assured of a source of valid u'\u005cu2014' if non-optimal u'\u005cu2014' solutions in line 13 of Algorithm 2.2
p6498
aVThe addition of this constraint to the relaxed LP reduces the rate of integral solutions drastically u'\u005cu2014' from 89% to approximately 33% u'\u005cu2014' but it serves to ensure that the resulting token configuration u'\u005cud835' u'\u005cudc31' ~ has at least as many non-zero elements as R , i.e.,, there are at least as many tokens activated in the LP solution as are required in a valid solution
p6499
aVSince the commodity flow constraints in ( 9 ) u'\u005cu2013' ( 11 ) ensure a connected u'\u005cud835' u'\u005cudc33' ~ , it is therefore possible to recover a maximum-weight spanning tree from G u'\u005cu2062' ( u'\u005cud835' u'\u005cudc33' ~ ) using the Chu-Liu Edmonds algorithm [ 4 , 14 ]
p6500
aVThe resulting spanning tree is a useful integral approximation of u'\u005cud835' u'\u005cudc33' ~ but, as mentioned previously, may contain more nodes than R due to fractional values in u'\u005cud835' u'\u005cudc31' ~ ; we therefore repeatedly prune leaves with the lowest incoming edge weight in the current tree until exactly R nodes remain
p6501
aVWe identify this phenomenon by counting repeated solutions and, if they exceed some threshold l max with at least one repeated solution from either subproblem, we terminate the update procedure for Lagrange multipliers and instead attempt to identify a good solution from the repeating ones by scoring them under ( 2
p6502
aVThe features used in this work are largely based on the features from Thadani and McKeown ( 2013 )
p6503
aVu'\u005cu03a6' dep contains features for the probability of a dependency edge under a smoothed dependency grammar constructed from the Penn Treebank and various conjunctions of the following features a) whether the edge appears as a dependency or ancestral relation in the input parse (b) the directionality of the dependency (c) the label of the edge (d) the POS tags of the tokens incident to the edge and (e) the labels of their surrounding chunks and whether the edge remains within the chunk
p6504
aVOverfitting was avoided by averaging parameters and monitoring performance against a held-out development set during training
p6505
aVGold dependency parses were approximated by running the Stanford dependency parser 7 7 http://nlp.stanford.edu/software/ over reference compressions
p6506
aVFollowing evaluations in machine translation as well as previous work in sentence compression [ 47 , 7 , 34 , 39 , 45 ] , we evaluate system performance using F 1 metrics over n-grams and dependency edges produced by parsing system output with RASP [ 3 ] and the Stanford parser
p6507
aVAn approximate inference approach based on an LP relaxation of ILP-Dep
p6508
aVAs discussed in § 2.4 , a maximum spanning tree is recovered from the output of the LP and greedily pruned in order to generate a valid integral solution while observing the imposed compression rate
p6509
aVThe learning rate schedule for the Lagrangian relaxation approaches was set as u'\u005cu0397' i u'\u005cu225c' u'\u005cu03a4' / ( u'\u005cu03a4' + i ) , 10 10 u'\u005cu03a4' was set to 100 for aggressive subgradient updates while the hyperparameter u'\u005cu03a8' was tuned using the development split of each corpus
p6510
aVTurning to the joint approaches, the strong performance of ILP-Joint is expected; less so is the relatively high but yet practically reasonable runtime that it requires
p6511
aVComparing the two approximation strategies shows a clear performance advantage for DP+LP over DP+LP u'\u005cu2192' MST the latter approach entails slower inference due to the overhead of running the Chu-Liu Edmonds algorithm at every dual update, and furthermore, the error introduced by approximating an integral solution results in a significant decrease in dependency recall
p6512
aVIn contrast, DP+LP directly optimizes the dual problem by using the relaxed dependency solution to update Lagrange multipliers and achieves the best performance on parse-based F 1 outside of the slower ILP approaches
p6513
aVThe variation in RASP F 1 % with input size indicates the viability of a hybrid approach which could balance accuracy and speed by using ILP-Joint for smaller problems and DP+LP for larger ones
p6514
aVMost approaches to sentence compression are supervised [ 25 , 42 , 46 , 36 , 47 , 18 , 40 , 9 , 17 , 19 , 38 , 15 ] following the release of datasets such as the Ziff-Davis corpus [ 24 ] and the Edinburgh compression corpora [ 5 , 7 ] , although unsupervised approaches u'\u005cu2014' largely based on ILPs u'\u005cu2014' have also received consideration [ 6 , 7 , 16 ]
p6515
aVCompression has also been used as a tool for document summarization [ 12 , 49 , 6 , 34 , 2 , 48 , 1 , 37 , 30 , 41 ] , with recent work formulating the summarization task as joint sentence extraction and compression and often employing ILP or Lagrangian relaxation
p6516
aVWe have presented approximate inference strategies to jointly compress sentences under bigram and dependency-factored objectives by exploiting the modularity of the task and considering the two subproblems in isolation
p6517
asg88
(lp6518
sg90
(lp6519
sg92
(lp6520
VWe have presented approximate inference strategies to jointly compress sentences under bigram and dependency-factored objectives by exploiting the modularity of the task and considering the two subproblems in isolation.
p6521
aVExperiments show that one of these approximation strategies produces results comparable to a state-of-the-art integer linear program for the same joint inference task with a 60% reduction in average inference time.
p6522
ag106
asg107
S'P14-1117'
p6523
sg109
(lp6524
VSentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming.
p6525
aVWe explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately.
p6526
aVWhile dynamic programming is viable for bigram-based sentence compression, finding optimal compressed trees within graphs is NP-hard.
p6527
aVWe recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms, yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers.
p6528
aVExperiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime.
p6529
ag106
asba(icmyPackage
FText
p6530
(dp6531
g3
(lp6532
VThis paper defines a systematic approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity and the type of comment and (ii) proposing robust shallow syntactic structures for improving model adaptability
p6533
aVYouTube is a unique environment, just like Twitter, but probably even richer multi-modal, with a social graph, and discussions between people sharing an interest
p6534
aVHence, doing sentiment research in such an environment is highly relevant for the community
p6535
aVWhile the linguistic conventions used on Twitter and YouTube indeed show similarities [ 2 ] , focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research
p6536
aVNevertheless, there is almost no work showing effective OM on YouTube comments
p6537
aVThe comment contains a product name xoom and some negative expressions, thus, a bag-of-words model would derive a negative polarity for this product
p6538
aVIn contrast, the opinion towards the product is neutral as the negative sentiment is expressed towards the video
p6539
aV1 1 The corpus and the annotation guidelines are publicly available at http://projects.disi.unitn.it/iKernels/projects/sentube/ It is the first manually annotated corpus that enables researchers to use supervised methods on YouTube for comment classification and opinion analysis
p6540
aVThe third contribution of the paper is a novel structural representation, based on shallow syntactic trees enriched with conceptual information, i.e.,, tags generalizing the specific topic of the video, e.g.,, iPad , Kindle , Toyota Camry
p6541
aVIn particular, we define an efficient tree kernel derived from the Partial Tree Kernel, [ 10 ] , suitable for encoding structural representation of comments into Support Vector Machines (SVMs
p6542
aVStructural models generally improve on both tasks u'\u005cu2013' polarity and type classification u'\u005cu2013' yielding up to 30% of relative improvement, when little data is available
p6543
aVHence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains
p6544
aVThe aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing
p6545
aVSimilar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies, which makes previous corpora less suitable to train models for OM on YouTube
p6546
aVA recent study focuses on sentiment analysis for Twitter [ 14 ] , however, their corpus was compiled automatically by searching for emoticons expressing positive and negative sentiment only
p6547
aV2010 ) focus on exploiting user ratings (counts of u'\u005cu2018' thumbs up/down u'\u005cu2019' as flagged by other users) of YouTube video comments to train classifiers to predict the community acceptance of new comments
p6548
aVHence, their goal is different predicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content
p6549
aVExploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work
p6550
aVThey help to build a system that is more robust across domains
p6551
aVTherefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation [ 3 , 4 ] , the domain adaptation problem boils down to finding a more robust system [ 25 , 17 ]
p6552
aVSuch classifiers are traditionally based on bag-of-words and more advanced features
p6553
aVIn the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods
p6554
aVFor each of the lexicons, we use the number of words found in the comment that have positive and negative sentiment as a feature
p6555
aV- negation the count of negation words, e.g.,, { don u'\u005cu2019' t, never, not, etc
p6556
aVMost of the videos come with a title and a short description, which can be used to encode the topicality of each comment by looking at their overlap
p6557
aVWe go beyond traditional feature vectors by employing structural models ( STRUCT ), which encode each comment into a shallow syntactic tree
p6558
aVMoreover, such taggers have been recently updated with models [ 18 , 5 ] trained specifically to process noisy texts showing significant reductions in the error rate on user-generated texts, e.g.,, Twitter
p6559
aVHence, we use the CMU Twitter pos-tagger [ 5 , 13 ] to obtain the part-of-speech tags
p6560
aVIn contrast, the STRUCT model relies on the fact that the negative word, destroy , refers to the PRODUCT ( xoom ) since they form a verbal phase (VP
p6561
aVMoreover, tree kernels generate all possible subtrees, thus producing generalized (back-off) features, e.g.,, [S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]] or [S [negative-VP [PRODUCT-NP]]]]
p6562
aVWe perform OM on YouTube using supervised methods, e.g.,, SVM
p6563
aVA binary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum prediction score
p6564
aVThe STRUCT model treats each comment as a tuple u'\u005cud835' u'\u005cudc99' = u'\u005cu27e8' u'\u005cud835' u'\u005cudc7b' , u'\u005cud835' u'\u005cudc97' u'\u005cu27e9' composed of a shallow syntactic tree u'\u005cud835' u'\u005cudc7b' and a feature vector u'\u005cud835' u'\u005cudc97'
p6565
aVHence, for each pair of comments u'\u005cud835' u'\u005cudc99' 1 and u'\u005cud835' u'\u005cudc99' 2 , we define the following comment similarity kernel
p6566
aVwhere N T 1 and N T 2 are the sets of the T 1 u'\u005cu2019' s and T 2 u'\u005cu2019' s nodes, respectively and u'\u005cu0394' u'\u005cu2062' ( n 1 , n 2 ) is equal to the number of common fragments rooted in the n 1 and n 2 nodes, according to several possible definition of the atomic fragments
p6567
aVTo improve the speed computation of T u'\u005cu2062' K , we consider pairs of nodes ( n 1 , n 2 ) belonging to the same tree level
p6568
aVThus, given H , the height of the STRUCT trees, where each level h contains nodes of the same type, i.e.,, chunk, POS, and lexical nodes, we define SHTK as the following 5 5 To have a similarity score between 0 and 1, a normalization in the kernel space, i.e., S u'\u005cu2062' H u'\u005cu2062' T u'\u005cu2062' K u'\u005cu2062' ( T 1 , T 2 ) S u'\u005cu2062' H u'\u005cu2062' T u'\u005cu2062' K u'\u005cu2062' ( T 1 , T 1 ) × S u'\u005cu2062' H u'\u005cu2062' T u'\u005cu2062' K u'\u005cu2062' ( T 2 , T 2 ) is applied
p6569
aVWhen applied to STRUCT trees, SHTK exactly computes the same feature space as PTK, but in faster time (on average
p6570
aVIndeed, SHTK required to be only applied to node pairs from the same level (see Eq
p6571
aVThis reduces the time for selecting the matching-node pairs carried out in PTK [ 10 , 11 ]
p6572
aVThe fragment space is obviously the same, as the node labels of different levels in STRUCT are different and will not be matched by PTK either
p6573
aVIf the comment contains several statements of different polarities, it is annotated as both positive and negative u'\u005cu201c' Love the video but waiting for iPad 4 u'\u005cu201d'
p6574
aVIn total we have annotated 208 videos with around 35k comments (128 videos TABLETS and 80 for AUTO
p6575
aVWe treat each comment as expressing positive , negative or neutral sentiment
p6576
aVHence, the task is a three-way classification
p6577
aVOne of the challenging aspects of sentiment analysis of YouTube data is that the comments may express the sentiment not only towards the product shown in the video, but also the video itself, i.e.,, users may post positive comments to the video while being generally negative about the product and vice versa
p6578
aVHence, it is of crucial importance to distinguish between these two types of comments
p6579
aVGiven that the main goal of sentiment analysis is to select sentiment-bearing comments and identify their polarity, distinguishing between off-topic and spam categories is not critical
p6580
aVThus, we merge the spam and off-topic into a single uninformative category
p6581
aVConsidering a real-life application, it is important not only to detect the polarity of the comment, but to also identify if it is expressed towards the product or the video
p6582
aV7 7 We exclude comments annotated as both video and product
p6583
aVThis enables the use of a simple flat multi-classifiers with seven categories for the full task, instead of a hierarchical multi-label classifiers (i.e.,, type classification first and then opinion polarity
p6584
aVSince the number of comments per video varies, the resulting sizes of each set are different (we use the larger split for TRAIN
p6585
aVIt is likely due to the nature of the TABLETS videos, that are more geek-oriented, where users are more prone to share their opinions and enter involved discussions about a product
p6586
aVWe start off by presenting the results for the traditional in-domain setting, where both TRAIN and TEST come from the same domain, e.g.,, AUTO or TABLETS
p6587
aVNext, we show the learning curves to analyze the behavior of FVEC and STRUCT models according to the training size
p6588
aVIn contrast, comments from TABLETS category tend to be more elaborated and well-argumented, thus, benefiting from the expressiveness of the structural representations
p6589
aVThe learning curves depict the behavior of FVEC and STRUCT models as we increase the size of the training set
p6590
aVIntuitively, the STRUCT model relies on more general syntactic patterns and may overcome the sparseness problems incurred by the FVEC model when little training data is available
p6591
aVNevertheless, as we see in Figure 2 , the learning curves for sentiment and type classification tasks across both product categories do not confirm this intuition
p6592
aVThe STRUCT model consistently outperforms the FVEC across all training sizes, but the gap in the performance does not increase when we move to smaller training sets
p6593
aVAs we will see next, this picture changes when we perform the cross-domain study
p6594
aVTo understand the performance of our classifiers on other YouTube domains, we perform a set of cross-domain experiments by training on the data from one product category and testing on the other
p6595
aVTable 3 reports the accuracy for three tasks when we use all comments (TRAIN + TEST) from AUTO to predict on the TEST from TABLETS and in the opposite direction ( TABLETS u'\u005cu2192' AUTO
p6596
aVWhen using AUTO as a source domain, STRUCT model provides additional 1-3% of absolute improvement, except for the sentiment task
p6597
aVSimilar to the in-domain experiments, we studied the effect of the source domain size on the target test performance
p6598
aVAUTO is used as the source domain to train models, which are tested on TABLETS
p6599
aVThis difference becomes smaller as we add data from the same domain
p6600
aVThis is an important advantage of our structural approach, since we cannot realistically expect to obtain manual annotations for 10k+ comments for each (of many thousands) product domains present on YouTube
p6601
aVOur STRUCT model is more accurate since it is able to induce structural patterns of sentiment
p6602
aVThe FVEC bag-of-words model misclassifies it to be positive , since it contains two positive expressions ( better , better functionality ) that outweigh a single negative expression ( bulky
p6603
aVThe structural model, in contrast, is able to identify the product of interest ( xoom ) and associate it with the negative expression through a structural feature and thus correctly classify the comment as negative
p6604
aVThe largest group of errors are implicit sentiments
p6605
aVThus, some comments do not contain any explicit positive or negative opinions, but provide detailed and well-argumented criticism, for example, this phone is heavy
p6606
aVWe carried out a systematic study on OM from YouTube comments by training a set of supervised multi-class classifiers distinguishing between video and product related opinions
p6607
asg88
(lp6608
sg90
(lp6609
sg92
(lp6610
VWe carried out a systematic study on OM from YouTube comments by training a set of supervised multi-class classifiers distinguishing between video and product related opinions.
p6611
aVWe use standard feature vectors augmented by shallow syntactic trees enriched with additional conceptual information.
p6612
aVThis paper makes several contributions i) it shows that effective OM can be carried out with supervised models trained on high quality annotations; (ii) it introduces a novel annotated corpus of YouTube comments, which we make available for the research community; (iii) it defines novel structural models and kernels, which can improve on feature vectors, e.g.,, up to 30% of relative improvement in type classification, when little data is available, and demonstrates that the structural model scales well to other domains.
p6613
aVIn the future, we plan to work on a joint model to classify all the comments of a given video, s.t it is possible to exploit latent dependencies between entities and the sentiments of the comment thread.
p6614
aVAdditionally, we plan to experiment with hierarchical multi-label classifiers for the full task (in place of a flat multi-class learner.
p6615
ag106
asg107
S'P14-1118'
p6616
sg109
(lp6617
VThis paper defines a systematic approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity and the type of comment and (ii) proposing robust shallow syntactic structures for improving model adaptability.
p6618
aVWe rely on the tree kernel technology to automatically extract and learn features with better generalization power than bag-of-words.
p6619
aVAn extensive empirical evaluation on our manually annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural models in a cross-domain setting.
p6620
ag106
asba(icmyPackage
FText
p6621
(dp6622
g3
(lp6623
VAutomatic keyphrase extraction concerns u'\u005cu201c' the automatic selection of important and topical phrases from the body of a document u'\u005cu201d' [ 50 ]
p6624
aVOwing to its importance, automatic keyphrase extraction has received a lot of attention
p6625
aVIn contrast, a scientific paper typically has at least 10 keyphrases and hundreds of candidate keyphrases, yielding a much bigger search space [ 16 ]
p6626
aVConsequently, it is harder to extract keyphrases from scientific papers, technical reports, and meeting transcripts than abstracts, emails, and news articles
p6627
aVFor this reason, structural information is likely to facilitate keyphrase extraction from scientific papers and technical reports because of their standard format (i.e.,, standard sections such as abstract, introduction, conclusion, etc
p6628
aVIn contrast, the lack of structural consistency in other types of structured documents (e.g.,, web pages, which can be blogs, forums, or reviews) may render structural information less useful
p6629
aVThe reason is simple in a conversation, the topics (i.e.,, its talking points) change as the interaction moves forward in time, and so do the keyphrases associated with a topic
p6630
aVThe presence of uncorrelated topics implies that it may no longer be possible to exploit relatedness and therefore increases the difficulty of keyphrase extraction
p6631
aVA keyphrase extraction system typically operates in two steps
p6632
aV1) extracting a list of words/phrases that serve as candidate keyphrases using some heuristics (Section 3.1 ); and (2) determining which of these candidate keyphrases are correct keyphrases using supervised (Section 3.2 ) or unsupervised (Section 3.3 ) approaches
p6633
aVHowever, for a long document, the resulting list of candidates can be long
p6634
aVConsequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases [ 17 , 29 , 10 , 59 , 40 ]
p6635
aVDifferent learning algorithms have been used to train this classifier, including naïve Bayes [ 12 , 56 ] , decision trees [ 49 , 50 ] , bagging [ 20 ] , boosting [ 18 ] , maximum entropy [ 58 , 26 ] , multi-layer perceptron [ 35 ] , and support vector machines [ 22 , 35 ]
p6636
aVRecasting keyphrase extraction as a classification problem has its weaknesses, however
p6637
aVNote that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others [ 21 , 55 ]
p6638
aV2009 ) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases
p6639
aVThis pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA [ 56 , 12 ] , a popular supervised baseline that adopts the traditional supervised classification approach [ 46 , 23 ]
p6640
aVStatistical features are computed based on statistical information gathered from the training documents
p6641
aVThe first one, tf*idf [ 45 ] , is computed based on candidate frequency in the given text and inverse document frequency (i.e.,, number of other documents where the candidate appears
p6642
aV2 2 A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches [ 63 , 9 , 44 , 13 ]
p6643
aVThe second one, the distance of a phrase, is defined as the number of words preceding its first occurrence normalized by the number of words in the document
p6644
aVThe third one, supervised keyphraseness , encodes the number of times a phrase appears as a keyphrase in the training set
p6645
aVThis feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document
p6646
aVA phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page
p6647
aVFor example, a candidate keyphrase has been encoded as (1) a PoS tag sequence , which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence , which is the sequence of morphological suffixes of its words [ 58 , 42 , 26 ]
p6648
aVExternal resource-based features are computed based on information gathered from resources other than the training documents, such as lexical knowledge bases (e.g.,, Wikipedia) or the Web, with the goal of improving keyphrase extraction performance by exploiting external knowledge
p6649
aVWikipedia-based keyphraseness is computed as a candidate u'\u005cu2019' s document frequency multiplied by the ratio of the number of Wikipedia articles where the candidate appears as a link to the number of articles where it appears [ 37 ]
p6650
aVThis feature is motivated by the observation that a candidate is likely to be a keyphrase if it occurs frequently as a link in Wikipedia
p6651
aVUnlike supervised keyphraseness, Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain
p6652
aV2006 ) employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine, exploiting the observation that a candidate is potentially important if it was used as a search query
p6653
aVNoting that candidate keyphrases that are not semantically related to the predicted keyphrases are unlikely to be keyphrases in technical reports, Turney employs coherence features to identify such candidate keyphrases
p6654
aVSemantic relatedness is encoded in the coherence features as two candidate keyphrases u'\u005cu2019' pointwise mutual information, which Turney computes by using the Web as a corpus
p6655
aVInformally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important
p6656
aVThe basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g.,, Brin and Page ( 1998 )
p6657
aVFor each node, each of its edges is treated as a u'\u005cu201c' vote u'\u005cu201d' from the other node connected by the edge
p6658
aVThe top-ranked candidates from the graph are then selected as keyphrases for the input document
p6659
aVTextRank [ 38 ] is one of the most well-known graph-based approaches to keyphrase extraction
p6660
aVThe underlying hypothesis is that each of these clusters corresponds to a topic covered in the document, and selecting the candidates close to the centroid of each cluster as keyphrases ensures that the resulting set of keyphrases covers all the topics of the document
p6661
aVWhile empirical results show that KeyCluster performs better than both TextRank and Hulth u'\u005cu2019' s [ 20 ] supervised system, KeyCluster has a potential drawback by extracting keyphrases from each topic cluster, it essentially gives each topic equal importance
p6662
aVBy running TextRank once for each topic, TPR ensures that the extracted keyphrases cover the main topics of the document
p6663
aVThe final score of a candidate is computed as the sum of its scores for each of the topics, weighted by the probability of that topic in that document
p6664
aVHence, unlike KeyCluster, candidates belonging to a less probable topic are given less importance
p6665
aVLike TPR, CommunityCluster gives more weight to more important topics, but unlike TPR, it extracts all candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic
p6666
aVCommunityCluster yields much better recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo term extractor
p6667
aVSince keyphrases represent a dense summary of a document, researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously
p6668
aVZha ( 2002 ) proposes the first graph-based approach for simultaneous summarization and keyphrase extraction, motivated by a key observation a sentence is important if it contains important words, and important words appear in important sentences
p6669
aV2007 ) extend Zha u'\u005cu2019' s work by adding two assumptions
p6670
aVOnce the graphs are constructed for an input document, an iterative reinforcement algorithm is applied to assign scores to each sentence and word
p6671
aVMany existing approaches have a separate, heuristic module for extracting candidate keyphrases prior to keyphrase ranking/extraction
p6672
aVLMA scores a candidate keyphrase based on two features, namely, phraseness (i.e.,, the extent to which a word sequence can be treated as a phrase) and informativeness (i.e.,, the extent to which a word sequence captures the central idea of the document it appears in
p6673
aVIntuitively, a phrase that has high scores for phraseness and informativeness is likely to be a keyphrase
p6674
aVPhraseness, defined using the foreground LM, is calculated as the loss of information incurred as a result of assuming a unigram LM (i.e.,, conditional independence among the words of the phrase) instead of an n-gram LM (i.e.,, the phrase is drawn from an n-gram LM
p6675
aVInformativeness is computed as the loss that results because of the assumption that the candidate is sampled from the background LM rather than the foreground LM
p6676
aVCandidates are ranked according to the sum of these two feature values
p6677
aVWhile the use of language models to identify phrases cannot be considered a major strength of this approach (because heuristics can identify phrases fairly reliably), the use of a background corpus to identify candidates that are unique to the foreground u'\u005cu2019' s domain is a unique aspect of this approach
p6678
aVWe believe that this idea deserves further investigation, as it would allow us to discover a keyphrase that is unique to the foreground u'\u005cu2019' s domain but may have a low tf*idf value
p6679
aVIn this section, we describe metrics for evaluating keyphrase extraction systems as well as state-of-the-art results on commonly-used datasets
p6680
aVConceivably, exact match is an overly strict condition, considering a predicted keyphrase incorrect even if it is a variant of a gold keyphrase
p6681
aVFor instance, given the gold keyphrase u'\u005cu201c' neural network u'\u005cu201d' , exact match will consider a predicted phrase incorrect even if it is an expanded version of the gold keyphrase ( u'\u005cu201c' artificial neural network u'\u005cu201d' ) or one of its morphological ( u'\u005cu201c' neural networks u'\u005cu201d' ) or lexical ( u'\u005cu201c' neural net u'\u005cu201d' ) variants
p6682
aVHuman evaluation has been suggested as a possibility [ 36 ] , but it is time-consuming and expensive
p6683
aVFor this reason, researchers have experimented with two types of automatic evaluation metrics
p6684
aVGiven that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) [ 32 ] will award more credit to A than to B if the ranks of the correct predictions in A u'\u005cu2019' s output are higher than those in B u'\u005cu2019' s output
p6685
aVThe motivation behind the design of R p is simple a system will achieve a perfect R p value if it ranks all the keyphrases above the non-keyphrases
p6686
aVOvergeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that appears frequently in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word
p6687
aVRecall that for many systems, it is not easy to reject a non-keyphrase containing a word with a high term frequency many unsupervised systems score a candidate by summing the score of each of its component words, and many supervised systems use unigrams as features to represent a candidate
p6688
aVTo be more concrete, consider the news article on athlete Ben Johnson in Figure 1, where the keyphrases are boldfaced
p6689
aVAs we can see, the word Olympic(s) has a significant presence in the document
p6690
aVConsequently, many systems not only correctly predict Olympics as a keyphrase, but also erroneously predict Olympic movement as a keyphrase, yielding overgeneration errors
p6691
aVInfrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document [ 31 ]
p6692
aVHandling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document
p6693
aVIn the Ben Johnson example, many keyphrase extractors fail to identify 100-meter dash and gold medal as keyphrases, resulting in infrequency errors
p6694
aVRedundancy errors are a type of precision error contributing to 8 u'\u005cu2013' 12% of the overall error
p6695
aVNevertheless, some researchers may argue that a system should not be penalized for redundancy errors because the extracted candidates are in fact keyphrases
p6696
aVIn our example, Olympics and Olympic games refer to the same concept, so a system that predicts both of them as keyphrases commits a redundancy error
p6697
aVAn evaluation error occurs when a system outputs a candidate that is semantically equivalent to a gold keyphrase, but is considered erroneous by a scoring program because of its failure to recognize that the predicted phrase and the corresponding gold keyphrase are semantically equivalent
p6698
aVIn other words, an evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program
p6699
aVHence, an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent
p6700
aVFirst, we discuss how redundancy errors could be addressed by using the background knowledge extracted from external databases
p6701
aVNote that if we can identify semantically equivalent candidates, then we can reduce redundancy errors
p6702
aVHence, before a system outputs a set of candidates as keyphrases, it can use Freebase to determine whether any of them is mapped to the same Freebase topic
p6703
aVReferring back to our running example, both Olympics and Olympic games are mapped to a Freebase topic called Olympic games
p6704
aVBased on this information, a keyphrase extractor can determine that the two candidates are aliases and should output only one of them, thus preventing a redundancy error
p6705
aVNext, we discuss how infrequency errors could be addressed using background knowledge
p6706
aVIn other words, if we could relate an infrequent keyphrase to other candidates in the text, we could boost its importance
p6707
aVAll four systems have managed to identify Ben Johnson as a keyphrase due to its significant presence
p6708
aVHence, we can boost the importance of 100-meter dash and gold medal if we can relate them to Ben Johnson
p6709
aVTo do so, note that Freebase maps a candidate to one or more pre-defined topics, each of which is associated with one or more types
p6710
aV100-meter dash is mapped to the topic Sprint of type Sports in the Sports domain, whereas gold medal is mapped to a topic with the same name of type Olympic medal in the Olympics domain
p6711
aVConsequently, we can relate 100-meter dash to Ben Johnson via the Sports domain (i.e.,, they belong to different types under the same domain
p6712
aVFirst, an ad-hoc window size cannot capture related candidates that are not inside the window
p6713
aVSo it is difficult to predict 100-meter dash and gold medal as keyphrases they are more than 10 tokens away from frequent words such as Johnson and Olympics
p6714
aV4 4 Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be related to each other (see Section 2
p6715
aVThe absence of such a mapping in the Olympics domain could be used by a keyphrase extractor as a supporting evidence against predicting Olympic movement as a keyphrase
p6716
aVFinally, as mentioned before, evaluation errors should not be considered errors made by a system
p6717
asg88
(lp6718
sg90
(lp6719
sg92
(lp6720
VWe have presented a survey of the state of the art in automatic keyphrase extraction.
p6721
aVWhile unsupervised approaches have started to rival their supervised counterparts in performance, the task is far from being solved, as reflected by the fairly poor state-of-the-art results on various commonly-used evaluation datasets.
p6722
aVOur analysis revealed that there are at least three major challenges ahead.
p6723
aVWhile much recent work has focused on algorithmic development, keyphrase extractors need to have a deeper understanding of a document in order to reach the next level of performance.
p6724
aVSuch an understanding can be facilitated by the incorporation of background knowledge.
p6725
aVWhile it may be possible to design better algorithms to handle the large number of candidates in long documents, we believe that employing sophisticated features, especially those that encode background knowledge, will enable keyphrases and non-keyphrases to be distinguished more easily even in the presence of a large number of candidates.
p6726
aVTo more accurately measure the performance of keyphrase extractors, they should not be penalized for evaluation errors.
p6727
aVWe have suggested several possibilities as to how this problem can be addressed.
p6728
ag106
asg107
S'P14-1119'
p6729
sg109
(lp6730
VWhile automatic keyphrase extraction has been examined extensively, state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks.
p6731
aVWe present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.
p6732
ag106
asba(icmyPackage
FText
p6733
(dp6734
g3
(lp6735
VThe main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced
p6736
aVThe bilingual lexicon extraction task from bilingual corpora was initially addressed by using parallel corpora (i.e., a corpus that contains source texts and their translation
p6737
aVHowever, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English
p6738
aVFor these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship [ 21 ]
p6739
aVAccording to Fung and Cheung (2004) , who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e., a kind of filiation
p6740
aVThe bilingual lexicon extraction task from comparable corpora inherits this filiation
p6741
aVFor instance, the historical context-based projection method [ 11 , 28 ] , known as the standard approach , dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e., each part of the corpus is composed of the same amount of data
p6742
aVMoreover, this assumption is prejudicial for specialized comparable corpora, especially when involving the English language for which many documents are available due the prevailing position of this language as a standard for international scientific publications
p6743
aVWithin this context, our main contribution consists in a re-reading of the standard approach putting emphasis on the unfounded assumption of the balance of the specialized comparable corpora
p6744
aVIn specialized domains, the comparable corpora are traditionally of small size (around 1 million words) in comparison with comparable corpus-based general language (up to 100 million words
p6745
aVConsequently, the observations of word co-occurrences which is the basis of the standard approach are unreliable
p6746
aVWe then present an extension of this approach based on regression models
p6747
aVThe main work in bilingual lexicon extraction from comparable corpora is based on lexical context analysis and relies on the simple observation that a word and its translation tend to appear in the same lexical contexts
p6748
aVIn order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure
p6749
aVThen, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary
p6750
aVThe implementation of the standard approach can be carried out by applying the following three steps [ 29 , 3 , 7 , 22 , 18 , among others]
p6751
aVIf the bilingual dictionary provides several translations for an element, we consider all of them but weight the different translations according to their frequency in the target language
p6752
aVThe standard approach is used by most researchers so far [ 28 , 12 , 25 , 29 , 3 , 7 , 13 , 22 , 18 , 26 , 2 , among others] with the implicit hypothesis that comparable corpora are balanced
p6753
aV21) observe, a specialized comparable corpus is built as balanced by analogy with a parallel corpus u'\u005cu201c' Therefore, in relation to parallel corpora, it is more likely for comparable corpora to be designed as general balanced corpora u'\u005cu201d'
p6754
aVSince the context vectors are computed from each part of the comparable corpus rather than through the parts of the comparable corpora, the standard approach is relatively insensitive to differences in corpus sizes
p6755
aVThe only precaution for using the standard approach with unbalanced corpora is to normalize the association measure (for instance, this can be done by dividing each entry of a given context vector by the sum of its association scores
p6756
aVSince comparable corpora are usually small in specialized domains (see Table 2 ), the discriminative power of context vectors (i.e., the observations of word co-occurrences) is reduced
p6757
aVThis consists in assigning to each observed co-occurrence count of a small comparable corpora, a new value learned beforehand from a large training corpus
p6758
aVIn order to make co-occurrence counts more discriminant and in the same way as Hazem and Morin [ 15 ] , one strategy consists in addressing this problem through regression given training corpora of small and large size (abundant in the general domain), we predict word co-occurrence counts in order to make them more reliable
p6759
aVWe then apply the resulting regression function to each word co-occurrence count as a pre-processing step of the standard approach
p6760
aVOur work differs from Hazem and Morin [ 15 ] in two ways
p6761
aVWe use regression analysis to describe the relationship between word co-occurrence counts in a large corpus (the response variable) and word co-occurrence counts in a small corpus (the predictor variable
p6762
aVAs most regression models have already been described in great detail [ 5 , 1 ] , the derivation of most models is only briefly introduced in this work
p6763
aVAs we can not claim that the prediction of word co-occurrence counts is a linear problem, we consider in addition to the simple linear regression model ( L u'\u005cu2062' i u'\u005cu2062' n ), a generalized linear model which is the logistic regression model ( L u'\u005cu2062' o u'\u005cu2062' g u'\u005cu2062' i u'\u005cu2062' t ) and non linear regression models such as polynomial regression model ( P u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' y n ) of order n
p6764
aVProchasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language
p6765
aVIsmail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors
p6766
aVIn another way, Rubino and Linarès (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities
p6767
aVKoehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts
p6768
aVAs regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis
p6769
aVThe rank of candidate translations can be improved by integrating different heuristics
p6770
aVFor instance, Chiao and Zweigenbaum (2002) introduce a heuristic based on word distribution symmetry
p6771
aVLaroche and Langlais (2010) suggest a heuristic based on the graphic similarity between source and target terms
p6772
aVAfter a manual selection, we only kept the documents which were relative to the medical domain
p6773
aVAs a result, 65 French documents were extracted (about 257,000 words
p6774
aVWe only kept the free fulltext available documents
p6775
aVAs a result, 2,339 English documents were extracted (about 3,5 million words
p6776
aVThe comparability measure [ 19 ] is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable
p6777
aVFor instance, Prochasson and Fung (2011) showed that the standard approach is not relevant for infrequent words (since the context vectors are very unrepresentative i.e., poor in information
p6778
aVAs a result of filtering, 169 French/English single words were extracted for the breast cancer corpus and 244 French/English single words were extracted for the diabetes corpus
p6779
aVWe first performed an experiment using each comparable corpus independently of the others (we refer to these corpora as balanced corpora
p6780
aVWe then conducted a second experiment where we varied the size of the English part of the comparable corpus, from 530,000 to 7.4 million words for the breast cancer corpus in 530,000 words steps, and from 250,000 to 3.5 million words for the diabetes corpus in 250,000 words steps (we refer to these corpora as unbalanced corpora
p6781
aVFor instance, the column 3 indicates the MAP obtained by using a comparable corpus that is composed i) only of [ breast cancer corpus 3 ] (MAP of 21.0%), and ii) of [ breast cancer corpus 1, 2 and 3 ] (MAP of 34.7%
p6782
aVAs a preliminary remark, we can notice that the results differ noticeably according to the comparable corpus used individually (MAP variation between 21.0% and 29.6% for the breast cancer corpora and between 10.5% and 16.5% for the diabetes corpora
p6783
aVWe can also note that the MAP of all the unbalanced comparable corpora is always higher than any individual comparable corpus
p6784
aVOverall, starting with a MAP of 26.1% as provided by the balanced [ breast cancer corpus 1 ] , we are able to increase it to 42.3% with the unbalanced [ breast cancer corpus 12 ] (the variation observed for some unbalanced corpora such as [ diabetes corpus 12, 13 and 14 ] can be explained by the fact that adding more data in the source language increases the error rate of the translation phase of the standard approach, which leads to the introduction of additional noise in the translated context vectors
p6785
aVWe contrast the prediction models presented in Section 2.2 to findout which is the most appropriate model to use as a pre-processing step of the standard approach
p6786
aVWe chose the balanced corpora where the standard approach has shown the best results in the previous experiment, namely [ breast cancer corpus 12 ] and [ diabetes corpus 7 ]
p6787
aVWe can notice that except for the L u'\u005cu2062' o u'\u005cu2062' g u'\u005cu2062' i u'\u005cu2062' t model, all the regression models outperform the baseline ( N u'\u005cu2062' o p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d u'\u005cu2062' i u'\u005cu2062' c u'\u005cu2062' t u'\u005cu2062' i u'\u005cu2062' o u'\u005cu2062' n
p6788
aVAlso, as we can see, the results obtained with the linear and polynomial regressions are very close
p6789
aVThis suggests that both linear and polynomial regressions are suitable as a pre-processing step of the standard approach, while the logistic regression seems to be inappropriate according to the results shown in Table 6
p6790
aVThis may be due to the regression parameters that have been learned from a training corpus of the general domain
p6791
aVIf prediction can not replace a large amount of data, it aims at increasing co-occurrence counts as if large amounts of data were at our disposal
p6792
aVThat said, the other regression models have shown the same behavior as L u'\u005cu2062' i u'\u005cu2062' n
p6793
aVWe can see that the best results are obtained by the S u'\u005cu2062' o u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' c u'\u005cu2062' e p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d approach for both comparable corpora
p6794
aVIt is not surprising that predicting the target side only leads to lower results, since it is well known that a better characterization of a word to translate (given from the source side) leads to better results
p6795
aVThe results of the experiments conducted on the diabetes corpus are shown in Figure 1
p6796
aVAs for the previous experiment, we can see that the U u'\u005cu2062' n u'\u005cu2062' b u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' c u'\u005cu2062' e u'\u005cu2062' d approach significantly outperforms the B u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' c u'\u005cu2062' e u'\u005cu2062' d approach
p6797
aVThis aspect represents a significant interest when working with specialized comparable corpora for which the quantity of the data collected may differ depending on the languages involved, especially when involving the English language as many scientific documents are available
p6798
aVMore precisely, our different experiments show that using an unbalanced specialized comparable corpus always improves the quality of word translations
p6799
aVThus, the MAP goes up from 29.6% (best result on the balanced corpora) to 42.3% (best result on the unbalanced corpora) in the breast cancer domain, and from 16.5% to 26.0% in the diabetes domain
p6800
aVAdditionally, these results can be improved by using a prediction model of the word co-occurrence counts
p6801
aVWe hope that this study will pave the way for using specialized unbalanced comparable corpora for bilingual lexicon extraction
p6802
asg88
(lp6803
sg90
(lp6804
sg92
(lp6805
VIn this paper, we have studied how an unbalanced specialized comparable corpus could influence the quality of the bilingual lexicon extraction.
p6806
aVThis aspect represents a significant interest when working with specialized comparable corpora for which the quantity of the data collected may differ depending on the languages involved, especially when involving the English language as many scientific documents are available.
p6807
aVMore precisely, our different experiments show that using an unbalanced specialized comparable corpus always improves the quality of word translations.
p6808
aVThus, the MAP goes up from 29.6% (best result on the balanced corpora) to 42.3% (best result on the unbalanced corpora) in the breast cancer domain, and from 16.5% to 26.0% in the diabetes domain.
p6809
aVAdditionally, these results can be improved by using a prediction model of the word co-occurrence counts.
p6810
aVHere, the MAP goes up from 42.3% (best result on the unbalanced corpora) to 46.9% (best result on the unbalanced corpora with prediction) in the breast cancer domain, and from 26.0% to 29.8% in the diabetes domain.
p6811
aVWe hope that this study will pave the way for using specialized unbalanced comparable corpora for bilingual lexicon extraction.
p6812
ag106
asg107
S'P14-1121'
p6813
sg109
(lp6814
VThe main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced.
p6815
aVHowever, the historical context-based projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus.
p6816
aVWithin this context, we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments.
p6817
aVMoreover, we have introduced a regression model that boosts the observations of word co-occurrences used in the context-based projection method.
p6818
aVOur results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons.
p6819
ag106
asba(icmyPackage
FText
p6820
(dp6821
g3
(lp6822
VFurthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable
p6823
aVHowever, these methods, too, are limited by the resources required for acquiring large numbers of responses
p6824
aVWhile prior efforts in NLP have incorporated games for performing annotation and validation [ 34 , 12 , 27 ] , these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task
p6825
aVWhile their game is among the most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game u'\u005cu2019' s objectives, which potentially decreases motivation for answering correctly
p6826
aVFor all three games, two players play the same game under time limits and then are rewarded if their answers match
p6827
aVIn a rapid-play style game, OntoPronto attempts to classify Wikipedia pages as either categories or individuals [ 33 ]
p6828
aVSpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the distinctions between classes [ 37 ]
p6829
aVWhile the computer can potentially act as a second player, such a simulated player is often limited to using preexisting knowledge or responses, which makes it difficult to validate new types of entities or create novel answers
p6830
aVIn contrast, we drop this requirement thanks to a new strategy for assigning confidence scores to the annotations based on negative associations
p6831
aVFor each task we developed a video game with a purpose that integrates the task within the game, as illustrated in Sections 4 and 5
p6832
aVKnowledge base As the reference knowledge base, we chose BabelNet 2 2 http://babelnet.org [ 22 ] , a large-scale multilingual semantic ontology created by automatically merging WordNet with other collaboratively-constructed resources such as Wikipedia and OmegaWiki
p6833
aVBabelNet data offers two necessary features for generating the games u'\u005cu2019' datasets
p6834
aVFirst, by connecting WordNet synsets to Wikipedia pages, most synsets are associated with a set of pictures; while often noisy, these pictures sometimes illustrate the target concept and are an ideal case for validation
p6835
aVUsing the same set of synsets, separate datasets were created for the two validation tasks
p6836
aVSpecifically, novel lemmas were selected by computing the u'\u005cu03a7' 2 statistic for co-occurrences between the lemmas of c and all other part of speech-tagged lemmas in Wikipedia
p6837
aVTo enable concept-to-concept annotations, we disambiguate novel lemmas using a simple heuristic based on link co-occurrence count [ 23 ]
p6838
aVFor the concept-image data, V c is the union of V c B , which contains all images associated with c in BabelNet, and V c n , which contains web-gathered images using a lemma of c as the query
p6839
aVWeb-gathered images were retrieved using Yahoo! Boss image search and the first result set (35 images) was added to V c
p6840
aVFor both datasets, each negative set N c is constructed as u'\u005cu222a' c u'\u005cu2032' u'\u005cu2208' C u'\u005cu2216' { c } V c u'\u005cu2032' B , i.e.,, from the items related in BabelNet to all other concepts in C
p6841
aVBy constructing N c directly from the knowledge base, play actions may be validated based on recognition of true negatives, removing the heavy burden for ever manually creating a gold standard test set
p6842
aVDesign Infection is designed as a top-down shooter game in the style of Commando
p6843
aVInfection features the classic game premise that a virus has partially infected humanity, turning people into zombies
p6844
aVBecause infected and uninfected humans look identical, the player uses a passphrase call-and-response mechanism to distinguish between the two
p6845
aVUninfected humans are expected to respond with a word or phrase related to the passphrase; in contrast, infected humans have become confused due to the infection and will say something completely unrelated in an attempt to sneak past
p6846
aVFurthermore, if any time after ten humans have been seen, the player has killed more than 80% of the uninfected humans, the player u'\u005cu2019' s gun is taken by the survivors and she loses the stage
p6847
aVScoring is based on both the number of zombies killed and the percentage of uninfected humans saved, motivating players to kill infected humans in order to increase their score
p6848
aVImportantly, Infection also includes a leaderboard where players compete for top positions based on their total scores
p6849
aVThe design of Infection enables annotating multiple types of conceptual relations such as synonymy or antonymy by changing only the description of the passphrase and how uninfected humans are expected to respond
p6850
aVThese mechanics ensure the game naturally produces better quality annotations; in contrast, common crowdsourcing platforms do not support analogous mechanics for enforcing this type of correctness at annotation time
p6851
aVDesign TKT is designed as a single-player role playing game (RPG) where the player explores a series of towers to unlock long-forgotten knowledge
p6852
aVAt the start of each tower, a target concept is shown, e.g.,, the tower of u'\u005cu201c' tango, u'\u005cu201d' along with a description of the concept (Figure 7
p6853
aVThe player must then recover the knowledge of the target concept by acquiring pictures of it
p6854
aVPictures are obtained through defeating monsters and opening treasure chests, such as those shown in Figure 7
p6855
aVOnce the player has collected enough pictures, the door to the boss room is unlocked and the player may enter to defeat the boss and complete the tower
p6856
aVPictures may also be deposited in special reward chests that grant experience bonuses if the deposited pictures are from V
p6857
aVIf the player finishes the level with a majority of unrelated pictures, the player u'\u005cu2019' s journey is unsuccessful and she must replay the tower
p6858
aVLast, TKT includes a leaderboard where players can compete for positions; a player u'\u005cu2019' s score is based on increasing her character u'\u005cu2019' s abilities and her accuracy at discarding images from N
p6859
aVHowever, its general design allows for other types of annotations, such as image labeling, by changing the tower u'\u005cu2019' s instructions and pictures
p6860
aVQuality Enforcement Mechanisms Similar to Infection, TKT includes analogous mechanisms for limiting adversarial player annotations
p6861
aVSimilarly, players who collect all images are likely to have half of their images from N and therefore fail the tower u'\u005cu2019' s quality-check after defeating the boss
p6862
aVA further analysis revealed differences in the annotators u'\u005cu2019' thresholds for determining association, with one annotator permitting more abstract relations
p6863
aVWe refer to these as the paid and free versions of the game, respectively
p6864
aVIn the paid setting, the five top-ranking players were offered gift cards valued at 25, 15, 15, 10, and 10 USD, starting from first place (a total of 75 USD per game
p6865
aVSeparate tasks were used for validating concept-concept and concept-image relations
p6866
aVEach tasks u'\u005cu2019' questions were shown as a binary choice of whether the item is related to the task u'\u005cu2019' s concept
p6867
aVWorkers were paid 0.05 USD per task
p6868
aVFollowing common practices for guarding against adversarial workers [ 19 ] , the tasks for concept c include quality check questions using items from N c
p6869
aVWorkers who rate too many relations from N c as valid are removed by CrowdFlower and prevented from participating further
p6870
aVWhile the crowdsourcing task could be adjusted to use an increased number of quality-check options, such a design is uncommon and artificially inflates the cost of the crowdsourcing comparison beyond what would be expected
p6871
aVTherefore, although the crowdsourcing and game-based annotation tasks differ slightly, we chose to use the common setup in order to create a fair cost-comparison between the two
p6872
aVNon-video Game with a Purpose To measure the impact of the video game itself on the annotation process, we developed a non-video game with a purpose, referred to as SuchGame
p6873
aVPlayers perform a single action in SuchGame after being shown a concept c and its textual definition, a player answers whether an item is related to the concept
p6874
aVSuchGame was promoted with same free recognition incentive as Infection and TKT
p6875
aVBoth video games were released to multiple online forums, social media sites, and Facebook groups
p6876
aVNotices promoting the game were separated so that audiences saw promotions for one of either the paid or free incentive version
p6877
aVIn contrast, concept-concept associations require more background knowledge to determine if a relation exists; furthermore, Infection gives players limited time to decide (due to board length) and also contains cognitive distractors (zombies
p6878
aVThe close proximity of players in the paid positions is a result of continued competition as players jostled for higher-paying prizes
p6879
aVFor images, crowdsourcing workers have a higher IAA than game players; however, this increased agreement is due to adversarial workers consistently selecting the same, incorrect answer
p6880
aVIn contrast, both video games contain mechanisms for limiting such behavior
p6881
aVThe strength of both crowdsourcing and games with a purpose comes from aggregating multiple annotations of a single item; i.e.,, while IAA may be low, the majority annotation of an item may be correct
p6882
aVTherefore, in Table 1 , we calculate the percentage agreement of the aggregated annotations with the gold standard annotations for approving valid relations (true positives; Col. 5), rejecting invalid relations (true negatives; Col. 6), and for both combined (Col. 7
p6883
aVExamining the difference in annotation quality for true positives and negatives, we see a strong bias with crowdsourcing towards rejecting all items
p6884
aVThis bias leads to annotations with few false positives, but as Column 5 shows, crowdflower workers consistently performed much worse than game players at identifying valid relations, producing many false negative annotations
p6885
aVHighlighting one example, the five most selected concept-concept relations for chord were all novel; BabelNet included many relations to highly-specific concepts (e.g.,, u'\u005cu201c' Circle of fifths u'\u005cu201d' ) but did not include relations to more commonly-associated concepts, like note and harmony
p6886
aVIt could be argued that the recognition incentive was motivating players in the free condition and thus some incentive was required
p6887
aVAfter the contest period ended, no players in the free setting registered for being acknowledged by name, which strongly suggests the incentive was not contributing to their motivation for playing
p6888
aVCrowdsourcing was slightly more cost-effective than both games in the paid condition, as shown in Table 1 , Column 8
p6889
aVHowever, three additional factors need to be considered
p6890
aVFirst, both games intentionally uniformly sample between V and N to increase player engagement, 4 4 Earlier versions that used mostly items from V proved less engaging due to players frequently performing the same action, e.g.,, saving most humans or collecting most pictures which generates a larger number of annotations for items in N than are produced by crowdsourcing
p6891
aVBased on agreement with the gold standard (Table 1 , Col. 5), the estimated cost for crowdsourcing a correct true positive annotation increases to $0.014 for a concept-image and a $0.048 for concepts-concept annotation
p6892
aVThird, we note that both video games in the paid setting incur a fixed cost (for the prizes) and therefore additional games played can only further decrease the cost per annotation
p6893
aVAssuming combining the audiences would produce the same number of annotations, both our games u'\u005cu2019' costs per annotation drop to $0.012
p6894
aVLast, video games can potentially come with indirect costs due to software development and maintenance
p6895
aVIn contrast, both games here were developed as a part of student projects using open source software and assets and thus incurred no cost; furthermore, games were created in a few months, rather than years
p6896
aVGiven that few online games attain significant sustained interest, we argue that our lightweight model is preferable for producing video games with a purpose
p6897
aVTwo video games have been presented for validating and extending knowledge bases
p6898
asg88
(lp6899
sg90
(lp6900
sg92
(lp6901
VTwo video games have been presented for validating and extending knowledge bases.
p6902
aVThe first game, Infection, validates concept-concept relations, and the second, The Knowledge Towers, validates image-concept relations.
p6903
aVIn experiments involving online players, we demonstrate three contributions.
p6904
aVFirst, games were released in two conditions whereby players either saw financial incentives for playing or a personal satisfaction incentive where they were thanked by us.
p6905
aVWe demonstrated that both conditions produced nearly identical numbers of annotations and, moreover, that players were disinterested in the satisfaction incentive, suggesting they played out of interest in the game itself.
p6906
aVFurthermore, we demonstrated the effectiveness of a novel design for games with a purpose which does not require two players for validation and instead reinforces behavior only using true negative items that required no manual annotation.
p6907
aVSecond, in a comparison with crowdsourcing, we demonstrate that video game-based annotations consistently generated higher-quality annotations.
p6908
aVLast, we demonstrate that video game-based annotation can be more cost-effective than crowdsourcing or annotation tasks with game-like features.
p6909
aVThe significant number of annotations generated by the satisfaction incentive condition shows that a fun game can generate high-quality annotations at virtually no cost.
p6910
aVAll annotated resources, demos of the games, and a live version of the top-ranking items for each concept are currently available online.
p6911
aV5 5 http://lcl.uniroma1.it/games/.
p6912
aVIn the future we will apply our video games to the validation of more data, such as the new Wikipedia bitaxonomy [ 11 ].
p6913
ag106
asg107
S'P14-1122'
p6914
sg109
(lp6915
VLarge-scale knowledge bases are important assets in NLP.
p6916
aVFrequently, such resources are constructed through automatic mergers of complementary resources, such as WordNet and Wikipedia.
p6917
aVHowever, manually validating these resources is prohibitively expensive, even when using methods such as crowdsourcing.
p6918
aVWe propose a cost-effective method of validating and extending knowledge bases using video games with a purpose.
p6919
aVTwo video games were created to validate concept-concept and concept-image relations.
p6920
aVIn experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated.
p6921
ag106
asba(icmyPackage
FText
p6922
(dp6923
g3
(lp6924
VInstead of focusing on grammatical errors that are found to be highly representative of language proficiency, our interest is in capturing the range of forms that surface in language production and the degree of sophistication of such forms, collectively referred to as syntactic complexity in [ 24 ]
p6925
aVThe choice and design of objective measures of language proficiency is governed by two crucial constraints
p6926
aVIn the domain of native language acquisition, the presence or absence of a grammatical structure indicates grammatical development
p6927
aVSpeaking in a non-native language requires diverse abilities, including fluency, pronunciation, intonation, grammar, vocabulary, and discourse
p6928
aVInformed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability
p6929
aVAutomatic recognition of non-native speakers u'\u005cu2019' spontaneous speech is a challenging task as evidenced by the error rate of the state-of-the-art speech recognizer
p6930
aVFor instance, Chen and Zechner ( 2011 ) reported a 50.5% word error rate (WER) and Yoon and Bhat ( 2012 ) reported a 30% WER in the recognition of ESL students u'\u005cu2019' spoken responses
p6931
aVThese high error rates at the recognition stage negatively affect the subsequent stages of the speech scoring system in general, and in particular, during a deep syntactic analysis, which operates on a long sequence of words as its context
p6932
aVAs a result, measures of grammatical complexity that are closely tied to a correct syntactic analysis are rendered unreliable
p6933
aVIn order to avoid the problems encountered with deep analysis-based measures, Yoon and Bhat ( 2012 ) explored a shallow analysis-based approach, based on the assumption that the level of grammar sophistication at each proficiency level is reflected in the distribution of part-of-speech (POS) tag bigrams
p6934
aVHence we will refer to this approach as u'\u005cu2018' shallow analysis u'\u005cu2019'
p6935
aVThe idea that the level of syntactic complexity (in terms of its range and sophistication) can be assessed based on the distribution of POS-tags is informed by prior studies in second language acquisition
p6936
aVBased on this idea, Yoon and Bhat ( 2012 ) developed a set of features of syntactic complexity based on POS sequences extracted from a large corpus of ESL learners u'\u005cu2019' spoken responses, grouped by human-assigned scores of proficiency level
p6937
aVThe syntactic complexity of a test spoken response was estimated based on its similarity to the proficiency groups in the reference corpus with respect to the score-specific constructions
p6938
aVWe first analyze the limitations of the model studied in [ 37 ] and then describe how our model can address those limitations
p6939
aVThe result is a new measure based on POS bigrams to assess ESL learners u'\u005cu2019' mastery of syntactic complexity
p6940
aVWe mentioned that the measure proposed in this study is derived from assumptions similar to those studied in [ 37 ]
p6941
aVAccordingly, we will summarize the previously studied model, outline its limitations, show how our proposed measure addresses those limitations and compare the two measures for the task of automatic scoring of speech
p6942
aVThey treat the concatenated collection of responses from a particular score-class as a u'\u005cu2018' super u'\u005cu2019' document
p6943
aVThen, regarding POS bigrams as terms, they construct POS-based vector space models for each score-class (there are four score classes denoting levels of proficiency as will be explained in Section 5.2 ), thus yielding four score-specific vector-space models (VSMs
p6944
aVThe terms of the VSM are weighted by the term frequency-inverse document frequency ( t u'\u005cu2062' f - i u'\u005cu2062' d u'\u005cu2062' f ) weighting scheme [ 29 ]
p6945
aVOf these, c u'\u005cu2062' o u'\u005cu2062' s 4 was selected based on its empirical performance (it showed the strongest correlation with human-assigned scores of proficiency among the distance-based measures
p6946
aVFirst, the VSM-based method is likely to over-estimate the contribution of the POS bigrams when highly correlated bigrams occur as terms in the VSM
p6947
aVConsider the presence of a grammar pattern represented by more than one POS bigram
p6948
aVGrammatical expressions that occur frequently in one score level but rarely in other levels can be assumed to be characteristic of a specific score level
p6949
aVTherefore, the more uneven the distribution of a grammatical expression across score classes, the more important that grammatical expression should be as an indicator of a particular score class
p6950
aVA pattern that occurs rarely but uniformly across different score groups can get the same weight as a pattern which is unevenly distributed to one score group
p6951
aVWhen using t u'\u005cu2062' f - i u'\u005cu2062' d u'\u005cu2062' f weighting to extract words that were strongly associated with positive sentiment in a movie review corpus (they considered each review as a document and a word as a term), it was found that a substantial proportion of words with the highest t u'\u005cu2062' f - i u'\u005cu2062' d u'\u005cu2062' f were rare words (e.g.,, proper nouns) which were not directly associated with the sentiment
p6952
aVThis is done by resorting to a maximum entropy model based approach, to which we turn next
p6953
aVTaking an approach different from previous studies, we formulate the task of assigning a score of syntactic complexity to a spoken response as a classification problem given a spoken response, assign the response to a proficiency class
p6954
aVA distinguishing feature of the current study is that the measure is based on a comparison of characteristics of the test response to models trained on large amounts of data from each score point, as opposed to measures that are simply characteristics of the responses themselves (which is how measures have been considered in prior studies
p6955
aVThe second limitation, related to the ineffective weighting of terms via the the t u'\u005cu2062' f - i u'\u005cu2062' d u'\u005cu2062' f scheme, seems to be addressed by the fact that the MaxEnt model assigns a weight to each feature (in our case, POS bigrams) on a per-class basis (in our case, score group), by taking every instance into consideration
p6956
aVTherefore, a MaxEnt model has an advantage over the model described in 4.1 in that it uses four different weight schemes (one per score level) and each scheme is optimized for each score level
p6957
aVTest takers read and/or listened to stimulus materials and then responded to questions based on the stimuli
p6958
aVThe distribution of proficiency scores, along with other details of the data sets, are presented in Table 1
p6959
aVAs seen in Table 1 , there is a strong bias towards the middle scores (score 2 and 3) with approximately 84-85% of the responses belonging to these two score levels
p6960
aVAlthough the skewed distribution limits the number of score-specific instances for the highest and lowest scores available for model training, we used the data without modifying the distribution since it is representative of responses in a large-scale language assessment scenario
p6961
aVThe first stage, ASR, yields an automatic transcription, which is followed by the POS tagging stage
p6962
aVSubsequently, the feature extraction stage (a VSM or a MaxEnt model as the case may be) generates the syntactic complexity feature which is then incorporated in a multiple linear regression model to generate a score
p6963
aVHowever, due to substantial amount of speech recognition errors in our data, the POS error rate (resulting from the combined errors of ASR and automated POS tagger) is expected to be higher
p6964
aVThe results that follow are based on MaxEnt classifier u'\u005cu2019' s parameter settings initialized to zero
p6965
aVSince a preliminary analysis of the effect of varying the feature (binary or frequency) revealed that the binary-valued feature was optimal (in terms of yielding the best agreement between human and machine scores), we only report our results for this case
p6966
aVTo illustrate this, consider a scenario where the classifier assigns two responses A and B to score level 2 (based on the maximum a posteriori condition
p6967
aVIt is apparent that the classifier has an overall tendency to assign a higher score to B, but looking at its top preference alone (2 for both responses), masks this tendency
p6968
aVWe thus capture the classifier u'\u005cu2019' s finer-grained scoring tendency by calculating the expected value of the classifier output
p6969
aVThis permits us to better represent the score assigned by the MaxEnt classifier as a relative preference over score assignments
p6970
aVWe consider a multiple regression automatic scoring model as studied in Zechner et al
p6971
aVA measure u'\u005cu2019' s utility has been evaluated according to its ability to discriminate between levels of proficiency assigned by human raters
p6972
aVThis is done by considering the Pearson correlation coefficient between the feature and the human scores
p6973
aVIn our case, however, with only access to the overall proficiency scores, we use scores of language proficiency as those of grammatical skill
p6974
aVA criterion for evaluating the performance of the scoring model is the extent to which the automatic scores of overall proficiency agree with the human scores
p6975
aVAs in prior studies, here too the level of agreement is evaluated by means of the weighted kappa measure as well as unrounded and rounded Pearson u'\u005cu2019' s correlations between machine and human scores (since the output of the regression model can either be rounded or regarded as is
p6976
aVSeeking to study the robustness of the measures derived using a shallow analysis, we next compare the two measures studied here, with respect to the impact of speech recognition errors on their correlation with scores of syntactic complexity
p6977
aVChen and Zechner ( 2011 ) found that while using measures of syntactic complexity obtained from transcriptions, errors in ASR transcripts caused over 0.40 drop in correlation from that found with manual transcriptions 5 5 Due to differences in the dataset and ASR system, a direct comparison between the current study and the cited prior study was not possible
p6978
aVThe effect of the measure of syntactic complexity is best studied by including it in an automatic scoring model of overall proficiency
p6979
aVAs seen in Table 3 , using the proposed measure, m u'\u005cu2062' e u'\u005cu2062' s u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e , leads to an improved agreement between human and machine scores of proficiency
p6980
aVThe performance gain of Base+cos4 over Base , however, is not statistically significant at level = 0.01
p6981
aVThus, the inclusion of the MaxEnt-based measure of syntactic complexity results in improved agreement between machine and human scores compared to the state-of-the-art model (here, Base
p6982
aVThe D-Level Scale categorizes grammatical development into 8 levels according to the presence of a set of diverse grammatical expressions varying in difficulty (for example, level 0 consists of simple sentences, while level 5 consists of sentences joined by a subordinating conjunction
p6983
aVSimilarly, Scarborough ( 1990 ) proposed the Index of Productive Syntax (IPSyn), according to which, the presence of particular grammatical structures, from a list of 60 structures (ranging from simple ones such as including only subjects and verbs, to more complex constructions such as conjoined sentences) is evidence of language acquisition milestones
p6984
aVDespite the functional differences between the indices, there is a fundamental operational similarity - that they both use the presence or absence of grammatical structures, rather than their occurrence count, as evidence of acquisition of certain grammatical levels
p6985
aVThe assumption that a presence-based view of grammatical level acquisition is also applicable to second language assessment helps validate our observation that binary-valued features yield a better performance when compared with frequency-valued features
p6986
aVLooking ahead, an important question is the extent to which our measure is sensitive to a mismatch between training and test data
p6987
aVSeeking alternatives to measuring syntactic complexity of spoken responses via syntactic parsers, we study a shallow-analysis based approach for use in automatic scoring
p6988
aVEmpirically, we show that the proposed measure, based on a maximum entropy classification, satisfied the constraints of the design of an objective measure to a high degree
p6989
aVThe measure outperformed a related measure of syntactic complexity (also based on shallow-analysis of spoken response) previously found to be well-suited for automatic scoring
p6990
asg88
(lp6991
sg90
(lp6992
sg92
(lp6993
VSeeking alternatives to measuring syntactic complexity of spoken responses via syntactic parsers, we study a shallow-analysis based approach for use in automatic scoring.
p6994
aVEmpirically, we show that the proposed measure, based on a maximum entropy classification, satisfied the constraints of the design of an objective measure to a high degree.
p6995
aVIn addition, the proposed measure was found to be relatively robust to ASR errors.
p6996
aVThe measure outperformed a related measure of syntactic complexity (also based on shallow-analysis of spoken response) previously found to be well-suited for automatic scoring.
p6997
aVIncluding the measure of syntactic complexity in an automatic scoring model resulted in statistically significant performance gains over the state-of-the-art.
p6998
aVWe also make an interesting observation that the impressionistic evaluation of syntactic complexity is better approximated by the presence or absence of grammar and usage patterns (and not by their frequency of occurrence), an idea supported by studies in native language acquisition.
p6999
ag106
asg107
S'P14-1123'
p7000
sg109
(lp7001
VDesigning measures that capture various aspects of language ability is a central task in the design of systems for automatic scoring of spontaneous speech.
p7002
aVIn this study, we address a key aspect of language proficiency assessment syntactic complexity.
p7003
aVWe propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical performance on real world data in multiple ways.
p7004
aVFirst, it is both robust and reliable , producing automatic scores that agree well with human rating compared to the state-of-the-art.
p7005
aVSecond, the measure makes sense theoretically, both from algorithmic and native language acquisition points of view.
p7006
ag106
asba(icmyPackage
FText
p7007
(dp7008
g3
(lp7009
VWe aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models
p7010
aVWe leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits
p7011
aVThe spoken term detection task arises as a key subtask in applying NLP applications to spoken content
p7012
aVTasks like topic identification and named-entity detection require transforming a continuous acoustic signal into a stream of discrete tokens which can then be handled by NLP and other statistical machine learning techniques
p7013
aVSpoken term detection converts the raw acoustics into time-marked keyword occurrences, which may subsequently be fed (e.g., as a bag-of-terms) to standard NLP algorithms
p7014
aVAlthough spoken term detection does not require the use of word-based automatic speech recognition (ASR), it is closely related
p7015
aVIf we had perfectly accurate ASR in the language of the corpus, term detection is reduced to an exact string matching task
p7016
aVWe consider term detection rather than the transcription task in considering how to exploit topic context, because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence
p7017
aVWe will show that by focusing on contextual information in the form of word repetition within documents, we obtain consistent improvement across five languages in the so called Base Phase of the IARPA BABEL program
p7018
aVThe BABEL task is modeled on the 2006 NIST Spoken Term Detection evaluation [] but focuses on limited resource conditions
p7019
aVWe focus specifically on the so called no target audio reuse (NTAR) condition to make our method broadly applicable
p7020
aVThe strength of this phenomenon suggests it may be more viable for improving term-detection than, say, topic-sensitive language models
p7021
aVWe validate this by developing an interpolation formula to boost putative word repetitions in the search results, and then investigate a method for setting interpolation weights without manually tuning on a development set
p7022
aVWe then demonstrate that the method generalizes well, by applying it to the 2006 English data and the remaining four 2013 BABEL languages
p7023
aVGiven the rise of unsupervised latent topic modeling with Latent Dirchlet Allocation [] and similar latent variable approaches for discovering meaningful word co-occurrence patterns in large text corpora, we ought to be able to leverage these topic contexts instead of merely N-grams
p7024
aVIndeed there is work in the literature that shows that various topic models, latent or otherwise, can be useful for improving language model perplexity and word error rate (Khudanpur and Wu, 1999; Chen, 2009; Naptali et al., 2012
p7025
aVFor example, if we determine the context of the detection hypothesis is about computers, containing words like u'\u005cu2018' monitor, u'\u005cu2019' u'\u005cu2018' internet u'\u005cu2019' and u'\u005cu2018' mouse, u'\u005cu2019' then we would be more confident of a term such as u'\u005cu2018' keyboard u'\u005cu2019' and less confident of a term such as u'\u005cu2018' cheese board u'\u005cu2019'
p7026
aVUsing topic information will be helpful if u'\u005cu2018' monitor, u'\u005cu2019' u'\u005cu2018' keyboard u'\u005cu2019' and u'\u005cu2018' mouse u'\u005cu2019' consistently predict that u'\u005cu2018' keyboard u'\u005cu2019' is present
p7027
aVWe illustrate this variability by looking at how consistent word co-occurrences are between two separate corpora in the same language i.e.,, if we observe words that frequently co-occur with a keyword in the training corpus, do they also co-occur with the keywords in a second held-out corpus
p7028
aVFigure 1 , based on the BABEL Tagalog corpus, suggests this is true only for high frequency keywords
p7029
aVFor each keyword k , we count how often it co-occurs in the same conversation as a vocabulary word w in the ASR training data and the development data, and designate the counts T u'\u005cu2062' ( k , w ) and D u'\u005cu2062' ( k , w ) respectively
p7030
aVThe x -coordinate of each point in Figure 1 is the frequency of k in the training data, and the y -coordinate is the correlation coefficient u'\u005cu03a1' k between T u'\u005cu2062' ( k , w ) and D u'\u005cu2062' ( k , w
p7031
aVA high u'\u005cu03a1' k implies that words w that co-occur frequently with k in the training data also do so in the search collection
p7032
aVTo further illustrate how Figure 1 was obtained, consider the high-frequency keyword bukas (count = u'\u005cud835' u'\u005cudfd6' u'\u005cud835' u'\u005cudfd5' u'\u005cud835' u'\u005cudfd7' ) and the low-frequency keyword Davao (count = u'\u005cud835' u'\u005cudfcf' u'\u005cud835' u'\u005cudfcf' ), and plot T u'\u005cu2062' ( k , u'\u005cu22c5' ) versus D u'\u005cu2062' ( k , u'\u005cu22c5' ) , as done in Figure 4
p7033
aVThe correlation coefficients u'\u005cu03a1' u'\u005cud835' u'\u005cudc4f' u'\u005cud835' u'\u005cudc62' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc4e' u'\u005cud835' u'\u005cudc60' and u'\u005cu03a1' u'\u005cud835' u'\u005cudc37' u'\u005cud835' u'\u005cudc4e' u'\u005cud835' u'\u005cudc63' u'\u005cud835' u'\u005cudc4e' u'\u005cud835' u'\u005cudc5c' from the two plots end up as two points in Figure 1
p7034
aVHowever, if the goal is to help a speech retrieval system detect content-rich (and presumably infrequent) keywords, then using word co-occurrence information (i.e., topic context) does not appear to be too promising, even though intuition suggests that such information ought to be helpful
p7035
aVIn light of this finding, we will restrict the type of context we use for term detection to the co-occurrence of the term itself elsewhere within the document
p7036
aVAs it turns out this u'\u005cu2018' burstiness u'\u005cu2019' of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context
p7037
aVIn all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models
p7038
aVMost recently, looked at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language
p7039
aVThe typical use of Document Frequency ( DF ) in information retrieval or text categorization is to emphasize words that occur in only a few documents and are thus more u'\u005cu201c' rich in content u'\u005cu201d'
p7040
aVClose examination of DF statistics by Church and Gale in their work on Poisson Mixtures (1995) resulted in an analysis of the burstiness of content words
p7041
aVThe first illustration of word burstiness can be seen by plotting observed inverse document frequency, IDF w , versus f w in the log domain (Figure 7
p7042
aVIn contrast, the AP English data exhibits a correlation of u'\u005cu03a1' = 0.93 []
p7043
aVThus the deviation in the Tagalog corpus is more pronounced, i.e., words are less uniformly distributed across documents
p7044
aVWe denote this as E u'\u005cu2062' [ k ] and can interpret burstiness as the expected word count given we see w at least once
p7045
aVIn general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model []
p7046
aVLikewise, Katz attempts to capture these two classes in his G model of word frequencies (1996
p7047
aVFor the first class, burstiness increases slowly but steadily as w occurs more frequently
p7048
aVSince our corpus size is fixed, we might expect this to occur, as more word occurrences must be pigeon-holed into the same number of documents
p7049
aVLooking close to the y -axis in Figure 9 , we observe a second class of exclusively low frequency words whose burstiness ranges from highly concentrated to singletons
p7050
aVIf we take the Class A concentration trend as typical, we can argue that most Class B words exhibit a larger than average concentration
p7051
aVIn applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence
p7052
aVWe encounter the burstiness property of words again by looking at unigram occurrence probabilities
p7053
aVHowever, conditioning on one occurrence, most word types are more likely to occur again, due to their burstiness
p7054
aVFor the Tagalog data, we let u'\u005cu0391' range from 0 (the baseline) to 0.4 and re-score each term detection score according to ( 6
p7055
aVTable 1 shows the results of this parameter sweep and yields us 1 to 2% absolute performance gains in a number of term detection metrics
p7056
aVimplies that cost of a miss is inversely proportional to the frequency of the term in the corpus, but the cost of a false alarm is fixed
p7057
aVFor this reason, we report both ATWV and the P u'\u005cu2062' ( Miss ) component
p7058
aVBut despite the strong evidence of the adaptation phenomenon in both high and low-frequency words (Figure 11 ), we have less confidence in the adaptation strength of any particular word
p7059
aVAs with word co-occurrence, we consider if estimates of P a u'\u005cu2062' d u'\u005cu2062' a u'\u005cu2062' p u'\u005cu2062' t u'\u005cu2062' ( w ) from training data are consistent when estimated on development data
p7060
aVGiven the variability in estimating P a u'\u005cu2062' d u'\u005cu2062' a u'\u005cu2062' p u'\u005cu2062' t u'\u005cu2062' ( w ) , an alternative approach would be take P w ^ as an upper bound on u'\u005cu0391' , reached as the DF w increases (cf
p7061
aVHowever, considering this estimate in light of the two classes of words in Figure 9 , there are clearly words in Class B with high burstiness that will be ignored by trying to compensate for the high adaptation variability in the low-frequency range
p7062
aVTable 2 contrasts the results for using the three different interpolation heuristics on the Tagalog development queries
p7063
aVNow that we have tested word repetition-based re-scoring on a small Tagalog development set we want to know if our approach, and particularly our u'\u005cu0391' ^ estimate is sufficiently robust to apply broadly
p7064
aVLastly, we re-score the search output by interpolating the top term detection score for a document with subsequent hits according to Equation 6 using the u'\u005cu0391' ^ estimated for this training condition
p7065
aVUsing our final algorithm, we are able to boost repeated term detections and improve results in all languages and training conditions
p7066
aVLastly, the reductions in P u'\u005cu2062' ( Miss ) suggests that we are improving the term detection metric, which is sensitive to threshold changes, by doing what we set out to do, which is to boost lower confidence repeated words and correctly asserting them as true hits
p7067
aVMoreover, we are able to accomplish this in a wide variety of languages
p7068
aVLeveraging the burstiness of content words, we have developed a simple technique to consistently boost term detection performance across languages
p7069
aVUsing word repetitions, we effectively use a broad document context outside of the typical 2-5 N-gram window
p7070
asg88
(lp7071
sg90
(lp7072
sg92
(lp7073
VLeveraging the burstiness of content words, we have developed a simple technique to consistently boost term detection performance across languages.
p7074
aVUsing word repetitions, we effectively use a broad document context outside of the typical 2-5 N-gram window.
p7075
aVFurthermore, we see improvements across a broad spectrum of languages languages with syllable-based word tokens (Vietnamese, Cantonese), complex morphology (Turkish), and dialect variability (Pashto.
p7076
aVSecondly, our results are not only effective but also intuitive, given that the interpolation weight parameter matches our expectations for the burstiness of the word tokens in the language on which it is estimated.
p7077
aVWe have focused primarily on re-scoring results for the term detection task.
p7078
aVGiven the effectiveness of the technique across multiple languages, we hope to extend our effort to exploit our human tendency towards redundancy to decoding or other aspects of the spoken document processing pipeline.
p7079
ag106
asg107
S'P14-1124'
p7080
sg109
(lp7081
VWe aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models.
p7082
aVInstead of taking a broad view of topic context in spoken documents, variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents.
p7083
aVWe show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document.
p7084
aVWe leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits.
p7085
aVWe then develop a principled approach to select interpolation weights using only the ASR training data.
p7086
aVUsing this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five languages in the BABEL program.
p7087
ag106
asba(icmyPackage
FText
p7088
(dp7089
g3
(lp7090
VWe present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resource-rich language
p7091
aVWe train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization
p7092
aVOur method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages
p7093
aVWe perform experiments on three Data sets u'\u005cu2014' Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages
p7094
aVIn recent years, dependency parsing has gained universal interest due to its usefulness in a wide range of applications such as synonym generation [ 43 ] , relation extraction [ 37 ] and machine translation [ 21 , 51 ]
p7095
aVHowever, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages
p7096
aVThis led to a vast amount of research on unsupervised grammar induction [ 9 , 22 , 47 , 12 , 48 , 4 , 29 , 49 ] , which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers
p7097
aV1 1 For the sake of simplicity, we refer to the resource-poor language as the u'\u005cu201c' target language u'\u005cu201d' , and resource-rich language as the u'\u005cu201c' source language u'\u005cu201d'
p7098
aVIn addition, in this study we use English as the source resource-rich language, but our methodology can be applied to any resource-rich languages
p7099
aVObviously, bilingual treebanks are much more difficult to acquire than the resources required in our scenario, since the labeled training data and the parallel text in our case are completely separated
p7100
aVIn this work, we propose a learning framework for transferring dependency grammars from a resource-rich language to resource-poor languages via parallel text
p7101
aVWe train probabilistic parsing models for resource-poor languages by maximizing a combination of likelihood on parallel data and confidence on unlabeled data
p7102
aVOur work is based on the learning framework used in Smith and Eisner [ 44 ] , which is originally designed for parser bootstrapping
p7103
aVWe extend this learning framework so that it can be used to transfer cross-lingual knowledge between different languages
p7104
aVThroughout this paper, English is used as the source language and we evaluate our approach on ten target languages u'\u005cu2014' Danish (da), Dutch (nl), French (fr), German (de), Greek (el), Italian (it), Korean (ko), Portuguese (pt), Spanish (es) and Swedish (sv
p7105
aVFor example, Figure 1 shows a dependency tree for the sentence, Economic news had little effect on financial markets , with the sentence u'\u005cu2019' s root-symbol as its root
p7106
aVThe focus of this work is on building dependency parsers for target languages, assuming that an accurate English dependency parser and some parallel text between the two languages are available
p7107
aVAnother advantage of the learning framework is that it combines both the likelihood on parallel data and confidence on unlabeled data, so that both parallel text and unlabeled data can be utilized in our approach
p7108
aVwhere F j are feature functions, u'\u005cu039b' = ( u'\u005cu039b' 1 , u'\u005cu039b' 2 , u'\u005cu2026' ) are parameters of the model, and Z u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) is a normalization factor, which is commonly referred to as the partition function
p7109
aVA common strategy to make this parsing model efficiently computable is to factor dependency trees into sets of edges
p7110
aVThat is, dependency tree y is treated as a set of edges e and each feature function F j u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9a' , u'\u005cud835' u'\u005cudc99' ) is equal to the sum of all the features f j u'\u005cu2062' ( e , u'\u005cud835' u'\u005cudc99' )
p7111
aVFirst, by transferring the weight function to the corresponding weight in the well-developed English parsing model, we can project syntactic information across language boundaries
p7112
aVBy reducing unaligned edges to their delexicalized forms, we can still use those delexicalized features, such as part-of-speech tags, for those unaligned edges, and can address problem that automatically generated word alignments include errors
p7113
aVDue to the normalizing factor Z ~ u'\u005cu2062' ( u'\u005cud835' u'\u005cudc99' ) , the transferring distribution is a valid one
p7114
aVWe introduce a multiplier u'\u005cu0393' as a trade-off between the two contributions (parallel and unsupervised) of the objective function K , and the final objective function K u'\u005cu2032' has the following form
p7115
aVOne may regard u'\u005cu0393' as a Lagrange multiplier that is used to constrain the parser u'\u005cu2019' s uncertainty H to be low, as presented in several studies on entropy regularization [ 5 , 17 , 20 ]
p7116
aVAccording to equation ( 9 ), p ~ ( u'\u005cud835' u'\u005cudc9a' u'\u005cud835' u'\u005cudc99' ) can also be factored into the multiplication of the weight of each edge, so both K P and its gradient can be calculated by running the O u'\u005cu2062' ( n 3 ) inside-outside algorithm [ 2 , 41 ] for projective parsing
p7117
aVFor non-projective parsing, the analogy to the inside algorithm is the O u'\u005cu2062' ( n 3 ) matrix-tree algorithm based on Kirchhoff u'\u005cu2019' s Matrix-Tree Theorem, which is dominated asymptotically by a matrix determinant [ 25 , 46 ]
p7118
aVThe gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O u'\u005cu2062' ( n 3 ) complexity as evaluating the function
p7119
aVSimilar with the calculation of K P , K U can also be computed by running the inside-outside algorithm [ 2 , 41 ] for projective parsing
p7120
aVPrepare parallel text by running word alignment method to obtain word alignments, 3 3 The word alignment methods do not require additional resources besides parallel text and prepare the unlabeled data
p7121
aVWe select target languages based on the availability of these resources
p7122
aVThe monolingual treebanks in our experiments are from the Google Universal Dependency Treebanks [ 31 ] , for the reason that the treebanks of different languages in Google Universal Dependency Treebanks have consistent syntactic representations
p7123
aVHowever, previous studies [ 34 , 31 ] have demonstrated that a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components, and the heterogenous representations used in CoNLL shared-tasks treebanks weaken any conclusion that can be drawn
p7124
aVThe three languages are Danish, Dutch and Greek
p7125
aVSo totally we have ten target languages
p7126
aVThe set of POS tags needs to be consistent across languages and treebanks
p7127
aVFor this reason we use the universal POS tag set of Petrov et al
p7128
aVPOS tags are not available for parallel data in the Europarl and Kaist corpus, so we need to provide the POS tags for these data
p7129
aVFor the purpose of evaluation of our approach and comparison with previous work, we need to exploit the gold POS tags to train the POS taggers
p7130
aVAs part-of-speech tags are also a form of syntactic analysis, this assumption weakens the applicability of our approach
p7131
aVIn this section, we will describe the details of our experiments and compare our results with previous methods
p7132
aVAs presented in Section 3.1 , we evaluate our parsing approach on both version 1.0 and version 2.0 of Google Univereal Treebanks for seven languages 6 6 Japanese and Indonesia are excluded as no practicable parallel data are available
p7133
aVIt is based on the transition-based dependency parsing paradigm [ 40 ]
p7134
aVIn addition to their original results, we also report results by re-implementing the direct transfer parser based on the first-order projective dependency parsing model [ 30 ] (DTP u'\u005cu2020'
p7135
aVOne may regard this system as an oracle of transfer parsing
p7136
aVParsing accuracy is measured with unlabeled attachment score (UAS the percentage of words with the correct head
p7137
aVOur approaches significantly outperform all the baseline systems across all the seven target languages
p7138
aVFor the results on Google Universal Treebanks version 1.0, the improvement on average over the projected transfer paper (PTP u'\u005cu2020' ) is 3.96% and up to 6.22% for Korean and 4.80% for German
p7139
aVBy adding entropy regularization from unlabeled data, our full model achieves average improvement of 0.29% over the u'\u005cu201c' -U u'\u005cu201d' setting
p7140
aVTable 6 gives the results comparing the model without unlabeled data (-U) presented in this work to those five baseline systems and the oracle (OR
p7141
aVTable 7 shows the results of our system and the results of baseline systems u'\u005cu201c' USR u'\u005cu2020' u'\u005cu201d' is the weakly supervised system of Naseem et al
p7142
aVIt should be noted that the u'\u005cu201c' NMG u'\u005cu201d' system utilizes more than one helper languages
p7143
aVSo it is not directly comparable to our work
p7144
aVFor example, if we want to make our model capable of utilizing more contextual information, we can extend our transferring weight to higher-order parts
p7145
aVBy presenting a model training framework, our approach can utilize parallel text to estimate transferring distribution with the help of a well-developed resource-rich language dependency parser, and use unlabeled data as entropy regularization
p7146
aVThe experimental results on three data sets across ten target languages show that our approach achieves significant improvement over previous studies
p7147
asg88
(lp7148
sg90
(lp7149
sg92
(lp7150
VIn this paper, we propose an unsupervised projective dependency parsing approach for resource-poor languages, using existing resources from a resource-rich source language.
p7151
aVBy presenting a model training framework, our approach can utilize parallel text to estimate transferring distribution with the help of a well-developed resource-rich language dependency parser, and use unlabeled data as entropy regularization.
p7152
aVThe experimental results on three data sets across ten target languages show that our approach achieves significant improvement over previous studies.
p7153
ag106
asg107
S'P14-1126'
p7154
sg109
(lp7155
VWe present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resource-rich language.
p7156
aVWe train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization.
p7157
aVOur method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages.
p7158
aVWe perform experiments on three Data sets Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages.
p7159
aVWe obtain state-of-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems.
p7160
ag106
asba(icmyPackage
FText
p7161
(dp7162
g3
(lp7163
VThe induced word boundary information is encoded as a graph propagation constraint
p7164
aVThe constrained model induction is accomplished by using posterior regularization algorithm
p7165
aVWord segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g.,, space in English
p7166
aVThese models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency [ 4 ]
p7167
aVBut one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task
p7168
aVIn recent years, a number of works [ 23 , 4 , 12 , 22 ] attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data
p7169
aVThey proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model
p7170
aVFrequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment
p7171
aVThe prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment [ 15 ]
p7172
aVCrucially, the GP expression with the bilingual knowledge is then used as side information to regularize a CRFs (conditional random fields) model u'\u005cu2019' s learning over treebank and bitext data, based on the posterior regularization (PR) framework [ 9 ]
p7173
aVThe former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations
p7174
aV[ 4 ] enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence
p7175
aV[ 29 ] produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications
p7176
aVMost importantly, the constraints have a better learning guidance since they originate from the bilingual texts
p7177
aVRather than playing the u'\u005cu201c' hard u'\u005cu201d' uses of the bilingual segmentation knowledge, i.e.,, directly merging u'\u005cu201c' char-to-word u'\u005cu201d' alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model u'\u005cu2019' s learning
p7178
aV[ 32 ] , proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g.,, CRFs
p7179
aVThese approaches are referred to as pipelined learning with GP
p7180
aVThis study also works with a similarity graph, encoding the learned bilingual knowledge
p7181
aVBut, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation
p7182
aVOne of our main objectives is to bias CRFs model u'\u005cu2019' s learning on unlabeled data, under a non-linear GP constraint encoding the bilingual knowledge
p7183
aVPR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters
p7184
aV[ 4 ] described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge
p7185
aVThe second step aims to collect word boundary distributions for all types, i.e.,, character-level trigrams, according to the n -to-1 mappings (Section 3.1
p7186
aVThis constrained learning is carried out based on posterior regularization (PR) framework [ 9 ]
p7187
aVThis relies on statistical character-based alignment first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special u'\u005cu201c' words u'\u005cu201d' or alignment targets, and second, they are connected with English words by using a statistical word aligner, e.g.,, GIZA++ [ 15 ]
p7188
aVThe primary idea is that consecutive Chinese characters are grouped to a candidate word, if they are aligned to the same foreign word
p7189
aVIt is worth mentioning that prior works presented a straightforward usage for candidate words, treating them as golden segmentations, either dictionary units or labeled resources
p7190
aVBut this study treats the induced candidate words in a different way
p7191
aVSecond, boundary distributions can play more flexible roles as constraints over labelings to bias the model learning
p7192
aVThe type-level word boundary extraction is formally described as follows
p7193
aVAn intuitive manner is to directly leverage the induced boundary distributions as label constraints to regularize segmentation model learning, based on a constrained learning algorithm
p7194
aVIn what follows, the graph setting and propagation expression are introduced
p7195
aVAs in conventional GP examples [ 7 ] , a similarity graph u'\u005cud835' u'\u005cudca2' = ( V , E ) is constructed over N types extracted from Chinese training data, including treebank u'\u005cud835' u'\u005cudc9f' l c and bitexts u'\u005cud835' u'\u005cudc9f' u c
p7196
aVThe similarities are measured based on co-occurrence statistics over a set of predefined features (introduced in Section 4.1
p7197
aVThe quality (smoothness) of the similarity graph can be estimated by using a standard propagation function, as shown in Equation 1
p7198
aVOur learning problem belongs to semi-supervised learning (SSL), as the training is done on treebank labeled data ( X L , Y L ) = { ( x 1 , y 1 ) , u'\u005cu2026' , ( x l , y l ) } , and bilingual unlabeled data ( X U ) = { x 1 , u'\u005cu2026' , x u } where x i = { x 1 , u'\u005cu2026' , x m } is an input word sequence and y i = { y 1 , u'\u005cu2026' , y m } , y u'\u005cu2208' T is its corresponding label sequence
p7199
aVThe conditional probabilities p u'\u005cu0398' are expressed as a log-linear form
p7200
aVWhere Z u'\u005cu0398' u'\u005cu2062' ( x i ) is a partition function that normalizes the exponential form to be a probability distribution, and f u'\u005cu2062' ( y i k - 1 , y i k , x i ) are arbitrary feature functions
p7201
aVWe follow the approach introduced by [ 10 ] to set up a penalty-based PR objective with GP the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints
p7202
aVRather than regularize CRFs model u'\u005cu2019' s posteriors p u'\u005cu0398' ( u'\u005cud835' u'\u005cudcb4' x i ) directly, our model uses an auxiliary distribution q ( u'\u005cud835' u'\u005cudcb4' x i ) over the possible labelings u'\u005cud835' u'\u005cudcb4' for x i , and penalizes the CRFs marginal log-likelihood by a KL-divergence term 4 4 The form of KL term
p7203
aVNote that the penalty is fired if the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration
p7204
aVThis nature requires that the penalty term u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( v ) should be formed as a function of posteriors q over CRFs model predictions 5 5 The original PR setting also requires that the penalty term should be a linear (Ganchev et al., 2010) or non-linear [ 10 ] function on q i.e.,, u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( q
p7205
aV{ 1 , u'\u005cu2026' , u } , { 1 , u'\u005cu2026' , m } ) u'\u005cu2192' V from words in the corpus to vertices in the graph is defined
p7206
aVWe can thus decompose v i , t into a function of q as follows
p7207
aVSince the penalty term u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( v ) is a non-linear form, the optimization method in [ 9 ] via projected gradient descent on the dual is inefficient 6 6 According to [ 10 ] , the dual of quadratic program implies an expensive matrix inverse
p7208
aVThis EM-style approach monotonically increases u'\u005cud835' u'\u005cudca5' u'\u005cu2062' ( u'\u005cu0398' , q ) and thus is guaranteed to converge to a local optimum
p7209
aVThere are four hyperparameters in our model to be tuned by using the development data ( dev MT ) among the following settings for the graph propagation, u'\u005cu039c' u'\u005cu2208' { 0.2 , 0.5 , 0.8 } and u'\u005cu03a1' u'\u005cu2208' { 0.1 , 0.3 , 0.5 , 0.8 } ; for the PR learning, u'\u005cu039b' u'\u005cu2208' { 0 u'\u005cu2264' u'\u005cu039b' i u'\u005cu2264' 1 } and u'\u005cu03a3' u'\u005cu2208' { 0 u'\u005cu2264' u'\u005cu03a3' i u'\u005cu2264' 1 } where the step is 0.1
p7210
aVThe MT experiment was conducted based on a standard log-linear phrase-based SMT model
p7211
aVThe heuristic strategy of grow-diag-final-and [ 11 ] was used to combine the bidirectional alignments for extracting phrase translations and reordering tables
p7212
aVThe standard four-tags ( B , M , E and S ) were used as the labels
p7213
aVThe stochastic gradient descent is adopted to optimize the parameters
p7214
aV[ 28 ] , is a hierarchical HMM segmenter that incorporates parts-of-speech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities
p7215
aVTo be fair, the same similarity graph settings introduced in this paper were used that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches
p7216
aVSelf-training Segmenters (STS two variant models were defined by the approach reported in [ 20 ] that uses the supervised CRFs model u'\u005cu2019' s decodings, incorporating empirical and constraint information, for unlabeled examples as additional labeled data to retrain a CRFs model
p7217
aVBut they only capture partial segmentation features so that less gains for SMT are achieved when comparing to other sophisticated models
p7218
aVThis behaviour illustrates that the conventional optimizations to the monolingual supervised model, e.g.,, accumulating more supervised data or predefined segmentation properties, are insufficient to help model for achieving better segmentations for SMT
p7219
aVThe second observation shifts the emphasis to SMS and UBS, based on the treebank and the bilingual segmentation, respectively
p7220
aVThrough analyzing both models u'\u005cu2019' segmentations for train MT and test MT , we attempted to get a closer inspection on the segmentation preferences and their influence on MT
p7221
aVFor example, UBS grouped u'\u005cu201c' {CJK} UTF8gbsnå½(country)_ {CJK} UTF8gbsné(border)_ {CJK} UTF8gbsné´(between) u'\u005cu201d' to a word u'\u005cu201c' {CJK} UTF8gbsnå½éé´(international) u'\u005cu201d' , rather than two words u'\u005cu201c' {CJK} UTF8gbsnå½é(international)_ {CJK} UTF8gbsné´(between) u'\u005cu201d' (as given by SMS), since these three characters are aligned to a single English word u'\u005cu201c' international u'\u005cu201d'
p7222
aVIn our opinion, the learning mechanism of our approach, joint coupling of GP and CRFs, rather than the pipelined one as the other two models, contributes to maximizing the graph smoothness effects to the CRFs estimation so that the error propagation of the pipelined approaches is alleviated
p7223
aV1) learn word boundaries from character-based alignments; 2) encode the learned word boundaries into a GP constraint; and 3) training a CRFs model, under the GP constraint, by using the PR framework
p7224
asg88
(lp7225
sg90
(lp7226
sg92
(lp7227
VThis paper proposed a novel CWS model for the SMT task.
p7228
aVThis model aims to maintain the linguistic segmentation supervisions from treebank data and simultaneously integrate useful bilingual segmentations induced from the bitexts.
p7229
aVThis objective is accomplished by three main steps.
p7230
aV1) learn word boundaries from character-based alignments; 2) encode the learned word boundaries into a GP constraint; and 3) training a CRFs model, under the GP constraint, by using the PR framework.
p7231
aVThe empirical results indicate that the proposed model can yield better segmentations for SMT.
p7232
ag106
asg107
S'P14-1128'
p7233
sg109
(lp7234
VThis study investigates on building a better Chinese word segmentation model for statistical machine translation.
p7235
aVIt aims at leveraging word boundary information, automatically learned by bilingual character-based alignments, to induce a preferable segmentation model.
p7236
aVWe propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model, trained by the treebank data (labeled), on the bilingual data (unlabeled.
p7237
aVThe induced word boundary information is encoded as a graph propagation constraint.
p7238
aVThe constrained model induction is accomplished by using posterior regularization algorithm.
p7239
aVThe experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality.
p7240
ag106
asba(icmyPackage
FText
p7241
(dp7242
g3
(lp7243
VRecent work has shown success in using neural network language models (NNLMs) as features in MT systems
p7244
aVHere, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window
p7245
aVAdditionally, we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding
p7246
aVThey have since been extended to translation modeling, parsing, and many other NLP tasks
p7247
aVWe also present a novel technique for training the neural network to be self-normalized , which avoids the costly step of posteriorizing over the entire vocabulary in decoding
p7248
aVAdditionally, on top of a simpler decoder equivalent to Chiang u'\u005cu2019' s [ 5 ] original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU u'\u005cu2013' as much as all of the other features in our strong baseline system combined
p7249
aVWe also show strong improvements on the NIST OpenMT12 Chinese-English task, as well as the DARPA BOLT (Broad Operational Language Translation) Arabic-English and Chinese-English conditions
p7250
aVIntuitively, we want to define u'\u005cud835' u'\u005cudcae' i as the window that is most relevant to t i
p7251
aVTo do this, we first say that each target word t i is affiliated with exactly one source word at index a i u'\u005cud835' u'\u005cudcae' i is then the m -word source window centered at a i
p7252
aVIf t i is unaligned, we inherit its affiliation from the closest aligned word, with preference given to the right
p7253
aVThe vocabulary is selected by frequency-sorting the words in the parallel training data
p7254
aVOut-of-vocabulary words are mapped to their POS tag (or OOV , if POS is not available), and in this case P ( P O S i t i - 1 , u'\u005cu22ef' ) is used directly without further normalization
p7255
aVWe chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u005cu2013' larger sizes did not improve results, while smaller sizes degraded results
p7256
aVAt every epoch, which we define as 20,000 minibatches, the likelihood of a validation set is computed
p7257
aVIf this likelihood is worse than the previous epoch, the learning rate is multiplied by 0.5
p7258
aV2012 ) require a 2-20k shortlist vocabulary, and are therefore still quite costly
p7259
aVHere, our goal is to be able to use a fairly large vocabulary without word classes, and to simply avoid computing the entire output layer at decode time
p7260
aV4 4 We are not concerned with speeding up training time, as we already find GPU training time to be adequate
p7261
aVIf we could guarantee that log u'\u005cu2061' ( Z u'\u005cu2062' ( x ) ) were always equal to 0 (i.e.,, Z u'\u005cu2062' ( x ) = 1) then at decode time we would only have to compute row r of the output layer instead of the whole matrix
p7262
aVWhile we cannot train a neural network with this guarantee, we can explicitly encourage the log-softmax normalizer to be as close to 0 as possible by augmenting our training objective function
p7263
aVAt decode time, we simply use U r u'\u005cu2062' ( x ) as the feature score, rather than log u'\u005cu2061' ( P u'\u005cu2062' ( x )
p7264
aVHere, we present a u'\u005cu201c' trick u'\u005cu201d' for pre-computing the first hidden layer, which further increases the speed of NNJM lookups by a factor of 1,000x
p7265
aVHowever, note that there are only 3 possible positions for each target word, and 11 for each source word
p7266
aVTherefore, for every word in the vocabulary, and for each position, we can pre-compute the dot product between the word embedding and the first hidden layer
p7267
aVThis can be reduced to just 5 scalar additions by pre-summing each 11-word source window when starting a test sentence
p7268
aVIf our neural network has only one hidden layer and is self-normalized, the only remaining computation is 512 calls to t u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' h u'\u005cu2062' ( ) and a single 513-dimensional dot product for the final output score
p7269
aV6 6 t u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' h u'\u005cu2062' ( ) is implemented using a lookup table
p7270
aVThus, only u'\u005cu223c' 3500 arithmetic operations are required per n -gram lookup, compared to u'\u005cu223c' 2.8M for self-normalized NNJM without pre-computation, and u'\u005cu223c' 35M for the standard NNJM
p7271
aVFor the sake of a fair comparison, these all use one hidden layer
p7272
aVThe decoding cost is based on a measurement of u'\u005cu223c' 1200 unique NNJM lookups per source word for our Arabic-English system
p7273
aVBy combining self-normalization and pre-computation, we can achieve a speed of 1.4M lookups/second, which is on par with fast back-off LM implementations [ 27 ]
p7274
aVBecause our NNJM is fundamentally an n -gram NNLM with additional source context, it can easily be integrated into any SMT decoder
p7275
aVFor aligned target words, the normal affiliation heuristic can be used, since the word alignment is available within the rule
p7276
aVFor unaligned words, the normal heuristic can also be used, except when the word is on the edge of a rule, because then the target neighbor words are not necessarily known
p7277
aVSpecifically, if unaligned target word t is on the right edge of an arc that covers source span [ s i , s j ] , we simply say that t is affiliated with source word s j
p7278
aVIf t is on the left edge of the arc, we say it is affiliated with s i
p7279
aVThe T2S variations cannot be used in decoding due to the large target context required, and are thus only used in k -best rescoring
p7280
aVThe S2T/R2L variant could be used in decoding, but we have not found this beneficial, so we only use it in rescoring
p7281
aVOne issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words
p7282
aVWe treat NULL as a normal target word, and if a source word aligns to multiple target words, it is treated as a single concatenated token
p7283
aVIt is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word
p7284
aVAlthough we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM
p7285
aVOn Arabic-English, the primary S2T/L2R NNJM gains +1.4 BLEU on top of our baseline, while the S2T NNLTM gains another +0.8, and the directional variations gain +0.8 BLEU more
p7286
aVThis leads to a total improvement of +3.0 BLEU from the NNJM and its variations
p7287
aV12 12 Note that the official 1st place OpenMT12 result was our own system, so we can assure that these comparisons are accurate
p7288
aVThe smaller improvement on Chinese-English compared to Arabic-English is consistent with the behavior of our baseline features, as we show in the next section
p7289
aVThe baseline used in the last section is a highly-engineered research system, which uses a wide array of features that were refined over a number of years, and some of which require linguistic resources
p7290
aVBecause of this, the baseline BLEU scores are much higher than a typical MT system u'\u005cu2013' especially a real-time, production engine which must support many language pairs
p7291
aVTherefore, we also present results using a simpler version of our decoder which emulates Chiang u'\u005cu2019' s original Hiero implementation [ 5 ]
p7292
aVThe baseline here uses the same feature set as the strong NIST system
p7293
aVOn Arabic, the total gain is +2.6 BLEU, while on Chinese, the gain is +1.3 BLEU
p7294
aV13 13 The difference in score for self-normalized vs pre-computed is entirely due to two vs one hidden layers
p7295
aVThe u'\u005cu201c' Simple Hierarchical u'\u005cu201d' baseline is used here because it more closely approximates a real-time MT engine
p7296
aVFor the sake of speed, these experiments only use the S2T/L2R NNJM+S2T NNLTM
p7297
aVHowever, the n -grams created for the NNJM can be shared with the Kneser-Ney LM, which reduces the cost of that feature
p7298
aVThus, the total cost increase of using the NNJM+NNLTM features in decoding is only u'\u005cu223c' 0.01 seconds per source word
p7299
aVA number of recent papers have proposed methods for creating neural network translation/joint models, but nearly all of these works have obtained much smaller BLEU improvements than ours
p7300
aVAlso, their model is recurrent, so it cannot be used in decoding
p7301
aVThey score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features (21.8 vs 21.7
p7302
aVHowever, additive results are not presented
p7303
aVHowever, Le u'\u005cu2019' s formulation could only be used in k -best rescoring, since it requires long-distance re-ordering and a large target context
p7304
aVHowever, we have demonstrated that we can obtain 50%-80% of the total improvement with only one model (S2T/L2R NNJM), and 70%-90% with only two models (S2T/L2R NNJM + S2T NNLTM
p7305
aVThus, the one and two-model conditions still significantly outperform any past work
p7306
aVWe have described a novel formulation for a neural network-based machine translation joint model, along with several simple variations of this model
p7307
aVWhen used as MT decoding features, these models are able to produce a gain of +3.0 BLEU on top of a very strong and feature-rich baseline, as well as a +6.3 BLEU gain on top of a simpler system
p7308
aVThe use of a large bilingual context vector, which is provided to the neural network in u'\u005cu201c' raw u'\u005cu201d' form, rather than as the output of some other algorithm
p7309
aVThe fact that the model is purely lexicalized, which avoids both data sparsity and implementation complexity
p7310
asg88
(lp7311
sg90
(lp7312
sg92
(lp7313
g1538
asg107
S'P14-1129'
p7314
sg109
(lp7315
VRecent work has shown success in using neural network language models (NNLMs) as features in MT systems.
p7316
aVHere, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window.
p7317
aVOur model is purely lexicalized and can be integrated into any MT decoder.
p7318
aVWe also present several variations of the NNJM which provide significant additive improvements.
p7319
aVAlthough the model is quite simple, it yields strong empirical results.
p7320
aVOn the NIST OpenMT12 Arabic-English condition, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, feature-rich baseline which already includes a target-only NNLM.
p7321
aVThe NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chiang s [ 5 ] original Hiero implementation.
p7322
aVAdditionally, we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding.
p7323
aVThese techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM.
p7324
ag106
asba(icmyPackage
FText
p7325
(dp7326
g3
(lp7327
VWe explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms
p7328
aVThese features are not redundant
p7329
aVTherefore, we may suffer a performance loss if we select only a small subset of the features
p7330
aVOn the other hand, by including all the rich features, we face over-fitting problems
p7331
aVWe depart from this view and leverage high-dimensional feature vectors by mapping them into low dimensional representations
p7332
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p7333
aVThe associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance
p7334
aVThis low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags
p7335
aVBy automatically selecting a small number of dimensions useful for parsing, we can leverage a wide array of (correlated) features
p7336
aVFollowing standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set
p7337
aVWord-level vector space embeddings have so far had limited impact on parsing performance
p7338
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p7339
aVBecause of this issue, Cirik and u'\u005cu015e' ensoy ( 2013 ) used word vectors only as unigram features (without combinations) as part of a shift reduce parser [ 32 ]
p7340
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p7341
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
p7342
aVMany machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters
p7343
aVSuch problems include, for example, multi-task learning and collaborative filtering
p7344
aVLow-rank constraints are commonly used for improving generalization [ 19 , 37 , 38 , 12 ]
p7345
aVIndeed, recent approaches to matrix problems decompose the parameter matrix as a sum of low-rank and sparse matrices [ 40 , 47 ]
p7346
aVThe sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace [ 42 , 4 ]
p7347
aVTensors are increasingly used as tools in spectral estimation [ 15 ] , including in parsing [ 6 ] and other NLP problems [ 10 ] , where the goal is to avoid local optima in maximum likelihood estimation
p7348
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p7349
aVWe will commence here by casting first-order dependency parsing as a tensor estimation problem
p7350
aVWe will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task
p7351
aVWe denote each element of the tensor as A i , j , k where i u'\u005cu2208' [ n ] , j u'\u005cu2208' [ n ] , k u'\u005cu2208' [ d ] and [ n ] is a shorthand for the set of integers { 1 , 2 , u'\u005cu22ef' , n }
p7352
aVWe define the inner product of two tensors (or matrices) as u'\u005cu27e8' A , B u'\u005cu27e9' = vec u'\u005cu2062' ( A ) T u'\u005cu2062' vec u'\u005cu2062' ( B ) , where vec u'\u005cu2062' ( u'\u005cu22c5' ) concatenates the tensor (or matrix) elements into a column vector
p7353
aVTheir orientation is defined based on usage
p7354
aVFor example, u u'\u005cu2297' v is a rank-1 matrix u u'\u005cu2062' v T when u and v are column vectors ( u T u'\u005cu2062' v if they are row vectors
p7355
aVWe will directly learn a low-rank tensor A (because r is small) in this form as one of our model parameters
p7356
aVEach y is understood as a collection of arcs h u'\u005cu2192' m where h and m index words in x
p7357
aV2 2 Note that in the case of high-order parsing, the sum S u'\u005cu2062' ( x , y ) may also include local scores for other syntactic structures, such as grandhead-head-modifier score s ( g u'\u005cu2192' h u'\u005cu2192' m
p7358
aVThe predicted parse is obtained as y ^ = arg u'\u005cu2062' max y u'\u005cu2208' u'\u005cud835' u'\u005cudcb4' u'\u005cu2062' ( x ) u'\u005cu2061' S u'\u005cu2062' ( x , y )
p7359
aVBased on this feature representation, we define the score of each arc as s u'\u005cu0398' ( h u'\u005cu2192' m ) = u'\u005cu27e8' u'\u005cu0398' , u'\u005cu03a6' h u'\u005cu2192' m u'\u005cu27e9' where u'\u005cu0398' u'\u005cu2208' u'\u005cu211d' L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in u'\u005cu03a6' h u'\u005cu2192' m
p7360
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p7361
aVNote that elements of this rank-1 tensor include feature combinations that are not part of the feature crossings in u'\u005cu03a6' h u'\u005cu2192' m
p7362
aVAs a result, the arc score for the tensor reduces to evaluating U u'\u005cu2062' u'\u005cu03a6' h , V u'\u005cu2062' u'\u005cu03a6' m , and W u'\u005cu2062' u'\u005cu03a6' h , m which are all r dimensional vectors and can be computed efficiently based on any sparse vectors u'\u005cu03a6' h , u'\u005cu03a6' m , and u'\u005cu03a6' h , m
p7363
aVBy learning parameters U , V , and W that function well in dependency parsing, we also learn context-dependent embeddings for words and arcs
p7364
aVSpecifically, U u'\u005cu2062' u'\u005cu03a6' h (for a given sentence, suppressed) is an r dimensional vector representation of the word corresponding to h as a head word
p7365
aVSimilarly, V u'\u005cu2062' u'\u005cu03a6' m provides an analogous representation for a modifier m
p7366
aVThe resulting embedding is therefore tied to the syntactic roles of the words (and arcs), and learned in order to perform well in parsing
p7367
aVFor example, we can easily incorporate additional useful features in the feature vectors u'\u005cu03a6' h , u'\u005cu03a6' m and u'\u005cu03a6' h , m , since the low-rank assumption (for small enough r ) effectively counters the otherwise uncontrolled feature expansion
p7368
aVMoreover, by controlling the amount of information we can extract from each of the component feature vectors (via rank r ), the statistical estimation problem does not scale dramatically with the dimensions of u'\u005cu03a6' h , u'\u005cu03a6' m and u'\u005cu03a6' h , m
p7369
aVIf the arc has not been seen in the available training data, it does not contribute to the traditional arc score s u'\u005cu0398' u'\u005cu2062' ( u'\u005cu22c5'
p7370
aVSpecifically, we define the arc score s u'\u005cu0393' ( h u'\u005cu2192' m ) as the combination
p7371
aVwhere u'\u005cu0398' u'\u005cu2208' u'\u005cu211d' L , U u'\u005cu2208' u'\u005cu211d' r × n , V u'\u005cu2208' u'\u005cu211d' r × n , and W u'\u005cu2208' u'\u005cu211d' r × d are the model parameters to be learned
p7372
aVThe constraints serve to separate the gold tree from other alternatives in u'\u005cud835' u'\u005cudcb4' u'\u005cu2062' ( x ^ i ) with a margin that increases with distance
p7373
aVThe objective as stated is not jointly convex with respect to U , V and W due to our explicit representation of the low-rank tensor
p7374
aVHowever, if we fix any two sets of parameters, for example, if we fix V and W , then the combined score S u'\u005cu0393' u'\u005cu2062' ( x , y ) will be a linear function of both u'\u005cu0398' and U
p7375
aVAs a result, the objective will be jointly convex with respect to u'\u005cu0398' and U and could be optimized using standard tools
p7376
aVIn an online learning setup, we update parameters successively based on each sentence
p7377
aVThis is possible since the objective function with respect to ( u'\u005cu0398' , U ) has a similar form as in the original passive-aggressive algorithm
p7378
aVWe then obtain parameter increments u'\u005cu0394' u'\u005cu2062' u'\u005cu0398' and u'\u005cu0394' u'\u005cu2062' U by solving
p7379
aVWhen u'\u005cu0393' = 0 , the arc scores are entirely based on the low-rank tensor and u'\u005cu0394' u'\u005cu2062' u'\u005cu0398' = 0
p7380
aVNote that u'\u005cu03a6' h , u'\u005cu03a6' m , u'\u005cu03a6' h , m , and u'\u005cu03a6' h u'\u005cu2192' m are typically very sparse for each word or arc
p7381
aVTherefore d u'\u005cu2062' u and d u'\u005cu2062' u'\u005cu0398' are also sparse and can be computed efficiently
p7382
aVThe alternating online algorithm relies on how we initialize U , V , and W since each update is carried out in the context of the other two
p7383
aVA random initialization of these parameters is unlikely to work well, both due to the dimensions involved, and the nature of the alternating updates
p7384
aVWe consider here instead a reasonable deterministic u'\u005cu201c' guess u'\u005cu201d' as the initialization method
p7385
aVWe begin by training our model without any low-rank parameters, and obtain parameters u'\u005cu0398'
p7386
aVThe majority of features in this MST component can be expressed as elements of the feature tensor, i.e.,, as [ u'\u005cu03a6' h u'\u005cu2297' u'\u005cu03a6' m u'\u005cu2297' u'\u005cu03a6' h , m ] i , j , k
p7387
aVWe can therefore create a tensor representation of u'\u005cu0398' such that B i , j , k equals the corresponding parameter value in u'\u005cu0398'
p7388
aVWe use a low-rank version of B as the initialization
p7389
aVSpecifically, we unfold the tensor B into a matrix B ( h ) of dimensions n and n u'\u005cu2062' d , where n = d u'\u005cu2062' i u'\u005cu2062' m u'\u005cu2062' ( u'\u005cu03a6' h ) = d u'\u005cu2062' i u'\u005cu2062' m u'\u005cu2062' ( u'\u005cu03a6' m ) and d = d u'\u005cu2062' i u'\u005cu2062' m u'\u005cu2062' ( u'\u005cu03a6' h , m
p7390
aVFor instance, a rank-1 tensor can be unfolded as u u'\u005cu2297' v u'\u005cu2297' w = u u'\u005cu2297' vec u'\u005cu2062' ( v u'\u005cu2297' w
p7391
aVWe compute the top-r SVD of the resulting unfolded matrix such that B ( h ) = P T u'\u005cu2062' S u'\u005cu2062' Q
p7392
aVIn other words, keeping updating the model may lead to large parameter values and over-fitting
p7393
aVThe decoding algorithm for the third-order parsing is based on [ 46 ]
p7394
aVFinally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing
p7395
aVTo add auxiliary word vector representations, we use the publicly available word vectors [ 5 ] , learned from raw data [ 13 , 20 ]
p7396
aVEach entry of the word vector is added as a feature value into feature vectors u'\u005cu03a6' h and u'\u005cu03a6' m
p7397
aVFor each word in the sentence, we add its own word vector as well as the vectors of its left and right words
p7398
aVWe should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U u'\u005cu2062' u'\u005cu03a6' h , V u'\u005cu2062' u'\u005cu03a6' m and W u'\u005cu2062' u'\u005cu03a6' h , m rather than explicitly calculate the feature tensor u'\u005cu03a6' h u'\u005cu2297' u'\u005cu03a6' m u'\u005cu2297' u'\u005cu03a6' h , m
p7399
aVTherefore updating parameters and decoding a sentence is still efficient, i.e.,, linear in the number of values of the feature vector
p7400
aVFollowing standard practices, we train our full model and the baselines for 10 epochs
p7401
aVAs the evaluation measure, we use unlabeled attachment scores (UAS) excluding punctuation
p7402
aVBy comparing NT-1st and NT-3rd (models without low-rank) with our full model (with low-rank), we obtain 0.7% absolute improvement on first-order parsing, and 0.3% improvement on third-order parsing
p7403
aVFirst, we test our model by varying the hyper-parameter u'\u005cu0393' which balances the tensor score and the traditional MST/Turbo score components
p7404
aVLearning of the tensor is harder because the scoring function is not linear (nor convex) with respect to parameters U , V and W
p7405
aVAs described in previous section, we do so by appending the values of different coordinates in the word vector into u'\u005cu03a6' h and u'\u005cu03a6' m
p7406
aVAs Table 3 shows, adding this information increases the parsing performance for all the three languages
p7407
aVSince our model learns a compressed representation of feature vectors, we are interested to measure its performance when part-of-speech tags are not provided (See Table 4
p7408
aVThe two r-dimension vectors are concatenated as an u'\u005cu201c' averaged u'\u005cu201d' vector
p7409
aVBased on these results, estimating a rank-50 tensor together with MST parameters only increases the running time by a factor of 1.7
p7410
aVOur method maintains the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms
p7411
aVIn particular, we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor
p7412
aVThis tensor will accordingly be a four or five-way array
p7413
aVThe online update algorithm remains applicable since each dimension is optimized in an alternating fashion
p7414
aVWe thank Volkan Cirik for sharing the unsupervised word vector data
p7415
asg88
(lp7416
sg90
(lp7417
sg92
(lp7418
VAccurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high-dimensional feature representations.
p7419
aVWe introduce a low-rank factorization method that enables to map high dimensional feature vectors into low dimensional representations.
p7420
aVOur method maintains the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms.
p7421
aVWe implement the approach on first-order to third-order dependency parsing.
p7422
aVOur parser outperforms the Turbo and MST parsers across 14 languages.
p7423
aVFuture work involves extending the tensor component to capture higher-order structures.
p7424
aVIn particular, we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor.
p7425
aVThis tensor will accordingly be a four or five-way array.
p7426
aVThe online update algorithm remains applicable since each dimension is optimized in an alternating fashion.
p7427
ag106
asg107
S'P14-1130'
p7428
sg109
(lp7429
VAccurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high-dimensional feature representations.
p7430
aVA small subset of such features is often selected manually.
p7431
aVThis is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features.
p7432
aVIn this paper, we use tensors to map high-dimensional feature vectors into low dimensional representations.
p7433
aVWe explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms.
p7434
aVOur parser consistently outperforms the Turbo and MST parsers across 14 different languages.
p7435
aVWe also obtain the best published UAS results on 5 languages.
p7436
aV1 1 Our code is available at https://github.com/taolei87/RBGParser.
p7437
ag106
asba(icmyPackage
FText
p7438
(dp7439
g3
(lp7440
VWe present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph
p7441
aVThese algorithms are often based on PageRank [ 2 ] and other centrality measures (e.g.,, [ 7 ]
p7442
aVSimRank is based on the simple intuition that nodes in a graph should be considered as similar to the extent that their neighbors are similar
p7443
aVUnfortunately, SimRank has time complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) (where n is the number of nodes in the graph) and therefore does not scale to the large graphs that are typical of NLP
p7444
aVOur key observation is that to compute the similarity of two nodes, we need not consider all other nodes in the graph as SimRank does; instead, CoSimRank starts random walks from the two nodes and computes their similarity at each time step
p7445
aVThis offers large savings in computation time if we only need the similarities of a small subset of all n 2 node similarities
p7446
aVCoSimRank can be used to compute many variations of basic node similarity u'\u005cu2013' including similarity for graphs with weighted and typed edges and similarity for sets of nodes
p7447
aVThus, CoSimRank has the added advantage of being a flexible tool for different types of applications
p7448
aVThe extension of CoSimRank to similarity across graphs is important for the application of bilingual lexicon extraction given a set of correspondences between nodes in two graphs A and B (corresponding to two different languages), a pair of nodes ( a u'\u005cu2208' A , b u'\u005cu2208' B ) is a good candidate for a translation pair if their node similarity is high
p7449
aVBy providing some useful extensions, we demonstrate the great flexibility of CoSimRank (Section 5
p7450
aVOur work is unsupervised
p7451
aVWe therefore do not review graph-based methods that make extensive use of supervised learning (e.g.,, de Melo and Weikum ( 2012 )
p7452
aVSince the original version of SimRank [ 15 ] has complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 4 ) , many extensions have been proposed to speed up its calculation
p7453
aVThe algorithm we propose below is an order of magnitude faster in such applications because it is based on a local formulation of the similarity measure
p7454
aV2006 ) introduce a similarity measure that is also based on the idea that nodes are similar when their neighbors are, but that is designed for bipartite graphs
p7455
aVWe will refer to this measure as PPR+cos
p7456
aVHughes and Ramage ( 2007 ) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs
p7457
aVLike CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below
p7458
aVPPR+cos performed best except for a new similarity measure based on commute time
p7459
aVWe do not compare against this new measure as it uses the graph Laplacian and so cannot be computed for a single node pair
p7460
aVOne reason CoSimRank is efficient is that we need only compute a few iterations of the random walk
p7461
aVWe use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below
p7462
aVA similar graph of dependency structures was built by Minkov and Cohen ( 2008
p7463
aVThe novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only needs a fraction of all u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) pairwise similarities
p7464
aVWe first first give an intuitive introduction of CoSimRank as a Personalized PageRank (PPR) derivative
p7465
aVLater on, we will give a matrix formulation to compare CoSimRank with SimRank
p7466
aVHaveliwala ( 2002 ) introduced Personalized PageRank u'\u005cu2013' or topic-sensitive PageRank u'\u005cu2013' based on the idea that the uniform damping vector p ( 0 ) can be replaced by a personalized vector, which depends on node i
p7467
aVNote that the personalization vector p ( 0 ) was eliminated, but is still present as the starting vector of the iteration
p7468
aVLet p u'\u005cu2062' ( i ) be the PPR vector of node i
p7469
aVThe cosine of two vectors u and v is computed by dividing the inner product u'\u005cu27e8' u , v u'\u005cu27e9' by the lengths of the vectors
p7470
aVThe cosine of two PPR vectors can be used as a similarity measure for the corresponding nodes [ 12 , 1 ]
p7471
aVThis measure s u'\u005cu2062' ( i , j ) looks at the probability that a random walker is on a certain edge after an unlimited number of steps
p7472
aVThis is potentially problematic as the example in Figure 1 shows
p7473
aVHowever, the PPR vector of law will also have a non-zero weight for tailor
p7474
aVSo law and dress are similar because of the node tailor
p7475
aVWe can prevent this type of spurious similarity by taking into account the path the random surfer took to get to a particular node
p7476
aVWe formalize this by defining CoSimRank s u'\u005cu2062' ( i , j ) as follows
p7477
aVWe add a damping factor c , so that early meetings are more valuable than later meetings
p7478
aVSince our vectors are probability vectors, we have
p7479
aVWe will see in Section 5 that this formulation is the basis for a very efficient version of CoSimRank
p7480
aVAs the PPR vectors have only positive values, we can easily see in Eq
p7481
aVWe also know from elementary functional analysis that the 1-norm is the biggest of all p-norms and so one has u'\u005cu2225' p ( k ) u'\u005cu2225' u'\u005cu2264' 1
p7482
aVIf an upper bound of 1 is desired for s u'\u005cu2062' ( i , j ) (instead of 1 / ( 1 - c ) ), then we can use s u'\u005cu2032'
p7483
aVWhereas SimRank sets each of these entries back to one at each iteration, CoSimRank adds one
p7484
aVThus, when computing the two similarity measures iteratively, the diagonal element ( i , i ) will be set to 1 by both methods for those initial iterations for which this entry is 0 for c u'\u005cu2062' A u'\u005cu2062' S ( k - 1 ) u'\u005cu2062' A T (i.e.,, before applying either max or add
p7485
aV8 ) have time complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) or u'\u005cu2013' if we want to take the higher efficiency of computation for sparse graphs into account u'\u005cu2013' u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n 2 ) where n is the number of nodes and d the average degree
p7486
aVIf d k , then the time complexity of CoSimRank is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k 2 u'\u005cu2062' n
p7487
aVIf we only compute a single similarity, then the complexity is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n
p7488
aVIt is not obvious how to design a lower-complexity version of SimRank for this case
p7489
aVThus, we have reduced SimRank u'\u005cu2019' s cubic time complexity to a quadratic time complexity for CoSimRank or u'\u005cu2013' assuming that the average degree d does not depend on n u'\u005cu2013' SimRank u'\u005cu2019' s quadratic time complexity to linear time complexity for the case of computing few similarities
p7490
aVSpace complexity for computing k 2 similarities is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2062' n ) since we need only store k vectors, not the complete similarity matrix
p7491
aVIf the matrix formulation cannot be used because the u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) similarity matrix is too big for available memory, then we can compute all similarities in batches u'\u005cu2013' and if desired in parallel u'\u005cu2013' whose size is chosen such that the vectors of each batch still fit in memory
p7492
aVIn summary, CoSimRank and SimRank have similar space and time complexities for computing all n 2 similarities
p7493
aVFor the more typical case that we only want to compute a fraction of all similarities, we have recast the global SimRank formulation as a local CoSimRank formulation
p7494
aVAs a result, time and space complexities are much improved
p7495
aVWe will show now that the basic CoSimRank algorithm can be extended in a number of ways and is thus a flexible tool for different NLP applications
p7496
aVThe use of weighted edges was first proposed in the PageRank patent
p7497
aVIt is straightforward and easy to implement by replacing the row normalized adjacency matrix A with an arbitrary stochastic matrix P
p7498
aVWe can interpret S ( 0 ) as a change of basis
p7499
aVA similar approach for word embeddings was published by Mikolov et al
p7500
aVWe are not including this method in our experiments, but we will give the equation here, as traditional document similarity measures (e.g.,, cosine similarity) perform poorly on this task although there also are known alternatives with good results [ 30 ]
p7501
aVOur motivation for this application is that two words that are synonyms of each other should have similar lexical neighbors and that two words that are translations of each other should have neighbors that correspond to each other; thus, in each case the nodes should be similar in the graph-theoretic sense and CoSimRank should be able to identify this similarity
p7502
aVWe propose CoSimRank as an efficient algorithm for computing the similarity of nodes in a graph
p7503
aVConsequently, we compare against the two main methods for this task in NLP
p7504
aVApart from SimRank, extensions of PageRank are the main methods for computing the similarity of nodes in graphs in NLP (e.g.,, Hughes and Ramage ( 2007 ) , Agirre et al
p7505
aVWe use TS68 , a test set of 68 synonym pairs published by Minkov and Cohen ( 2012 ) for evaluation
p7506
aVThis gold standard lists a single word as the correct synonym even if there are several equally acceptable near-synonyms (see Table 3 for examples
p7507
aVIf all three of them agreed on one word as being a synonym in at least one meaning, we added this as a correct answer to the test set
p7508
aVCoSimRank is better than PPR+cos on both evaluations, but as this test set is very small, the results are not significant
p7509
aVMRR is equivalent to MAP as reported by Minkov and Cohen ( 2012 ) when there is only one correct answer.) Their best number (0.59) is better than our one-synonym result; however, they performed manual postprocessing of results u'\u005cu2013' e.g.,, discarding words that are morphologically or semantically related to other words in the list u'\u005cu2013' so our fully automatic results cannot be directly compared
p7510
aVFor lexicon extraction, we use the same parameters as in the synonym extraction task for all four similarity measures
p7511
aVWe use a seed dictionary of 12,630 word pairs to establish node-node correspondences between the two graphs
p7512
aVWe remove a search keyword from the seed dictionary before calculating similarities for it, something that the architecture of CoSimRank makes easy because we can use a different seed dictionary S ( 0 ) for every keyword
p7513
aVThis is also true for CoSimRank, but it seems that CoSimRank is more stable because we compare more than one vector
p7514
aVWe tried a number of different ways of modifying it for weighted graphs i) running the random walks with the weighted adjacency matrix as Markov matrix, (ii) storing the weight (product of each edge weight) of a random walk and using it as a factor if two walks meet and (iii) a combination of both
p7515
aVAs a result, the computational time was approximately 30 minutes per test word, so this method is even slower than SimRank for our application
p7516
aVThe actual wall clock time was significantly lower as we used up to 64 CPUs
p7517
aVSimRank is at a disadvantage because it computes all similarities in the graph regardless of the size of the test set; it is particularly inefficient on synonym extraction because the English graph contains a large number of edges (see Table 1
p7518
aVThis effect is only visible on the larger test set (lexicon extraction) because the general computation overhead is about the same on a smaller test set
p7519
aVHere we address inducing a bilingual lexicon from a seed set based on grammatical relations found by a parser
p7520
aVMost of the 226 missing word pairs are adverbs, prepositions and plural forms that are not covered by our graphs due to the construction algorithm we use lemmatization, restriction to adjectives, nouns and verbs etc
p7521
aV4 4 We achieved better results for CoSimRank by optimizing the damping factor, but in this paper, we only present results for a fixed damping factor of 0.8
p7522
aVThe results on TS774 can be considered conservative since only one translation is accepted as being correct
p7523
aVAdditionally, TS774 was created by translating English words into German (using Google translate
p7524
aVWe are now testing the reverse direction
p7525
aVSo we are doomed to fail if the original English word is a less common translation of an ambiguous German word
p7526
aVFor example, the English word gulf was translated by Google to Golf , but the most common sense of Golf is the sport
p7527
aVHence our algorithm will incorrectly translate it back to golf
p7528
aVAs we can see in Table 7 , we also face the problems discussed by Laws et al
p7529
aV2010 the algorithm sometimes picks cohyponyms (which can still be seen as reasonable) and antonyms (which are clear errors
p7530
aVLooking at Table 1 , we see that there is only one edge type connecting adjectives
p7531
aVWe also presented extensions of CoSimRank for a number of applications, thus demonstrating the flexibility of CoSimRank as a similarity measure
p7532
aVWe showed that CoSimRank is superior to SimRank in time and space complexity; and we demonstrated that CoSimRank performs better than PPR+cos on two similarity computation tasks
p7533
asg88
(lp7534
sg90
(lp7535
sg92
(lp7536
VWe have presented CoSimRank , a new similarity measure that can be computed for a single node pair without relying on the similarities in the whole graph.
p7537
aVWe gave two different formalizations of CoSimRank i) a derivation from Personalized PageRank and (ii) a matrix representation that can take advantage of fast matrix multiplication algorithms.
p7538
aVWe also presented extensions of CoSimRank for a number of applications, thus demonstrating the flexibility of CoSimRank as a similarity measure.
p7539
aVWe showed that CoSimRank is superior to SimRank in time and space complexity; and we demonstrated that CoSimRank performs better than PPR+cos on two similarity computation tasks.
p7540
ag106
asg107
S'P14-1131'
p7541
sg109
(lp7542
VWe present CoSimRank , a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph.
p7543
aVWe present equivalent formalizations that show CoSimRank s close relationship to Personalized PageRank and SimRank and also show how we can take advantage of fast matrix multiplication algorithms to compute CoSimRank.
p7544
aVAnother advantage of CoSimRank is that it can be flexibly extended from basic node-node similarity to several other graph-theoretic similarity measures.
p7545
aVIn an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches.
p7546
ag106
asba(icmyPackage
FText
p7547
(dp7548
g3
(lp7549
VFollowing up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images, we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word
p7550
aVBy combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words
p7551
aVHowever, a possibly even more serious pitfall of vector models is lack of reference natural language is, fundamentally, a means to communicate, and thus our words must be able to refer to objects, properties and events in the outside world [ 1 ]
p7552
aVThe model might suggest that the concepts of dog and cat are semantically related, but it has no means to determine the visual appearance of dogs, and consequently no way to verify the truth of such a simple statement
p7553
aVThis is achieved by means of a simple neural network trained to project image-extracted feature vectors to text-based vectors through a hidden layer that can be interpreted as a cross-modal semantic space
p7554
aVWe first test the effectiveness of our cross-modal semantic space on the so-called zero-shot learning task [ 40 ] , which has recently been explored in the machine learning community [ 18 , 49 ]
p7555
aVOn the contrary, the first time a learner is exposed to a new object, the linguistic information available is likely also very limited
p7556
aVThus, in order to consider vision-to-language mapping under more plausible conditions, similar to the ones that children or robots in a new environment are faced with, we next simulate a scenario akin to fast mapping
p7557
aVWe show that the induced cross-modal semantic space is powerful enough that sensible guesses about the correct word denoting an object can be made, even when the linguistic context vector representing the word has been created from as little as 1 sentence containing it
p7558
aVThe contributions of this work are three-fold
p7559
aVFinally, we provide preliminary evidence that cross-modal projections can be used effectively to simulate a fast mapping scenario, thus strengthening the claims of this approach as a full-fledged, fully inductive theory of meaning acquisition
p7560
aVThe problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning (see Fazly et al
p7561
aVMost importantly, by projecting visual representations of objects into a shared semantic space , we do not limit ourselves to establishing a link between objects and words
p7562
aVIn our zero-shot experiments, we assume no access to an outlier detector, and thus, the search for the correct label is performed in the full concept space
p7563
aVA natural question we aim to answer is whether the success of cross-modal mapping is due to the high-quality embeddings or to the general algorithmic design
p7564
aVIf the latter is the case, then these results could be extended to traditional distributional vectors bearing other desirable properties, such as high interpretability of dimensions
p7565
aVMoreover, if this is also the first linguistic encounter of that concept, then we refer to the task as fast mapping
p7566
aVConcretely, we assume that concepts, denoted for convenience by word labels, are represented in linguistic terms by vectors in a text-based distributional semantic space (see Section 4.3
p7567
aVDuring training, this cross-modal vocabulary is used to induce a projection function (Section 4.4 ), which u'\u005cu2013' intuitively u'\u005cu2013' represents a mapping between visual and linguistic dimensions
p7568
aVThus, this function, given a visual vector, returns its corresponding linguistic representation
p7569
aVThe fast mapping setting can be seen as a special case of the zero-shot task
p7570
aVWhereas for the latter our system assumes that all concepts have rich linguistic representations (i.e.,, representations estimated from a large corpus), in the case of the former, new concepts are assumed to be encounted in a limited linguistic context and therefore lacking rich linguistic representations
p7571
aVThis is operationalized by constructing the text-based vector for these concepts from a context of just a few occurrences
p7572
aVUnlike the CIFAR-100 images, which were chosen specifically for image object recognition tasks (i.e.,, each image is clearly depicting a single object in the foreground), ESP contains a random selection of images from the Web
p7573
aVConsequently, objects do not appear in most images in their prototypical display, but rather as elements of complex scenes (see Figure 2
p7574
aVThus, ESP constitutes a more realistic, and at the same time more challenging, simulation of how things are encountered in real life, testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks
p7575
aVWe do not attempt any parameter tuning of the pipeline
p7576
aVAs low-level features, we use Scale Invariant Feature Transform (SIFT) features [ 35 ]
p7577
aVFor ESP, given the size and amount of noise in this dataset, we build vectors for visual concepts , by normalizing and summing the BoVW vectors of all the images that have the relevant concept as a tag
p7578
aVNote that relevant literature [ 41 ] has emphasized the importance of learners self-generating multiple views when faced with new objects
p7579
aVThus, our multiple-image assumption should not be considered as problematic in the current setup
p7580
aVFinally, similarly to the visual semantic space, raw counts are transformed by applying LMI and then reduced to 300 dimensions with SVD
p7581
aVThe process of learning to map objects to the their word label is implemented by training a projection function f proj v u'\u005cu2192' w from the visual onto the linguistic semantic space
p7582
aVWe implement 4 alternative learning algorithms for inducing the cross-modal projection function f proj v u'\u005cu2192' w
p7583
aVOur first model is a very simple linear mapping between the two modalities estimated by solving a least-squares problem
p7584
aVIn our setup, we can see the two different modalities as if they were different languages
p7585
aVBy using least-squares regression, the projection function f proj v u'\u005cu2192' w can be derived as
p7586
aVThis is achieved by finding a pair of matrices, in our case u'\u005cud835' u'\u005cudc02' V u'\u005cu2208' u'\u005cu211d' d v × d and u'\u005cud835' u'\u005cudc02' W u'\u005cu2208' u'\u005cu211d' d w × d , such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized
p7587
aVIn our setup, after applying CCA on the two spaces u'\u005cud835' u'\u005cudc15' s and u'\u005cud835' u'\u005cudc16' s , we obtain the two projection mappings onto the common space and thus our projection function can be derived as
p7588
aVThe weights are estimated by minimizing the objective function
p7589
aVIn our experiments we used cosine as similarity function, so that s u'\u005cu2062' i u'\u005cu2062' m u'\u005cu2062' ( u'\u005cud835' u'\u005cudc00' , u'\u005cud835' u'\u005cudc01' ) = A u'\u005cu2062' B u'\u005cu2225' A u'\u005cu2225' u'\u005cu2062' u'\u005cu2225' B u'\u005cu2225' , thus penalizing parameter settings leading to a low cosine between the target linguistic representations u'\u005cud835' u'\u005cudc16' s and those produced by the projection function u'\u005cud835' u'\u005cudc16' ^ s
p7590
aV7 7 We also experimented with the same objective function as Socher et al
p7591
aV2013 ) , however, our objective function yielded consistently better results in all experimental settings
p7592
aVTable 2 reports results obtained by averaging the performance on the 11,400 distinct vectors of the 19 unseen concepts
p7593
aVHowever, NN , an architecture that can capture more complex, non-linear relations in features across modalities, emerges as the best performing model, confirming on a larger scale the recent findings of Socher et al
p7594
aV2013 )
p7595
aVWe achieve this by looking at which visual concepts result in the highest hidden unit activation
p7596
aV8 8 For this post-hoc analysis, we include a sparsity parameter in the objective function of Equation 5 in order to get more interpretable results; hidden units are therefore maximally activated by a only few concepts
p7597
aVThe analysis demonstrates that, although prior knowledge about categories was not explicitly used to train the network, the latter induced an organization of concepts into superordinate categories in which the hidden layer acts as a cross-modal concept categorization/organization system
p7598
aVWhen the induced projection function maps an object onto the linguistic space, the derived text vector will inherit a mixture of textual features from the concepts that activated the same hidden unit as the object
p7599
aV2013 ) , thus preventing a direct comparison, the results reported in Table 5 are on a comparable scale to theirs
p7600
aVTo the best of our knowledge, this is the first time this task has been performed on a dataset as noisy as ESP
p7601
aVOverall, the results suggest that cross-modal mapping could be applied in tasks where images exhibit a more complex structure, e.g.,, caption generation and event recognition
p7602
aVIn this section, we aim at simulating a fast mapping scenario in which the learner has been just exposed to a new concept, and thus has limited linguistic evidence for that concept
p7603
aVWe operationalize this by considering the 34 concrete concepts introduced by Frassinelli and Keller ( 2012 ) , and deriving their text-based representations from just a few sentences randomly picked from the corpus
p7604
aVThe zero-shot framework leads us to frame fast mapping as the task of projecting visual representations of new objects onto language space for retrieving their word labels ( v u'\u005cu2192' w
p7605
aVIf we think about how linguistic reference is acquired, a scenario in which a learner first encounters a new object and then seeks its reference in the language of the surrounding environment (e.g.,, adults having a conversation, the text of a book with an illustration of an unknown object) is very natural
p7606
aVFurthermore, since not all new concepts in the linguistic environment refer to new objects (they might denote abstract concepts or out-of-scene objects), it seems more reasonable for the learner to be more alerted to linguistic cues about a recently-spotted new object than vice versa
p7607
aVMoreover, once the learner observes a new object, she can easily construct a full visual representation for it (and the acquisition literature has shown that humans are wired for good object segmentation and recognition [ 50 ] ) u'\u005cu2013' the more challenging task is to scan the ongoing and very ambiguous linguistic communication for contexts that might be relevant and informative about the new object
p7608
aVHowever, fast mapping is often described in the psychological literature as the opposite task
p7609
aVThe learner is exposed to a new word in context and has to search for the right object referring to it
p7610
aVFurthermore, all models perform better than Chance , including those that are based on just 1 or 5 sentences
p7611
aVRegarding the sources of error, a qualitative analysis of predicted word labels and objects as presented in Table 6 suggests that both textual and visual representations, although capturing relevant u'\u005cu201c' topical u'\u005cu201d' or u'\u005cu201c' domain u'\u005cu201d' information, are not enough to single out the properties of the target concept
p7612
aVAs an example, the textual vector of dishwasher contains kitchen-related dimensions such as u'\u005cu27e8' fridge , oven , gas , hob , u'\u005cu2026' , sink u'\u005cu27e9'
p7613
aVThe latter is shown in Figure 5 , with a gas hob well in evidence
p7614
aVAs a further example, the visual vector for cooker is extracted from pictures such as the one in Figure 5
p7615
aVThe neural network architecture emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts
p7616
aVMost importantly, our results suggest the viability of cross-modal mapping for grounded word-meaning acquisition in a simulation of fast mapping
p7617
aVFurthermore, we intend to adopt visual attributes [ 14 , 44 ] as visual representations, since they should allow a better understanding of how cross-modal mapping works, thanks to their linguistic interpretability
p7618
aVSimilarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties [ 28 ] might lead to more informative and discriminative linguistic vectors
p7619
asg88
(lp7620
sg90
(lp7621
sg92
(lp7622
VAt the outset of this work, we considered the problem of linking purely language-based distributional semantic spaces with objects in the visual world by means of cross-modal mapping.
p7623
aVWe compared recent models for this task both on a benchmark object recognition dataset and on a more realistic and noisier dataset covering a wide range of concepts.
p7624
aVThe neural network architecture emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts.
p7625
aVMost importantly, our results suggest the viability of cross-modal mapping for grounded word-meaning acquisition in a simulation of fast mapping.
p7626
aVGiven the success of NN , we plan to experiment in the future with more sophisticated neural network architectures inspired by recent work in machine translation [ 19 ] and multimodal deep learning [ 51 ].
p7627
aVFurthermore, we intend to adopt visual attributes [ 14 , 44 ] as visual representations, since they should allow a better understanding of how cross-modal mapping works, thanks to their linguistic interpretability.
p7628
aVThe error analysis in Section 5.3 suggests that automated localization techniques [ 54 ] , distinguishing an object from its surroundings, might drastically improve mapping accuracy.
p7629
aVSimilarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties [ 28 ] might lead to more informative and discriminative linguistic vectors.
p7630
aVFinally, the lack of large child-directed speech corpora constrained the experimental design of fast mapping simulations; we plan to run more realistic experiments with true nonce words and using source corpora (e.g.,, the Simple Wikipedia, child stories, portions of CHILDES) that contain sentences more akin to those a child might effectively hear or read in her word-learning years.
p7631
ag106
asg107
S'P14-1132'
p7632
sg109
(lp7633
VFollowing up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images, we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word.
p7634
aVWe then introduce fast mapping , a challenging and more cognitively plausible variant of the zero-shot task, in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts.
p7635
aVBy combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words.
p7636
aVOur results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts.
p7637
ag106
asba(icmyPackage
FText
p7638
(dp7639
g3
(lp7640
VScaling semantic parsers to large knowledge bases has attracted substantial attention recently [ 2 , 1 , 19 ] , since it drives applications such as question answering (QA) and information extraction (IE
p7641
aVFor instance, the utterances u'\u005cu201c' Where is ACL in 2014 u'\u005cu201d' and u'\u005cu201c' What is the location of ACL 2014 u'\u005cu201d' cannot be used in traditional semantic parsing methods, since the KB does not contain an entity ACL2014 , but this pair clearly contains valuable linguistic information
p7642
aVAs another reference point, out of 500,000 relations extracted by the ReVerb Open IE system [ 9 ] , only about 10,000 can be aligned to Freebase [ 1 ]
p7643
aVIn this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB (Figure 1
p7644
aVNext, we heuristically generate canonical utterances for each logical form based on the text descriptions of predicates from the KB
p7645
aVWe use two complementary paraphrase models an association model based on aligned phrase pairs extracted from a monolingual parallel corpus, and a vector space model , which represents each utterance as a vector and learns a similarity score between them
p7646
aV2013 ) presented a QA system that maps questions onto simple queries against Open IE extractions, by learning paraphrases from a large monolingual parallel corpus, and performing a single paraphrasing step
p7647
aVThe denotation of a logical form z with respect to a KB u'\u005cud835' u'\u005cudca6' is given by \u005cllbracket u'\u005cu2062' z u'\u005cu2062' \u005crrbracket u'\u005cud835' u'\u005cudca6'
p7648
aVThis strategy is feasible in factoid QA where compositionality is low, and so the size of u'\u005cud835' u'\u005cudcb5' x is limited (Section 4
p7649
aVFirst, the paraphrase model is decoupled from the KB, so we can train it from large text corpora
p7650
aVParaphrasing methods are well-suited for handling such text-to-text gaps
p7651
aVwhere the parameters u'\u005cu0398' pr define the paraphrase model (Section 5 ), which is based on features extracted from text only (the input and canonical utterance
p7652
aVThe parameters u'\u005cu0398' lf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section
p7653
aVWe also add all binary predicates in z as features
p7654
aVMoreover, we extract a popularity feature for predicates based on the number of instances they have in u'\u005cud835' u'\u005cudca6'
p7655
aVFor Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb [ 22 ]
p7656
aVLastly, Freebase formulas have types (see Section 4 ), and we conjoin the type of z with the first word of x , to capture the correlation between a word (e.g.,, u'\u005cu201c' where u'\u005cu201d' ) with the Freebase type (e.g.,, Location
p7657
aVAs our training data consists of question-answer pairs ( x i , y i ) , we maximize the log-likelihood of the correct answer
p7658
aVThe probability of an answer y is obtained by marginalizing over canonical utterances c and logical forms z whose denotation is y
p7659
aVThe strength u'\u005cu039b' of the L 1 regularizer is set based on cross-validation
p7660
aVWe optimize the objective by initializing the parameters u'\u005cu0398' to zero and running AdaGrad [ 8 ]
p7661
aVBoth steps are performed with a small and simple set of deterministic rules, which suffices for our datasets, as they consist of factoid questions with a modest amount of compositional structure
p7662
aVDue to its soporific effect though, we advise the reader to skim it quickly
p7663
aVTo construct candidate logical forms u'\u005cud835' u'\u005cudcb5' x for a given utterance x , our strategy is to find an entity in x and grow the logical form from that entity
p7664
aVAs we show later, this procedure actually produces a set with better coverage than constructing logical forms recursively from spans of x , as is done in traditional semantic parsing
p7665
aVThen, we add the logical form p p 1 e 1 u'\u005cu2293' p 2 e 2 ) , if there exists a binary p 2 with a compatible type signature ( t 1 , t 2 ) , where t 2 is one of e 2 u'\u005cu2019' s types
p7666
aVFor example, for the logical form Character.Actor.BradPitt , if we match the entity Troy in x , we obtain Character.(Actor.BradPitt u'\u005cu2293' Film.Troy
p7667
aVWe further modify logical forms by intersecting with a unary filter (#4 given a formula z with some Freebase type (e.g.,, People ), we look at all Freebase sub-types t (e.g.,, Composer ), and check whether one of their Freebase descriptions (e.g.,, u'\u005cu201c' composer u'\u005cu201d' ) appears in x
p7668
aVIf so, we add the formula u'\u005cud835' u'\u005cude83' u'\u005cud835' u'\u005cudea2' u'\u005cud835' u'\u005cude99' u'\u005cud835' u'\u005cude8e' t u'\u005cu2293' z to u'\u005cud835' u'\u005cudcb5' x
p7669
aVFinally, we check whether x is an aggregation formula by identifying whether it starts with phrases such as u'\u005cu201c' how many u'\u005cu201d' or u'\u005cu201c' number of u'\u005cu201d' (#5
p7670
aVClearly, we can increase the expressivity of this step by expanding the template set
p7671
aVFor example, we could handle superlative utterances ( u'\u005cu201c' What NBA player is tallest u'\u005cu201d' ) by adding a template with an argmax operator
p7672
aVThe template p p 1 e 1 u'\u005cu2293' p 2 e 2 ) (#3) is generated by appending the prepositional phrase in d u'\u005cu2062' ( e 2 ) , e.g, u'\u005cu201c' What character is the character of Brad Pitt in Troy u'\u005cu201d'
p7673
aVIn Section 6 , we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount
p7674
aVOnce the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs ( c , z ) based on a paraphrase model
p7675
aVThis is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases
p7676
aVWe can see that our model learns a positive score for associating u'\u005cu201c' type u'\u005cu201d' with u'\u005cu201c' genres u'\u005cu201d' , and a negative score for associating u'\u005cu201c' is u'\u005cu201d' with u'\u005cu201c' play u'\u005cu201d'
p7677
aVWe define associations in x and c primarily by looking up phrase pairs in a phrase table constructed using the Paralex corpus [ 10 ]
p7678
aVParalex is suitable for our needs since it focuses on question paraphrases
p7679
aVWe use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic [ 24 ] to all 5-grams
p7680
aVFor a pair ( x , c ) , we also consider as candidate associations the set u'\u005cu212c' (represented implicitly), which contains token pairs ( x i , c i u'\u005cu2032' ) such that x i and c i u'\u005cu2032' share the same lemma, the same POS tag, or are linked through a derivation link on WordNet [ 11 ]
p7681
aVThis results in many poor associations (e.g.,, u'\u005cu201c' play u'\u005cu201d' and u'\u005cu201c' the u'\u005cu201d' ), but as illustrated in Figure 3 , we learn weights that discriminate good from bad associations
p7682
aVBy extracting POS features, we obtain soft syntactic rules, e.g.,, the feature u'\u005cu201c' JJ N u'\u005cu2227' N u'\u005cu201d' indicates that omitting adjectives before nouns is possible
p7683
aVOnce associations are constructed, we mark tokens in x and c that were not part of any association, and extract deletion features for their lemmas and POS tags
p7684
aVThus, we learn that deleting pronouns is acceptable, while deleting nouns is not
p7685
aVWe start by constructing vector representations of words
p7686
aVFor example, the association model identifies that the paraphrase for u'\u005cu201c' What type of music did Richard Wagner Play u'\u005cu201d' is u'\u005cu201c' What is the musical genres of Richard Wagner u'\u005cu201d' , by relating phrases such as u'\u005cu201c' type of music u'\u005cu201d' and u'\u005cu201c' musical genres u'\u005cu201d'
p7687
aVThe VS model ranks the canonical utterance u'\u005cu201c' What composition has Richard Wagner as lyricist u'\u005cu201d' higher, as this utterance is also in the music domain
p7688
aVThus, we combine the two models to benefit from their complementary nature
p7689
aVThis dataset was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk
p7690
aVSince we train from question-answer pairs, we collect answers by executing the gold logical forms against Freebase
p7691
aVWe execute u'\u005cu039b' -DCS queries by converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine
p7692
aVThis demonstrates that our method for constructing candidate logical forms is reasonable
p7693
aVOur system generates relatively natural utterances from logical forms using simple rules based on Freebase descriptions (Section 4
p7694
aVWe surmise this is because questions in Free917 were generated by annotators prompted by Freebase facts, whereas questions in WebQuestions originated independently of Freebase
p7695
aVThus, word choice in Free917 is often close to the generated Freebase descriptions, allowing simple baselines to perform well
p7696
aVFor example, ParaSempre suggests that the best paraphrase for u'\u005cu201c' What company did Henry Ford work for u'\u005cu201d' is u'\u005cu201c' What written work novel by Henry Ford u'\u005cu201d' rather than u'\u005cu201c' The employer of Henry Ford u'\u005cu201d' , due to the exact match of the word u'\u005cu201c' work u'\u005cu201d'
p7697
aVAnother example is the question u'\u005cu201c' Where is the Nascar hall of fame u'\u005cu201d' , where ParaSempre suggests that u'\u005cu201c' What hall of fame discipline has Nascar hall of fame as halls of fame u'\u005cu201d' is the best canonical utterance
p7698
aVThis is because our simple model allows to associate u'\u005cu201c' hall of fame u'\u005cu201d' with the canonical utterance three times
p7699
aVA fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment [ 4 ]
p7700
aVWhile it has been shown that paraphrasing methods are useful for question answering [ 15 ] and relation extraction [ 27 ] , this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing
p7701
aVOne can view this work as a generalization of Fader et al along three dimensions
p7702
aVFirst, Fader et al use a KB over natural language extractions rather than a formal KB and so querying the KB does not require a generation step u'\u005cu2013' they paraphrase questions to KB entries directly
p7703
aVSince our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization
p7704
aVIn conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model
p7705
aVWe believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB
p7706
aVWe thank Kai Sheng Tai for performing the error analysis
p7707
asg88
(lp7708
sg90
(lp7709
sg92
(lp7710
g1538
asg107
S'P14-1133'
p7711
sg109
(lp7712
VA central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed.
p7713
aVTraditionally, semantic parsers are trained primarily from text paired with knowledge base information.
p7714
aVOur goal is to exploit the much larger amounts of raw text not tied to any knowledge base.
p7715
aVIn this paper, we turn semantic parsing on its head.
p7716
aVGiven an input utterance, we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each.
p7717
aVThen, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form.
p7718
aVWe present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs.
p7719
aVOur system ParaSempre improves state-of-the-art accuracies on two recently released question-answering datasets.
p7720
aVleftmargin=0cm,labelindent=0cm.
p7721
ag106
asba(icmyPackage
FText
p7722
(dp7723
g3
(lp7724
VBy providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis [ 24 ] , topic classification [ 16 ] or word-word similarity [ 20 ]
p7725
aVAccording to the theory of frame semantics [ 12 ] , a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles ) that participate in the event
p7726
aVMoreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date
p7727
aVSince the CoNLL 2004-2005 shared tasks [ 4 , 5 ] on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem
p7728
aVFrameNet The FrameNetproject [ 2 ] is a lexical database that contains information about words and phrases (represented as lemmas conjoined with a coarse part-of-speech tag) termed as lexical units, with a set of semantic frames that they could evoke
p7729
aVFor each frame, there is a list of associated frame elements (or roles, henceforth), that are also distinguished as core or non-core
p7730
aV2 2 Additional information such as finer distinction of the coreness properties of roles, the relationship between frames, and that of roles are also present, but we do not leverage that information in this work
p7731
aVA word embedding is a distributed representation of meaning where each word is represented as a vector in u'\u005cu211d' n
p7732
aVSuch representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content [ 6 , 25 , inter alia ]
p7733
aVWe could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' n in the vector correspond to the subject dependent, n + 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' 2 u'\u005cu2062' n correspond to the clausal complement dependent, and so forth
p7734
aVThus, the context is a vector in u'\u005cu211d' n u'\u005cu2062' k with the embedding of He at the subject position, the embedding of company in direct object position and zeros everywhere else
p7735
aVThe model computes a composed representation of the predicate instance by using distributed vector representations for words (3) u'\u005cu2013' the (red) vertical embedding vectors for each word are concatenated into a long vector
p7736
aVFirst, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation
p7737
aVSubsequently, we learn a mapping from this initial representation into a low-dimensional space; we also learn an embedding for each possible frame label in the same low-dimensional space
p7738
aVIf we have F possible frames we can store those parameters in an F × m matrix, one m -dimensional point for each frame, which we will refer to as the linear mapping Y
p7739
aVLet the lexical unit (the lemma conjoined with a coarse POS tag) for the marked predicate be u'\u005cu2113'
p7740
aVWe denote the frames that associate with u'\u005cu2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u005cu2113'
p7741
aVWsabie performs gradient-based updates on an objective that tries to minimize the distance between M u'\u005cu2062' ( g u'\u005cu2062' ( x ) ) and the embedding of the correct label Y u'\u005cu2062' ( y ) , while maintaining a large distance between M u'\u005cu2062' ( g u'\u005cu2062' ( x ) ) and the other possible labels Y u'\u005cu2062' ( y ¯ ) in the confusion set F u'\u005cu2113'
p7742
aVAt disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u005cu2062' s u'\u005cu2062' ( x , y ) where s u'\u005cu2062' ( x , y ) = M u'\u005cu2062' ( g u'\u005cu2062' ( x ) ) u'\u005cu22c5' Y u'\u005cu2062' ( y ) , where the argmax iterates over the possible frames y u'\u005cu2208' F u'\u005cu2113' if u'\u005cu2113' was seen in the lexicon or the training data, or y u'\u005cu2208' F , if it was unseen
p7743
aVSince Wsabie learns a single mapping from g u'\u005cu2062' ( x ) to u'\u005cu211d' m , parameters are shared between different words and different frames
p7744
aVSo for example u'\u005cu201c' He runs the company u'\u005cu201d' could help the model disambiguate u'\u005cu201c' He owns the company u'\u005cu201d' Moreover, since g u'\u005cu2062' ( x ) relies on word embeddings rather than word identities, information is shared between words
p7745
aVTo elaborate, the positions of interest are the labels of the direct dependents of the predicate, so k is the number of labels that the dependency parser can produce
p7746
aVFor example, if the label on the edge between runs and He is nsubj , we would put the embedding of He in the block corresponding to nsubj
p7747
aVIf a label occurs multiple times, then the embeddings of the words below this label are averaged
p7748
aVThis set of dependency paths were deemed as possible positions in the initial vector space representation
p7749
aVIn addition, akin to the first context function, we also added all dependency labels to the context set
p7750
aVThus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels
p7751
aVGiven a predicate in its sentential context, we therefore extract only those context words that appear in positions warranted by the above set
p7752
aVFor all our experiments, setting 3) which concatenates the direct dependents and dependency path always dominated the other two, so we only report results for this setting
p7753
aVThe mapping from g u'\u005cu2062' ( x ) to the low dimensional space u'\u005cu211d' m is a linear transformation, so the model parameters to be learnt are the matrix M u'\u005cu2208' u'\u005cu211d' n u'\u005cu2062' k × m as well as the embedding of each possible frame label, represented as another matrix Y u'\u005cu2208' u'\u005cu211d' F × m where there are F frames in total
p7754
aVChoosing L u'\u005cu2062' ( u'\u005cu0397' ) = C u'\u005cu2062' u'\u005cu0397' for any positive constant C optimizes the mean rank, whereas a weighting such as L u'\u005cu2062' ( u'\u005cu0397' ) = u'\u005cu2211' i = 1 u'\u005cu0397' 1 / i (adopted here) optimizes the top of the ranked list, as described in [ 26 ]
p7755
aVAdditionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usually only consider a handful of candidate labels
p7756
aVIf we used all training examples for a given predicate for finding a nearest-neighbor match at inference time, we would have to consider many more candidates, making the process very slow
p7757
aVFrom x , a rule-based candidate argument extraction algorithm extracts a set of spans u'\u005cud835' u'\u005cudc9c' that could potentially serve as the overt 7 7 By overtness, we mean the non-null instantiation of a semantic role in a frame-semantic parse arguments u'\u005cud835' u'\u005cudc9c' y for y (see § 5.4 -§ 5.5 for the details of the candidate argument extraction algorithms
p7758
aVa set of tuples that associates each role r in u'\u005cu211b' y with a span a according to the gold data
p7759
aVInference Although our learning mechanism uses a local log-linear model, we perform inference globally on a per-frame basis by applying hard structural constraints
p7760
aV2008 ) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in § 5.4 and § 5.5
p7761
aVWe used the same test set as Das et al containing 23 documents with 4,458 predicates
p7762
aVOf the remaining 55 documents, 16 documents were randomly chosen for development
p7763
aVAll the verb frame files in Ontonotes were used for creating our frame lexicon
p7764
aVAt test time, this model chooses the best frame as argmax y u'\u005cu2062' u'\u005cud835' u'\u005cudf4d' u'\u005cu22c5' u'\u005cud835' u'\u005cudc1f' u'\u005cu2062' ( y , x , u'\u005cu2113' ) where argmax iterates over the possible frames y u'\u005cu2208' F u'\u005cu2113' if u'\u005cu2113' was seen in the lexicon or the training data, or y u'\u005cu2208' F , if it was unseen, like the disambiguation scheme of § 3
p7765
aVWe train this model by maximizing L 2 regularized log-likelihood, using L-BFGS; the regularization constant was set to 0.1 in all experiments
p7766
aVThe first one computes the direct dependents and dependency paths as described in § 3.1 but conjoins them with the word identity rather than a word embedding
p7767
aVAdditionally, this model uses the un-conjoined words as backoff features
p7768
aVThis would be a standard NLP approach for the frame identification problem, but is surprisingly competitive with the state of the art
p7769
aVThe second baseline, tries to decouple the Wsabie training from the embedding input, and trains a log linear model using the embeddings
p7770
aVSo the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie
p7771
aVWe search for the stochastic gradient learning rate in { 0.0001 ¯ , 0.001 , 0.01 } , the margin u'\u005cu0393' u'\u005cu2208' { 0.001 , 0.01 ¯ , 0.1 , 1 } and the dimensionality of the final vector space m u'\u005cu2208' { 256 ¯ , 512 } , to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame
p7772
aVSince the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate u'\u005cu2019' s head; this helped capture cases like u'\u005cu201c' a few months u'\u005cu201d' (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate u'\u005cu2019' s head to the position immediately before the predicate, for cases like u'\u005cu201c' your gift to Goodwill u'\u005cu201d' (where to is the predicate and your gift is the argument
p7773
aVWe noted that this renders every lexical unit as seen ; in other words, at frame disambiguation time on our test set, for all instances, we only had to score the frames in F u'\u005cu2113' for a predicate with lexical unit u'\u005cu2113' (see § 3 and § 5.2
p7774
aVFor fair comparison, we took the lexical units for the predicates that Das et al. considered as seen, and constructed a lexicon with only those; training instances, if any, for the unseen predicates under Das et al u'\u005cu2019' s setup were thrown out as well
p7775
aV1) each span could have only one role, 2) each core role could be present only once, and 3) all overt arguments had to be non-overlapping
p7776
aVHyperparameters As in § 5.4 , we made a hyperparameter sweep in the same space
p7777
aVWe see the same trend as in Table 4
p7778
aVFinally, Table 6 presents SRL results that measures argument performance only, irrespective of the frame; we use the evaluation script from CoNLL 2005 [ 5 ]
p7779
aV14 14 The last row of Table 6 refers to a system which used the combination of two syntactic parsers as input
p7780
aVFor FrameNet, the Wsabie Embedding model we propose strongly outperforms the baselines on all metrics, and sets a new state of the art
p7781
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space
p7782
aVConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p7783
aVSince the Log-Linear Words model always performs better than the Log-Linear Embedding model, we conclude that the primary benefit does not come from the input embedding representation
p7784
aVOn the PropBankdata, we see that the Log-Linear Words baseline has roughly the same performance as our model on most metrics slightly better on the test data and slightly worse on the development data
p7785
aVThis can be partially explained with the significantly larger training set size for PropBank, making features based on words more useful
p7786
aVIn other words, it must estimate 512 parameters based on at most 10 training examples
p7787
aVHowever, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M
p7788
aVFor FrameNet, estimating the label embedding is not as much of a problem because even if a lexical unit is rare, the potential frames can be frequent
p7789
aV2014 ) , our model does not rely on heuristics to construct a similarity graph and leverage WordNet; hence, in principle it is generalizable to varying domains, and to other languages
p7790
asg88
(lp7791
sg90
(lp7792
sg92
(lp7793
VWe have presented a simple model that outperforms the prior state of the art on FrameNet-style frame-semantic parsing, and performs at par with one of the previous-best single-parser systems on PropBankSRL.
p7794
aVUnlike Das et al.
p7795
aV2014 ) , our model does not rely on heuristics to construct a similarity graph and leverage WordNet; hence, in principle it is generalizable to varying domains, and to other languages.
p7796
aVFinally, we presented results on PropBank-style semantic role labeling with a system that included the task of automatic verb frame identification, in tune with the FrameNet literature; we believe that such a system produces more interpretable output, both from the perspective of human understanding as well as downstream applications, than pipelines that are oblivious to the verb frame, only focusing on argument analysis.
p7797
ag106
asg107
S'P14-1136'
p7798
sg109
(lp7799
VWe present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings.
p7800
aVGiven labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation.
p7801
aVThe latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-of-the-art results on FrameNet-style frame-semantic analysis.
p7802
aVAdditionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work.
p7803
ag106
asba(icmyPackage
FText
p7804
(dp7805
g3
(lp7806
VThis study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers
p7807
aVTo overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training
p7808
aVHowever, with this approach, errors induced by probabilistic models are learned as correct alignments; thus, generalization capabilities are limited
p7809
aVNCE artificially generates bilingual sentences through samplings as pseudo-negative samples, and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences
p7810
aVIt has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently [ 22 , 21 , 14 , 11 ]
p7811
aVBased on this motivation, our directional models are also simultaneously trained
p7812
aVSpecifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function
p7813
aVFor example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm
p7814
aVAs an instance of discriminative models, we describe an FFNN-based word alignment model [ 40 ] , which is our baseline
p7815
aVNote that the model uses nonprobabilistic scores rather than probabilities because normalization over all words is computationally expensive
p7816
aVNote that alignments in the FFNN-based model are also governed by first-order Markov dynamics because an alignment score depends on the previous alignment a j - 1
p7817
aVFigure 1 shows the network structure with one hidden layer for computing a lexical translation probability t l u'\u005cu2062' e u'\u005cu2062' x ( f j , e a j c ( f j ) , c ( e a j )
p7818
aVThe model receives a source and target word with their contexts as inputs, which are words in a predefined window (the window size is three in Figure 1
p7819
aVFirst, the lookup layer converts each input word into its word embedding by looking up its corresponding column in the embedding matrix ( L ), and then concatenates them
p7820
aV[ 40 ] , a u'\u005cu201c' hard u'\u005cu201d' version of the hyperbolic tangent, htanh ( x ) 3 3 htanh ( x ) = - 1 for x - 1 , htanh ( x ) = 1 for x 1 , and htanh ( x ) = x for others is used as f u'\u005cu2062' ( x ) in this study
p7821
aVThe alignment model based on an FFNN is formed in the same manner as the lexical translation model
p7822
aVEach model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent (SGD) 4 4 In our experiments, we used a mini-batch SGD instead of a plain SGD where gradients are computed by the back-propagation algorithm [ 31 ]
p7823
aVThe Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e.,, the Viterbi algorithm) due to the long alignment history of y i
p7824
aVThus, the Viterbi alignment is computed approximately using heuristic beam search
p7825
aVIn the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings ( x j ) is fed to the hidden layer in the same manner as the FFNN-based model
p7826
aVNext, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
p7827
aVThe proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight matrices
p7828
aVNote that y j - 1 y j f u'\u005cu2062' ( x ) is an activation function, which is a hard hyperbolic tangent, i.e.,, htanh ( x ) , in this study
p7829
aVAs described above, the RNN-based model has a hidden layer with recurrent connections
p7830
aVUnder the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i
p7831
aVTherefore, the proposed model can find alignments by taking advantage of the long alignment history, while the FFNN-based model considers only the last alignment
p7832
aVThe RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq
p7833
aV7 (Section 2.2
p7834
aVHowever, this approach requires gold standard alignments
p7835
aV[ 9 ] presented an unsupervised alignment model based on contrastive estimation (CE) [ 32 ]
p7836
aVIn a simple implementation, each u'\u005cud835' u'\u005cudc86' - is generated by repeating a random sampling from a set of target words ( V e u'\u005cud835' u'\u005cudc86' + times and lining them up sequentially
p7837
aVThe IBM Model 1 with l 0 prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1
p7838
aVBoth of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e.,, they can represent one-to-many relations from the target side
p7839
aVThe proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings
p7840
aVLines 3-1 and 3-2 generate N pseudo-negative samples for each u'\u005cud835' u'\u005cudc1f' + and u'\u005cud835' u'\u005cudc1e' + based on the translation candidates of u'\u005cud835' u'\u005cudc1f' + and u'\u005cud835' u'\u005cudc1e' + found by the IBM Model 1 with l 0 prior, I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 1 (Section 4.1
p7841
aVIn addition, we evaluated the end-to-end translation performance of three tasks a Chinese-to-English translation task with the FBIS corpus ( F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S ), the IWSLT 2007 Japanese-to-English translation task ( I u'\u005cu2062' W u'\u005cu2062' S u'\u005cu2062' L u'\u005cu2062' T ) [ 10 ] , and the NTCIR-9 Japanese-to-English patent translation task ( N u'\u005cu2062' T u'\u005cu2062' C u'\u005cu2062' I u'\u005cu2062' R ) [ 13 ] 6 6 We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable
p7842
aVNote that the development data was not used in the alignment tasks, i.e.,, B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C and H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s , because the hyperparameters of the alignment models were set by preliminary small-scale experiments
p7843
aVWe split these pairs into the first 9,000 for training data and the remaining 960 as test data
p7844
aVAll the data in B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C is word-aligned, and the training data in H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s is unlabeled data
p7845
aVIn F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S , we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data ( N u'\u005cu2062' I u'\u005cu2062' S u'\u005cu2062' T u'\u005cu2062' 03 and N u'\u005cu2062' I u'\u005cu2062' S u'\u005cu2062' T u'\u005cu2062' 04
p7846
aVFor the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer z 1 to 100, and the window size of contexts to 5
p7847
aVHence z 0 is 300 ( 30 × 5 × 2
p7848
aV[ 40 ] , the FFNN-based model was trained by the supervised approach described in Section 2.2 ( F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s
p7849
aVFor the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer y j to 100
p7850
aVThus x j is 60 ( 30 × 2
p7851
aVUsing the SRILM Toolkits [ 33 ] with modified Kneser-Ney smoothing, we trained a 5-gram language model on the English side of each training data for I u'\u005cu2062' W u'\u005cu2062' S u'\u005cu2062' L u'\u005cu2062' T and N u'\u005cu2062' T u'\u005cu2062' C u'\u005cu2062' I u'\u005cu2062' R , and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S
p7852
aVIn H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s , all models were trained from randomly sampled 100 K data 10 10 Due to high computational cost, we did not use all the training data
p7853
aVWe evaluated the word alignments produced by first applying each model in both directions and then combining the alignments using the u'\u005cu201c' grow-diag-final-and u'\u005cu201d' heuristic [ 18 ]
p7854
aVTable 2 also shows that R u'\u005cu2062' N u'\u005cu2062' N s + c u'\u005cu2062' ( R / I ) and R u'\u005cu2062' N u'\u005cu2062' N u + c achieve significantly better performance than R u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( R / I ) and R u'\u005cu2062' N u'\u005cu2062' N u in both tasks, respectively
p7855
aVThis indicates that the proposed agreement constraint is effective in training better models in both the supervised and unsupervised approaches
p7856
aVThis indicates that our unsupervised learning benefits our models because the supervised models are adversely affected by errors in the automatically generated training data
p7857
aVHowever, R u'\u005cu2062' N u'\u005cu2062' N u and R u'\u005cu2062' N u'\u005cu2062' N u + c outperform F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( I ) and I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 in all tasks
p7858
aVThese results indicate that our proposals contribute to improving translation performance 12 12 We also confirmed the effectiveness of our models on the NIST05 and NTCIR-10 evaluation data
p7859
aVFigure 3 (a) shows that R u'\u005cu2062' R u'\u005cu2062' N s adequately identifies complicated alignments with long distances compared to F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s (e.g.,, jaggy alignments of u'\u005cu201c' have you been learning u'\u005cu201d' in Fig 3 (a)) because R u'\u005cu2062' N u'\u005cu2062' N s captures alignment paths based on long alignment history, which can be viewed as phrase-level alignments, while F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s employs only the last alignment
p7860
aVIn French-English word alignment, the most valuable clues are located locally because English and French have similar word orders and their alignment has more one-to-one mappings than Japanese-English word alignment (Figure 3
p7861
aVFigure 3 (b) shows that both R u'\u005cu2062' R u'\u005cu2062' N s and F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s work for such simpler alignments
p7862
aVTherefore, the RNN-based model has less effect on French-English word alignment than Japanese-English word alignment, as indicated in Table 2
p7863
aVNote that R u'\u005cu2062' N u'\u005cu2062' N s + c u'\u005cu2062' ( R ) cannot be trained from the 40 K data because the 40 K data does not have gold standard word alignments
p7864
aVTable 4 demonstrates that the proposed RNN-based model outperforms I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data, which is less than 25% of the training data for I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4
p7865
aVConsequently, the SMT system using R u'\u005cu2062' N u'\u005cu2062' N u + c trained from a small part of training data can achieve comparable performance to that using I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 trained from all training data, which is shown in Table 3
p7866
aVThe performance of these models is comparable in H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s
p7867
aVThese results indicate that the proposed unsupervised learning and agreement constraint benefit the FFNN-based model, similar to the RNN-based model
p7868
aVWe have proposed a word alignment model based on an RNN, which captures long alignment history through recurrent architectures
p7869
aVFurthermore, we proposed an unsupervised method for training our model using NCE and introduced an agreement constraint that encourages word embeddings to be consistent across alignment directions
p7870
asg88
(lp7871
sg90
(lp7872
sg92
(lp7873
VWe have proposed a word alignment model based on an RNN, which captures long alignment history through recurrent architectures.
p7874
aVFurthermore, we proposed an unsupervised method for training our model using NCE and introduced an agreement constraint that encourages word embeddings to be consistent across alignment directions.
p7875
aVOur experiments have shown that the proposed model outperforms the FFNN-based model [ 40 ] for word alignment and machine translation, and that the agreement constraint improves alignment performance.
p7876
aVIn future, we plan to employ contexts composed of surrounding words (e.g.,, c ( f j ) or c ( e a j ) in the FFNN-based model) in our model, even though our model implicitly encodes such contexts in the alignment history.
p7877
aVWe also plan to enrich each hidden layer in our model with multiple layers following the success of Yang et al.
p7878
aV[ 40 ] , in which multiple hidden layers improved the performance of the FFNN-based model.
p7879
aVIn addition, we would like to prove the effectiveness of the proposed method for other datasets.
p7880
ag106
asg107
S'P14-1138'
p7881
sg109
(lp7882
VThis study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers.
p7883
aVWe perform unsupervised learning using noise-contrastive estimation [ 15 , 26 ] , which utilizes artificially generated negative samples.
p7884
aVOur alignment model is directional, similar to the generative IBM models [ 4 ].
p7885
aVTo overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training.
p7886
aVThe RNN-based model outperforms the feed-forward neural network-based model [ 40 ] as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks.
p7887
ag106
asba(icmyPackage
FText
p7888
(dp7889
g3
(lp7890
V1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner
p7891
aVApplying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first
p7892
aV2013 ) propose a joint language and translation model, based on a recurrent neural network
p7893
aVWord embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model
p7894
aVIn R 2 NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure can be built, as recursive neural networks
p7895
aVTo generate the translation candidates in a commonly used bottom-up manner, recursive neural networks are naturally adopted to build the tree structure
p7896
aVIn recursive neural networks, all the representations of nodes are generated based on their child nodes, and it is difficult to integrate additional global information, such as language model and distortion model
p7897
aVIn order to integrate these crucial information for better translation prediction, we combine recurrent neural networks into the recursive neural networks, so that we can use global information to generate the next hidden state, and select the better translation candidate
p7898
aVWe propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy
p7899
aVSo as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network
p7900
aVIn their work, not only the target word embedding is used as the input of the network, but also the embedding of the source word, which is aligned to the current target word
p7901
aVTo tackle the large search space due to the weak independence assumption, a lattice algorithm is proposed to re-rank the n-best translation candidates, generated by a given SMT decoder
p7902
aVR 2 NN is a combination of recursive neural network and recurrent neural network, which not only integrates the conventional global features as input information for each combination, but also generates the representation of the parent node for the future candidate generation
p7903
aVRecurrent neural network is proposed to use unbounded history information, and it has recurrent connections on hidden states, so that history information can be used circularly inside the network for arbitrarily long time
p7904
aVAs shown in Figure 1 , the network contains three layers, an input layer, a hidden layer, and an output layer
p7905
aVBased on h t , we can predict the probability of the next word, which forms the output layer y t
p7906
aVThe commonly used binary recursive neural networks generate the representation of the parent node, with the representations of two child nodes as the input
p7907
aVAs shown in Figure 2 , s [ l , m ] and s [ m , n ] are the representations of the child nodes, and they are concatenated into one vector to be the input of the network s [ l , n ] is the generated representation of the parent node y [ l , n ] is the confidence score of how plausible the parent node should be created l , m , n are the indexes of the string
p7908
aVWord embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no additional input information is used except the two representation vectors of the child nodes
p7909
aVHowever, some global information , which cannot be generated by the child representations, is crucial for SMT performance, such as language model score and distortion model score
p7910
aVSo as to integrate such global information, and also keep the ability to generate tree structure, we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network (R 2 NN
p7911
aVAs shown in Figure 3 , based on the recursive network, we add three input vectors x [ l , m ] for child node [ l , m ] , x [ m , n ] for child node [ m , n ] , and x [ l , n ] for parent node [ l , n ]
p7912
aVWe call them recurrent input vectors, since they are borrowed from recurrent neural networks
p7913
aVThe two recurrent input vectors x [ l , m ] and x [ m , n ] are concatenated as the input of the network, with the original child node representations s [ l , m ] and s [ m , n ]
p7914
aVOnly the n-best translation candidates are kept for upper combination, according to their plausible scores
p7915
aVThe commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x
p7916
aVDuring decoding, recurrent input vectors x for internal nodes are calculated accordingly
p7917
aVOur model generates the representation of a translation pair based on its child nodes
p7918
aVIn their work, the representation is optimized to learn a distortion model using recursive neural network, only based on the representation of the child nodes
p7919
aVThe main idea of auto encoding is to initialize the parameters of the neural network, by minimizing the information lost, which means, capturing as much information as possible in the hidden states from the input vector
p7920
aVAs shown in Figure 5 , RAE contains two parts, an encoder with parameter W , and a decoder with parameter W u'\u005cu2032'
p7921
aVWith the parent node representation s as the input vector, the decoder reconstructs the representation of two child nodes s 1 u'\u005cu2032' and s 2 u'\u005cu2032'
p7922
aVThe loss function is defined as following so as to minimize the information lost
p7923
aVTranslation candidates generated by forced decoding [ 18 ] are used as oracle translations, which are the positive samples
p7924
aVForced decoding performs sentence pair segmentation using the same translation system as decoding
p7925
aVFor each sentence pair in the training data, SMT decoder is applied to the source side, and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding
p7926
aVIn this subsection, a supervised global training is proposed to tune the model according to the final translation performance of the whole source sentence
p7927
aVDue to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training
p7928
aVWe use negative log-likelihood to penalize all the other translation candidates except the oracle ones, so as to leverage all the translation candidates as training samples
p7929
aVThe next question is how to initialize the phrase pair embedding in the translation table, so as to generate the leaf nodes of the derivation tree
p7930
aVFor each term, we have a vector with length 20 as parameters, so there are 20 × 500K parameters totally
p7931
aVBut for source-target word pair, we may only have 7M bilingual corpus for training (taking IWSLT data set as an example), and there are 20 × (500K) 2 parameters to be tuned
p7932
aVIt is very difficult to learn the phrase pair embedding brute-forcedly as word embedding is learnt [ 12 , 3 ] , since we may not have enough training data
p7933
aVOne problem is that, word embedding may not be able to model the translation relationship between source and target phrases at phrase level, since some phrases cannot be decomposed
p7934
aVThe one-hot representation vector is used as the input, and a one-hidden-layer network generates a confidence score
p7935
aVThe neural network is used to reduce the space dimension of sparse features, and the hidden layer of the network is used as the phrase pair embedding
p7936
aVThe length of the hidden layer is empirically set to 20
p7937
aVWe use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings
p7938
aVThe recurrent neural network is trained with word aligned bilingual corpus, similar as [ 1 ]
p7939
aVIn this section, we conduct experiments to test our method on a Chinese-to-English translation task
p7940
aVAll these commonly used features are used as recurrent input vector x in our R 2 NN
p7941
aVAs we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable
p7942
aVWe first train the source and target word embeddings separately using large monolingual data, following [ 3 ]
p7943
aVUsing monolingual word embedding as the initialization, we fine tune them to get bilingual word embedding [ 20 ]
p7944
aVHere the length of the word embedding is also set to 20
p7945
aVTherefore, the length of the phrase pair embedding is 20 × 4 = 80
p7946
aVWord embedding can model translation relationship at word level, but it may not be powerful to model the phrase pair respondents at phrasal level, since the meaning of some phrases cannot be decomposed into the meaning of words
p7947
aVAnd also, translation task is difference from other NLP tasks, that, it is more important to model the translation confidence directly (the confidence of one target phrase as a translation of the source phrase), and our TCBPPE is designed for such purpose
p7948
asg88
(lp7949
sg90
(lp7950
sg92
(lp7951
VIn this paper, we propose a Recursive Recurrent Neural Network(R 2 NN) to combine the recurrent neural network and recursive neural network.
p7952
aVOur proposed R 2 NN cannot only integrate global input information during each combination, but also can generate the tree structure in a recursive way.
p7953
aVWe apply our model to SMT decoding, and propose a three-step semi-supervised training method.
p7954
aVIn addition, we explore phrase pair embedding method, which models translation confidence directly.
p7955
aVWe conduct experiments on a Chinese-to-English translation task, and our method outperforms a state-of-the-art baseline about 1.5 points BLEU.
p7956
aVFrom the experiments, we find that, phrase pair embedding is crucial to the performance of SMT.
p7957
aVIn the future, we will explore better methods for phrase pair embedding to model the translation equivalent between source and target phrases.
p7958
aVWe will apply our proposed R 2 NN to other tree structure learning tasks, such as natural language parsing.
p7959
ag106
asg107
S'P14-1140'
p7960
sg109
(lp7961
VIn this paper, we propose a novel recursive recurrent neural network (R 2 NN) to model the end-to-end decoding process for statistical machine translation.
p7962
aVR 2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities.
p7963
aV1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner.
p7964
aVA semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly.
p7965
aVExperiments on a Chinese to English translation task show that our proposed R 2 NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU.
p7966
ag106
asba(icmyPackage
FText
p7967
(dp7968
g3
(lp7969
VHowever, every Chinese word inputted into computer or cellphone cannot be typed through one-to-one mapping of key-to-letter inputting directly, but has to go through an IME as there are thousands of Chinese characters for inputting while only 26 letter keys are available in the keyboard
p7970
aVPinyin is originally designed as the phonetic symbol of a Chinese character (based on the standard modern Chinese, mandarin) , using Latin letters as its syllable notation
p7971
aVMost characters usually have unique pinyin representations, while a few Chinese characters may be pronounced in several different ways, so they may have multiple pinyin representations
p7972
aVThe advantage of pinyin IME is that it only adopts the pronunciation perspective of Chinese characters so that it is simple and easy to learn
p7973
aVSince Chinese characters are entered via IME, those user-made typos do not immediately lead to spelling errors
p7974
aVNormally, the user may immediately notice the inputting error and then make corrections, which usually means doing a bunch of extra operations like cursor movement, deletion and re-typing
p7975
aVThus there are two separated sub-tasks for Chinese spell checking
p7976
aVThe user may fail to input the completely right pinyin simply because he/she is a dialect speaker and does not know the exact pronunciation for the expected character
p7977
aVThis may be a very common situation since there are about seven quite different dialects in Chinese, among which being spoken languages, six are far different from the standard modern Chinese, mandarin
p7978
aVWith the boom of smart-phones, pinyin typos worsen due to the limited size of soft keyboard, and the lack of physical feedback on the touch screen
p7979
aVThe idea of u'\u005cu201c' statistical input method u'\u005cu201d' was proposed by modeling PTC conversion as a hidden Markov model (HMM), and using Viterbi [ 26 ] algorithm to decode the sequence
p7980
aVThey solved the typo correction problem by decomposing the conditional probability P ( H
p7981
aVHowever, real user input data can be very noisy and not very convenient to obtain
p7982
aVAs we will propose a joint model in this paper, such an individual typing model is not necessarily built in our approach
p7983
aVWe release this assumption since our model solves segmentation, typo correction and PTC conversion jointly
p7984
aVWithout word delimiters, linguists have argued on what a Chinese word really is for a long time and that is why there is always a primary word segmentation treatment in most Chinese language processing tasks [ 40 , 13 , 41 , 39 , 42 , 43 ]
p7985
aVA Chinese word may contain from 1 to over 10 characters due to different word segmentation conventions
p7986
aVNon-Chinese users may feel confused or even surprised if they know that when typing pinyin through an IME, Chinese IME users will never enter delimiters such as u'\u005cu201c' Space u'\u005cu201d' key to segment either pinyin syllables or pinyin words, but just input the entire un-segmented pinyin sequence
p7987
aVFor example, if one wants to input u'\u005cu201c' \u005csong u'\u005cu4f60' u'\u005cu597d' u'\u005cu4e16' u'\u005cu754c'  (Hello world) u'\u005cu201d' , he will just type u'\u005cu201c' nihaoshijie u'\u005cu201d' instead of segmented pinyin sequence u'\u005cu201c' ni hao shi jie u'\u005cu201d'
p7988
aVSince pinyin syllables have a very limited vocabulary and follow a set of regularities strictly, it is convenient to perform pinyin syllable segmentation by using rules
p7989
aVBut as the pinyin input is not segmented, it is nearly impossible to adopt previous spell checking methods for English to pinyin typo checking, although techniques for English spell checking have been well developed
p7990
aVA bit confusing but interesting, pinyin typo correction and segmentation come as two sides of one problem when a pinyin sequence is mistyped, it is unlikely to be correctly segmented; when it is segmented in an awkward way, it is likely to be mistyped
p7991
aVInspired by [ 36 ] and [ 14 ] , we adopt the graph model for Chinese spell checking for pinyin segmentation and typo correction, which is based on the shortest path word segmentation algorithm [ 1 ]
p7992
aVThe shortest path segmentation algorithm is based on the idea that a reasonable segmentation should minimize the number of segmented units
p7993
aVPossible legal syllables fetched from dictionary u'\u005cud835' u'\u005cudd3b' p according to the input pinyin sequence
p7994
aVThis is the single source shortest path (SSSP) problem on DAG which has an efficient algorithm by preprocessing the DAG with topology sort, then traversing vertices and edges in topological order
p7995
aVFor example, one intends to input u'\u005cu201c' \u005csong u'\u005cu4f60' u'\u005cu597d' u'\u005cu4e16' u'\u005cu754c'  (Hello world) u'\u005cu201d' by typing u'\u005cu201c' nihaoshijie u'\u005cu201d' , but mistyped as u'\u005cu201c' m ihaoshiji w u'\u005cu201d'
p7996
aVIf the adjacent syllables can be merged into a legal syllable, the merged syllable is also added into u'\u005cud835' u'\u005cudd4d'
p7997
aVSyllables with Levenshtein distance under a certain threshold are considered as similar
p7998
aVThe vertex weight is the Levenshtein distance multiply by a normalization parameter
p7999
aVMerely using the above model, the typo correction result is not satisfying yet, no matter how much effort is paid
p8000
aVThe major reason is that the basic semantic unit of Chinese language is actually word (tough vaguely defined) which is usually composed of several characters
p8001
aVThus the conditional probability between characters does not make much sense
p8002
aVHowever, using pinyin words instead of syllables is not a wise choice because pinyin word segmentation is not so easy a task as syllable segmentation
p8003
aVTo make typo correction better, we consider to integrate it with PTC conversion using a joint model
p8004
aVThe graph G = ( u'\u005cud835' u'\u005cudd4d' , u'\u005cud835' u'\u005cudd3c' ) is constructed based on graph G c for typo correction in Section 3.2
p8005
aVCorresponding Chinese words are fetched from a PTC dictionary u'\u005cud835' u'\u005cudd3b' c , which is a dictionary maps pinyin words to Chinese words, and added as vertices
p8006
aVThe vertex weight consists of two parts
p8007
aVIf the corresponding pinyin syllables in G c have an edge between them, the vertices in G also have an edge
p8008
aVAlthough the model is formulated on first order HMM, i.e.,, the LM used for transition probability is a bigram one, it is easy to extend the model to take advantage of higher order n -gram LM, by tracking longer history while traversing the graph
p8009
aVAccording to our empirical statistics, when setting threshold T = 2 , for a sentence of M characters, the joint graph will have u'\u005cud835' u'\u005cudd4d'
p8010
aVTo reduce the scale of graph G , we filter graph G c by searching its K -shortest paths first to get G c u'\u005cu2032' and construct G on top of G c u'\u005cu2032'
p8011
aVThe scale of graph may be thus drastically reduced
p8012
aVAn efficient heap data structure is required in K -shortest paths algorithm [ 7 ] for backtracking the best paths to current vertex while traversing
p8013
aVThe heap is implemented as a priority queue of size K sorted according to path length that should support efficient push and pop operations
p8014
aVFibonacci heap [ 9 ] is adopted for the heap implementation since it has a push complexity of O u'\u005cu2062' ( 1 ) which is better than the O u'\u005cu2062' ( K ) for other heap structures
p8015
aVAnother benefit provided by K -shortest paths is that it can be used for generating N -best candidates of PTC conversion, which may be helpful for further performance improvement
p8016
aVThe pinyin part is segmented according to the Chinese part
p8017
aVThis vocabulary u'\u005cud835' u'\u005cudcb1' also serves as the PTC dictionary
p8018
aVThe original vocabulary is not labeled with pinyin, thus we use the PTC dictionary of sunpinyin 1 1 http://code.google.com/p/sunpinyin/ which is an open source Chinese pinyin IME, to label the vocabulary u'\u005cud835' u'\u005cudcb1' with pinyin
p8019
aVThe emission probabilities are estimated using the lexical translation module of MOSES [ 17 ] as u'\u005cu201c' translation probability u'\u005cu201d' from pinyin to Chinese
p8020
aVWe will use conventional sequence labeling evaluation metrics such as sequence accuracy and character accuracy 2 2 We only work on the PTC conversion part of IME, thus we are unable to use existing evaluation systems [ 15 ] for full Chinese IME functions
p8021
aVWe will also report the conversion error rate (ConvER) proposed by [ 44 ] , which is the ratio of the number of mistyped pinyin word that is not converted to the right Chinese word over the total number of mistyped pinyin words 3 3 Other evaluation metrics are also proposed by [ 44 ] which is only suitable for their system since our system uses a joint model
p8022
aVAccording to our empirical observation, emission probabilities are mostly 1 since most Chinese words have unique pronunciation
p8023
aVSo in this step we set u'\u005cu0393' = 0
p8024
aVAccording to the results, we then choose the trigram LM using Kneser-Ney smoothing with interpolation
p8025
aVWe can observe that MIU-Acc slightly decreases while N goes up, but Ch-Acc largely increases
p8026
aVWe therefore choose N = 10 as trade-off
p8027
aVThe typo rate is set according to previous Human-Computer Interaction (HCI) studies
p8028
aVDue to few works have been done on modeling Chinese text entry, we have to refer to those corresponding results on English [ 32 , 22 , 6 ] , which show that the average typo rate is about 2%
p8029
aVThe collected data consists of 775 mistyped pinyin words caused by one edit operation, and 85 caused by two edit operations
p8030
aVAs we observe on \u005cmsc Train that the average pinyin word length is 5.24, then typo rate in the experiment of [ 44 ] can be roughly estimated as
p8031
aVwhich is similar to the conclusion on English
p8032
aVThus we generate corpora from \u005cmsc Dev with typo rate of 0% ( 0-P ), 2% ( 2-P ), and 5% ( 5-P ) to evaluate the system
p8033
aVSince pinyin syllable is much shorter than pinyin word, this ratio can be higher for pinyin syllables
p8034
aVFrom our statistics on \u005cmsc Train , with 2% randomly generated typos, P r ( u'\u005cu2112' ( S u'\u005cu2032' , S ) 2 ) = 99.86 %
p8035
aVThus we set the threshold T for u'\u005cu2112' to 2
p8036
aVWe choose K = 20 since there is no significant improvement when K 20
p8037
aVSince sunpinyin does not have typo correction module and performs much poorer than our baseline system, we do not include it in the comparison
p8038
aVThough no direct proofs can be found to indicate if Google Input Tool has an independent typo correction component, its outputs show that such a component is unlikely available
p8039
aVSince Google Input Tool has to be accessed through a web interface and the network connection cannot be guaranteed we only take a subset of 10K sentences of \u005cmsc Test to perform the experiments, and the results are shown in Table 3
p8040
aVThe scores reported in [ 44 ] are not listed in Table 4 since the data set is different
p8041
asg88
(lp8042
sg90
(lp8043
sg92
(lp8044
VIn this paper, we have developed a joint graph model for pinyin-to-Chinese conversion with typo correction.
p8045
aVThis model finds a joint global optimal for typo correction and PTC conversion on the entire input pinyin sequence.
p8046
aVThe evaluation results show that our model outperforms both previous academic systems and existing commercial products.
p8047
aVIn addition, the joint model is efficient enough for practical use.
p8048
ag106
asg107
S'P14-1142'
p8049
sg109
(lp8050
VIt is very import for Chinese language processing with the aid of an efficient input method engine (IME), of which pinyin-to-Chinese (PTC) conversion is the core part.
p8051
aVMeanwhile, though typos are inevitable during user pinyin inputting, existing IMEs paid little attention to such big inconvenience.
p8052
aVIn this paper, motivated by a key equivalence of two decoding algorithms, we propose a joint graph model to globally optimize PTC and typo correction for IME.
p8053
aVThe evaluation results show that the proposed method outperforms both existing academic and commercial IMEs.
p8054
ag106
asba(icmyPackage
FText
p8055
(dp8056
g3
(lp8057
VA major weakness of many existing scoring engines such as the Intelligent Essay Assessor u'\u005cu2122' [ 13 ] is that they adopt a holistic scoring scheme, which summarizes the quality of an essay with a single score and thus provides very limited feedback to the writer
p8058
aVEssay grading software that provides feedback along multiple dimensions of essay quality such as E- rater /Criterion [ 1 ] has also begun to emerge
p8059
aVOur goal in this paper is to develop a computational model for scoring an essay along an under-investigated dimension u'\u005cu2014' prompt adherence
p8060
aVRegarding task formulation, while Higgins et al. focus on classifying each sentence as having either good or bad adherence to the prompt, we focus on assigning a prompt adherence score to the entire essay , allowing the score to range from one to four points at half-point increments
p8061
aVAs far as the approach is concerned, Higgins et al. adopt a knowledge-lean approach to the task, where almost all of the features they employ are computed based on a word-based semantic similarity measure known as Random Indexing [ 10 ]
p8062
aVSince progress in prompt adherence modeling is hindered in part by the lack of a publicly annotated corpus, we believe that our data set will be a valuable resource to the NLP community
p8063
aVWe use as our corpus the 4.5 million word International Corpus of Learner English (ICLE) [ 5 ] , which consists of more than 6000 essays written by university undergraduates from 16 countries and 16 native languages who are learners of English as a Foreign Language
p8064
aV91% of the ICLE texts are argumentative
p8065
aVFor the sake of our experiments, whenever annotators disagree on an essay u'\u005cu2019' s prompt adherence score, we assign the essay the average of all annotations rounded to the nearest half point
p8066
aVIn this section, we describe in detail our system for predicting essays u'\u005cu2019' prompt adherence scores
p8067
aVWe cast the problem of predicting an essay u'\u005cu2019' s prompt adherence score as 13 regression problems, one for each prompt
p8068
aVEach essay is represented as an instance whose label is the essay u'\u005cu2019' s true score (one of the values shown in Table 3 ) with up to seven types of features including baseline (Section 4.2) and six other feature types proposed by us (Section 4.3
p8069
aVIf he was alive at the end of the 20th century, he would replace religion with television, u'\u005cu201d' students sometimes write essays about all the evils of television, forgetting that their essay is only supposed to be about whether it is u'\u005cu201c' the opium of the masses u'\u005cu201d'
p8070
aVThe test instances are created in the same way as the training instances
p8071
aVOur baseline system for score prediction employs various features based on Random Indexing
p8072
aVWe expect that features based on RI will be useful for prompt adherence scoring because they may help us find text related to the prompt even if some of its concepts have have been rephrased (e.g.,, an essay may talk about u'\u005cu201c' jail u'\u005cu201d' rather than u'\u005cu201c' prison u'\u005cu201d' , which is mentioned in one of the prompts), and because they have already proven useful for the related task of determining which sentences in an essay are related to the prompt [ 7 ]
p8073
aVFor each essay, we therefore attempt to adapt the RI features used by Higgins et al
p8074
aVWe do this by generating one feature encoding the entire essay u'\u005cu2019' s similarity to the prompt, another encoding the essay u'\u005cu2019' s highest individual sentence u'\u005cu2019' s similarity to the prompt, a third encoding the highest entire essay similarity to one of the prompt sentences, another encoding the highest individual sentence similarity to an individual prompt sentence, and finally one encoding the entire essay u'\u005cu2019' s similarity to a manually rewritten version of the prompt that excludes extraneous material (such as u'\u005cu201c' In his novel Animal Farm, George Orwell wrote, u'\u005cu201d' which is introductory material from the third prompt in Table 1
p8075
aVOur RI feature set necessarily excludes those features from Higgins et al. that are not easily translatable to our problem since we are concerned with an entire essay u'\u005cu2019' s adherence to its prompt rather than with each of its sentences u'\u005cu2019' relatedness to the prompt
p8076
aVSince RI does not provide a straightforward way to measure similarity between groups of words such as sentences or essays, we use Higgins and Burstein u'\u005cu2019' s [ 8 ] method to generate these features
p8077
aVNext, we introduce six types of novel features
p8078
aVAs our first novel feature, we use the 10,000 most important lemmatized unigram, bigram, and trigram features that occur in the essay
p8079
aVN-grams can be useful for prompt adherence scoring because they can capture useful words and phrases related to a prompt
p8080
aVFor example, words and phrases like u'\u005cu201c' university degree u'\u005cu201d' , u'\u005cu201c' student u'\u005cu201d' , and u'\u005cu201c' real world u'\u005cu201d' are relevant to the first prompt in Table 1 , so it is more likely that an essay adheres to the prompt if they appear in the essay
p8081
aVSince the essays vary greatly in length, we normalize each essay u'\u005cu2019' s set of n-gram features to unit length
p8082
aVThe keyword features were formed by first examining the 13 essay prompts, splitting each into its component pieces
p8083
aVAs an example of what is meant by a u'\u005cu201c' component piece u'\u005cu201d' , consider the first prompt in Table 1
p8084
aVThen the most important (primary) and second most important (secondary) words were selected from each prompt component, where a word was considered u'\u005cu201c' important u'\u005cu201d' if it would be a good word for a student to use when stating her thesis about the prompt
p8085
aVSo since the lemmatized version of the third component of the second prompt in Table 1 is u'\u005cu201c' it should rehabilitate they u'\u005cu201d' , u'\u005cu201c' rehabilitate u'\u005cu201d' was selected as a primary keyword and u'\u005cu201c' society u'\u005cu201d' as a secondary keyword
p8086
aVFeatures are then computed based on these keywords
p8087
aVFor instance, one thesis clarity keyword feature is computed as follows
p8088
aVThe greatest of the fractions generated in this way is encoded as a feature because if it has a low value, that indicates the essay u'\u005cu2019' s thesis may not be very relevant to the prompt
p8089
aVThe thesis clarity keyword features described above were intended for the task of determining how clear an essay u'\u005cu2019' s thesis is, but since our goal is instead to determine how well an essay adheres to its prompt, it makes sense to adapt keyword features to our task rather than to adopt keyword features exactly as they have been used before
p8090
aVFor this reason, we construct a new list of keywords for each prompt component, though since prompt adherence is more concerned with what the student says about the topics than it is with whether or not what she says about them is stated clearly, our keyword lists look a little different than the ones discussed above
p8091
aVIf he was alive at the end of the 20th century, he would replace religion with television u'\u005cu201d' Since the question suggests that students discuss whether television is analogous to religion in this way, our set of prompt adherence keywords for this prompt contains the word u'\u005cu201c' religion u'\u005cu201d' while the previously discussed keyword sets do not
p8092
aVThis is because a thesis like u'\u005cu201c' Television is bad u'\u005cu201d' can be stated very clearly without making any reference to religion at all, and so an essay with a thesis like this can potentially have a very high thesis clarity score
p8093
aVIt should not, however, have a very high prompt adherence score, as the prompt asked the student to discuss whether television is like religion in a particular way, so religion should be at least briefly addressed for an essay to be awarded a high prompt adherence score
p8094
aVAdditionally, our prompt adherence keyword sets do not adopt the notions of primary and secondary groups of keywords for each prompt component, instead collecting all the keywords for a component into one set because u'\u005cu201c' secondary u'\u005cu201d' keywords tend to be things that are important when we are concerned with what a student is saying about the topic rather than just how clearly she said it
p8095
aVTo obtain feature values of the first type, we take the RI similarities between the whole essay and each set of prompt adherence keywords from the prompt u'\u005cu2019' s components
p8096
aVThis results in one to three features, as some prompts have one component while others have up to three
p8097
aVThis results in one to three features since a prompt has one to three components
p8098
aVThese topics should not diminish the essay u'\u005cu2019' s prompt adherence score because they are at least related to prompt concepts
p8099
aVFor example, consider the prompt u'\u005cu201c' All armies should consist entirely of professional soldiers there is no value in a system of military service u'\u005cu201d' An essay containing words like u'\u005cu201c' peace u'\u005cu201d' , u'\u005cu201c' patriotism u'\u005cu201d' , or u'\u005cu201c' training u'\u005cu201d' are probably not digressions from the prompt, and therefore should not be penalized for discussing these topics
p8100
aVWhile n-gram features do not have exactly the same problem, they would still only notice that these example words are related to the prompt if multiple essays use the same words to discuss these concepts
p8101
aVFor this reason, we introduce Latent Dirichlet Allocation (LDA) [ 2 ] features
p8102
aVThis results in what we can think of as a soft clustering of words into 1,000 sets for each prompt, where each set of words represents one of the topics LDA identified being discussed in the essays for that prompt
p8103
aVSo for example, the five most important words in the most frequently discussed topic for the military prompt we mentioned above are u'\u005cu201c' man u'\u005cu201d' , u'\u005cu201c' military u'\u005cu201d' , u'\u005cu201c' service u'\u005cu201d' , u'\u005cu201c' pay u'\u005cu201d' , and u'\u005cu201c' war u'\u005cu201d'
p8104
aVSince the latter topic is discussed so much in the essay and does not appear to have much to do with the military prompt, this essay should probably get a bad prompt adherence score
p8105
aVEach feature u'\u005cu2019' s value is obtained by using the topic model to tell us how much of the essay was spent discussing the feature u'\u005cu2019' s corresponding topic
p8106
aVA weakness of the LDA topics feature type is that it may result in a regressor that has trouble distinguishing between an infrequent topic that is adherent to the prompt and one that just represents an irrelevant digression
p8107
aVThis is because an infrequent topic may not appear in the training set often enough for the regressor to make this judgment
p8108
aVIn order to construct manually annotated LDA topic features, we first build 13 topic models, one for each prompt, just as described in the section on LDA topic features
p8109
aVRather than requesting models of 1,000 topics, however, we request models of only 100 topics 2 2 We use 100 topics for each prompt in the manually annotated version of LDA features rather than the 1,000 topics we use in the regular version of LDA features because 1,300 topics are not too costly to annotate, but manually annotating 13,000 topics would take too much time
p8110
aVThe first five features encode the sum of the contributions to an essay of topics annotated with a number u'\u005cu2265' 1 , the sum of the contributions to an essay of topics annotated with a number u'\u005cu2265' 2 , and so on up to 5
p8111
aVThe next five features are similar to the last, with one feature taking on the sum of the contributions to an essay of topics annotated with the number 0, another feature taking on the sum of the contributions to an essay of topics annotated with the number 1, and so on up to 4
p8112
aVWe do not include a feature for topics annotated with the number 5 because it would always have the same value as the feature for topics u'\u005cu2265' 5
p8113
aVFeatures like these should give the regressor a better idea how much of an essay is composed of prompt-related arguments and discussion and how much of it is irrelevant to the prompt, even if some of the topics occurring in it are too infrequent to judge just from training data
p8114
aVFor instance, an essay that has a Relevance to Prompt error or an Incomplete Prompt Response error should intuitively receive a low prompt adherence score
p8115
aVFor this reason, we introduce features based on these errors to our feature set for prompt adherence scoring 3 3 See our website at http://www.hlt.utdallas.edu/~persingq/ICLE/ for the complete list of error annotations
p8116
aVWhile each of the essays in our data set was previously annotated with these thesis clarity errors, in a realistic setting a prompt adherence scoring system will not have access to these manual error labels
p8117
aVAs a result, we first need to predict which of these errors is present in each essay
p8118
aVIf a training essay is written in response to p , it will be used to generate a training instance whose label is 1 if e was annotated for it or 0 otherwise
p8119
aVSince error prediction and prompt adherence scoring are related problems, the features we associate with this instance are features 1 - 6 which we have described earlier in this section
p8120
aVAs we will see below, S u'\u005cu2062' 1 , S u'\u005cu2062' 2 , and S u'\u005cu2062' 3 are error metrics, so lower scores imply better performance
p8121
aVIn contrast, P u'\u005cu2062' C is a correlation metric, so higher correlation implies better performance
p8122
aVThe simplest metric, S u'\u005cu2062' 1 , measures the frequency at which a system predicts the wrong score out of the seven possible scores
p8123
aVHence, a system that predicts the right score only 25% of the time would receive an S u'\u005cu2062' 1 score of 0.75
p8124
aVThis metric reflects the idea that a system that predicts scores close to the annotator-assigned scores should be preferred over a system whose predictions are further off, even if both systems estimate the correct score at the same frequency
p8125
aVwhere A j , E j , and E j u'\u005cu2032' are the annotator assigned, system predicted, and rounded system predicted scores 4 4 Since our regressor assigns each essay a real value rather than an actual valid score, it would be difficult to obtain a reasonable S u'\u005cu2062' 1 score without rounding the system estimated score to one of the possible values
p8126
aVFor that reason, we round the estimated score to the nearest of the seven scores the human annotators were permitted to assign (1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0) only when calculating S u'\u005cu2062' 1
p8127
aVFor other scoring metrics, we only round the predictions to 1.0 or 4.0 if they fall outside the 1.0 - 4.0 range respectively for essay j , and N is the number of essays
p8128
aVA positive (negative) P u'\u005cu2062' C implies that the two sets of predictions are positively (negatively) correlated
p8129
aVAs mentioned earlier, for each prompt p i , we train a linear regressor r i using LIBSVM with regularization parameter c i
p8130
aVNote that each of the c i values can be tuned independently because a c i value that is optimal for predicting scores for p i essays with respect to any of the error performance measures is necessarily also the optimal c i when measuring that error on essays from all prompts
p8131
aVHowever, this is not case with Pearson u'\u005cu2019' s correlation coefficient, as the P u'\u005cu2062' C value for essays from all 13 prompts cannot be simplified as a weighted sum of the P u'\u005cu2062' C values obtained on each individual prompt
p8132
aVIn order to obtain an optimal result as measured by P u'\u005cu2062' C , we jointly tune the c i parameters to optimize the P u'\u005cu2062' C value achieved by our system on the same held-out validation data
p8133
aVHowever, an exact solution to this optimization problem is computationally expensive, as there are too many ( 7 13 ) possible combinations of c values to exhaustively search
p8134
aVConsequently, we find a local maximum by employing the simulated annealing algorithm [ 11 ] , altering one c i value at a time to optimize P u'\u005cu2062' C while holding the remaining parameters fixed
p8135
aVThese results mean that the greatest improvements our system makes are that it ensures that our score predictions are not too often very far away from an essay u'\u005cu2019' s actual score, as making such predictions would tend to drive up S u'\u005cu2062' 3 , yielding a relative error reduction in S u'\u005cu2062' 3 of 15.8%, and it also ensures a better correlation between predicted and actual scores, thus yielding the 16.6% improvement in P u'\u005cu2062' C
p8136
aV7 7 These numbers are calculated B - O B - P where B is the baseline system u'\u005cu2019' s score, O is our system u'\u005cu2019' s score, and P is a perfect score
p8137
aVThe top line of each subtable shows what our system u'\u005cu2019' s score would be if we removed just one of the feature types from our system
p8138
aVSo to see how our system performs by the S u'\u005cu2062' 1 metric if we remove only predicted thesis clarity error features, we would look at the first row of results of Table d (a) under the column headed by the number 7 since predicted thesis clarity errors are the seventh feature type introduced in Section 4
p8139
aVSince Table 4 shows that when our system includes this feature type (along with all the other feature types), it obtains an S u'\u005cu2062' 1 score of .488, this feature type u'\u005cu2019' s removal costs our system .014 S u'\u005cu2062' 1 points, and thus its inclusion has a beneficial effect on the S u'\u005cu2062' 1 score
p8140
aVFrom row 1 of Table d (a), we can see that removing feature 4 yields a system with the best S u'\u005cu2062' 1 score in the presence of the other feature types in this row
p8141
aVFor this reason, we permanently remove feature 4 from the system before we generate the results on line 2
p8142
aVThus, we can see what happens when we remove both feature 4 and feature 5 by looking at the second entry in row 2
p8143
aVAnd since removing feature 6 harms performance least in the presence of row 2 u'\u005cu2019' s other feature types, we permanently remove both 4 and 6 from our feature set when we generate the third row of results
p8144
aVSince the feature type whose removal yields the best system is always the rightmost entry in a line, the order of column headings indicates the relative importance of the feature types, with the leftmost feature types being most important to performance and the rightmost feature types being least important in the presence of the other feature types
p8145
aVThis being the case, it is interesting to note that while the relative importance of different feature types does not remain exactly the same if we measure performance in different ways, we can see that some feature types tend to be more important than others in a majority of the four scoring metrics
p8146
aVFeatures 2 (n-grams), 3 (thesis clarity keywords), and 6 (manually annotated LDA topics) tend to be the most important feature types, as they tend to be the last feature types removed in the ablation subtables
p8147
aVFinally, while features 4 (prompt adherence keywords) and 7 (predicted thesis clarity errors) may by themselves provide useful information to our system, in the presence of the other feature types they tend to be the least important to performance as they are often the first feature types removed
p8148
aVFor example, while we identified feature 3 (thesis clarity keywords) as one of the most important feature types generally due to its tendency to have a large beneficial impact on performance, when we are measuring performance using S u'\u005cu2062' 3 , it is the least useful feature type
p8149
aVThough feature 3 is an extreme example, all feature types fluctuate in importance, as we see when we compare their orders of removal among the four ablation subtables
p8150
aVHence, it is important to know how performance is measured when building a system for scoring prompt adherence
p8151
aVFeature 3 is not the only feature type whose removal sometimes has a beneficial impact on performance
p8152
aVAs we can see in Table d (b), the removal of features 4, 5, and 7 improves our system u'\u005cu2019' s S u'\u005cu2062' 2 score by .001 points
p8153
aVFortunately, this effect does not occur in any other cases than the two listed above, as most feature types usually have a beneficial or at least neutral impact on our system u'\u005cu2019' s performance
p8154
aVWe can see this is the case by noting that they are not all the least important feature types in their respective subtables as indicated by column order
p8155
aVFor example, by the time feature 1 gets permanently removed in Table d (c), its removal harms performance by .002 S u'\u005cu2062' 3 points
p8156
aVTo more closely examine the behavior of our system, in Table 6 we chart the distributions of scores it predicts for essays having each gold standard score
p8157
aVAs an example of how to read this table, consider the number 3.06 appearing in row 2.0 in the .25 column of the S u'\u005cu2062' 3 region
p8158
aVThis means that 25% of the time, when our system with parameters tuned for optimizing S u'\u005cu2062' 3 is presented with a test essay having a gold standard score of 2.0, it predicts that the essay has a score less than or equal to 3.06
p8159
aVFrom this table, we see that our system has a strong bias toward predicting more frequent scores as there are no numbers less than 3.0 in the table, and about 93.7% of all essays have gold standard scores of 3.0 or above
p8160
aVAnother interesting point to note about this table is that the difference in error weighting between the S u'\u005cu2062' 2 and S u'\u005cu2062' 3 scoring metrics appears to be having its desired effect, as every entry in the S u'\u005cu2062' 3 subtable is less than its corresponding entry in the S u'\u005cu2062' 2 subtable due to the greater penalty the S u'\u005cu2062' 3 metric imposes for predictions that are very far away from the gold standard scores
p8161
asg88
(lp8162
sg90
(lp8163
sg92
(lp8164
VWe proposed a feature-rich approach to the under-investigated problem of predicting essay-level prompt adherence scores on student essays.
p8165
aVIn an evaluation on 830 argumentative essays selected from the ICLE corpus, our system significantly outperformed a Random Indexing based baseline by several evaluation metrics.
p8166
aVTo stimulate further research on this task, we make all our annotations, including our prompt adherence scores, the LDA topic annotations, and the error annotations publicly available.
p8167
ag106
asg107
S'P14-1144'
p8168
sg109
(lp8169
VRecently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence, technical errors, and prompt adherence.
p8170
aVThe work on modeling prompt adherence, however, has been focused mainly on whether individual sentences adhere to the prompt.
p8171
aVWe present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension.
p8172
aVOur approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system yielding improvements of up to 16.6%.
p8173
ag106
asba(icmyPackage
FText
p8174
(dp8175
g3
(lp8176
VUnderstanding the rich and complex layers of connotation remains to be a challenging task
p8177
aVAs a starting point, we study a more feasible task of learning the polarity of connotation
p8178
aVHowever, as for the second sense, for which u'\u005cu201c' burst u'\u005cu201d' and u'\u005cu201c' bristle u'\u005cu201d' can be used interchangeably with respect to this particular sense, 1 1 Hence a sense in WordNet is defined by synset (= synonym set) , which is the set of words sharing the same sense the general overtone is slightly more negative with a touch of unpleasantness, or at least not as positive as that of the first sense
p8179
aVEspecially if we look up the WordNet entry for u'\u005cu201c' bristle u'\u005cu201d' , there are noticeably more negatively connotative words involved in its gloss and examples
p8180
aVEncouraged by these recent successes, in this study, we investigate if we can attain similar gains if we model the connotative polarity of senses separately
p8181
aVEnd-users of such a lexicon may not wish to deal with Word Sense Disambiguation (WSD), which is known to be often too noisy to be incorporated into the pipeline with respect to other NLP tasks
p8182
aVAs a result, researchers often would need to aggregate labels across different senses to derive the word-level label
p8183
aVAlthough such aggregation is not entirely unreasonable, it does not seem to be the most optimal and principled way of integrating available resources
p8184
aVTherefore, in this work, we present the first unified approach that learns both sense- and word-level connotations simultaneously
p8185
aVAnother contribution of our work is the introduction of loopy belief propagation (loopy-BP) as a lexicon induction algorithm
p8186
aVLoopy-BP in our study achieves statistically significantly better performance over the constraint optimization approaches previously explored
p8187
aVIn addition, it runs much faster and it is considerably easier to implement
p8188
aVLast but not least, by using probabilistic representation of pairwise-MRF in conjunction with Loopy-BP as inference, the resulting solution has the natural interpretation as the intensity of connotation
p8189
aVThe connotation graph, called G Word+Sense , is a heterogeneous graph with multiple types of nodes and edges
p8190
aVAs shown in Figure 1 , it contains two types of nodes; (i) lemmas (i.e.,, words, 115K) and (ii) synsets (63K), and four types of edges; ( t 1 ) predicate-argument (179K), ( t 2 ) argument-argument (144K), ( t 3 ) argument-synset (126K), and ( t 4 ) synset-synset (3.4K) edges
p8191
aV2011 ) , depict the selectional preference of connotative predicates (i.e.,, the polarity of a predicate indicates the polarity of its arguments) and encode their co-occurrence relations based on the Google Web 1T corpus
p8192
aVThe argument-argument edges are based on the distributional similarities among the arguments
p8193
aV2013 ) ), could be a source of noise, as one needs to assume that the semantic relation between a pair of synsets transfers over the pair of words corresponding to that pair of synsets
p8194
aVWe formulate the task of learning sense- and word-level connotation lexicon as a graph-based classification task [ 26 ]
p8195
aVIn our collective classification formulation, each node in V is represented as a random variable that takes a value from an appropriate class label domain; in our case, u'\u005cu2112' = { + , - } for positive and negative connotation
p8196
aVMRFs are a class of probabilistic graphical models that are suited for solving inference problems in networked data
p8197
aVIn pairwise MRFs, the joint probability of the graph can be written as a product of pairwise factors, parameterized over the edges
p8198
aVThese factors are referred to as clique potentials in general MRFs, which are essentially functions that collectively determine the graph u'\u005cu2019' s joint probability
p8199
aVSpecifically, let G = ( V , E ) denote a network of random variables, where V consists of the unobserved variables u'\u005cud835' u'\u005cudcb4' that need to be assigned values from label set u'\u005cu2112'
p8200
aVHaving introduced our graph-based classification task and objective formulation, we define our problem more formally
p8201
aVThe brute force approach through enumeration of all possible assignments is exponential and thus intractable
p8202
aVIn general, exact inference is known to be NP-hard and there is no known algorithm which can be theoretically shown to solve the inference problem for general MRFs
p8203
aVTherefore in this work, we employ a computationally tractable (in fact linearly scalable with network size) approximate inference algorithm called Loopy Belief Propagation (LBP) [ 37 ] , which we extend to handle typed graphs like our connotation graph
p8204
aVOur inference algorithm is based on iterative message passing and the core of it can be concisely expressed as the following two equations
p8205
aVA message m i u'\u005cu2192' j is sent from node i to node j and captures the belief of i about j , which is the probability distribution over the labels of j ; i.e., what i u'\u005cu201c' thinks u'\u005cu201d' j u'\u005cu2019' s label is, given the current label of i and the type of the edge that connects i and j
p8206
aVAt every iteration, each node computes its belief based on messages received from its neighbors, and uses the compatibility mapping to transform its belief into messages for its neighbors
p8207
aVIt then proceeds by making each Y i u'\u005cu2208' u'\u005cud835' u'\u005cudcb4' communicate messages with their neighbors in an iterative fashion until the messages stabilize (lines 10-14), i.e., convergence is reached
p8208
aVAt convergence, we calculate the marginal probabilities, that is of assigning Y i with label y i , by computing the final beliefs b i u'\u005cu2062' ( y i ) (lines 15-17
p8209
aVThe prior beliefs u'\u005cu03a8' i of nodes can be suitably initialized if there is any prior knowledge for their connotation sentiment (e.g.,, enjoy is positive, suffer is negative
p8210
aVIn case there is no prior knowledge available, each node is initialized equally likely to have any of the possible labels, i.e.,, 1 u'\u005cu2112' as in Algorithm 3.2 (line 9
p8211
aVThe compatibility potentials can be thought of as matrices, with entries u'\u005cu03a8' i u'\u005cu2062' j t u'\u005cu2062' ( y i , y j ) that give the likelihood of a node having label y i , given that it has a neighbor with label y j to which it is connected through a type t edge
p8212
aVA key difference of our method from earlier models is that we use clique potentials that differ for edge types, since the connotation graph is heterogeneous
p8213
aV4 4 a u'\u005cu2062' r u'\u005cu2062' g u'\u005cu2062' - u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' g edges are based on co-occurrence (see Section 2 ), which does not carry as strong indication of the same connotation as e.g.,, synonymy
p8214
aVThus, we enforce less homophily for nodes connected through edges of a u'\u005cu2062' r u'\u005cu2062' g u'\u005cu2062' - u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' g type
p8215
aVOn the other hand, s u'\u005cu2062' y u'\u005cu2062' n u'\u005cu2062' - u'\u005cu2062' s u'\u005cu2062' y u'\u005cu2062' n edges connect nodes that are antonyms of each other, and thus the compatibilities capture the reverse relationship among their labels
p8216
aVOften, l is quite small (in our case, l = 2 ) and r u'\u005cu226a' m
p8217
aVThus running time grows linearly with the number of edges and is scalable to large datasets
p8218
aVConnotationWordNet is expected to be the superset of a sentiment lexicon, as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity
p8219
aVThus, we use two conventional sentiment lexicons, General Inquirer ( GenInq ) [ 27 ] and MPQA [ 36 ] , as surrogates to measure the performance of our inference algorithm
p8220
aVThis is a (bipartite) subgraph of G Word+Sense , which only includes the connotative predicates and their arguments
p8221
aVAs such, it contains only type t 1 edges
p8222
aVThe edges between the predicates and the arguments can be weighted by their Point-wise Mutual Information (PMI) 5 5 PMI scores are widely used in previous studies to measure association between words (e.g.,, [ 7 ] , [ 31 ] , [ 19 ] based on the Google Web 1T corpus
p8223
aVIn addition, argument pairs ( a 1 , a 2 ) are connected if they occurred together in the u'\u005cu201c' a 1 and a 2 u'\u005cu201d' or u'\u005cu201c' a 2 and a 1 u'\u005cu201d' coordination [ 11 , 24 ]
p8224
aVThe edges can also be weighted based on the distributional similarities of the word pairs
p8225
aVTo weigh the edges, we use the cosine similarity between the gloss vectors of the synsets based on the TF-IDF values of the words the glosses contain
p8226
aVNote that the connotation inference algorithm, as given in Algorithm 3.2 , remains exactly the same for all the graphs described above
p8227
aVThe only difference is the set of parameters used; while G Word w/ Pred-Arg and G Word w/ Overlay contain one and two edge types, respectively and only use compatibilities ( t 1 ) and ( t 2 ) , G Word uses all four as given in Table 1
p8228
aVThe G Word+Sense w/ SynSim graphs use an additional compatibility matrix for the synset similarity edges of type t 5 , which is the same as the one used for t 1 , i.e.,, similar synsets are likely to have the same connotation label
p8229
aVThis flexibility is one of the key advantages of our algorithm as new types of nodes and edges can be added to the graph seamlessly
p8230
aVThe sentiment lexicons we use as gold standard are small, compared to the size (i.e.,, number of words) our graphs contain
p8231
aVThus, we first find the overlap between each graph and a sentiment lexicon
p8232
aVNote that the overlap size may be smaller than the lexicon size , as some sentiment words may be missing from our graphs
p8233
aVThen, we calculate the number of correct label assignments
p8234
aVAs such, precision is defined as ( correct / overlap ), and recall as ( correct / lexicon size
p8235
aVFinally, F1-score is their harmonic mean and reflects the overall accuracy
p8236
aVAs shown in Table 2 (top), we first observe that including the synonym and antonym relations in the graph, as with G Word and G Word+Sense , improve the performance significantly, almost by an order of magnitude, over graphs G Word w/ Pred-Arg and G Word w/ Overlay that do not contain those relation types
p8237
aVFinally, we note that using the unweighted versions of the graphs provide relatively more robust performance, potentially due to noise in the relative edge weights
p8238
aVNext we analyze the performance when the new edges between synsets are introduced, as given in Table 2 (bottom
p8239
aVWe observe that connecting the synset nodes by their gloss-similarity (at least in the ways we tried) does not yield better performance than on our original G Word+Sense graph
p8240
aVThis suggests that glossary similarity would be a more robust means to correlate nodes; we leave it as future work to explore this direction for predicate-argument and argument-argument relations
p8241
aVOur belief propagation based connotation sentiment inference algorithm has one user-specified parameter u'\u005cu0395' (see Table 1
p8242
aVTo study the sensitivity of its performance to the choice of u'\u005cu0395' , we reran our experiments for u'\u005cu0395' = { 0.02 , 0.04 , u'\u005cu2026' , 0.24 } 6 6 Note that for u'\u005cu0395' 0.25 , compatibilities of u'\u005cu03a8' t 2 in Table 1 are reversed, hence the maximum of 0.24 and report the accuracy results on our G Word+Sense in Figure 2 for the two lexicons
p8243
aV7 7 Because senses in WordNet can be tricky to understand, care should be taken in designing the task so that the Turkers will focus only on the corresponding sense of a word
p8244
aVTherefore, we provided the part of speech tag, the WordNet gloss of the selected sense, and a few examples as given in WordNet
p8245
aVAs an incentive, each Turker was rewarded $0.07 per hit which consists of 10 words to label
p8246
aVWe labeled a word as negative if its intensity score is less than 0 and positive otherwise
p8247
aVFor word-level labels we apply similar procedure as above
p8248
aVWe first evaluate the word-level assignment of connotation, as shown in Table 3
p8249
aV2005a ) ) show low agreement rate with human, which is somewhat as expected human judges in this study are labeling for subtle connotation, not for more explicit sentiment
p8250
aVOpinionFinder u'\u005cu2019' s low agreement rate was mainly due to the low hit rate of the words (successful look-up rate, 33.43%
p8251
aVSince OpinionFinder and Feng2013 do not provide the polarity scores at the sense-level, we excluded them from this evaluation
p8252
aVBecause sense-level polarity assignment is a harder (more subtle) task, the performance of all lexicons decreased to some degree in comparison to that of word-level evaluations
p8253
aVA notable goodness of our induction algorithm is that the outcome of the algorithm can be interpreted as an intensity of the corresponding connotation
p8254
aVBut are these values meaningful
p8255
aVSince we collect human labels based on scales , we already have this information at hand
p8256
aVBecause different human judges have different notion of scales however, subtle differences are more likely to be noisy
p8257
aVTherefore, we experiment with varying degrees of differences in their scales, as shown in Figure 3
p8258
aVThreshold values (ranging from 0.5 to 3.0) indicate the minimum differences in scales for any pair of words, for the pair to be included in the test set
p8259
aVAs expected, we observe that the performance improves as we increase the threshold (as pairs get better separated
p8260
aVSuch cases seems to be due to the limited score patterns of SentiWordNet
p8261
aVThe ratio of such cases are accounted as Undecided in Table 4
p8262
aVFinally, to show the utility of the resulting lexicon in the context of a concrete sentiment analysis task, we perform lexicon-based sentiment analysis
p8263
aVWe experiment with SemEval dataset [ 28 ] that includes the human labeled dataset for predicting whether a news headline is a good news or a bad news , which we expect to have a correlation with the use of connotative words that we focus on in this paper
p8264
aVWe construct several data sets by applying different thresholds on scores
p8265
aVNote that there is a difference in how humans judge the orientation and the degree of connotation for a given word out of context, and how the use of such words in context can be perceived as good/bad news
p8266
aVIn particular, we conjecture that humans may have a bias toward the use of positive words, which in turn requires calibration from the readers u'\u005cu2019' minds [ 22 ]
p8267
aVWith this in mind, we tune the appropriate calibration from a small training data, by using 1 fold from N fold cross validation, and using the remaining N - 1 folds as testing
p8268
aVWe tune this parameter u'\u005cu039b' 8 8 What is reported is based on u'\u005cu039b' u'\u005cu2208' { 20 , 40 , 60 , 80 }
p8269
aVNote that due to this parameter learning, we are able to report better performance for the connotation lexicon of [ 10 ] than what the authors have reported in their paper (labeled with *) in Table 5
p8270
aVIn addition, Figure 4 shows that the performance does not change much based on the size of training data used for parameter tuning ( N = { 5 , 10 , 15 , 20 }
p8271
aVOur work introduces the use of loopy belief propagation over pairwise-MRF as an alternative solution to these tasks
p8272
aVAt a high-level, both approaches share the general idea of propagating confidence or belief over the graph connectivity
p8273
aVThe key difference, however, is that in our MRF representation, we can explicitly model various types of word-word, sense-sense and word-sense relations as edge potentials
p8274
aVIn particular, we can naturally encode relations that encourage the same assignment (e.g.,, synonym) as well as the opposite assignment (e.g.,, antonym) of the polarity labels
p8275
aV2013 ) share this spirit by targeting more subtle, nuanced sentiment even from those words that would be considered as objective in early studies of sentiment analysis
p8276
aVWe have introduced a novel formulation of lexicon induction operating over both words and senses, by exploiting the innate structure between the words and senses as encoded in WordNet
p8277
aVA notable strength of our approach is its expressiveness various types of prior knowledge and lexical relations can be encoded as node potentials and edge potentials
p8278
aVIn addition, it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation
p8279
aVThe resulting lexicon, called ConnotationWordNet , is the first lexicon that has polarity labels over both words and senses
p8280
asg88
(lp8281
sg90
(lp8282
sg92
(lp8283
VWe have introduced a novel formulation of lexicon induction operating over both words and senses, by exploiting the innate structure between the words and senses as encoded in WordNet.
p8284
aVIn addition, we introduce the use of loopy belief propagation over pairwise -Markov Random Fields as an effective lexicon induction algorithm.
p8285
aVA notable strength of our approach is its expressiveness various types of prior knowledge and lexical relations can be encoded as node potentials and edge potentials.
p8286
aVIn addition, it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation.
p8287
aVThe resulting lexicon, called ConnotationWordNet , is the first lexicon that has polarity labels over both words and senses.
p8288
aVConnotationWordNet is publicly available for research and practical use.
p8289
ag106
asg107
S'P14-1145'
p8290
sg109
(lp8291
VWe introduce ConnotationWordNet , a connotation lexicon over the network of words in conjunction with senses.
p8292
aVWe formulate the lexicon induction problem as collective inference over pairwise-Markov Random Fields, and present a loopy belief propagation algorithm for inference.
p8293
aVThe key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations, exploiting the innate bipartite graph structure encoded in WordNet.
p8294
aVWe present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons.
p8295
ag106
asba(icmyPackage
FText
p8296
(dp8297
g3
(lp8298
VThis is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad , to neighboring word vectors
p8299
aVExperiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set
p8300
aVThe objective is to classify the sentiment polarity of a tweet as positive, negative or neutral
p8301
aVThe majority of existing approaches follow Pang et al
p8302
aVIt is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering [ 4 ]
p8303
aVFor the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains [ 40 , 47 ]
p8304
aVAccordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word
p8305
aVThe most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text
p8306
aVAs a result, words with opposite polarity, such as good and bad , are mapped into close vectors
p8307
aVWe encode the sentiment information into the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum
p8308
aVWe learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations
p8309
aVThese automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentiment-specific word embedding
p8310
aVWe apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013
p8311
aVIn the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%
p8312
aVThe quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons
p8313
aVMany studies on Twitter sentiment classification [ 32 , 10 , 1 , 22 , 48 ] leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision [ 17 ]
p8314
aVIt has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0
p8315
aVHowever, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words
p8316
aVWith the revival of interest in deep learning [ 2 ] , incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing [ 35 ] , language modeling [ 3 , 29 ] and NER [ 43 ]
p8317
aVSocher et al propose Recursive Neural Network (RNN) [ 38 ] , matrix-vector RNN [ 37 ] and Recursive Neural Tensor Network (RNTN) [ 40 ] to learn the compositionality of phrases of any length based on the representation of each pair of children recursively
p8318
aV2011 ) introduce C W model to learn word embedding based on the syntactic contexts of words
p8319
aVThe original and corrupted ngrams are treated as inputs of the feed-forward neural network, respectively
p8320
aVThe output f c u'\u005cu2062' w is the language model score of the input, which is calculated as given in Equation 2 , where L is the lookup table of word embedding, w 1 , w 2 , b 1 , b 2 are the parameters of linear layers
p8321
aVFollowing the traditional C W model [ 9 ] , we incorporate the sentiment information into the neural network to learn sentiment-specific word embedding
p8322
aVWe develop three neural networks with different strategies to integrate the sentiment information of tweets
p8323
aVAs an unsupervised approach, C W model does not explicitly capture the sentiment information of texts
p8324
aVAn intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram
p8325
aVWe do not utilize the entire sentence as input because the length of different sentences might be variant
p8326
aVWe therefore slide the window of ngram across a sentence, and then predict the sentiment polarity based on each ngram with a shared neural network
p8327
aVIn the neural network, the distributed representation of higher layer are interpreted as features describing the input
p8328
aVThus, we utilize the continuous vector of top layer to predict the sentiment distribution of text
p8329
aVAssuming there are K labels, we modify the dimension of top layer in C W model as K and add a s u'\u005cu2062' o u'\u005cu2062' f u'\u005cu2062' t u'\u005cu2062' m u'\u005cu2062' a u'\u005cu2062' x layer upon the top layer
p8330
aVS u'\u005cu2062' o u'\u005cu2062' f u'\u005cu2062' t u'\u005cu2062' m u'\u005cu2062' a u'\u005cu2062' x layer is suitable for this scenario because its outputs are interpreted as conditional probabilities
p8331
aVSSWE h is trained by predicting the positive ngram as [1,0] and the negative ngram as [0,1]
p8332
aVHowever, the constraint of SSWE h is too strict
p8333
aVThe distribution of [0.7,0.3] can also be interpreted as a positive label because the positive score is larger than the negative score
p8334
aVSimilarly, the distribution of [0.2,0.8] indicates negative polarity
p8335
aVBased on the above observation, the hard constraints in SSWE h should be relaxed
p8336
aVIf the sentiment polarity of a tweet is positive, the predicted positive score is expected to be larger than the predicted negative score, and the exact reverse if the tweet has negative polarity
p8337
aVCompared with SSWE h , the s u'\u005cu2062' o u'\u005cu2062' f u'\u005cu2062' t u'\u005cu2062' m u'\u005cu2062' a u'\u005cu2062' x layer is removed because SSWE r does not require probabilistic interpretation
p8338
aVSSWE u is illustrated in Figure 1 (c
p8339
aVGiven an original (or corrupted) ngram and the sentiment polarity of a sentence as the input, SSWE u predicts a two-dimensional vector for each input ngram
p8340
aVwhere l u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' s c u'\u005cu2062' w u'\u005cu2062' ( t , t r ) is the syntactic loss as given in Equation 1 , l u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' s u u'\u005cu2062' s u'\u005cu2062' ( t , t r ) is the sentiment loss as described in Equation 9
p8341
aVWe train SSWE h , SSWE r and SSWE u by taking the derivative of the loss through back-propagation with respect to the whole set of parameters [ 9 ] , and use AdaGrad [ 12 ] to update the parameters
p8342
aVWe empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models
p8343
aVWe learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting
p8344
aVWe apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work [ 33 ]
p8345
aVInstead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet
p8346
aVThe sentiment classifier is built from tweets with manually annotated sentiment polarity
p8347
aVWe explore m u'\u005cu2062' i u'\u005cu2062' n , a u'\u005cu2062' v u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' g u'\u005cu2062' e and m u'\u005cu2062' a u'\u005cu2062' x convolutional layers [ 9 , 36 ] , which have been used as simple and effective methods for compositionality learning in vector-based semantics [ 28 ] , to obtain the tweet representation
p8348
aVThe result is the concatenation of vectors derived from different convolutional layers
p8349
aVWe conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification
p8350
aVWe also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons
p8351
aVHowever, we were unable to download all the training and development sets because some tweets were deleted or not available due to modified authorization status
p8352
aVRecursive Autoencoder [ 39 ] has been proven effective in many sentiment analysis tasks by learning compositionality automatically
p8353
aVWe re-implement this system because the codes are not publicly available 3 3 For 3-class sentiment classification in SemEval 2013, our re-implementation of NRC achieved 68.3%, 0.7% lower than NRC (69%) due to less training data
p8354
aVWe do not compare with RNTN [ 40 ] because we cannot efficiently train the RNTN model
p8355
aVThe reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases
p8356
aVAnother reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains [ 8 ]
p8357
aVDistant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier
p8358
aVThe results of bag-of-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words
p8359
aVWe achieve 84.98% by using only SSWE u as features without borrowing any sentiment lexicons or hand-crafted rules
p8360
aVWe also compare SSWE u with the ngram feature by integrating SSWE into NRC-ngram
p8361
aVThe concatenated features SSWE u +NRC-ngram (86.48%) outperform the original feature set of NRC (84.73%
p8362
aVAs a reference, we apply SSWE u on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWE u as feature
p8363
aVWe compare sentiment-specific word embedding (SSWE h , SSWE r , SSWE u ) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification
p8364
aVWe use the embedding of unigrams, bigrams and trigrams in the experiment
p8365
aVWe utilize the Skip-gram model because it performs better than CBOW in our experiments
p8366
aVWe compare with C W and word2vec as they have been proved effective in many NLP tasks
p8367
aVTable 3 shows the performance on the positive/negative classification of tweets 5 5 MVSA and ReEmb are not suitable for learning bigram and trigram embedding because their sentiment predictor functions only utilize the unigram embedding
p8368
aVFrom the first column of Table 3 , we can see that the performance of C W and word2vec are obviously lower than sentiment-specific word embeddings by only using unigram embedding as features
p8369
aVThe reason is that C W and word2vec do not explicitly exploit the sentiment information of the text, resulting in that the words with opposite polarity such as good and bad are mapped to close word vectors
p8370
aVWhen such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected
p8371
aVThe underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit
p8372
aVA typical case in sentiment analysis is that the composed phrase and multiword expression may have a different sentiment polarity than the individual words it contains, such as not [bad] and [great] deal of (the word in the bracket has different sentiment polarity with the ngram
p8373
aVWe tune the hyper-parameter u'\u005cu0391' of SSWE u on the development set by using unigram embedding as features
p8374
aVAs given in Equation 8 , u'\u005cu0391' is the weighting score of syntactic loss of SSWE u and trades-off the syntactic and sentiment losses
p8375
aVWe set the u'\u005cu0391' of SSWE u as 0.5, according to the experiments shown in Figure 2
p8376
aVThe underlying reason is that when more tweets are incorporated, the word embedding is better estimated as the vocabulary size is larger and the context and sentiment information are richer
p8377
aVWhen we have more than 10 million tweets, the performance remains stable as the contexts of words have been mostly covered
p8378
aVWe only use unigram embedding in this section because these sentiment lexicons do not contain phrases
p8379
aVThe accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word
p8380
aVSSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively
p8381
aVIn this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework
p8382
aVWe show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification
p8383
aVThese methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g., good and bad
p8384
aVWe learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks
p8385
aVThe effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons
p8386
asg88
(lp8387
sg90
(lp8388
sg92
(lp8389
VIn this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.
p8390
aVWe show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.
p8391
aVThese methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g., good and bad.
p8392
aVWe learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.
p8393
aVWe train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.
p8394
aVThe effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.
p8395
aVOur unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments.
p8396
ag106
asg107
S'P14-1146'
p8397
sg109
(lp8398
VWe present a method that learns word embedding for Twitter sentiment classification in this paper.
p8399
aVMost existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.
p8400
aVThis is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad , to neighboring word vectors.
p8401
aVWe address this issue by learning sentiment-specific word embedding ( SSWE ), which encodes sentiment information in the continuous representation of words.
p8402
aVSpecifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g., sentences or tweets) in their loss functions.
p8403
aVTo obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.
p8404
aVExperiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.
p8405
ag106
asba(icmyPackage
FText
p8406
(dp8407
g3
(lp8408
VTwo kernels are defined based on history sequences and context trees constructed based on the extracted features
p8409
aVTo analyze and maintain dialog topics from a more systematic perspective in a given dialog flow, some researchers [ 12 , 8 , 1 ] have considered this dialog topic identification as a separate sub-problem of dialog management and attempted to solve it with text categorization approaches for the recognized utterances in a given turn
p8410
aVHowever, the dialog topic at each turn can be determined not only by the user u'\u005cu2019' s intentions captured from the given utterances, but also by the system u'\u005cu2019' s decisions for dialog management purposes
p8411
aVThus, the text categorization approaches can only be effective for the user-initiative cases when users tend to mention the topic-related expressions explicitly in their utterances
p8412
aVThese knowledge-based methods have an advantage of dealing with system-initiative dialogs, because dialog flows can be controlled by the system based on given resources
p8413
aVMoreover, these approaches face cost problems for building a sufficient amount of resources to cover broad states of complex dialogs, because these resources should be manually prepared by human experts for each specific domain
p8414
aVComposite kernels have been successfully applied to improve the performances in other NLP problems [ 17 , 16 ] by integrating multiple individual kernels, which aim to overcome the errors occurring at one level by information from other levels
p8415
aVOur composite kernel consists of a history sequence and a domain context tree kernels, both of which are composed based on similar textual units in Wikipedia articles to a given dialog context
p8416
aVDialog topic tracking can be considered as a classification problem to detect topic transitions
p8417
aVThe most probable pair of topics at just before and after each turn is predicted by the following classifier f u'\u005cu2062' ( x t ) = ( y t - 1 , y t ) , where x t contains the input features obtained at a turn t , y t u'\u005cu2208' C , and C is a closed set of topic categories
p8418
aVIf a topic transition occurs at t , y t should be different from y t - 1
p8419
aVThis conversation is divided into three segments, since f detects three topic transitions at t 1 , t 4 and t 6
p8420
aVAlthough some fundamental features extracted from the utterances mentioned at a given turn or in a certain number of previous turns can be used for training the model, this information obtained solely from an ongoing dialog is not sufficient to identify not only user-initiative, but also system-initiative topic transitions
p8421
aVBoth represent the current dialog context at a given turn with a set of relevant Wikipedia paragraphs which are selected based on the cosine similarity between the term vectors of the recently mentioned utterances and each paragraph in the Wikipedia collection as follows
p8422
aVThe term vector for the input x , u'\u005cu03a6' u'\u005cu2062' ( x ) , is computed by accumulating the weights in the previous turns as follows
p8423
aVwhere u'\u005cu0391' i = u'\u005cu2211' j = 0 h ( u'\u005cu039b' j u'\u005cu22c5' t u'\u005cu2062' f u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f u'\u005cu2062' ( w i , u ( t - j ) ) ) , u t is the utterance mentioned in a turn t , t u'\u005cu2062' f u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f u'\u005cu2062' ( w i , u t ) is the product of term frequency of a word w i in u t and inverse document frequency of w i , u'\u005cu039b' is a decay factor for giving more importance to more recent turns
p8424
aVSince our hypothesis is that the more similar the dialog histories of the two inputs are, the more similar aspects of topic transtions occur for them, we propose a sub-sequence kernel [ 11 ] to map the data into a new feature space defined based on the similarity of each pair of history sequences as follows
p8425
aVSince this constructed tree structure represents semantic, discourse, and structural information extracted from the similar Wikipedia paragraphs to each given instance, we can explore these more enriched features to build the topic tracking model using a subset tree kernel [ 5 ] which computes the similarity between each pair of trees in the feature space as follows
p8426
aVIn this work, a composite kernel is defined by combining the individual kernels including history sequence and domain context tree kernels, as well as the linear kernel between the vectors representing fundamental features extracted from the utterances themselves and the results of linguistic preprocessors
p8427
aVSince we aim at developing the system which acts as a guide communicating with tourist users, an instance for both training and prediction of topic transition was created for each turn of tourists
p8428
aVThen, the history sequence and tree context structures for our composite kernel were constructed based on 3,155 articles related to Singapore collected from Wikipedia database dump as of February 2013
p8429
aVFor the linear kernel baseline, we used the following features n-gram words, previous system actions, and current user acts which were manually annotated
p8430
aVFinally, 8,318 instances were used for training the model
p8431
aVWe trained the SVM models using SVM light 1 1 http://svmlight.joachims.org/ [ 7 ] with the following five different combinations of kernels
p8432
aVK l only, K l with u'\u005cud835' u'\u005cudcab' as features, K l + K s , K l + K t , and K l + K s + K t
p8433
aVWhen just the paragraph IDs were included as additional features, it failed to improve the performances from the baseline without any external features
p8434
aVThe error distributions in Figure 4 indicate that these performance improvements were achieved by resolving the errors not only on user-initiative topic transitions, but also on system-initiative cases, which implies the effectiveness of the structured knowledge from Wikipedia to track the topics in mixed-initiative dialogs
p8435
aVThis paper presented a composite kernel approach for dialog topic tracking
p8436
asg88
(lp8437
sg90
(lp8438
sg92
(lp8439
VThis paper presented a composite kernel approach for dialog topic tracking.
p8440
aVThis approach aimed to represent various types of domain knowledge obtained from Wikipedia as two structures history sequences and domain context trees; then incorporate them into the model with kernel methods.
p8441
aVExperimental results show that the proposed approaches helped to improve the topic tracking performances in mixed-initiative human-human dialogs with respect to the baseline model.
p8442
ag106
asg107
S'P14-2004'
p8443
sg109
(lp8444
VDialog topic tracking aims at analyzing and maintaining topic transitions in on-going dialogs.
p8445
aVThis paper proposes a composite kernel approach for dialog topic tracking to utilize various types of domain knowledge obtained from Wikipedia.
p8446
aVTwo kernels are defined based on history sequences and context trees constructed based on the extracted features.
p8447
aVThe experimental results show that our composite kernel approach can significantly improve the performances of topic tracking in mixed-initiative human-human dialogs.
p8448
ag106
asba(icmyPackage
FText
p8449
(dp8450
g3
(lp8451
VBLANC is a link-based coreference evaluation metric for measuring the quality of coreference systems on gold mentions
p8452
aVThe proposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data
p8453
aVB-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based
p8454
aVIn particular, MUC measures the degree of agreement between key coreference links (i.e.,, links among mentions within entities) and response coreference links, while non-coreference links (i.e.,, links formed by mentions from different entities) are not explicitly taken into account
p8455
aVThis leads to a phenomenon where coreference systems outputting large entities are scored more favorably than those outputting small entities [ 3 ]
p8456
aVIt calculates recall, precision and F-measure separately on coreference and non-coreference links in the usual way, and defines the overall recall, precision and F-measure as the mean of the respective measures for coreference and non-coreference links
p8457
aVThe BLANC-gold metric was developed with the assumption that response mentions and key mentions are identical
p8458
aVK where k i is the i t u'\u005cu2062' h key entity; accordingly, R = { r j } j = 1
p8459
aVLet C k u'\u005cu2062' ( i ) and C r u'\u005cu2062' ( j ) be the set of coreference links formed by mentions in k i and r j
p8460
aVAs can be seen, a link is an undirected edge between two mentions, and it can be equivalently represented by a pair of mentions
p8461
aVClearly, C k and N k form a partition of T k since C k u'\u005cu2229' N k = u'\u005cu2205' , T k = C k u'\u005cu222a' N k
p8462
aVWe write l 1 = l 2 if two links are equal
p8463
aVIt is easy to see that the gold mention assumption u'\u005cu2014' same set of response mentions as the set of key mentions u'\u005cu2014' can be equivalently stated as T k = T r (this does not necessarily mean that C k = C r or N k = N r
p8464
aVRand Index is defined as the ratio between the number of correct within-cluster links plus the number of correct cross-cluster links, and the total number of links
p8465
aVWhen T k = T r , Rand Index can be applied directly since coreference resolution reduces to a clustering problem where mentions are partitioned into clusters (entities
p8466
aVIn practice, though, the simple-minded adoption of Rand Index is not satisfactory since the number of non-coreference links often overwhelms that of coreference links [ 7 ] , or
p8467
aVBLANC-gold solves this problem by averaging the F-measure computed over coreference links and the F-measure over non-coreference links
p8468
aVUsing the notations in Section 2 , the recall, precision, and F-measure on coreference links are
p8469
aVIn Eqn. ( 2 ) - ( 3 ) and Eqn. ( 5 ) - ( 6 ), denominators are written as a sum of disjoint subsets so they can be related to the contingency table in [ 7 ]
p8470
aVUnder the assumption that T k = T r , it is clear that C k = ( C k u'\u005cu2229' C r ) u'\u005cu222a' ( C k u'\u005cu2229' N r ) , C r = ( C k u'\u005cu2229' C r ) u'\u005cu222a' ( N k u'\u005cu2229' C r ) , and so on
p8471
aVUnder the assumption that the key and response mention sets are identical (which implies that T k = T r ), Equations ( 2 ) to ( 7 ) make sense
p8472
aVFor example, R c is the ratio of the number of correct coreference links over the number of key coreference links; P c is the ratio of the number of correct coreference links over the number of response coreference links, and so on
p8473
aVHowever, when response mentions are not identical to key mentions, a key coreference link may not appear in either C r or N r , so Equations ( 2 ) to ( 7 ) cannot be applied directly to systems with imperfect mentions
p8474
aVFor instance, if the key entities are { a,b,c } { d,e }; and the response entities are { b,c } { e,f,g }, then the key coreference link (a,b) is not seen on the response side; similarly, it is possible that a response link does not appear on the key side either c,f) and (f,g) are not in the key in the above example
p8475
aVWe observe that the definition of the proposed BLANC, Equ. ( 9 )-( 14 ) subsume the BLANC-gold ( 2 ) to ( 7 ) due to the following proposition
p8476
aVIf T k = T r , then B u'\u005cu2062' L u'\u005cu2062' A u'\u005cu2062' N u'\u005cu2062' C = B u'\u005cu2062' L u'\u005cu2062' A u'\u005cu2062' N u'\u005cu2062' C ( g )
p8477
aVWe prove the first one (the other proofs are similar and elided due to space limitations
p8478
aVThis establishes that R c = R c ( g )
p8479
aVIndeed, since C k is a union of three disjoint subsets
p8480
aVUnification for other component recalls and precisions can be done similarly
p8481
aVSo the final definition of BLANC can be succinctly stated as
p8482
aVThis can happen when all key (or response) mentions are in one cluster or are all singletons the former case will lead to N k = u'\u005cu2205' (or N r = u'\u005cu2205' ); the latter will lead to C k = u'\u005cu2205' (or C r = u'\u005cu2205'
p8483
aV0 , F n in ( 18 ) is well-defined
p8484
aVSo we only need to augment the BLANC definition for the following cases
p8485
aV1) If C k = C r = u'\u005cu2205' and N k = N r = u'\u005cu2205' , then BLANC = I ( M k = M r ) , where I u'\u005cu2062' ( u'\u005cu22c5' ) is an indicator function whose value is 1 if its argument is true, and 0 otherwise
p8486
aVSince there is no coreference link, BLANC reduces to the non-coreference F-measure F n
p8487
aVSince there is no non-coreference link, BLANC reduces to the coreference F-measure F c
p8488
aVObviously, C k = { ( a u'\u005cu2062' b ) , ( b u'\u005cu2062' c ) , ( a u'\u005cu2062' c ) } ; N k = { ( a u'\u005cu2062' d ) , ( b u'\u005cu2062' d ) , ( c u'\u005cu2062' d ) } ; C r = { ( b u'\u005cu2062' c ) , ( d u'\u005cu2062' e ) } ; N r = { ( b u'\u005cu2062' d ) , ( b u'\u005cu2062' e ) , ( c u'\u005cu2062' d ) , ( c u'\u005cu2062' e ) }
p8489
aVTherefore, C k u'\u005cu2229' C r = { ( b u'\u005cu2062' c ) } , N k u'\u005cu2229' N r = { ( b u'\u005cu2062' d ) , ( c u'\u005cu2062' d ) } , and R c = 1 3 , P c = 1 2 , F c = 2 5 ; R n = 2 3 , P n = 2 4 , F n = 4 7
p8490
aVSince N k = { ( a u'\u005cu2062' b ) , ( b u'\u005cu2062' c ) , ( c u'\u005cu2062' a ) } , N r = { ( a u'\u005cu2062' b ) , ( b u'\u005cu2062' d ) , ( a u'\u005cu2062' d ) } , we have N k u'\u005cu2229' N r = { ( a u'\u005cu2062' b ) } , and R n = 1 3 , P n = 1 3
p8491
aVSo BLANC = F n = 1 3
p8492
aVThis is boundary case (3 there are no non-coreference links
p8493
aVSince C k = { ( a u'\u005cu2062' b ) , ( b u'\u005cu2062' c ) , ( c u'\u005cu2062' a ) } , and C r = { ( b u'\u005cu2062' c ) } , we have C k u'\u005cu2229' C r = { ( b u'\u005cu2062' c ) } , and R c = 1 3 , P c = 1 , So BLANC = F c = 2 4 = 1 2
p8494
aVHowever, the CoNLL data sets come from OntoNotes [ 2 ] , where singleton entities are not annotated, and BLANC has a wider dynamic range on data sets with singletons [ 7 ]
p8495
aVSo the correlations will likely be lower on data sets with singleton entities
p8496
aVThe original BLANC-gold [ 7 ] requires that system mentions be identical to gold mentions, which limits the metric u'\u005cu2019' s utility since detected system mentions often have missing key mentions or spurious mentions
p8497
aVSince BLANC works on imperfect system mentions, we have used it to score the CoNLL 2011 and 2012 coreference systems
p8498
asg88
(lp8499
sg90
(lp8500
sg92
(lp8501
VThe original BLANC-gold [ 7 ] requires that system mentions be identical to gold mentions, which limits the metric s utility since detected system mentions often have missing key mentions or spurious mentions.
p8502
aVThe proposed BLANC is free from this assumption, and we have shown that it subsumes the original BLANC-gold.
p8503
aVSince BLANC works on imperfect system mentions, we have used it to score the CoNLL 2011 and 2012 coreference systems.
p8504
aVThe BLANC scores show strong correlation with existing metrics, especially B-cubed and CEAF-m.
p8505
ag106
asg107
S'P14-2005'
p8506
sg109
(lp8507
VBLANC is a link-based coreference evaluation metric for measuring the quality of coreference systems on gold mentions.
p8508
aVThis paper extends the original BLANC ( BLANC-gold henceforth) to system mentions, removing the gold mention assumption.
p8509
aVThe proposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data.
p8510
ag106
asba(icmyPackage
FText
p8511
(dp8512
g3
(lp8513
VAs for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking
p8514
aVUsing linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation
p8515
aVThe two are lexically and structurally similar
p8516
aVHowever, because of the sarcasm in the second tweet (in u'\u005cu201c' cold u'\u005cu201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u005cu201c' just what I wanted u'\u005cu201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation
p8517
aVThus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others
p8518
aVIf the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example
p8519
aVAlso, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences
p8520
aVThe next level of sentiment annotation complexity arises due to syntactic complexity
p8521
aVConsider the review u'\u005cu201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u005cu201d'
p8522
aVAn annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review
p8523
aVSarcasm expressed in u'\u005cu201c' It u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' leads to difficulty in sentiment annotation because of positive words like u'\u005cu201c' an all-star salute u'\u005cu201d'
p8524
aVManual annotation of complexity scores may not be intuitive and reliable
p8525
aVHence, we use a cognitive technique to create our annotated dataset
p8526
aVThe underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
p8527
aVUsing the idea of u'\u005cu201c' annotation time u'\u005cu201d' linked with complexity, we devise a technique to create a dataset annotated with SAC
p8528
aVHowever, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise
p8529
aVHowever, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds
p8530
aVThis indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself
p8531
aVWe wish to predict sentiment annotation complexity of the text using a supervised technique
p8532
aVAs stated above, the time-to-annotate is one good candidate
p8533
aVHowever, u'\u005cu201c' simple time measurement u'\u005cu201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
p8534
aVHowever, saccade duration is not significant for annotation of short text, as in our case
p8535
aVHence, the SAC labels of our dataset are fixation durations with appropriate normalization
p8536
aVThis is to prevent fatigue over a period of time
p8537
aVThus, each annotator participates in this experiment over a number of sittings
p8538
aVWe ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
p8539
aVHowever, we want to capture the most natural form of sentiment annotation
p8540
aVSo, the guidelines are kept to a bare minimum of u'\u005cu201c' annotating a sentence as positive, negative and objective as per the speaker u'\u005cu201d'
p8541
aVSince we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels
p8542
aVWe carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit
p8543
aVThe model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC
p8544
aVThe correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
p8545
aVThe cross-domain MPE is higher than the rest, as expected
p8546
aVTo understand how each of the features performs, we conducted ablation tests by considering one feature at a time
p8547
aVBased on the MPE values, the best features are
p8548
aVNote that some errors may be introduced in feature extraction due to limitations of the NLP tools
p8549
aVUsing NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets
p8550
aVUsing this data set and a set of linguistic features, we trained a regression model to predict SAC
p8551
aVFinally, we observe a negative correlation between the classifier confidence scores and a SAC, as expected
p8552
aVAs a future work, we would like to investigate how SAC of a test sentence can be used to choose a classifier from an ensemble, and to determine the pre-processing steps (entity-relationship extraction, for example
p8553
asg88
(lp8554
sg90
(lp8555
sg92
(lp8556
VWe presented a metric called Sentiment Annotation Complexity (SAC), a metric in SA research that has been unexplored until now.
p8557
aVFirst, the process of data preparation through eye tracking, labeled with the SAC score was elaborated.
p8558
aVUsing this data set and a set of linguistic features, we trained a regression model to predict SAC.
p8559
aVOur predictive framework for SAC resulted in a mean percentage error of 22.02%, and a moderate correlation of 0.57 between the predicted and observed SAC values.
p8560
aVFinally, we observe a negative correlation between the classifier confidence scores and a SAC, as expected.
p8561
aVAs a future work, we would like to investigate how SAC of a test sentence can be used to choose a classifier from an ensemble, and to determine the pre-processing steps (entity-relationship extraction, for example.
p8562
ag106
asg107
S'P14-2007'
p8563
sg109
(lp8564
VThe effort required for a human annotator to detect sentiment is not uniform for all texts, irrespective of his/her expertise.
p8565
aVWe aim to predict a score that quantifies this effort, using linguistic properties of the text.
p8566
aVOur proposed metric is called Sentiment Annotation Complexity (SAC.
p8567
aVAs for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking.
p8568
aVThe sentences in our dataset are labeled with SAC scores derived from eye-fixation duration.
p8569
aVUsing linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation.
p8570
aVWe also study the correlation between a human annotator s perception of complexity and a machine s confidence in polarity determination.
p8571
aVThe merit of our work lies in (a) deciding the sentiment annotation cost in, for example, a crowdsourcing setting, (b) choosing the right classifier for sentiment prediction.
p8572
ag106
asba(icmyPackage
FText
p8573
(dp8574
g3
(lp8575
VFirst, we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains
p8576
aVThen, to avoid over-engineering specific citation features for a particular scientific domain, we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features
p8577
aVWe expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications
p8578
aVThis choice also allows us to access the wealth of existing data containing polarity annotation and then frame the task as a domain adaptation problem
p8579
aVOf course the risk in approaching the problem as domain adaptation is that the domains are so different that the representation of a positive instance of a movie or product review, for example, will not coincide with that of a positive scientific citation
p8580
aVOn the other hand, because there is a limited amount of annotated citation data available, by leveraging large amounts of annotated polarity data we could potentially even improve citation classification
p8581
aVWe treat citation polarity classification as a sentiment analysis domain adaptation task and therefore must be careful not to define features that are too domain specific
p8582
aVSince mSDA achieved state-of-the-art performance in Amazon product domain adaptation, we are hopeful it will also be effective when switching to a more distant domain like scientific citations
p8583
aVWe are interested in domain adaptation for citation classification and therefore need a target dataset of citations and a non-citation source dataset
p8584
aVDue to the infrequent use of negative citations, a substantial annotation effort (annotating over 5 times more data) would be necessary to reach 1000 negative citation instances, which is the number of negative instances in a single domain in the multi-domain corpus described below
p8585
aVThe DFKI Citation Corpus 2 2 https://aclbib.opendfki.de/repos/trunk/citation_classification_dataset/ has been used for classifying citation function [ 13 ] , but the dataset also includes polarity annotation
p8586
aV190 are labeled as positive , 57 as negative , and the vast majority, 1521, are left neutral
p8587
aVBecause each of the citation corpora is of modest size we combine them to form one citation dataset, which we will refer to as CITD
p8588
aVSince mSDA also makes use of large amounts of unlabeled data, we extend our CITD corpus with citations from the proceedings of the remaining years of the ACL, 1979 u'\u005cu2013' 2003, 2005 u'\u005cu2013' 2006, and 2009
p8589
aVReviews were preprocessed so that for each review you find a list of unigrams and bigrams with their frequency within the review
p8590
aVWe omit the citations labeled neutral from the DFKI corpus because the IMS corpus does not contain neutral annotation nor does the MDSD
p8591
aVThe MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as u'\u005cu201c' unlabeled u'\u005cu201d' rather balanced
p8592
aVFor this reason, in line with previous work using MDSD, we balance the labeled portion of the CITD corpus
p8593
aVThis is done by taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences
p8594
aV2012 ) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features
p8595
aVPrevious work in citation classification has largely focused on identifying new features for improving classification accuracy
p8596
aVA significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g.,, [ 30 , 13 ]
p8597
aV2005 ) to be useful, and neither study lists dependency relations as significant features
p8598
aVAthar ( 2011 ) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy
p8599
aVThe dark gray bar indicates the F 1 scores for the SVM baseline using the 30,000 features and the lighter gray bar shows the mSDA results
p8600
aVThe black horizontal line indicates the F 1 score for in-domain citation classification, which sometimes represents the goal for domain adaptation
p8601
aVUsing a larger training set, along with mSDA, which makes use of the unlabeled data, leads to the best results for citation classification
p8602
aVAccording to this measure, citations are most similar to the books domain
p8603
aVTherefore, it is not surprising that training on books performs well on citations, and intuitively, among the domains in the Amazon dataset, a book review is most similar to a scientific citation
p8604
aVTo see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daumé III ( 2007
p8605
aVThe results of this comparison can be seen in Table 2
p8606
aVBriefly, u'\u005cu201c' All u'\u005cu201d' trains on source and target data; u'\u005cu201c' Weight u'\u005cu201d' is the same as u'\u005cu201c' All u'\u005cu201d' except that instances may be weighted differently based on their domain (weights are chosen on a development set); u'\u005cu201c' Pred u'\u005cu201d' trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions; u'\u005cu201c' LinInt u'\u005cu201d' linearly interpolates predictions using the source-only and target-only models (the interpolation parameter is chosen on a development set); u'\u005cu201c' Augment u'\u005cu201d' uses a larger feature set with source-specific and target-specific copies of features; see [ 12 ] for further details
p8607
aVAlthough they are not quite as high as other published results for citation polarity [ 1 ] 7 7 Their work included a CRF model to identify the citation context that gave them an increase of 9.2 percent F 1 over a single sentence citation context
p8608
aVOur approach achieves similar macro- F 1 on only the citation sentence, but using a different corpus we have shown that you can improve citation polarity classification by leveraging large amounts of annotated data from other domains and using a simple set of features
p8609
aVWe do not present those results here due to space constraints
p8610
aVThe combination led to mixed results adding mSDA to the supervised approaches tended to improve F 1 over those approaches but results never exceeded the top mSDA numbers in Table 2
p8611
aVWe thank the DFG for funding this work (SPP 1335 Scalable Visual Analytics
p8612
asg88
(lp8613
sg90
(lp8614
sg92
(lp8615
VRobust citation classification has been hindered by the relative lack of annotated data.
p8616
aVIn this paper we successfully use a large, out-of-domain, annotated corpus to improve the citation polarity classification.
p8617
aVOur approach uses a deep learning neural network for domain adaptation with labeled out-of-domain data and unlabeled in-domain data.
p8618
aVThis semi-supervised domain adaptation approach outperforms the in-domain citation polarity classification and other fully supervised domain adaptation approaches.
p8619
aVAcknowledgments.
p8620
aVWe thank the DFG for funding this work (SPP 1335 Scalable Visual Analytics.
p8621
ag106
asg107
S'P14-2008'
p8622
sg109
(lp8623
VRecent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering.
p8624
aVWhile this result confirms that citation classification is feasible, there are two drawbacks to this approach i) it requires a large annotated corpus for supervised classification, which in the case of scientific literature is quite expensive; and (ii) feature engineering that is too specific to one area of scientific literature may not be portable to other domains, even within scientific literature.
p8625
aVIn this paper we address these two drawbacks.
p8626
aVFirst, we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains.
p8627
aVThen, to avoid over-engineering specific citation features for a particular scientific domain, we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features.
p8628
aVWe achieve better citation classification results with this cross-domain approach than using in-domain classification.
p8629
ag106
asba(icmyPackage
FText
p8630
(dp8631
g3
(lp8632
VWe then use this weak supervision to u'\u005cu201c' sprinkle u'\u005cu201d' artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations
p8633
aVIn this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) [] which does not need labeled documents
p8634
aV[] used LDA topics as features in text classification, but they use labeled documents while learning a classifier sLDA [] , DiscLDA [] and MedLDA [] are few extensions of LDA which model both class labels and words in the documents
p8635
aVThese models can be used for text classification, but they need expensive labeled documents
p8636
aVIn this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words
p8637
aVWe extend ClassifyLDA algorithm by u'\u005cu201c' sprinkling u'\u005cu201d' topics to unlabeled documents
p8638
aVThe basic idea involves encoding of class labels as artificial words which are u'\u005cu201c' sprinkled u'\u005cu201d' (appended) to training documents
p8639
aVAs LSI uses higher order word associations [] , sprinkling of artificial words gives better and class-enriched latent semantic structure
p8640
aVHowever, Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents
p8641
aVThe paper revolves around the idea of labeling topics (which are far fewer in number compared to documents) as in ClassifyLDA, and using these labeled topic for sprinkling
p8642
aVAs in ClassifyLDA, we ask an annotator to assign class labels to a set of topics inferred on the unlabeled training documents
p8643
aVWe then infer a set of topics on the sprinkled training documents
p8644
aVAs LDA uses higher order word associations [] while discovering topics, we hypothesize that sprinkling will improve text classification performance of ClassifyLDA
p8645
aVAn important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling
p8646
aVThe third type of semi-supervised text classification algorithms is based on active learning
p8647
aVThese topics are very few, when compared to the number of documents
p8648
aVAs the most probable words of topics are representative of the dataset, there is no need for the annotator to search for the right set of features for each class
p8649
aVAs LDA topics are semantically more meaningful than individual words and can be acquired easily, our approach overcomes limitations of the semi-supervised methods discussed above
p8650
aVIf a document d belongs to the class c1 then a set of artificial words which represent the class c1 are appended into the document d, otherwise a set of artificial words which represent the class c2 are appended
p8651
aVSingular Value Decomposition (SVD) is then performed on the sprinkled training documents and a lower rank approximation is constructed by ignoring dimensions corresponding to lower singular values
p8652
aVWe then ask a human annotator to assign one or more class labels to the topics based on their most probable words
p8653
aVIf the topic assigned to the word w at the position n in document d is t, then we replace it by the class label assigned to the topic t
p8654
aVIf more than one class labels are assigned to the topic t, then we randomly select one of the class labels assigned to the topic t
p8655
aVIf the annotator is unable to label a topic then we randomly select a class label from the set of all class labels
p8656
aVIf a word in a document is a sprinkled word then while sampling a class label for it, we sample the class label associated with the sprinkled word, otherwise we sample a class label for the word using Gibbs update in Equation 1
p8657
aVWe name this model as Topic Sprinkled LDA (TS-LDA
p8658
aVWhile classifying a test document, its probability distribution over class labels is inferred using TS-LDA model and it is classified to its most probable class label
p8659
aVWe evaluate and compare our text classification algorithm by computing Macro averaged F1
p8660
aVAs the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average MacroF1
p8661
aVWe randomly split SRAA dataset such that 80% is used as training data and remaining is used as test data
p8662
aVThe task is to classify the webpages as student, course, faculty or project
p8663
aVWe randomly split this dataset such that 80% is used as training and 20% is used as test data
p8664
aVWe preprocess these datasets by removing HTML tags and stop-words
p8665
aVWe should note here that in TS-LDA, the annotator only labels a few topics and not a single document
p8666
aVHence, our approach exerts a low cognitive load on the annotator, at the same time achieves text classification performance close to LDA-SVM which needs labeled documents
p8667
aVWe can observe here that these two topics are more coherent than the topics in Table 3
p8668
aVHence, we can say here that, in addition to text classification, sprinkling improves coherence of topics
p8669
aVIf the annotator assigns a wrong class label to a topic representing multiple classes (e.g., first topic in Table 3), then it may affect the performance of the resulting classifier
p8670
aVHowever, in our approach the annotator can assign multiple class labels to a topic, hence our approach is more flexible for the annotator to encode her domain knowledge efficiently
p8671
aVIn this paper we propose a novel algorithm that classifies documents based on class labels over few topics
p8672
asg88
(lp8673
sg90
(lp8674
sg92
(lp8675
VIn this paper we propose a novel algorithm that classifies documents based on class labels over few topics.
p8676
aVThis reduces the need to label a large collection of documents.
p8677
aVWe have used the idea of sprinkling originally proposed in the context of supervised Latent Semantic Analysis, but the setting here is quite different.
p8678
aVUnlike the work in (Chakraborti et al., 2007), we do not assume that we have class labels over the set of training documents.
p8679
aVInstead, to realize our goal of reducing knowledge acquisition overhead, we propose a way of propagating knowledge of few topic labels to the words and inducing a new topic distribution that has its topics more closely aligned to the class labels.
p8680
aVThe results show that the approach can yield performance comparable to entirely supervised settings.
p8681
aVIn future work, we also envision the possibility of sprinkling knowledge from background knowledge sources like Wikipedia (Gabrilovich and Markovitch, 2007) to realize an alignment of topics to Wikipedia concepts.
p8682
aVWe would like to study effect of change in number of topics on the text classification performance.
p8683
aVWe will also explore techniques which will help annotators to encode their domain knowledge efficiently when the topics are not well aligned to the class labels.
p8684
ag106
asg107
S'P14-2010'
p8685
sg109
(lp8686
VSupervised text classification algorithms require a large number of documents labeled by humans, that involve a labor-intensive and time consuming process.
p8687
aVIn this paper, we propose a weakly supervised algorithm in which supervision comes in the form of labeling of Latent Dirichlet Allocation (LDA) topics.
p8688
aVWe then use this weak supervision to sprinkle artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations.
p8689
aVWe evaluate this approach to improve performance of text classification on three real world datasets.
p8690
ag106
asba(icmyPackage
FText
p8691
(dp8692
g3
(lp8693
VWe systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information
p8694
aVRecent research in this area, whether feature-based [ Kambhatla2004 , Boschee et al.2005 , Zhou et al.2005 , Grishman et al.2005 , Jiang and Zhai2007a , Chan and Roth2010 , Sun et al.2011 ] or kernel-based [ Zelenko et al.2003 , Bunescu and Mooney2005a , Bunescu and Mooney2005b , Zhang et al.2006 , Qian et al.2008 , Nguyen et al.2009 ] , attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources
p8695
aV+ It is unclear if this approach can encode real-valued features of words (such as word embeddings [ Mnih and Hinton2007 , Collobert and Weston2008 ] ) effectively
p8696
aVAs the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains
p8697
aVIn this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more naturally and effectively
p8698
aVIn DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains
p8699
aVIn traditional machine learning where the challenge is to utilize the training data to make predictions on unseen data points (generated from the same distribution as the training data), the classifier with a good generalization performance is the one that not only fits the training data, but also avoids ovefitting over it
p8700
aVExploiting the shared interest in generalization performance with traditional machine learning, in domain adaptation for RE, we would prefer the relation extractor that fits the source domain data, but also circumvents the overfitting problem over this source domain 1 1 domain overfitting [ Jiang and Zhai2007b ] so that it could generalize well on new domains
p8701
aVEventually, regularization methods can be considered naturally as a simple yet general technique to cope with DA problems
p8702
aVFollowing Plank and Moschitti [ Plank and Moschitti2013 ] , we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data
p8703
aVFinally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization [ Jiang and Zhai2007b ]
p8704
aV[ Socher et al.2012 ] and Khashabi [ Khashabi2013 ] use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE
p8705
aVHowever, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper
p8706
aV[ Blitzer et al.2006 ] propose structural correspondence learning (SCL) while Huang and Yates [ Huang and Yates2010 ] attempt to learn a multi-dimensional feature representation
p8707
aVUnfortunately, these methods require unlabeled target domain data which are unavailable in our single-system setting of DA
p8708
aVHowever, these methods assume some labeled data in target domains and are thus not applicable in our setting of unsupervised DA
p8709
aVAbove all, we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research
p8710
aVWe consider two types of word representations and use them as additional features in our DA system, namely Brown word clustering [ Brown et al.1992 ] and word embeddings [ Bengio et al.2001 ]
p8711
aVWhile word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, low-dimensional, and real-valued vectors (distributed representations
p8712
aVThis issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available
p8713
aVTherefore, following the settings of Plank and Moschitti [ Plank and Moschitti2013 ] , we will only assume entity boundaries and not rely on the gold standard information in the experiments
p8714
aVWe apply the same feature set as Sun et al
p8715
aV[ Sun et al.2011 ] but remove the entity and mention type information 2 2 We have the same observation as Plank and Moschitti [ Plank and Moschitti2013 ] that when the gold-standard labels are used, the impact of word representations is limited since the gold-standard information seems to dominate
p8716
aVHowever, whenever the gold labels are not available or inaccurate, the word representations would be useful for improving adaptability performance
p8717
aV[ Sun et al.2011 ] , we first group lexical features into 4 groups and rank their importance based on linguistic intuition and illustrations of the contributions of different lexical features from various feature-based RE systems
p8718
aVAfter that, we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance
p8719
aVOur relation extraction system is hierarchical [ Bunescu and Mooney2005b , Sun et al.2011 ] and apply maximum entropy (MaxEnt) in the MALLET 3 3 http://mallet.cs.umass.edu/ toolkit as the machine learning tool
p8720
aVFor Brown word clusters, we directly apply the clustering trained by Plank and Moschitti [ Plank and Moschitti2013 ] to facilitate system comparison later
p8721
aVWe use the ACE 2005 corpus for DA experiments (as in Plank and Moschitti [ Plank and Moschitti2013 ]
p8722
aVIt involves 6 relation types and 6 domains broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un
p8723
aVWe follow the standard practices on ACE [ Plank and Moschitti2013 ] and use news (the union of bn and nw) as the source domain and bc , cts and wl as our target domains
p8724
aVWe take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already
p8725
aVIntroducing embeddings to words of mentions alone has mild impact while it is generally a bad idea to augment chunk heads and words in the contexts
p8726
aVWe evaluate word cluster and embedding (denoted by ED) features by adding them individually as well as simultaneously into the baseline feature set
p8727
aVFor word clusters, we experiment with two possibilities i) only using a single prefix length of 10 (as Plank and Moschitti [ Plank and Moschitti2013 ] did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC
p8728
aVRow 4 shows that word embedding itself is also very useful for domain adaptation in RE since it improves the baseline system for all the target domains
p8729
aVHowever, in domain cts, the improvement that word embeddings provide for word clusters is modest
p8730
aVThis is because the RCV1 corpus used to induce the word embeddings [ Turian et al.2010 ] does not cover spoken language words in cts very well
p8731
asg88
(lp8732
sg90
(lp8733
sg92
(lp8734
g1538
asg107
S'P14-2012'
p8735
sg109
(lp8736
VRelation extraction suffers from a performance loss when a model is applied to out-of-domain data.
p8737
aVThis has fostered the development of domain adaptation techniques for relation extraction.
p8738
aVThis paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems.
p8739
aVWe systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information.
p8740
aVFinally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors.
p8741
ag106
asba(icmyPackage
FText
p8742
(dp8743
g3
(lp8744
VAll possible NE candidates are represented as nodes in the graph and associations between different candidates are represented by edges between the nodes
p8745
aVNamed entities (NEs) have received much attention over the last two decades [ 14 ] , mostly focused on recognizing the boundaries of textual NE mentions and classifying them as, e.g.,, Person, Organization or Location
p8746
aVHowever, references to entities in the real world are often ambiguous there is a many-to-many relation between NE mentions and the entities they denote in the real world
p8747
aVThe problem is that other textual mentions in the document are also ambiguous
p8748
aVSo, what is needed is a collective disambiguation approach that jointly disambiguates all NE textual mentions
p8749
aVOur approach first ranks all nodes in the solution graph using the Page-Rank algorithm, then re-ranks all nodes by combining the initial confidence and graph ranking scores
p8750
aVWe consider several different measures for computing the initial confidence assigned to each node and several measures for determining and weighting the graph edges
p8751
aVEL is a similar but broader task than NED because NED is concerned with disambiguating a textual NE mention where the correct entity is known to be one of the KB entries, while EL also requires systems to deal with the case where there is no entry for the NE in the reference KB
p8752
aVThese approaches try to model the interdependence between the different candidate entities for different NE mentions in the query document, and reformulate the problem of NED as a global optimization problem whose aim is to find the best set of entities
p8753
aVAs this new formulation is NP-hard, many approximations have been proposed
p8754
aVHoffert [ 10 ] poses the problem as one of finding a dense sub-graph, which is infeasible in a huge graph
p8755
aVSo, an algorithm originally used to find strongly interconnected, size-limited groups in social media is adopted to prune the graph, and then a greedy algorithm is used to find the densest graph
p8756
aVThe graph nodes are formulated as a set V = { ( m i , e i , j ) u'\u005cu2223' u'\u005cu2200' e i , j u'\u005cu2208' E i , u'\u005cu2200' m i u'\u005cu2208' M }
p8757
aVNodes are represented as ordered pairs of textual mentions and candidate entities, since the same entity may be found multiple times as a candidate for different textual mentions and each occurrence must be evaluated independently
p8758
aVWe adopted a new mention-candidate similarity function, j u'\u005cu2062' w u'\u005cu2062' S u'\u005cu2062' i u'\u005cu2062' m , which uses Jaro-Winkler similarity as a first estimate of the initial confidence value for each candidate
p8759
aVThis function considers all terms found in the candidate entity KB entry title, but not in the textual mention as disambiguation terms
p8760
aVThe percentage of disambiguation terms found in the query document is used to boost in the initial j u'\u005cu2062' w u'\u005cu2062' S u'\u005cu2062' i u'\u005cu2062' m value, in addition to an acronym check (whether the NE textual mention could be an acronym for a specific candidate entity title
p8761
aVThe cosine similarity between the sentence containing the NE mention in the query document and the textual description of the candidate NE in the KB (we use the first section of the Wikipedia article as the candidate entity description
p8762
aVGlobal confidence is a measure of the global importance of the candidate entity
p8763
aVEntity popularity has been used successfully as a discriminative feature for NED [ 15 ]
p8764
aVFreebase provides an API to get an entity u'\u005cu2019' s popularity score ( FB ), which is computed during Freebase indexing
p8765
aVThe initial confidence is not normalized across all NEs because each score is calculated independently
p8766
aVIt is not based on context, so it is always the same regardless of the query document
p8767
aVCoherence is represented as an edge between nodes in the solution graph
p8768
aVWe used two measures for coherence, described as follows
p8769
aVUses the Wikipedia documents for both entity candidates to check if either document has a link to the other
p8770
aVThis relation is directed, but we assume an inverse relation also exists; so this relation is represented as undirected
p8771
aVThe first step is initial graph ranking, where all nodes are ranked according to the link structure
p8772
aVThe second step is to re-rank the nodes by combining the graph rank with the initial confidence
p8773
aVThe highest rank is not always correct, so in the third step a selection algorithm is used to choose the best candidate
p8774
aVAll nodes in the graph are ranked according to these relations using PR
p8775
aVInitial confidence is used as an initial rank for the graph nodes, while entities u'\u005cu2019' coherence measures are used as link weights which play a role in distributing a node u'\u005cu2019' s rank over its outgoing nodes
p8776
aVIn our case this is not appropriate, so the final rank for each mention is determined after graph ranking, by combining the graph rank with the initial confidence
p8777
aVLet us refer to the graph rank of a candidate as P u'\u005cu2062' R u'\u005cu2062' ( e i
p8778
aVTwo combination schemes are used
p8779
aVThe simplest approach is to select the highest ranked entity in the list for each mention m i according to equation 5 , where R could refer to R m or R s
p8780
aVHowever, we found that a dynamic choice between the re-ranking schemes, based on the difference between the top two candidates, as described in algorithm 4 and indicated by e g ,works best
p8781
aVThe underlying intuition of this algorithm is that a greater difference between the top ranks reflects more confident discrimination between candidates
p8782
aVSo, the two combination schemes assign different ranks to the candidates and the algorithm selects the scheme which appears more discriminative
p8783
aVWe used AIDA dataset 3 3 http://www.mpi-inf.mpg.de/yago-naga/aida/ , which is based on the CoNLL 2003 data for NER tagging
p8784
aVWe only consider NE mentions with an entry in the Wikipedia KB, ignoring the 20% of query mentions (7136) without a link to the KB, as Hoffart did
p8785
aVTo study graph ranking using PR, and the contributions of the initial confidence and entity coherence, experiments were carried out using PR in different modes and with different selection techniques
p8786
aVIn the first experiment, referred to as P u'\u005cu2062' R I , initial confidence is used as an initial node rank for PR and edge weights are uniform, edges, as in the PR baseline, being created wherever REF or JProb are not zero
p8787
aVWhen comparing these results to the PR baseline we notice a slight positive effect when using the initial confidence as an initial rank instead of uniform ranking
p8788
aVThe major improvement comes from re-ranking nodes by combining initial confidence with PR score
p8789
aVIn our second experiment, P u'\u005cu2062' R C , entity coherence features are tested by setting the edge weights to the coherence score and using uniform initial node weights
p8790
aVWe compared JProb and Ref edge weighting approaches, where for each approach edges were created only where the coherence score according to the approach was non-zero
p8791
aVTo compare our results with the state-of-the-art, we report Hoffart et al u'\u005cu2019' s [ 10 ] results as they re-implemented two other systems and also ran them over the AIDA dataset
p8792
aVOur results show that Page-Rank in conjunction with re-ranking by initial confidence score can be used as an effective approach to collectively disambiguate named entity textual mentions in a document
p8793
aVIn future work we plan to explore enriching the edges between nodes by incorporating semantic relations extracted from an ontology
p8794
asg88
(lp8795
sg90
(lp8796
sg92
(lp8797
VOur results show that Page-Rank in conjunction with re-ranking by initial confidence score can be used as an effective approach to collectively disambiguate named entity textual mentions in a document.
p8798
aVOur proposed features are very simple and easy to extract, and work well when employed in PR.
p8799
aVIn future work we plan to explore enriching the edges between nodes by incorporating semantic relations extracted from an ontology.
p8800
ag106
asg107
S'P14-2013'
p8801
sg109
(lp8802
VNamed Entity Disambiguation (NED) refers to the task of mapping different named entity mentions in running text to their correct interpretations in a specific knowledge base (KB.
p8803
aVThis paper presents a collective disambiguation approach using a graph model.
p8804
aVAll possible NE candidates are represented as nodes in the graph and associations between different candidates are represented by edges between the nodes.
p8805
aVEach node has an initial confidence score, e.g., entity popularity.
p8806
aVPage-Rank is used to rank nodes and the final rank is combined with the initial confidence for candidate selection.
p8807
aVExperiments on 27,819 NE textual mentions show the effectiveness of using Page-Rank in conjunction with initial confidence.
p8808
aV87% accuracy is achieved, outperforming both baseline and state-of-the-art approaches.
p8809
ag106
asba(icmyPackage
FText
p8810
(dp8811
g3
(lp8812
VThis paper presents an approach to query construction to detect multilingual dictionaries for predetermined language combinations on the web, based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents
p8813
aVParallel corpus construction is the task of automatically detecting document sets that contain the same content in different languages, commonly based on a combination of site-structural and content-based features [ 3 , 19 ]
p8814
aVSuch methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis
p8815
aVMethods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g., based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English [ 16 , 24 ]
p8816
aVAlternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity [ 14 , 7 ]
p8817
aVWhile the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not
p8818
aVHere, multi-label document classification methods have been adapted to identify what mix of languages is present in a given document, which could be used as a pre-filter to locate documents containing a given mixture of languages, although there is, of course, no guarantee that a multilingual document is a dictionary
p8819
aVFinally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exhaustively to all documents on the web
p8820
aVA variety of document genre methods have been proposed, generally based on a mixture of structural and content-based features [ 12 , 5 , 25 ]
p8821
aVWhile all of these lines of research are relevant to this work, as far as we are aware, there has not been work which has proposed a direct method for identifying pre-existing multilingual dictionaries in document collections
p8822
aVOur method is based on a query formulation approach, and querying against a pre-existing index of a document collection (e.g., the web) via an information retrieval system
p8823
aVThe first intuition underlying our approach is that certain words are a priori more u'\u005cu201c' language-discriminating u'\u005cu201d' than others, and should be preferred in query construction (e.g., sushi occurs as a [transliterated] word in a wide variety of languages, whereas anti-discriminatory is found predominantly in English documents
p8824
aVAs such, we prefer search terms w i with a higher value for max l P ( l w i ) , where l is the language of interest
p8825
aVThe second intuition is that the lexical coverage of dictionaries varies considerably, especially with multilingual lexicons, which are often compiled by a single developer or small community of developers, with little systematicity in what is including or not included in the dictionary
p8826
aVAs such, if we are to follow a query construction approach to lexicon discovery, we need to be able to predict the likelihood of a given word w i being included in an arbitrarily-selected dictionary D l incorporating language l (i.e., P ( w i
p8827
aVThe third intuition is that certain word combinations are more selective of multilingual dictionaries than others, i.e., if certain words are found together (e.g., cruiser , gospel and noodle ), the containing document is highly likely to be a dictionary of some description rather than a u'\u005cu201c' conventional u'\u005cu201d' document
p8828
aVBelow, we describe our methodology for query construction based on these elements in greater detail
p8829
aVThe only assumption on the method is that we have access to a selection of dictionaries D (mono- or multilingual) and a corpus of conventional (non-dictionary) documents C , and knowledge of the language(s) contained in each dictionary and document
p8830
aVFor instance, a well-developed, mature multilingual dictionary may contain over 100,000 multilingual lexical records, while a specialised 5-way multilingual domain dictionary may contain as few as 100 multilingual lexical records
p8831
aVIn line with our second criterion, we want to select words which have a higher likelihood of occurrence in a multilingual dictionary involving that language
p8832
aVIn all experiments in this paper, we assume that we have access to at least one multilingual dictionary containing each of our target languages, but in absence of such a dictionary, sdict u'\u005cu2062' ( w i , l ) could be set to 1 for all words w i , l in the language
p8833
aVThe result of this term weighing is a ranked list of words for each language
p8834
aVWe perform query construction for each language based on frequent item set mining, using the Apriori algorithm [ 1 ]
p8835
aVFor a given combination of languages (e.g., English and Swaheli), queries are then formed simply by combining monolingual queries for the component languages
p8836
aVBased on the assumption that querying a (pre-indexed) document collection is relatively simple, we generate a range of queries of decreasing length and increasing likelihood of term co-occurrence in standard documents, and query until a non-empty set of results is returned
p8837
aVNote that the actual calculation of this co-occurrence can be performed efficiently, as a) for a given iteration of Apriori, it only needs to be performed between the new word that we are adding to the query ( u'\u005cu201c' item set u'\u005cu201d' in the terminology of Apriori) and each of the other words in a non-zero support itemset from the previous iteration of the algorithm (which are guaranteed to not co-occur with each other); and (b) the determination of whether two terms collocate can be performed efficiently using an inverted index of Wikipedia for that language
p8838
aVagainst a synthetic dataset, whereby we injected bilingual dictionaries into a collection of web documents, and evaluated the ability of the method to return multilingual dictionaries for individual languages; in this, we naively assume that all web documents in the background collection are not multilingual dictionaries, and as such, the results are potentially an underestimate of the true retrieval effectiveness
p8839
aVNote that the first evaluation with the synthetic dataset is based on monolingual dictionary retrieval effectiveness because we have very few (and often no) multilingual dictionaries for a given pairing of our target languages
p8840
aVFor a given language, we are thus evaluating the ability of our method to retrieve multilingual dictionaries containing that language (and other indeterminate languages
p8841
aVFor both the synthetic dataset and open web experiments, we evaluate our method based on mean average precision (MAP), that is the mean of the average precision scores for each query which returns a non-empty result set
p8842
aVTo train our method, we use 52 bilingual Freedict [ 6 ] dictionaries and Wikipedia 1 1 Based on 2009 dumps documents for each of our target languages
p8843
aVAs there are no bilingual dictionaries in Freedict for Chinese and Japanese, the training of Score values is based on the Wikipedia documents only
p8844
aVThe synthetic dataset was constructed using a subset of ClueWeb09 [ 4 ] as the background web document collection
p8845
aVThe original ClueWeb09 dataset consists of around 1 billion web pages in ten languages that were collected in January and February 2009
p8846
aVFirst, we present results over the synthetic dataset in Table 3
p8847
aVAs our baseline, we simply query for the language name and the term dictionary in the local language (e.g., English dictionary , for English) in the given language
p8848
aVThe comparably low result for English is potentially affected by its prevalence both in the bilingual dictionaries in training (restricting the effective vocabulary size due to our L l filtering), and in the document collection
p8849
aVLooking next to the open web, we present in Table 4 results based on querying the Google search API with the 1000 longest queries for English paired with each of the other 7 target languages
p8850
aVThe results in Table 4 are based on manual evaluation of all documents returned for the first 50 queries, and determination of whether they were multilingual dictionaries containing the indicated languages
p8851
aVDespite this, the results for our method are lower than those over the synthetic dataset, we suspect largely as a result of the style of queries we issue being so far removed from standard Google query patterns
p8852
aVThis research was supported by funding from the Group of Eight and the Australian Research Council
p8853
asg88
(lp8854
sg90
(lp8855
sg92
(lp8856
VWe have described initial results for a method designed to automatically detect multilingual dictionaries on the web, and attained highly credible results over both a synthetic dataset and an experiment over the open web using a web search engine.
p8857
aVIn future work, we hope to explore the ability of the method to detect domain-specific dictionaries (e.g., training over domain-specific dictionaries from other language pairs), and low-density languages where there are few dictionaries and Wikipedia articles to train the method on.
p8858
ag106
asg107
S'P14-2016'
p8859
sg109
(lp8860
VThis paper presents an approach to query construction to detect multilingual dictionaries for predetermined language combinations on the web, based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents.
p8861
aVWe use eight target languages for our case study, and train our method on pre-identified multilingual dictionaries and the Wikipedia dump for each of our languages.
p8862
ag106
asba(icmyPackage
FText
p8863
(dp8864
g3
(lp8865
VBased on the assumption that these linguistic changes follow certain rules, we propose a method for automatically detecting pairs of cognates employing an orthographic alignment method which proved relevant for sequence alignment in computational biology
p8866
aVWe use aligned subsequences as features for machine learning algorithms in order to infer rules for linguistic changes undergone by words when entering new languages and to discriminate between cognates and non-cognates
p8867
aVThe wide range of applications in which cognates prove useful attracted more and more attention on methods for detecting such related pairs of words
p8868
aVThis task is most challenging for resource-poor languages, for which etymologically related information is not accessible
p8869
aVTherefore, the research [ 17 , 25 , 16 ] focused on automatic identification of cognate pairs, starting from lists of known cognates
p8870
aVFinally, in Section 5 we draw the conclusions of our study and describe our plans for extending the method
p8871
aVFor measuring phonetic and orthographic proximity of cognate candidates, string similarity metrics can be applied, using the phonetic or orthographic word forms as input
p8872
aVVarious measures were investigated and compared [ 17 , 14 ] ; Levenshtein distance [ 22 ] , XDice [ 3 ] and the longest common subsequence ratio [ 24 ] are among the most frequently used metrics in this field
p8873
aVGomes and Lopes ( 2011 ) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words
p8874
aVAlgorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic
p8875
aVKondrak ( 2000 ) developed the ALINE system, which aligns words u'\u005cu2019' phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming
p8876
aV2013 ) proposed a method for cognate production relying on statistical character-based machine translation, learning orthographic production patterns, and Mulloni ( 2007 ) introduced an algorithm for cognate production based on edit distance alignment and the identification of orthographic cues when words enter a new language
p8877
aVWe assume that rules for adapting foreign words to the orthographic system of the target languages might not have been very well defined in their period of early development, but they may have since become complex and probably language-specific
p8878
aVDetecting pairs of cognates based on etymology is useful and reliable, but, for resource-poor languages, methods which require less linguistic knowledge might be necessary
p8879
aVAccording to Gusfield ( 1997 ) , an edit transcript (representing the conversion of one string to another) and an alignment are mathematically equivalent ways of describing relationships between strings
p8880
aVTherefore, because the edit distance was widely used in this research area and produced good results, we are encouraged to employ orthographic alignment for identifying pairs of cognates, not only to compute similarity scores, as was previously done, but to use aligned subsequences as features for machine learning algorithms
p8881
aVOur intuition is that inferring language-specific rules for aligning words will lead to better performance in the task of cognate identification
p8882
aVString alignment is closely related to the task of sequence alignment in computational biology
p8883
aVTherefore, to align pairs of words we employ the Needleman-Wunsch global alignment algorithm [ 29 ] , which is mainly used for aligning sequences of proteins or nucleotides
p8884
aVThe algorithm uses dynamic programming and, thus, guarantees to find the optimal alignment
p8885
aVIts main idea is that any partial path of the alignment along the optimal path should be the optimal path leading up to that point
p8886
aVTherefore, the optimal path can be determined by incremental extension of the optimal subpaths [ 31 ]
p8887
aVFor orthographic alignment, we consider words as input sequences and we use a very simple substitution matrix, which gives equal scores to all substitutions, disregarding diacritics (e.g.,, we ensure that e and è are matched
p8888
aVUsing aligned pairs of words as input, we extract features around mismatches in the alignments
p8889
aVThe second alternative leads to better performance, so we account for all mismatches
p8890
aVAs for the length of the grams, we experiment with n u'\u005cu2208' { 1 , 2 , 3 }
p8891
aVWe achieve slight improvements by combining n -grams, i.e.,, for a given n , we use all i -grams, where i u'\u005cu2208' { 1 , u'\u005cu2026' , n }
p8892
aVIn order to provide information regarding the position of the features, we mark the beginning and the end of the word with a $ symbol
p8893
aVThus, for the above-mentioned pair of cognates, (exhaustiv, esaustivo) , we extract the following features when n = 2
p8894
aVFor identical features we account only once
p8895
aVTherefore, because there is one feature ( xh s- ) which occurs twice in our example, we have 8 features for the pair (exhaustiv, esaustivo)
p8896
aVWe use Naive Bayes as a baseline and we experiment with Support Vector Machines (SVMs) to learn orthographic changes and to discriminate between pairs of cognates and non-cognates
p8897
aVWe use the radial basis function kernel (RBF), which can handle the case when the relation between class labels and attributes is non-linear, as it maps samples non-linearly into a higher dimensional space
p8898
aVWe discard pairs of words for which the forms across languages are identical (i.e.,, the Romanian word matrice and its Italian cognate pair matrice , having the same form), because these pairs do not provide any orthographic changes to be learned
p8899
aVFinally, we obtain 445 pairs of cognates for Romanian-French 2 2 The number of pairs of cognates is much lower for French than for the other languages because there are numerous Romanian words which have French etymology and, in this paper, we do not consider these words to be cognate candidates
p8900
aVBecause we need sets of approximately equal size for comparison across languages, we keep 400 pairs of cognates and 400 pairs of non-cognates for each pair of languages
p8901
aVFor Portuguese, both Naive Bayes and SVM misclassify more non-cognates as cognates than viceversa
p8902
aVA possible explanation might be the occurrence, in the dataset, of more remotely related words, which are not labeled as cognates
p8903
aVWe investigate the performance of the method we propose in comparison to previous approaches for automatic detection of cognate pairs based on orthographic similarity
p8904
aVIn addition, we use SpSim [ 11 ] , which outperformed the longest common subsequence ratio and a similarity measure based on the edit distance in previous experiments
p8905
aVTo evaluate these metrics on our dataset, we use the same train/test sets as we did in our previous experiments and we follow the strategy described in [ 17 ]
p8906
aVFirst, we compute the pairwise distances between pairs of words for each orthographic metric individually, as a single feature 5 5 SpSim cannot be computed directly, as the other metrics, so we introduce an additional step in which we use 1/3 of the training set (only cognates are needed) to learn orthographic changes
p8907
aVIn order to detect the best threshold for discriminating between cognates and non-cognates, we run a decision stump classifier (provided by Weka) on the training set for each pair of languages and for each metric
p8908
aVUsing the best threshold value selected for each metric and pair of languages, we further classify the pairs of words in our test sets as cognates or non-cognates
p8909
aVIn this paper we proposed a method for automatic detection of cognates based on orthographic alignment
p8910
aVWe applied our method on an automatically extracted dataset of cognates for four pairs of languages
p8911
aVAs future work, we plan to extend our method on a few levels
p8912
aVAn important achievement in this direction belongs to Delmestri and Cristianini ( 2010 ) , who introduced PAM-like matrices, linguistic-inspired substitution matrices which are based on information regarding orthographic changes
p8913
aVWe intend to investigate other approaches to string alignment, such as local alignment [ 33 ] , and other learning algorithms for discriminating between cognates and non-cognates
p8914
aVWe are interested to find out if the orthographic rules depend on the source language, or if they are rather specific to the target language
p8915
asg88
(lp8916
sg90
(lp8917
sg92
(lp8918
VIn this paper we proposed a method for automatic detection of cognates based on orthographic alignment.
p8919
aVWe employed the Needleman-Wunsch algorithm [ 29 ] for sequence alignment widely-used in computational biology and we used aligned pairs of words to extract rules for lexical changes occurring when words enter new languages.
p8920
aVWe applied our method on an automatically extracted dataset of cognates for four pairs of languages.
p8921
aVAs future work, we plan to extend our method on a few levels.
p8922
aVIn this paper we used a very simple substitution matrix for the alignment algorithm, but the method can be adapted to integrate historical information regarding language evolution.
p8923
aVThe substitution matrix for the alignment algorithm can be customized with language-specific information, in order to reflect the probability of a character to change into another.
p8924
aVAn important achievement in this direction belongs to Delmestri and Cristianini ( 2010 ) , who introduced PAM-like matrices, linguistic-inspired substitution matrices which are based on information regarding orthographic changes.
p8925
aVWe plan to investigate the contribution of using this type of substitution matrices for our method.
p8926
aVWe intend to investigate other approaches to string alignment, such as local alignment [ 33 ] , and other learning algorithms for discriminating between cognates and non-cognates.
p8927
aVWe plan to extend our analysis with more language-specific features, where linguistic knowledge is available.
p8928
aVFirst, we intend to use the part of speech as an additional feature.
p8929
aVWe assume that some orthographic changes are dependent on the part of speech of the words.
p8930
aVSecondly, we want to investigate whether accounting for the common ancestor language influences the results.
p8931
aVWe are interested to find out if the orthographic rules depend on the source language, or if they are rather specific to the target language.
p8932
aVFinally, we plan to make a performance comparison on cognate pairs versus word-etymon pairs and to investigate false friends [ 27 ].
p8933
aVWe further intend to adapt our method for cognate detection to a closely related task, namely cognate production, i.e.,, given an input word w , a related language L and a set of learned rules for orthographic changes, to produce the cognate pair of w in L.
p8934
ag106
asg107
S'P14-2017'
p8935
sg109
(lp8936
VWords undergo various changes when entering new languages.
p8937
aVBased on the assumption that these linguistic changes follow certain rules, we propose a method for automatically detecting pairs of cognates employing an orthographic alignment method which proved relevant for sequence alignment in computational biology.
p8938
aVWe use aligned subsequences as features for machine learning algorithms in order to infer rules for linguistic changes undergone by words when entering new languages and to discriminate between cognates and non-cognates.
p8939
aVGiven a list of known cognates, our approach does not require any other linguistic information.
p8940
aVHowever, it can be customized to integrate historical information regarding language evolution.
p8941
ag106
asba(icmyPackage
FText
p8942
(dp8943
g3
(lp8944
VA parallel treebank is a parallel corpus where the sentences in each language are syntactically (if necessary morphologically) annotated, and the sentences and words are aligned
p8945
aVOur approach converts English parse trees into equivalent Turkish parse trees by applying several transformation heuristics
p8946
aVThe treebank is based on deep Lexical-Functional Grammars that were developed within the framework of the Parallel Grammar effort
p8947
aVWhen translating an English word to a gloss in Turkish, the translator may choose from a list of glosses sorted according their likelihood calculated over their previous uses in similar cases
p8948
aVThus, as the corpus grows in size, the translators use the leverage of their previous choices
p8949
aVIn general, we try to permute the nodes so as to correspond to the order of inflectional morphemes in the chosen gloss
p8950
aVIf we embed a constituent in the morphemes of a Turkish stem, we replace the English constituent leaf with *NONE*
p8951
aVIn some cases, the personal pronouns acting as subjects are naturally embedded in the verb inflection
p8952
aVQuestion sentences require special attention during transformation
p8953
aVAs opposed to movement in English question sentences, any constituent in Turkish can be questioned by replacing it with an inflected question word
p8954
aVThe proper nouns are translated with their common Turkish gloss if there is one
p8955
aVSo, u'\u005cu201c' London u'\u005cu201d' becomes u'\u005cu201c' Londra u'\u005cu201d'
p8956
aVSubordinating conjunctions, marked as u'\u005cu201c' IN u'\u005cu201d' in English sentences, are transformed to *NONE* and the appropriate participle morpheme is appended to the stem in the Turkish translation
p8957
aVIn the first case, we use the multiword expression as the gloss
p8958
aVIn the latter case, we replace some English words with *NONE*
p8959
aVThis work constitutes the preliminary step of parallel treebank generation
p8960
aVAs a next step, we will focus on morphological analysis and disambiguation of Turkish words
p8961
aVAfter determining the correct morphological analysis of Turkish words, we will use the parts of these analyses to replace the leaf nodes that we intentionally left as u'\u005cu201c' *NONE* u'\u005cu201d'
p8962
aVAs a future work, we plan to expand the dataset to include all Penn Treebank sentences
p8963
asg88
(lp8964
sg90
(lp8965
sg92
(lp8966
VParallel treebank construction efforts increased significantly in the recent years.
p8967
aVMany parallel treebanks are produced to build statistically strong language models for different languages.
p8968
aVIn this study, we report our preliminary efforts to build such a parallel corpus for Turkish-English pair.
p8969
aVWe translated and transformed a subset of parse trees of Penn Treebank to Turkish.
p8970
aVWe cover more than 50% of all sentences with a maximum length of 15-words including punctuation.
p8971
aVThis work constitutes the preliminary step of parallel treebank generation.
p8972
aVAs a next step, we will focus on morphological analysis and disambiguation of Turkish words.
p8973
aVAfter determining the correct morphological analysis of Turkish words, we will use the parts of these analyses to replace the leaf nodes that we intentionally left as *NONE* .
p8974
aVAs a future work, we plan to expand the dataset to include all Penn Treebank sentences.
p8975
ag106
asg107
S'P14-2019'
p8976
sg109
(lp8977
VIn this paper, we report our preliminary efforts in building an English-Turkish parallel treebank corpus for statistical machine translation.
p8978
aVIn the corpus, we manually generated parallel trees for about 5,000 sentences from Penn Treebank.
p8979
aVEnglish sentences in our set have a maximum of 15 tokens, including punctuation.
p8980
aVWe constrained the translated trees to the reordering of the children and the replacement of the leaf nodes with appropriate glosses.
p8981
aVWe also report the tools that we built and used in our tree translation task.
p8982
ag106
asba(icmyPackage
FText
p8983
(dp8984
g3
(lp8985
VIn statistical machine translation (SMT), syntax-based pre-ordering of the source language is an effective method for dealing with language pairs where there are great differences in their respective word orders
p8986
aVThis paper introduces a novel pre-ordering approach based on dependency parsing for Chinese-English SMT
p8987
aVWe also investigate the accuracy of the rule set by conducting human evaluations
p8988
aVSMT systems have difficulties translating between distant language pairs such as Chinese and English
p8989
aVThe reason for this is that there are great differences in their word orders
p8990
aVReordering therefore becomes a key issue in SMT systems between distant language pairs
p8991
aVPrevious work has shown that the approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective
p8992
aVSyntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French [ 2004 ] , German-English [ 2005 ] , Chinese-English [ 2007 , 2008 ] , and English-Japanese [ 2010 ]
p8993
aVAs a kind of constituent structure, HPSG [ 1994 ] parsing-based pre-ordering showed improvements in SVO-SOV translations, such as English-Japanese [ 2010 , 2011 ] and Chinese-Japanese [ 2012 ]
p8994
aVSince dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre-ordering approaches for language pairs such as Arabic-English [ 2007 ] , and English-SOV languages [ 2009 , 2011 ]
p8995
aVThe purpose of this paper is to introduce a novel dependency-based pre-ordering approach through creating a pre-ordering rule set and applying it to the Chinese-English PBSMT system
p8996
aVBy applying our rules and Wang et al u'\u005cu2019' s rules, one can use both dependency and constituency parsers for pre-ordering in Chinese-English PBSMT
p8997
aVThis is especially important on the point of the system combination of PBSMT systems, because the diversity of outputs from machine translation systems is important for system combination [ 2013 ]
p8998
aVBy using both our rules and Wang et al u'\u005cu2019' s rules, one can obtain diverse machine translation results because the pre-ordering results of these two rule sets are generally different
p8999
aVBecause there are a lot of language specific decisions that reflect specific aspects of the source language and the language pair combination, our rule set provides a valuable resource for pre-ordering in Chinese-English PBSMT
p9000
aVBecause dependency parse trees are generally more concise than the constituent ones, they can conduct long-distance reorderings in a finer way
p9001
aVThus, we attempted to conduct pre-ordering based on dependency parsing
p9002
aVFor Chinese, there are 45 types of grammatical relations for Stanford typed dependencies [ 2009 ] and 25 for CoNLL typed dependencies
p9003
aVAs we thought that Stanford typed dependencies could describe language phenomena more meticulously owing to more types of grammatical relations, we preferred to use it for searching candidate pre-ordering rules
p9004
aVHere, both x and y are dependency relations (e.g.,, plmod or lobj in Figure 2
p9005
aVWe define the dependency structure of a dependency relation as the structure containing the dependent word (e.g.,, the word directly indicated by plmod, or u'\u005cu201c' å u'\u005cu201d' in Figure 2) and the whole subtree under the dependency relation (all of the words that directly or indirectly depend on the dependent word, or the words under u'\u005cu201c' å u'\u005cu201d' in Figure 2
p9006
aVFurther, we define X and Y as the corresponding dependency structures of the dependency relations x and y , respectively
p9007
aVWe define X \u005c Y as structure X except Y
p9008
aVSearch the Chinese dependency parse trees in the corpus and rank all of the structures matching the two types of rules respectively according to their frequencies
p9009
aV2) Filter out the structures from which it was almost impossible to derive candidate pre-ordering rules because x or y was an u'\u005cu201c' irrespective u'\u005cu201d' dependency relation, for example, root, conj, cc and so on
p9010
aVInvestigate the remaining structures
p9011
aVFor each kind of structure, we selected some of the sample dependency parse trees that contained it, tried to restructure the parse trees according to the matched rule and judged the reordered Chinese phrases
p9012
aVIf the reordering produced a Chinese phrase that had a closer word order to that of the English one, this structure would be a candidate pre-ordering rule
p9013
aVConduct primary experiments which used the same training set and development set as the experiments described in Section 3
p9014
aVIn the primary experiments, we tested the effectiveness of the candidate rules and filtered the ones that did not work based on the BLEU scores on the development set
p9015
aVAs a result, we obtained eight pre-ordering rules in total, which can be divided into three dependency relation categories
p9016
aVHowever, in English, this kind of word (e.g.,, u'\u005cu201c' front u'\u005cu201d' in the caption of Figure 2) always occur directly after prepositions, which is to say, in the second position in a prepositional phrase
p9017
aVTherefore, we applied a rule plmod lobj (localizer object) to reposition the dependent word of the plmod relation (e.g.,, u'\u005cu201c' å u'\u005cu201d' in Figure 2) to the position before the lobj structure (e.g.,, u'\u005cu201c' ç¾å½ å¤§ä½¿é¦ u'\u005cu201d' in Figure 2
p9018
aVHere u'\u005cu201c' mw u'\u005cu201d' means u'\u005cu201c' measure word u'\u005cu201d'
p9019
aVAs shown in the figure, relative clause modifiers in Chinese (e.g.,, u'\u005cu201c' æ¥è¿ å¤é ç u'\u005cu201d' in Figure 3) occurs before the noun being modified, which is in contrast to English (e.g.,, u'\u005cu201c' close to Sharon u'\u005cu201d' in the caption of Figure 3), where they come after
p9020
aVThus, we introduced a series of rules NOUN rcmod to restructure rcmod structures so that the noun is moved to the head
p9021
aVSince a noun can be nsubj, dobj (direct object), pobj (prepositional object
p9022
aVRecognizing that prep structures occur before the verb in Chinese (e.g.,, u'\u005cu201c' å¨ æ­¤ å° u'\u005cu201d' in Figure 5) but after the verb in English (usually in the last position of a verb phrase, e.g.,, u'\u005cu201c' here u'\u005cu201d' in the caption of Figure 5), we applied a rule prep - dobj to reposition prep structures after their sibling dobj structures
p9023
aVThe Berkeley Parser [ 2006 ] was employed for parsing the Chinese sentences
p9024
aVFirst, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser [ 2003 ]
p9025
aVFor the Mate Parser, POS tagged inputs are required both in training and in inference
p9026
aVThus, we then extracted the POS information from the results of the Berkeley Parser and used these as the pre-specified POS tags for the Mate Parser
p9027
aVHowever, both of them substantially decreased the total times about 60% (or 1,600,000) for pre-ordering rule applications on the training set, compared with WR07
p9028
aVIn our opinion, the reason for the great decrease was that the dependency parse trees were more concise than the constituent parse trees in describing sentences and they could also describe the reordering at the sentence level in a finer way
p9029
aVIn this case, the affect of the performance of the constituent parsers on pre-ordering is larger than that of the dependency ones so that the constituent parsers are likely to bring about more incorrect pre-orderings
p9030
aV[ 2007 ] , we carried out human evaluations to assess the accuracy of our dependency-based pre-ordering rules by employing the system u'\u005cu201c' OUR DEP 2 u'\u005cu201d' in Table 1
p9031
aVSince the accuracy check for dependency parse trees took great deal of time, we did not try to select error free (100% accurately parsed) sentences
p9032
aVThe overall accuracy of this rule set is 60.0%, which is almost at the same level as the WR07 rule set (62.1%), according to the similar evaluation (200 sentences and one annotator) conducted in Wang et al
p9033
aVIn this paper, we introduced a novel pre-ordering approach based on dependency parsing for a Chinese-English PBSMT system
p9034
aVMoreover, our dependency-based pre-ordering rule set substantially decreased the time for applying pre-ordering rules about 60% compared with WR07, on the training set of 1M sentences pairs
p9035
aVThe overall accuracy of our rule set is 60.0%, which is almost at the same level as the WR07 rule set
p9036
aVThese results indicated that dependency parsing is more effective for conducting pre-ordering for Chinese-English PBSMT
p9037
aVIn the future, we attempt to create more efficient pre-ordering rules by exploiting the rich information in dependency structures
p9038
asg88
(lp9039
sg90
(lp9040
sg92
(lp9041
VIn this paper, we introduced a novel pre-ordering approach based on dependency parsing for a Chinese-English PBSMT system.
p9042
aVThe results showed that our approach achieved a BLEU score gain of 1.61.
p9043
aVMoreover, our dependency-based pre-ordering rule set substantially decreased the time for applying pre-ordering rules about 60% compared with WR07, on the training set of 1M sentences pairs.
p9044
aVThe overall accuracy of our rule set is 60.0%, which is almost at the same level as the WR07 rule set.
p9045
aVThese results indicated that dependency parsing is more effective for conducting pre-ordering for Chinese-English PBSMT.
p9046
aVAlthough our work focused on Chinese, the ideas can also be applied to other languages.
p9047
aVIn the future, we attempt to create more efficient pre-ordering rules by exploiting the rich information in dependency structures.
p9048
ag106
asg107
S'P14-2026'
p9049
sg109
(lp9050
VIn statistical machine translation (SMT), syntax-based pre-ordering of the source language is an effective method for dealing with language pairs where there are great differences in their respective word orders.
p9051
aVThis paper introduces a novel pre-ordering approach based on dependency parsing for Chinese-English SMT.
p9052
aVWe present a set of dependency-based pre-ordering rules which improved the BLEU score by 1.61 on the NIST 2006 evaluation data.
p9053
aVWe also investigate the accuracy of the rule set by conducting human evaluations.
p9054
aVUTF8gbsn.
p9055
ag106
asba(icmyPackage
FText
p9056
(dp9057
g3
(lp9058
VIn all these cases, the misspelled word contains many errors and the corresponding error model penalty cannot be compensated by the LM weight of its proper form
p9059
aVAs a result, either the misspelled word itself, or the other (less complicated, more frequent) misspelling of the same word wins the likelihood race
p9060
aVFor this purpose we used a method based on the simulated annealing algorithm
p9061
aVThese techniques can be combined with the proposed method by replacing posterior probability of single correction in our method with an estimate obtained via discriminative training method
p9062
aVIn our work, we focus on isolated word-error correction [ 7 ] , which, in a sense, is a harder task, than multi-word correction, because there is no context available for misspelled words
p9063
aVIf hypotheses constitute a major part of the posterior probability mass, it is highly unlikely that the intended word is not among them
p9064
aVHypotheses generator is based on A* beam search in a trie of words, and yields K hypotheses h k , for which the noisy channel scores P dist ( h k u'\u005cu2192' q 1 ) P LM ( h k ) are highest possible
p9065
aVWhile choosing arg u'\u005cu2062' max of the posterior probability is an optimal decision rule in theory, in practice it might not be optimal, due to limitations of the language and error modeling
p9066
aVFor example, vobemzin is corrected to more frequent misspelling vobenzin (instead of correct form wobenzym ) by the noisy channel, because P dist ( v o b e m z i n u'\u005cu2192' w o b e n z y m ) is too low (see Table 1
p9067
aVIt is motivated by the assumption, that we are more likely to successfully correct the query if we take several short steps instead of one big step [ 3 ]
p9068
aVIterative correction is hill climbing in the space of possible corrections on each iteration we make a transition to the best point in the neighbourhood, i.e., to correction, that has maximal posterior probability P ( c q
p9069
aVAs any local search method, iterative correction is prone to local minima, stopping before reaching the correct word
p9070
aVA common method of avoiding local minima in optimization is the simulated annealing algorithm, key ideas from which can be adapted for spelling correction task
p9071
aVWith that probability defined, our correction algorithm is the following given query q , pick c = arg max c E P ( c E q ) as a correction
p9072
aVProbability of getting from c 0 = q to some c E = c is a sum, over all possible paths, of probabilities of getting from q to c via specific path q = c 0 u'\u005cu2192' c 1 u'\u005cu2192' u'\u005cu2026' u'\u005cu2192' c E - 1 u'\u005cu2192' c E = c
p9073
aVwhere W is the set of all possible words, and P observe u'\u005cu2062' ( w ) is the probability of observing w as a query in the noisy-channel model
p9074
aVExample if we start a random walk from vobemzin and make 3 steps, we most probably will end up in the correct form wobenzym with P = 0.361
p9075
aVAlso note, that the method works only because multiple misspellings of the same word are presented in our model; for related research see [ 2 ]
p9076
aVBasic building block of every mentioned algorithm is one-step noisy-channel correction
p9077
aVEach basic correction proceeds as described in Section 2.1 a small number of hypotheses h 1 , u'\u005cu2026' , h K is generated for the query q , hypotheses are scored, and scores are recomputed into normalized posterior probabilities (see Equation 5
p9078
aVA standard log-linear weighing trick was applied to noisy-channel model components, see e.g., [ 9 ] u'\u005cu039b' is the parameter that controls the trade-off between precision and recall (see Section 4.2 ) by emphasizing the importance of either the high frequency of the correction or its proximity to the query
p9079
aVTo compensate for that, posteriors were smoothed by raising each probability to some power u'\u005cu0393' 1 and re-normalizing them afterward
p9080
aVFinally, if posterior probability of the best hypothesis was lower than threshold u'\u005cu0391' , then the original query q was used as the spell-checker output
p9081
aVPosterior is defined by Equation 6 for the baseline and simple iterative methods and by Equations 3 and 6 for the proposed method
p9082
aVThe difference between datasets is that one of them contained only queries with low search performance for which the number of documents retrieved by the search engine was less than a fixed threshold (we will address it as the u'\u005cu201d' hard u'\u005cu201d' dataset), while the other dataset had no such restrictions (we will call it u'\u005cu201d' common u'\u005cu201d'
p9083
aVSupposedly, it is because the iterative method benefits primarily from the sequential application of split/join operations altering query decomposition into words; since we are considering only one-word queries, such decomposition does not matter
p9084
aVWe tested all three methods on the u'\u005cu201d' common u'\u005cu201d' dataset as well to evaluate if our handling of hard cases affects the performance of our approach on the common cases of spelling error
p9085
aVHowever, if our method is applied to shorter and more frequent queries (as opposed to u'\u005cu201d' hard u'\u005cu201d' dataset), it tends to suggest frequent words as false-positive corrections (for example, grid is corrected to creed u'\u005cu2013' Assassin u'\u005cu2019' s Creed is popular video game
p9086
asg88
(lp9087
sg90
(lp9088
sg92
(lp9089
VIn this paper we introduced the stochastic iterative correction method for spell check corrections.
p9090
aVOur experimental evaluation showed that the proposed method improved the performance of popular spelling correction approach   the noisy channel model   in the correction of difficult spelling errors.
p9091
aVWe showed how to eliminate the local minima issue of simulated annealing and proposed a technique to make our algorithm deterministic.
p9092
aVThe experiments conducted on the specialized datasets have shown that our method significantly improves the performance of the correction of hard spelling errors (by 6.6% F 1 ) while maintaining good performance on common spelling errors.
p9093
aVIn continuation of the work we are considering to expand the method to correct errors in multi-word queries, extend the method to work with discriminative models, and use a query performance prediction method, which tells for a query whether our algorithm needs to be applied.
p9094
ag106
asg107
S'P14-2028'
p9095
sg109
(lp9096
VNoisy channel models, widely used in modern spellers, cope with typical misspellings, but do not work well with infrequent and difficult spelling errors.
p9097
aVIn this paper, we have improved the noisy channel approach by iterative stochastic search for the best correction.
p9098
aVThe proposed algorithm allowed us to avoid local minima problem and improve the F 1 measure by 6.6% on distant spelling errors.
p9099
ag106
asba(icmyPackage
FText
p9100
(dp9101
g3
(lp9102
VWe develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above
p9103
aVWith this unique data set, which we will release to the research community, it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality
p9104
aVWe created a dataset consisting of 3,129 sentences randomly selected from essays written by non-native speakers of English as part of a test of English language proficiency
p9105
aVWe oversampled lower-scoring essays to increase the chances of finding ungrammatical sentences
p9106
aVDue to these errors, the sentence may have multiple plausible interpretations, as in Example ( 3
p9107
aVThe sentence contains so many errors that it would be difficult to correct, as in Example ( 4
p9108
aVThe phrase u'\u005cu201c' do not everything u'\u005cu201d' makes the sentence practically incomprehensible since the subject of u'\u005cu201c' do u'\u005cu201d' is not clear
p9109
aVThese sentences, such as Example ( 5 ), appear in our corpus due to the nature of timed tests
p9110
aVIn preliminary experiments, averaging the six judgments (1 expert, 5 crowdsourced) for each item led to higher human-machine agreement
p9111
aVFor all experiments reported later, we used this average of six judgments as our gold standard
p9112
aVFor our experiments (§ 4 ), we randomly split the data into training (50%), development (25%), and testing (25%) sets
p9113
aV4 4 Regression models typically produce conservative predictions with lower variance than the original training data
p9114
aVSo that predictions better match the distribution of labels in the training data, the system rescales its predictions
p9115
aVGiven a sentence with with n word tokens, the model filters out tokens containing nonalphabetic characters and then computes the number of misspelled words n m u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' s (later referred to as num_misspelled ), the proportion of misspelled words n m u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' s n , and log u'\u005cu2061' ( n m u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' s + 1 ) as features
p9116
aV10 10 The complete list of relevant statistics used as features is trees, unify_cost_succ, unify_cost_fail, unifications_succ, unifications_fail, subsumptions_succ, subsumptions_fail, words, words_pruned, aedges, pedges, upedges, raedges, rpedges, medges
p9117
aVa binary feature that captures whether the top node of the tree is sentential or not (i.e., the assumption is that if the top node is non-sentential, then the sentence is a fragment
p9118
aVWe also trained and evaluated on binarized versions of the ordinal GUG labels a sentence was labeled 1 if the average judgment was at least 3.5 (i.e.,, would round to 4), and 0 otherwise
p9119
aVTo train our system on binarized data, we replaced the u'\u005cu2113' 2 -regularized linear regression model with an u'\u005cu2113' 2 -regularized logistic regression and used Kendall u'\u005cu2019' s u'\u005cu03a4' rank correlation between the predicted probabilities of the positive class and the binary gold standard labels as the grid search metric (§ 3.1 ) instead of Pearson u'\u005cu2019' s r
p9120
aVFor the ordinal task, we report Pearson u'\u005cu2019' s r between the averaged human judgments and each system
p9121
aVSince the predictions from the binary and ordinal systems are on different scales, we include the nonparametric statistic Kendall u'\u005cu2019' s u'\u005cu03a4' as a secondary evaluation metric for both tasks
p9122
aVIt is very different from our system since it relies on partial tree-substitution grammar derivations as features
p9123
aVAdditionally, its classifier implementation does not output scores or probabilities
p9124
aVTherefore, we used the same learning algorithms as for our system (i.e.,, ridge regression for the ordinal task and logistic regression for the binary task
p9125
aVTo create further baselines for comparison, we selected the following features that represent ways one might approximate grammaticality if a comprehensive model was unavailable whether the link parser can fully parse the sentence ( complete_link ), the Gigaword language model score ( gigaword_avglogprob ), and the number of misspelled tokens ( num_misspelled
p9126
aVIn this paper, we developed a system for predicting grammaticality on an ordinal scale and created a labeled dataset that we have released publicly (§ 2 ) to enable more realistic evaluations in future research
p9127
aVThis is the most realistic evaluation of methods for predicting sentence-level grammaticality to date
p9128
aVWe speculate that this is due to the fact that the Post system relies heavily on features extracted from automatic syntactic parses
p9129
aVIn future work, it may be possible to improve grammaticality measurement by integrating such features into a larger system
p9130
asg88
(lp9131
sg90
(lp9132
sg92
(lp9133
VIn this paper, we developed a system for predicting grammaticality on an ordinal scale and created a labeled dataset that we have released publicly (§ 2 ) to enable more realistic evaluations in future research.
p9134
aVOur system outperformed an existing state-of-the-art system [ 16 ] in evaluations on binary and ordinal scales.
p9135
aVThis is the most realistic evaluation of methods for predicting sentence-level grammaticality to date.
p9136
aVSurprisingly, the system from Post ( 2011 ) performed quite poorly on the GUG dataset.
p9137
aVWe speculate that this is due to the fact that the Post system relies heavily on features extracted from automatic syntactic parses.
p9138
aVWhile Post found that such a system can effectively distinguish grammatical news text sentences from sentences generated by a language model, measuring the grammaticality of real sentences from language learners seems to require a wider variety of features, including n -gram counts, language model scores, etc.
p9139
aVOf course, our findings do not indicate that syntactic features such as those from Post ( 2011 ) are without value.
p9140
aVIn future work, it may be possible to improve grammaticality measurement by integrating such features into a larger system.
p9141
ag106
asg107
S'P14-2029'
p9142
sg109
(lp9143
VAutomated methods for identifying whether sentences are grammatical have various potential applications (e.g.,, machine translation, automated essay scoring, computer-assisted language learning.
p9144
aVIn this work, we construct a statistical model of grammaticality using various linguistic features (e.g.,, misspelling counts, parser outputs, n -gram language model scores.
p9145
aVWe also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale.
p9146
aVIn evaluations, we compare our system to the one from Post ( 2011 ) and find that our approach yields state-of-the-art performance.
p9147
ag106
asba(icmyPackage
FText
p9148
(dp9149
g3
(lp9150
VWith the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences [ 9 , 19 , 4 , 22 , 24 ]
p9151
aVPredicting social roles such as doctor , teacher , vegetarian , christian , may open the door to large-scale passive surveys of public discourse that dwarf what has been previously available to social scientists
p9152
aVFor example, work on tracking the spread of flu infections across Twitter [ 12 ] might be enhanced with a factor based on aggregate predictions of author occupation
p9153
aVIn the first study, we seek to determine whether such a signal exists in self-identification we rely on variants of a single pattern, u'\u005cu201c' I am a u'\u005cu201d' , to bootstrap data for training balanced-class binary classifiers using unigrams observed in tweet content
p9154
aVAs compared to prior research that required actively polling users for ground truth in order to construct predictive models for demographic information [ 11 ] , we demonstrate that some users specify such properties publicly through direct natural language
p9155
aVIn the second study we exploit a complementary signal based on characteristic conceptual attributes of a social role, or concept class [ 20 , 1 , 17 ]
p9156
aVWe identify typical attributes of a given social role by collecting terms in the Google n-gram corpus that occur frequently in a possessive construction with that role
p9157
aVAll role-representative users were drawn from the free public 1% sample of the Twitter Firehose, over the period 2011-2013, from the subset that selected English as their native language (85,387,204 unique users
p9158
aVTo identify users of a particular role, we performed a case-agnostic search of variants of a single pattern
p9159
aVI am a(n) , and I u'\u005cu2019' m a(n) , where all single tokens filling the slot were taken as evidence of the author self-reporting for the given u'\u005cu201c' role u'\u005cu201d'
p9160
aVExample tweets can be seen in Table 1 , examples of frequency per role in Table 2
p9161
aV3 3 This removes users that selected English as their primary language, used a self-identification phrase, e.g., I am a belieber , but otherwise tended to communicate in non-English
p9162
aVThese tweets served as representative content for that role, with any tweet matching the self-reporting patterns filtered
p9163
aVThree sets of background populations were extracted based on randomly sampling users that self-reported English (post-filtered via LID
p9164
aVAny given user taken to be representative based on a previously posted tweet may no longer be available to query on
p9165
aVAs a hint of the sort of user studies one might explore given access to social role prediction, we see in Figure 1 a correlation between self-reported role and the chance of an account still being publicly visible, with roles such as belieber and directioner on the one hand, and doctor and teacher on the other
p9166
aVWhile these samples are small (and thus estimates of quality come with wide variance), it is noteworthy that a non-trivial number for each were judged as actually self-identifying
p9167
aVBaseline accuracy in these experiments was thus 50 u'\u005cu2062' %
p9168
aVEach training set had a target of 600 users (300 background, 300 self-identified); for those roles with less than 300 users self-identifying, all users were used, with an equal number background
p9169
aVThese results qualitatively suggest many roles under consideration may be teased out from a background population by focussing on language that follows expected use patterns
p9170
aVBergsma and Van Durme ( 2013 ) showed that the task of mining attributes for conceptual classes can relate straightforwardly to author attribute prediction
p9171
aVIf one views a role, in their case gender, as two conceptual classes, male and female , then existing attribute extraction methods for third-person content (e.g.,, news articles) can be cheaply used to create a set of bootstrapping features for building classifiers over first-person content (e.g.,, tweets
p9172
aVFor example, if we learn from news corpora that a man may have a wife , then a tweet saying u'\u005cu2026' my wife u'\u005cu2026' can be taken as potential evidence of membership in the male conceptual class
p9173
aVIn our second study, we test whether this idea extends to our wider set of fine-grained roles
p9174
aVWe approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using pointwise mutual information (PMI) [ 5 ]
p9175
aVFirst, we counted all terms matching a target social role u'\u005cu2019' s possessive pattern (e.g.,, doctor u'\u005cu2019' s ) in the web-scale n-gram corpus Google V2 [ 13 ] 5 5 In this corpus, follower-type roles like belieber and directioner are not at all prevalent
p9176
aVWe therefore focused on occupational and habitual roles (e.g.,, doctor, smoker
p9177
aVProbabilities were estimated from counts of the class-attribute pairs along with counts matching the generic possessive patterns his and her which serve as general background categories
p9178
aVFollowing suggestions by Bergsma and Van Durme, we manually filtered the ranked list
p9179
aVUsing this smaller high-precision set of attribute terms, we collected tweets from the Twitter Firehose over the period 2011-2013
p9180
aVAttribute terms are less indicative overall than self-ID, e.g.,, the phrase I u'\u005cu2019' m a barber is a clearer signal than my scissors
p9181
aVWe therefore include a role verification step in curating a collection of positively identified users
p9182
aVBased on this tweet, would you think this person is a Barber/Hairdresser along with four response options
p9183
aVWe found that making conceptual class assignments based on a single tweet was often a subtle task
p9184
aVAttribute terms shown in red were manually discarded as being inaccurate (low on the y-axis) or non-prevalent (small shape
p9185
aVFrom the remaining attribute terms, we identified users with tweets scoring 4.0 or better as positive examples of the associated roles
p9186
aVTweets from those users were scraped via the Twitter API to construct corpora for each role
p9187
aVFor example, if one sends a tweet saying my coach , then how likely is it that author is an athlete
p9188
aVUsing the same collection as the previous experiment, we trained classifiers conditioned on a given attribute term
p9189
aVOur results are based on simple, intuitive search patterns with minimal additional filtering this establishes the feasibility of the task, but leaves wide room for future work, both in the sophistication in methodology as well as the diversity of roles to be targeted
p9190
aVThis material is partially based on research sponsored by the NSF under grants DGE-123285 and IIS-1249516 and by DARPA under agreement number FA8750-13-2-0017 (the DEFT program
p9191
asg88
(lp9192
sg90
(lp9193
sg92
(lp9194
VWe have shown that Twitter contains sufficiently robust signal to support more fine-grained author attribute prediction tasks than have previously been attempted.
p9195
aVOur results are based on simple, intuitive search patterns with minimal additional filtering this establishes the feasibility of the task, but leaves wide room for future work, both in the sophistication in methodology as well as the diversity of roles to be targeted.
p9196
aVWe exploited two complementary types of indicators self-identification and self-possession of conceptual class (role) attributes.
p9197
aVThose interested in identifying latent demographics can extend and improve these indicators in developing ways to identify groups of interest within the general population of Twitter users.
p9198
aVThis material is partially based on research sponsored by the NSF under grants DGE-123285 and IIS-1249516 and by DARPA under agreement number FA8750-13-2-0017 (the DEFT program.
p9199
ag106
asg107
S'P14-2030'
p9200
sg109
(lp9201
VMotivated by work predicting coarse-grained author categories in social media, such as gender or political preference, we explore whether Twitter contains information to support the prediction of fine-grained categories, or social roles.
p9202
aVWe find that the simple self-identification pattern I am a supports significantly richer classification than previously explored, successfully retrieving a variety of fine-grained roles.
p9203
aVFor a given role (e.g.,, writer ), we can further identify characteristic attributes using a simple possessive construction (e.g.,, writer s.
p9204
aVTweets that incorporate the attribute terms in first person possessives ( my ) are confirmed to be an indicator that the author holds the associated social role.
p9205
ag106
asba(icmyPackage
FText
p9206
(dp9207
g3
(lp9208
VBased on properties of the involved edit and turn, we have defined constraints for corresponding edit-turn-pairs
p9209
aVMost of the resources used for collaborative writing do not explicitly allow their users to interact directly, so that the implicit effort of coordination behind the actual writing is not documented
p9210
aVWikipedia, as one of the most prominent collaboratively created resources, offers its users a platform to coordinate their writing, the so called talk or discussion pages [ 18 ]
p9211
aVRoughly five hours after that turn was issued on the discussion page, user Sbharris added a wikilink to the u'\u005cu201c' History and etymology u'\u005cu201d' section of the article by performing the following edit
p9212
aV{mdframed} [backgroundcolor=gray!30, linewidth=0pt, leftmargin=0, rightmargin=0, skipabove=4pt, skipbelow=3pt, innertopmargin=3pt, innerbottommargin=3pt] '' borax '' u'\u005cu2192' [[borax]] This is what we define as a corresponding edit-turn-pair
p9213
aVMore details follow in Section 2
p9214
aVEdits are coherent modifications based on a pair of adjacent revisions from Wikipedia article pages
p9215
aVIndividual turns are retrieved from topics by considering the revision history of the discussion page
p9216
aVAn edit-turn-pair is defined as a pair of an edit from a Wikipedia article u'\u005cu2019' s revision history and a turn from the discussion page bound to the same article
p9217
aVIf an article has no discussion page, there are no edit-turn-pairs for this article
p9218
aVDue to their performative nature, we assume that these dialog acts make the turn they belong to a good candidate for a corresponding edit-turn-pair
p9219
aVWe therefore define an edit-turn-pair as corresponding, if i) The turn is an explicit suggestion, recommendation or request and the edit performs this suggestion, recommendation or request, ii) the turn is an explicit reference or pointer and the edit adds or modifies this reference or pointer, iii) the turn is a commitment to an action in the future and the edit performs this action, and iv) the turn is a report of a performed action and the edit performs this action
p9220
aVThe search space for corresponding edit-turn-pairs is quite big, as any edit to an article may correspond to any turn from the article u'\u005cu2019' s discussion page
p9221
aVAssuming that most edit-turn-pairs are non-corresponding, we expect a heavy imbalance in the class distribution
p9222
aVIt was important to find a reasonable amount of corresponding edit-turn-pairs before the actual annotation could take place, as we needed a certain amount of positive seeds to keep turkers from simply labeling pairs as non-corresponding all the time
p9223
aVBased on an automatic classification using the model presented in our previous work [ 7 ] , we excluded edits classified as Vandalism, Revert or Other
p9224
aVFurthermore, we removed all edits which are part of a revision created by bots, based on the Wikimedia user group 2 2 http://meta.wikimedia.org/wiki/User_classes scheme
p9225
aVTo keep the class imbalance within reasonable margins, we limited the time span between edits and turns to 86,000 seconds (about 24 hours
p9226
aVThe result is a set of 13,331 edit-turn-pairs, referred to as ETP-all
p9227
aVFrom ETP-all, a set of 262 edit-turn-pairs have been annotated as corresponding as part of a preliminary annotation study with one human annotator
p9228
aVThis step is intended to make sure that we have a substantial number of corresponding pairs in the data for the final annotation study
p9229
aVHowever, we still expect a certain amount of non-corresponding edit-turn-pairs in this data, as the annotator judged the correspondence based on the entire revision and not the individual edit
p9230
aVWe refer to this 262 edit-turn-pairs as ETP-unconfirmed
p9231
aVFinally, for the Mechanical Turk annotation study, we selected 500 random edit-turn-pairs from ETP-all excluding ETP-unconfirmed
p9232
aVThe context of an edit is defined as one preceding and one following paragraph of the edited paragraph
p9233
aVEach edit-turn-pair could be labeled as u'\u005cu201c' corresponding u'\u005cu201d' , u'\u005cu201c' non-corresponding u'\u005cu201d' or u'\u005cu201c' can u'\u005cu2019' t tell u'\u005cu201d'
p9234
aVTo select good turkers and to block spammers, we carried out a pilot study on a small portion of manually confirmed corresponding and non-corresponding pairs, and required turkers to pass a qualification test
p9235
aVThis was calculated as 1 N u'\u005cu2062' u'\u005cu2211' i = 1 N u'\u005cu2211' c = 1 C v i c C , where N = 750 is the overall number of annotated edit-turn-pairs, C = R 2 - R 2 is the number of pairwise comparisons, R = 5 is the number of raters per edit-turn-pair, and v i c = 1 if a pair of raters c labeled edit-turn-pair i equally, and 0 otherwise
p9236
aVWe counted an edit-turn-pair as corresponding, if it was annotated as u'\u005cu201c' corresponding u'\u005cu201d' by least three out of five annotators, and likewise for non-corresponding pairs
p9237
aVFurthermore, we deleted 21 pairs for which the turn segmentation algorithm clearly failed (e.g., when the turn text was empty
p9238
aVTo assess the reliability of these annotations, one of the co-authors manually annotated a random subset of 100 edit-turn-pairs contained in ETP-gold as corresponding or non-corresponding
p9239
aVThe inter-rater agreement between ETP-gold (majority votes over Mechanical Turk annotations) and our expert annotations on this subset is Cohen u'\u005cu2019' s u'\u005cu039a' = .72
p9240
aVHowever, given the high price for a new corresponding edit-turn-pair (due to the high class imbalance in random data), we consider it as a useful starting point for research on edit-turn-pairs in Wikipedia
p9241
aVIn our 24 hours search space, the probability to find a corresponding edit-turn-pair drops steeply for time spans of more than 6 hours
p9242
aVWe therefore expect to cover the vast majority of corresponding edit-turn-pairs within a search space of 24 hours
p9243
aVWe propose a number of features which are purely based on the textual similarity between the text of the turn, and the edited text and context
p9244
aVSeveral of our features are based on metadata from both the edit and the turn
p9245
aVSome features are based on the edit or the turn alone and do not take into account the pair itself
p9246
aVThe 1,000 most frequent uni-, bi- and trigrams from the turn text are represented as binary features
p9247
aVWe treat the automatic classification of edit-turn-pairs as a binary classification problem
p9248
aVGiven the small size of ETP-gold, we did not assign a fixed train/test split to the data
p9249
aVFor the same reason, we did not further divide the data into train/test and development data
p9250
aVA reduction of the feature set as judged by a u'\u005cu03a7' 2 ranker improved the results for both Random Forest as well as the SVM, so we limited our feature set to the 100 best features
p9251
aVPrevious work [ 11 , 4 ] has shown that these algorithms work well for edit and turn classification
p9252
aVAs baseline, we defined a majority class classifier, which labels all edit-turn-pairs as non-corresponding
p9253
aVDue to the high class imbalance in the data, the majority class baseline sets a challenging accuracy score of .80
p9254
aVThe low F1 on corresponding pairs is likely due to the small number of training examples
p9255
aVTo understand the mistakes of the classifier, we manually assessed error patterns within the model of the Random Forest classifier
p9256
aVSome of the false positives (i.e., non-corresponding pairs classified as corresponding) were caused by pairs where the revision (as judged by its comment or the edit context) is related to the turn text, however the specific edit in this pair is not
p9257
aVBoth of these may be part of a corresponding edit-turn-pair, according to our definition in Section 2
p9258
aVIn the present study, we have analyzed cases where explicit coordination lead to implicit coordination and vice versa
p9259
aVBased on the types of turn and edit in an edit-turn-pair, we have operationalized the notion of corresponding and non-corresponding edit-turn-pairs
p9260
asg88
(lp9261
sg90
(lp9262
sg92
(lp9263
VThe novelty of this paper is a computational analysis of the relationship between the edit history and the discussion of a Wikipedia article.
p9264
aVAs far as we are aware, this is the first study to automatically analyze this relationship involving the textual content of edits and turns.
p9265
aVBased on the types of turn and edit in an edit-turn-pair, we have operationalized the notion of corresponding and non-corresponding edit-turn-pairs.
p9266
aVThe basic assumption is that in a corresponding pair, the turn contains an explicit performative and the edit corresponds to this performative.
p9267
aVWe have presented a machine learning system to automatically detect corresponding edit-turn-pairs.
p9268
aVTo test this system, we manually annotated a corpus of corresponding and non-corresponding edit-turn-pairs.
p9269
aVTrained and tested on this data, our system shows a significant improvement over the baseline.
p9270
aVWith regard to future work, an extension of the manually annotated corpus is the most important issue.
p9271
aVOur classifier can be used to bootstrap the annotation of additional edit-turn-pairs.
p9272
ag106
asg107
S'P14-2031'
p9273
sg109
(lp9274
VIn this study, we analyze links between edits in Wikipedia articles and turns from their discussion page.
p9275
aVOur motivation is to better understand implicit details about the writing process and knowledge flow in collaboratively created resources.
p9276
aVBased on properties of the involved edit and turn, we have defined constraints for corresponding edit-turn-pairs.
p9277
aVWe manually annotated a corpus of 636 corresponding and non-corresponding edit-turn-pairs.
p9278
aVFurthermore, we show how our data can be used to automatically identify corresponding edit-turn-pairs.
p9279
aVWith the help of supervised machine learning, we achieve an accuracy of .87 for this task.
p9280
ag106
asba(icmyPackage
FText
p9281
(dp9282
g3
(lp9283
VChinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing
p9284
aVUnknown words, also known as out-of-vocabulary ( oov ) words, lead to difficulties for word- or dictionary-based approaches
p9285
aVThere are two primary classes of models character-based, where the foundational units for processing are individual Chinese characters [ 23 , 19 , 24 , 20 ] , and word-based, where the units are full words based on some dictionary or training lexicon [ 1 , 25 ]
p9286
aV\u005cnewcite Sun:2010:COLING details their respective theoretical strengths character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new oov words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans
p9287
aVIn this work, we propose a simple and principled joint decoding method for combining character-based and word-based segmenters based on dual decomposition
p9288
aVIn this section, we describe the character-based and word-based models we use as baselines, review existing approaches to combination, and describe our algorithm for joint decoding with dual decomposition
p9289
aVIn the most commonly used contemporary approach to character-based segmentation, first proposed by [ 23 ] , CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word
p9290
aVConditional random fields (CRF) [ 11 ] have been widely adopted for this task, and give state-of-the-art results [ 19 ]
p9291
aVSearching through the entire GEN u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' ) space is intractable even with a local model, so a beam-search algorithm is used
p9292
aVThese mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation
p9293
aVDual decomposition (DD) [ 14 ] offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to [ 18 ] ) or decoding efficiency (in contrast to bagging in [ 22 , 17 ]
p9294
aVDD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing [ 9 ] , bilingual sequence tagging [ 21 ] and word alignment [ 6 ]
p9295
aVThe idea is that jointly modelling both character-sequence and word information can be computationally challenging, so instead we can try to find outputs that the two models are most likely to agree on
p9296
aVwhere u'\u005cud835' u'\u005cudc32' u'\u005cud835' u'\u005cudc50' is the output of character-based CRF, u'\u005cud835' u'\u005cudc32' u'\u005cud835' u'\u005cudc64' is the output of word-based perceptron, and the agreements are expressed as constraints s.t is a shorthand for u'\u005cu201c' such that u'\u005cu201d'
p9297
aVWe can then form the dual of this problem by taking the min outside of the max , which is an upper bound on the original problem
p9298
aVIn each iteration, if the best segmentations provided by the two models do not agree, then the two models will receive penalties for the decisions they made that differ from the other
p9299
aVThis penalty exchange is similar to message passing, and as the penalty accumulates over iterations, the two models are pushed towards agreeing with each other
p9300
aVWe adopt a learning rate update rule from \u005cnewcite Koo:2010:EMNLP where u'\u005cu0391' t is defined as 1 N , where N is the number of times we observed a consecutive dual value increase from iteration 1 to t
p9301
aVWe conduct experiments on the SIGHAN 2003 [ 16 ] and 2005 [ 7 ] bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm
p9302
aVWe use the publicly available Stanford CRF segmenter [ 19 ] 2 2 http://nlp.stanford.edu/software/segmenter.shtml as our character-based baseline model, and reproduce the perceptron-based segmenter from \u005cnewcite Zhang:2007:ACL as our word-based baseline model
p9303
aVWe also report out-of-vocabulary recall (R oov ) as an estimation of the model u'\u005cu2019' s generalizability to previously unseen words
p9304
aVA powerful feature of the dual decomposition approach is that it can generate correct segmentation decisions in cases where a voting or product-of-experts model could not, since joint decoding allows the sharing of information at decoding time
p9305
aVFinally, since dual decomposition is a method of joint decoding, it is still liable to reproduce errors made by the constituent systems
p9306
asg88
(lp9307
sg90
(lp9308
sg92
(lp9309
VIn this paper we presented an approach to Chinese word segmentation using dual decomposition for system combination.
p9310
aVWe demonstrated that this method allows for joint decoding of existing CWS systems that is more accurate and consistent than either system alone, and further achieves the best performance reported to date on standard datasets for the task.
p9311
aVPerhaps most importantly, our approach is straightforward to implement and does not require retraining of the underlying segmentation models used.
p9312
aVThis suggests its potential for broader applicability in real-world settings than existing approaches to combining character-based and word-based models for Chinese word segmentation.
p9313
ag106
asg107
S'P14-2032'
p9314
sg109
(lp9315
VThere are two dominant approaches to Chinese word segmentation word-based and character-based models, each with respective strengths.
p9316
aVPrior work has shown that gains in segmentation performance can be achieved from combining these two types of models; however, past efforts have not provided a practical technique to allow mainstream adoption.
p9317
aVWe propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference.
p9318
aVOur method is simple and easy to implement.
p9319
aVExperiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets.
p9320
aVUTF8gbsn.
p9321
ag106
asba(icmyPackage
FText
p9322
(dp9323
g3
(lp9324
VSegmentation is a common practice in Arabic NLP due to the language u'\u005cu2019' s morphological richness
p9325
aVGreen and DeNero use a linear-chain model with u'\u005cud835' u'\u005cudc17' as the sequence of input characters , and u'\u005cud835' u'\u005cudc18' * chosen according to the decision rule
p9326
aVTheir model classifies each y i as one of I (continuation of a segment), O (whitespace outside any segment), B (beginning of a segment), or F (pre-grouped foreign characters
p9327
aVOur segmenter expands this label space in order to handle two Arabic-specific orthographic rules
p9328
aVIn particular, it does not depend on the existence of a dialect-specific lexicon or morphological analyzer
p9329
aVAs a result, we expect this model to perform similarly well when applied to other Arabic dialects
p9330
aVF 1 scores provide a more informative assessment of performance than word-level or character-level accuracy scores, as over 80% of tokens in the development sets consist of only one segment, with an average of one segmentation every 4.7 tokens (or one every 20.4 characters
p9331
aVerrors that can be fixed with a fuller analysis of just the problematic token, and therefore represent a deficiency in the feature set; and
p9332
aVerrors that would require additional context or sophisticated semantic awareness to fix
p9333
aVOf the 100 errors we sampled, 33 are due to typographical errors or inconsistencies in the gold data
p9334
aVWe classify 7 as typos and 26 as annotation inconsistencies, although the distinction between the two is murky typos are intentionally preserved in the treebank data, but segmentation of typos varies depending on how well they can be reconciled with standard Arabic orthography
p9335
aVFour of the seven typos are the result of a missing space, such as
p9336
aVThe first example is segmented in the Egyptian treebank but is left unsegmented by our system; the second is left as a single token in the treebank but is split into the above three segments by our system
p9337
aVTwo examples of these are
p9338
aVwafi.tarIqaT u'\u005cu201c' and in the way u'\u005cu201d' segmented as wa - + fi.tarIqaT (correct analysis is wa - + fi u'\u005cu2014' + .tarIqaT f.tr u'\u005cu2018' u'\u005cu2018' break u'\u005cu2019' u'\u005cu2019' / u'\u005cu2018' u'\u005cu2018' breakfast u'\u005cu2019' u'\u005cu2019' is a common Arabic root, but the presence of q should indicate that f.tr is not the root in this case
p9339
aVwalAyuhimmhum u'\u005cu2018' u'\u005cu2018' and it u'\u005cu2019' s not important to them u'\u005cu2019' u'\u005cu2019' segmented as wa - + li u'\u005cu2014' + u'\u005cu2014' ayuhimm + u'\u005cu2014' hum (correct analysis is wa - + lA + yuhimm + u'\u005cu2014' hum
p9340
aVWe evaluated our segmenter on broadcast news and Egyptian Arabic due to the current availability of annotated data in these domains
p9341
asg88
(lp9342
sg90
(lp9343
sg92
(lp9344
VIn this paper we demonstrate substantial gains on Arabic clitic segmentation for both formal and dialectal text using a single model with dialect-independent features and a simple domain adaptation strategy.
p9345
aVWe present a new Arabic segmenter which performs better than tools employing sophisticated linguistic analysis, while also giving impressive speed improvements.
p9346
aVWe evaluated our segmenter on broadcast news and Egyptian Arabic due to the current availability of annotated data in these domains.
p9347
aVHowever, as data for other Arabic dialects and genres becomes available, we expect that the model s simplicity and the domain adaptation method we use will allow the system to be applied to these dialects with minimal effort and without a loss of performance in the original domains.
p9348
ag106
asg107
S'P14-2034'
p9349
sg109
(lp9350
VSegmentation of clitics has been shown to improve accuracy on a variety of Arabic NLP tasks.
p9351
aVHowever, state-of-the-art Arabic word segmenters are either limited to formal Modern Standard Arabic, performing poorly on Arabic text featuring dialectal vocabulary and grammar, or rely on linguistic knowledge that is hand-tuned for each dialect.
p9352
aVWe extend an existing MSA segmenter with a simple domain adaptation technique and new features in order to segment informal and dialectal Arabic text.
p9353
aVExperiments show that our system outperforms existing systems on newswire, broadcast news and Egyptian dialect, improving segmentation F 1 score on a recently released Egyptian Arabic corpus to 95.1%, compared to 90.8% for another segmenter designed specifically for Egyptian Arabic.
p9354
ag106
asba(icmyPackage
FText
p9355
(dp9356
g3
(lp9357
VThis paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition
p9358
aVIn contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression
p9359
aVThe results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is model-independent
p9360
aVThe provision of compositionality in distributional models of meaning, where a word is represented as a vector of co-occurrence counts with every other word in the vocabulary, offers a solution to the fact that no text corpus, regardless of its size, is capable of providing reliable co-occurrence statistics for anything but very short text constituents
p9361
aVBy composing the vectors for the words within a sentence, we are still able to create a vectorial representation for that sentence that is very useful in a variety of natural language processing tasks, such as paraphrase detection, sentiment analysis or machine translation
p9362
aVHence, given a sentence w 1 u'\u005cu2062' w 2 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' w n , a compositional distributional model provides a function f such that
p9363
aVWhat makes the models of the above work u'\u005cu2018' partial u'\u005cu2019' is that the authors used simplified versions of the linear maps, projected onto spaces of order lower than that required by the theoretical framework
p9364
aVAs a result, a certain amount of transformational power was traded off for efficiency
p9365
aVA potential explanation then for the effectiveness of the proposed prior disambiguation method can be sought on the limitations imposed by the compositional models under test
p9366
aVAfter all, the idea of having disambiguation emerge as a direct consequence of the compositional process, without the introduction of any explicit step, seems more natural and closer to the way the human mind resolves lexical ambiguities
p9367
aVWe create such a model by using linear regression, and we explain how an explicit disambiguation step can be introduced to this model prior to composition
p9368
aVWe then proceed by comparing the composite vectors produced by this approach with those produced by the model alone in a number of experiments
p9369
aVCompositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication [] to deep learning techniques based on neural networks []
p9370
aVN u'\u005cu2192' N (where N is our basic vector space for nouns), which takes as input a noun and returns a modified version of it
p9371
aVSince every map of this sort can be represented by a matrix living in the tensor product space N u'\u005cu2297' N , we now see that the meaning of a phrase such as u'\u005cu2018' red car u'\u005cu2019' is given by r u'\u005cu2062' e u'\u005cu2062' d ¯ × c u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2192' , where r u'\u005cu2062' e u'\u005cu2062' d ¯ is an adjective matrix and × indicates matrix multiplication
p9372
aVThe same concept applies for functions of higher order, such as a transitive verb (a function of two arguments, so a tensor of order 3
p9373
aVFor these cases, matrix multiplication generalizes to the more generic notion of tensor contraction
p9374
aVHere, the model does not fully exploit the space provided by the theoretical framework (i.e., an order-3 tensor), which has two disadvantages firstly, we lose space that could hold valuable information about the verb in this case and relational words in general; secondly, the generally non-commutative tensor contraction operation is now partly relying on element-wise multiplication, which is commutative, thus forgets (part of the) order of composition
p9375
aVIn the next section we will see how to apply linear regression in order to create full tensors for verbs and use them for a compositional model that avoids these pitfalls
p9376
aVIn order to create a matrix for, say, the intransitive verb u'\u005cu2018' play u'\u005cu2019' , we first collect all instances of the verb occurring with some subject in the training corpus, and then we create non-compositional holistic vectors for these elementary sentences following exactly the same methodology as if they were words
p9377
aVWe now have a dataset with instances of the form u'\u005cu27e8' s u'\u005cu2062' u u'\u005cu2062' b u'\u005cu2062' j i u'\u005cu2192' , s u'\u005cu2062' u u'\u005cu2062' b u'\u005cu2062' j i u'\u005cu2062' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' y u'\u005cu2192' u'\u005cu27e9' (e.g., the vector of u'\u005cu2018' kids u'\u005cu2019' paired with the holistic vector of u'\u005cu2018' kids play u'\u005cu2019' , and so on), that can be used to train a linear regression model in order to produce an appropriate matrix for verb u'\u005cu2018' play u'\u005cu2019'
p9378
aVThe premise of a model like this is that the multiplication of the verb matrix with the vector of a new subject will produce a result that approximates the distributional behaviour of all these elementary two-word exemplars used in training
p9379
aVWe present examples and experiments based on this method, constructing ambiguous and disambiguated tensors of order 2 (that is, matrices) for verbs taking one argument
p9380
aVInstead of using subject-verb constructs as above we concentrate on elementary verb phrases of the form verb-object (e.g., u'\u005cu2018' play football u'\u005cu2019' , u'\u005cu2018' admit student u'\u005cu2019' ), since in general objects comprise stronger contexts for disambiguating the usage of a verb
p9381
aVOur basic vector space is trained from the ukWaC corpus [] , originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content
p9382
aVThe verb phrases of our dataset are based on the 5 ambiguous verbs of Table 1
p9383
aVAs an example, in the verb u'\u005cu2018' play u'\u005cu2019' we impose the two distinct meanings of using a musical instrument and participating in a sport; so the first set of objects contains nouns such as u'\u005cu2018' oboe u'\u005cu2019' , u'\u005cu2018' piano u'\u005cu2019' , u'\u005cu2018' guitar u'\u005cu2019' , and so on, while in the second set we see nouns such as u'\u005cu2018' football u'\u005cu2019' , u'\u005cu2019' baseball u'\u005cu201d' etc
p9384
aVThen, each object in the list was manually annotated as exclusively belonging to one of the two senses; so, an object could be selected only if it was related to a single sense, but not both
p9385
aVFor example, u'\u005cu2018' attention u'\u005cu2019' was a valid object for the attract sense of verb u'\u005cu2018' draw u'\u005cu2019' , since it is unrelated to the sketch sense of that verb
p9386
aVOn the other hand, u'\u005cu2018' car u'\u005cu2019' is not an appropriate object for either sense of u'\u005cu2018' draw u'\u005cu2019' , since it could actually appear under both of them in different contexts
p9387
aVWe apply linear regression in order to train verb matrices using jointly the object sets for both meanings of each verb, as well as separately u'\u005cu2014' so in this latter case we get two matrices for each verb, one for each sense
p9388
aVFor each verb phrase, we create a composite vector by matrix-multiplying the verb matrix with the vector of the specific object
p9389
aVThis is done by comparing each holistic vector with all the composite ones, and then evaluating the rank of the correct composite vector within the list of results
p9390
aVFor every occurrence of the verb, we create a vector representing the surrounding context by averaging the vectors of every other word in the same sentence
p9391
aVThe clustering algorithm uses Ward u'\u005cu2019' s method as inter-cluster measure, and Pearson correlation for measuring the distance of vectors within a cluster
p9392
aVSince HAC returns a dendrogram embedding all possible groupings, we measure the quality of each partitioning by using the variance ratio criterion [] and we select the partitioning that achieves the best score (so the number of senses varies from verb to verb
p9393
aVThe next step is to classify every noun that has been used as an object with that verb to the most probable verb sense, and then use these sets of nouns as before for training tensors for the various verb senses
p9394
aVBeing equipped with a number of sense clusters created as above for every verb, the classification of each object to a relevant sense is based on the cosine distance of the object vector from the centroids of the clusters
p9395
aVThe union of all object sets is used for training a single unambiguous tensor for the verb
p9396
aVAs usual, data points are presented to learning algorithm in random order
p9397
aVFor the ambiguous regression model, the composition is done by matrix-multiplying the ambiguous verb matrix (learned by the union of all object sets) with the vector of the noun
p9398
aVFor the disambiguated version, we first detect the most probable sense of the verb given the noun, again by comparing the vector of the noun with the centroids of the verb clusters; then, we matrix-multiply the corresponding unambiguous tensor created exclusively from objects that have been classified as closer to this specific sense of the verb with the noun
p9399
aVWe also test a number of baselines the u'\u005cu2018' verbs-only u'\u005cu2019' model is a non-compositional baseline where only the two verbs are compared; u'\u005cu2018' additive u'\u005cu2019' and u'\u005cu2018' multiplicative u'\u005cu2019' compose the word vectors of each phrase by applying simple element-wise operations
p9400
aVFirst of all, the regression model is based on the assumption that the holistic vectors of the exemplar verb phrases follow an ideal distributional behaviour that the model aims to approximate as close as possible
p9401
aVThis is very important, since a regression model can only perform as well as its training dataset allows it; and in our case this is achieved to a very satisfactory level
p9402
aVThe use of a robust regression model rejects the hypothesis that the proposed methodology is helpful only for relatively u'\u005cu201c' weak u'\u005cu201d' compositional approaches
p9403
aVAs for future work, an interesting direction would be to see how a prior disambiguation step can affect deep learning compositional settings similar to [] and []
p9404
asg88
(lp9405
sg90
(lp9406
sg92
(lp9407
VThis paper adds to existing evidence from previous research that the introduction of an explicit disambiguation step before the composition improves the quality of the produced composed representations.
p9408
aVThe use of a robust regression model rejects the hypothesis that the proposed methodology is helpful only for relatively weak compositional approaches.
p9409
aVAs for future work, an interesting direction would be to see how a prior disambiguation step can affect deep learning compositional settings similar to [] and [].
p9410
ag106
asg107
S'P14-2035'
p9411
sg109
(lp9412
VThis paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition.
p9413
aVIn contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression.
p9414
aVThe results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is model-independent.
p9415
ag106
asba(icmyPackage
FText
p9416
(dp9417
g3
(lp9418
VIn this paper, we propose a novel model for enriching the content of microblogs by exploiting external knowledge, thus improving the data sparseness problem in short text classification
p9419
aVWe assume that microblogs share the same topics with external knowledge
p9420
aVWe first build an optimization model to infer the topics of microblogs by employing the topic-word distribution of the external knowledge
p9421
aVPrevious researches [ Phan et al.2008 , Guo and Diab2012 ] show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area
p9422
aVMeanwhile, external knowledge has been found helpful [ Hu et al.2009 ] in tackling the data scarcity problem by enriching short texts with informative context
p9423
aVNowadays, most of the work on short text focuses on microblog
p9424
aVAs a new form of short text, microblog has some unique features like informal spelling and emerging words, and many microblogs are strongly related to up-to-date topics as well
p9425
aVEvery day, a great quantity of microblogs more than we can read is pushed to us, and finding what we are interested in becomes rather difficult, so the ability of choosing what kind of microblogs to read is urgently demanded by common user
p9426
aVTreating microblogs as standard texts and directly classifying them cannot achieve the goal of effective classification because of sparseness problem
p9427
aVOn the other hand, news on the Internet is of information abundance and many microblogs are news-related
p9428
aVThey share up-to-date topics and sometimes quote each other
p9429
aVThus, external knowledge, such as news, provides rich supplementary information for analysing and mining microblogs
p9430
aVWe first infer the topic distribution of each microblog based on the topic-word distribution of news corpus obtained by the LDA estimation
p9431
aVWith the above two distributions, we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog, since words not appearing in the microblog may still be highly relevant
p9432
aVWe enrich the content of microblogs by inferring the association between microblogs and external words in a probabilistic perspective
p9433
aVBased on the idea of exploiting external knowledge, many methods are proposed to improve the representation of short texts for classification and clustering
p9434
aVBanerjee et al.2007 use the title and the description of news article as two separate query strings to select related concepts as additional feature
p9435
aVHu et al.2009 present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet
p9436
aVPhan et al.2008 present a framework including an approach for short text topic inference and adds abstract words as extra feature
p9437
aVGuo and Diab2012 modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks
p9438
aVOur task is to enrich each microblog with additional information so as to improve microblog u'\u005cu2019' s representation
p9439
aVWe do topic analysis for E u'\u005cu2062' K using LDA estimation [ Blei et al.2003 ] in this section and we choose LDA as the topic analysis model because of its broadly proved effectivity and ease of understanding
p9440
aVThe optimization problem is formulated as maximizing the log likelihood on the corpus
p9441
aVIn this formulation, X i u'\u005cu2062' j represents the term frequency of word w i in document d j
p9442
aVEstimating parameters for LDA by directly and exactly maximizing the likelihood of the corpus in (1) is intractable, so we use Gibbs Sampling for estimation
p9443
aVBecause of the assumption that microblogs share the same topics with external corpus, the u'\u005cu201c' topic distribution u'\u005cu201d' here refers to a distribution over all topics on E u'\u005cu2062' K
p9444
aVCompared with the original LDA optimization problem (1), the topic inference problem for microblog (2) follows the idea of document generation process, but replaces topics to be estimated with known topics from other corpus
p9445
aVAs a result, parameters to be inferred are only the topic distribution of every microblog
p9446
aVIt is noteworthy that since the word distribution of every topic P ( w i e z k e ) is known, Formula ( 2 ) can be further solved by separating it into M m subproblems
p9447
aVBased on the results of step (a) (b), we calculate the word distributions of microblogs as follows
p9448
aVIn other words, though some words may not actually appears in a microblog, there is still a probability that it is highly relevant to the microblog
p9449
aVIntuitively, this probability indicates the strength of association between a word and a microblog
p9450
aVThe word distribution of every microblog is based on topic analysis and its accuracy relies heavily on the accuracy of topic inference in step (b
p9451
aVHaving known the top L relevant words according to the result of sorting, we redefine the u'\u005cu201c' term frequency u'\u005cu201d' of every word after adding these L words to microblog d j m as additional content
p9452
aVwhere R u'\u005cu2062' T u'\u005cu2062' F u'\u005cu2062' ( u'\u005cu22c5' ) is the revised term frequency
p9453
aVAs the Equation ( 5 ) shows, the revised term frequency of every word is proportional to probability P ( w i d j m ) rather than a constant
p9454
aVSo far, we can add these L words and their revised term frequency as additional information to microblog d j m
p9455
aVThe revised term frequency plays the same role as TF in common text representation vector, so we calculate the TFIDF of the added words as
p9456
aVNote that I u'\u005cu2062' D u'\u005cu2062' F u'\u005cu2062' ( w ) is changed as arrival of new words for each microblog
p9457
aVThe TFIDF vector of a microblog with additional words is called enhanced vector
p9458
aVAfter preprocessing, these news documents are used as external knowledge
p9459
aVAs for microblog, we crawl a number of microblogs from Sina Weibo u'\u005cu2020' u'\u005cu2020' Sina Weibo http://www.weibo.com/ , and ask unbiased assessors to manually classify them into 9 categories following the column setting of Sina News
p9460
aVFinally, we get 1671 classified microblogs as our microblog dataset
p9461
aVThe size of each category is shown in Table 1
p9462
aVBy studying some cases, we find out that if we add too many words, the proportion of u'\u005cu201c' noisy words u'\u005cu201d' will increase
p9463
aVWe reach the best result when number of added words is 300
p9464
aVThe experiment corresponding to Figure 2 is to discover how the classification accuracy changes when we fix the number of added words ( L = 300 ) and change the number of topics ( K ) in our method
p9465
aVAs we can see, the accuracy does not grow monotonously as the number of topics increases
p9466
aVThe experiment corrsponding to Figure 3 is to discover whether our redefining u'\u005cu201c' term frequency u'\u005cu201d' as revised term frequency in step (c) of Section 3.3 will affect the classification accuracy and how
p9467
aVOn one hand, without redefinition, the accuracy remains in a stable high level and tends to decrease as we add more words
p9468
aVOne reason for the decreasing is that u'\u005cu201c' noisy words u'\u005cu201d' have a increasing negative impact on the accuracy as the proportion of u'\u005cu201c' noisy words u'\u005cu201d' grows with the number of added words
p9469
aVIn the first case, we successfully find the country name according to its leader u'\u005cu2019' s name and limited information in the sentence
p9470
aVOther related countries and events are also selected by our model as they often appear together in news
p9471
aVIn the other case, relevant words are among the most frequently used words in news and have close semantic relations with the microblogs in certain aspects
p9472
aVAs we can see, based on topic analysis, our model shows strong ability of mining relevant words
p9473
aVOther cases show that the model can be further improved by removing the noisy and meaningless ones among added words
p9474
aVNews corpus is exploited as external knowledge
p9475
aVAs for techniques, our method uses LDA as its topic analysis model and formulates topic inference for new data as convex optimization problems
p9476
aVCompared with traditional representation, enriched microblog shows great improvement in classification tasks
p9477
aVAs we do not control the quality of added words, our future work starts from building a filter to select better additional information
p9478
asg88
(lp9479
sg90
(lp9480
sg92
(lp9481
VWe propose an effective content enriching method for microblog, to enhance classification accuracy.
p9482
aVNews corpus is exploited as external knowledge.
p9483
aVAs for techniques, our method uses LDA as its topic analysis model and formulates topic inference for new data as convex optimization problems.
p9484
aVCompared with traditional representation, enriched microblog shows great improvement in classification tasks.
p9485
aVAs we do not control the quality of added words, our future work starts from building a filter to select better additional information.
p9486
aVAnd to make the most of external knowledge, better ways to build topic space should be considered.
p9487
ag106
asg107
S'P14-2036'
p9488
sg109
(lp9489
VIn this paper, we propose a novel model for enriching the content of microblogs by exploiting external knowledge, thus improving the data sparseness problem in short text classification.
p9490
aVWe assume that microblogs share the same topics with external knowledge.
p9491
aVWe first build an optimization model to infer the topics of microblogs by employing the topic-word distribution of the external knowledge.
p9492
aVThen the content of microblogs is further enriched by relevant words from external knowledge.
p9493
aVExperiments on microblog classification show that our approach is effective and outperforms traditional text classification methods.
p9494
ag106
asba(icmyPackage
FText
p9495
(dp9496
g3
(lp9497
VIn this paper, we extend the state of the art fixed-window alignment algorithm [] for combining the individual captions into a final output sequence
p9498
aVThe accuracy of the ASR systems can be improved using the u'\u005cu2018' re-speaking u'\u005cu2019' technique, which requires a person that the ASR has been trained on to repeat the words said by a speaker as he hears them
p9499
aVFor the caption alignment task, we treat each individual word as a symbol in our alphabet u'\u005cu03a3'
p9500
aVThe special gap symbol u'\u005cu2018' - u'\u005cu2019' represents a missing word and does not belong to u'\u005cu03a3'
p9501
aVLet A = ( a i u'\u005cu2062' j ) be a K × N f matrix, where a i u'\u005cu2062' j u'\u005cu2208' u'\u005cu03a3' u'\u005cu222a' { - } , and the i t u'\u005cu2062' h row has exactly ( N f - N i ) gaps and is identical to S i if we ignore the gaps
p9502
aVEvery column of A must have at least one non-gap symbol
p9503
aVTherefore, the j t u'\u005cu2062' h column of A indicates an alignment state for the j t u'\u005cu2062' h position, where the state can have one of the 2 K - 1 possible combinations
p9504
aVIf a i u'\u005cu2062' l and a j u'\u005cu2062' l are identical, the substitution cost is zero
p9505
aVThe substitution cost for two words is estimated based on the edit distance between two words
p9506
aVOur approach is based on weighted A * search for approximately solving the MSA problem []
p9507
aVThe A * search algorithm treats each node position n = [ n 1 , u'\u005cu2026' , n K ] as a search state, and estimates the cost function g u'\u005cu2062' ( n ) and the heuristic function h u'\u005cu2062' ( n ) for each state
p9508
aVIf the beam size is set to b seconds, then any state that aligns two words having more than b seconds time lag is ignored
p9509
aVWe ignore the alignment columns where the majority vote is below a certain threshold t v (typically t v = 2 ), and thus filter out spurious errors and spelling mistakes
p9510
aVThis is because the chunk boundaries are defined with respect to the timestamps associated with each word in the captions, but the timestamps can vary greatly between words that should in fact be aligned
p9511
aVAfter all, if the timestamps corresponded precisely to the original time at which each word was spoken, the entire alignment problem would be trivial
p9512
aVThe fact that the various instances of a single word in each transcription may fall on either side of a chunk boundary leads to errors where a word is either duplicated in the final output for more than one chunk, or omitted entirely
p9513
aVThis problem also causes errors in ordering among the words remaining within one chunk, because there is less information available to constrain the ordering relations between transcriptions
p9514
aVSecond, the fixed window alignment algorithm requires longer chunks ( u'\u005cu2265' 10 seconds) to obtain reasonable accuracy, and thus introduces unsatisfactory latency
p9515
aVIn order to address the problems described above, we explore a technique based on a sliding alignment window, shown in Algorithm 3
p9516
aVWe use a single point in the aligned output as the starting point for the next chunk, and determine the corresponding starting position within each original transcription
p9517
aVThe materials in the output alignment that follow this point is thrown away, and replaced with the output produced by aligning the next chunk starting from this point (line 8
p9518
aVThe chunk size directly determines the latency of the system to the end user, as alignment cannot begin until an entire chunk is captured
p9519
aVFurthermore, we can see that for smaller values of the chunk size parameter, increasing the keep-length makes the system less accurate
p9520
aVAs the chunk size parameter increases, the performance of sliding window systems with different values of keep-length parameter converges
p9521
aVTherefore, at larger chunk sizes, for which there are smaller number of boundaries, the keep-length parameter has lower impact
p9522
aVNext, we show the trade-off between computation speed and accuracy in Figure 3, as we fix the heuristic weight and vary the chunk size over the range [5, 10, 15, 20, 30] seconds
p9523
aVLarger chunks are more accurately aligned but require computation time that grows as N K in the chunk size N in the worst case
p9524
aVFurthermore, smaller weights allow faster alignment, but provide lower accuracy
p9525
aVBy effectively addressing the problem of alignment errors at chunk boundaries, our sliding window approach outperforms the existing fixed window based system [] in terms of word error rate, particularly when the chunk size is small, and thus achieves higher accuracy at lower latency
p9526
asg88
(lp9527
sg90
(lp9528
sg92
(lp9529
VIn this paper, we present a novel sliding window based text alignment algorithm for real-time crowd captioning.
p9530
aVBy effectively addressing the problem of alignment errors at chunk boundaries, our sliding window approach outperforms the existing fixed window based system [] in terms of word error rate, particularly when the chunk size is small, and thus achieves higher accuracy at lower latency.
p9531
aVFunded by NSF awards IIS-1218209 and IIS-0910611.
p9532
aV/msa.
p9533
ag106
asg107
S'P14-2039'
p9534
sg109
(lp9535
VThe primary way of providing real-time speech to text captioning for hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates.
p9536
aVRecent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom is able to type only part of what they hear.
p9537
aVIn this paper, we extend the state of the art fixed-window alignment algorithm [] for combining the individual captions into a final output sequence.
p9538
aVOur method performs alignment on a sliding window of the input sequences, drastically reducing both the number of errors and the latency of the system to the end user over the previously published approaches.
p9539
ag106
asba(icmyPackage
FText
p9540
(dp9541
g3
(lp9542
VThis paper presents a method for detecting words related to a topic (we call them topic words) over time in the stream of documents
p9543
aVWe propose a method to reinforce topic words with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation [ 4 ] to these documents
p9544
aVFor the results of LDA, we identified topic words by using Moving Average Convergence Divergence
p9545
aVThe results showed that the method was effective for sentence selection in summarization
p9546
aVAs the volume of online documents has drastically increased, the analysis of topic bursts, topic drift or detection of topic is a practical problem attracting more and more attention [ 1 , 23 , 2 , 13 , 15 , 9 ]
p9547
aVThey have attempted to handle concept changes by focusing a window with documents sufficiently close to the target concept
p9548
aVScholz et al have attempted to use different ensembles obtained by training several data streams to detect concept drift [ 22 ]
p9549
aVHe and Parket attempted to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates [ 11 ]
p9550
aVHowever, the fact that topics are widely distributed in the stream of documents, and sometimes they frequently appear in the documents, and sometimes not often hamper such attempts
p9551
aVThis paper proposes a method for detecting topic over time in series of documents
p9552
aVWe reinforced words related to a topic with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation (LDA) [ 4 ] to these documents in order to extract topic candidates
p9553
aVIt shows the relationship between two moving averages of prices modeling bursts as intervals of topic dynamics, i.e., , positive acceleration
p9554
aVFukumoto et al also applied MACD to find topics
p9555
aVWe identified event words by using the traditional tf u'\u005cu2217' idf method applied to the results of named entities
p9556
aVFinally, we selected a certain number of sentences according to the rank score into a summary
p9557
aVLDA presented by [ 4 ] models each document as a mixture of topics (we call it lda_topic to discriminate our t u'\u005cu2062' o u'\u005cu2062' p u'\u005cu2062' i u'\u005cu2062' c candidates), and generates a discrete probability distribution over words for each lda_topic
p9558
aVz \u005c i refers to a topic set Z , not including the current assignment z i n \u005c i , j v is the count of word v in topic j that does not include the current assignment z i , and n \u005c i , j u'\u005cu22c5' indicates a summation over that dimension
p9559
aVAfter a sufficient number of sampling iterations, the approximated posterior can be used to estimate u'\u005cu03a6' and u'\u005cu0398' by examining the counts of word assignments to topics and topic occurrences in documents
p9560
aVWe used documents prepared by summarization tasks, NTCIR and DUC data as each task consists of series of documents with the same topic
p9561
aVFor example, DUC2005 consists of 50 tasks
p9562
aVTherefore the number of different clusters is 50
p9563
aVWe estimated k u'\u005cu2032' and d u'\u005cu2032' by using Entropy measure given by
p9564
aVThe value of E ranges from 0 to 1, and the smaller value of E indicates better result
p9565
aVFor each lda_topic, we extracted words whose probabilities are larger than zero, and regarded these as topic candidates
p9566
aVThe proposed method does not simply use MACD to find bursts, but instead determines topic words in series of documents
p9567
aVUnlike Dynamic Topic Models [ 3 ] , it does not assume Gaussian distribution so that it is a natural way to analyze bursts which depend on the data
p9568
aVWe assume that if a term w is informative for summarizing a particular documents in a collection, its burstiness approximates the burstiness of documents in the collection
p9569
aVBecause w is a representative word of each document in the task
p9570
aVBased on this assumption, we computed similarity between correct and word histograms by using KL-distance 2 2 We tested KL-distance, histogram intersection and Bhattacharyya distance to obtain similarities
p9571
aVWe reported only the result obtained by KL-distance as it was the best results among them
p9572
aVIf the value of D ( P u'\u005cu2223' u'\u005cu2223' Q ) is smaller than a certain threshold value, w is regarded as a topic word
p9573
aVIt refers to notions of who(person), where(place), when(time) including what, why and how in a document
p9574
aVTherefore, we can assume that named entities(NE) are linguistic features for event detection
p9575
aVAn event word refers to the t u'\u005cu2062' h u'\u005cu2062' e u'\u005cu2062' m u'\u005cu2062' e of the document itself, and frequently appears in the document but not frequently appear in other documents
p9576
aVTherefore, we first applied NE recognition to the target documents to be summarized, and then calculated tf u'\u005cu2217' idf to the results of NE recognition
p9577
aVTwo vertices are connected if their affinity weight is larger than 0 and we let f ( i u'\u005cu2192' i ) = 0 to avoid self transition
p9578
aVWe selected a certain number of sentences according to rank score into the summary
p9579
aVThere are two types of correct summary according to the character length, u'\u005cu201c' long u'\u005cu201d' and u'\u005cu201c' short u'\u005cu201d' , All series of documents were tagged by CaboCha [ 14 ]
p9580
aVFBFREE DryRun data is used to tuning parameters, i.e., , the number of extracted words according to the tf u'\u005cu2217' idf value, and the threshold value of KL-distance
p9581
aVThe size that optimized the average Rouge-1(R-1) score across 30 tasks was chosen
p9582
aVAs a result, we set tf u'\u005cu2217' idf and KL-distance to 100 and 0.104, respectively
p9583
aVWe used FormalRun as a test data, and another set consisted of 218,724 documents from 1998 to 1999 of Mainichi newspaper as a corpus used in LDA and MACD
p9584
aVTopic candidates include surplus words that are not related to the topic because the results obtained by u'\u005cu201c' LDA u'\u005cu201d' were worse than those obtained by u'\u005cu201c' LDA MACD u'\u005cu201d' , and even worse than u'\u005cu201c' Event u'\u005cu201d' in both short and long summary
p9585
aVPrior work including u'\u005cu201c' TTM u'\u005cu201d' has demonstrated the usefulness of semantic concepts for extracting salient sentences
p9586
aVFor future work, we should be able to obtain further advantages in efficacy in our topic detection and summarization approach by disambiguating topic senses
p9587
aVThe research described in this paper explores a method for detecting topic words over time in series of documents
p9588
asg88
(lp9589
sg90
(lp9590
sg92
(lp9591
VThe research described in this paper explores a method for detecting topic words over time in series of documents.
p9592
aVThe results of extrinsic evaluation showed that integration of LDA and MACD is effective for topic detection.
p9593
ag106
asg107
S'P14-2040'
p9594
sg109
(lp9595
VThis paper presents a method for detecting words related to a topic (we call them topic words) over time in the stream of documents.
p9596
aVTopic words are widely distributed in the stream of documents, and sometimes they frequently appear in the documents, and sometimes not.
p9597
aVWe propose a method to reinforce topic words with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation [ 4 ] to these documents.
p9598
aVFor the results of LDA, we identified topic words by using Moving Average Convergence Divergence.
p9599
aVIn order to evaluate the method, we applied the results of topic detection to extractive multi-document summarization.
p9600
aVThe results showed that the method was effective for sentence selection in summarization.
p9601
ag106
asba(icmyPackage
FText
p9602
(dp9603
g3
(lp9604
VWe present a new approach to inducing the syntactic categories of words, combining their distributional and morphological properties in a joint nonparametric Bayesian model based on the distance-dependent Chinese Restaurant Process
p9605
aVDistributional similarity varies smoothly with syntactic function, so that words with similar syntactic functions should have similar distributional properties
p9606
aVBut these features are difficult to combine because of their disparate representations
p9607
aVDistributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or u'\u005cu201c' embeddings u'\u005cu201d' [ 16 , 15 , 13 , 24 ]
p9608
aVMoreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity
p9609
aVIn this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Blei and Frazier, 2011
p9610
aVWe use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning
p9611
aVRecent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically [ 9 , 3 ]
p9612
aVSince then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters [ 1 ] , or as features in a generative model [ 14 , 8 , 21 ] , or a representation-learning algorithm [ 27 ]
p9613
aVIn contrast, we use pairwise morphological similarity as a prior in a non-parametric clustering model
p9614
aVThis means that the membership of a word in a cluster requires only morphological similarity to some other element in the cluster, not to the cluster centroid; which may be more appropriate for languages with multiple morphological paradigms
p9615
aVBy identifying the connected components in this graph, the ddCRP equivalently defines a prior over clusterings
p9616
aVIf c i is the index of the customer followed by customer i , then the ddCRP prior can be written
p9617
aVA ddCRP is sequential if customers can only follow previous customers, i.e.,, d i u'\u005cu2062' j = u'\u005cu221e' when i j and f u'\u005cu2062' ( u'\u005cu221e' ) = 0
p9618
aVIn this case, if d i u'\u005cu2062' j = 1 for all i j then the ddCRP reduces to the CRP
p9619
aVBecause we do not know which suffixes are meaningful a priori , we use a maximum entropy model whose features include all suffixes up to length three that are shared by at least one pair of words
p9620
aVwhere g s u'\u005cu2062' ( i , j ) is 1 if suffix s is shared by i th and j th words, and 0 otherwise
p9621
aVWe can create an infinite mixture model by combining the ddCRP prior with a likelihood function defining the probability of the data given the cluster assignments
p9622
aVSince we are using continuous-valued vectors (word embeddings) to represent the distributional characteristics of words, we use a multivariate Gaussian likelihood
p9623
aVSince we marginalize over the cluster parameters, computing P ( c i = j ) requires computing the likelihood P ( fol ( i ) , u'\u005cud835' u'\u005cudc17' j u'\u005cu0398' ) , where u'\u005cud835' u'\u005cudc17' j are the k customers already clustered with j
p9624
aVHowever, if we do not merge fol u'\u005cu2062' ( i ) with u'\u005cud835' u'\u005cudc17' j , then we have P ( u'\u005cud835' u'\u005cudc17' j u'\u005cu0398' ) in the overall joint probability
p9625
aVTherefore, we can decompose P ( fol ( i ) , u'\u005cud835' u'\u005cudc17' j u'\u005cu0398' ) = P ( fol ( i u'\u005cud835' u'\u005cudc17' j , u'\u005cu0398' ) P ( u'\u005cud835' u'\u005cudc17' j u'\u005cu0398' ) and need only compute the change in likelihood due to merging in fol u'\u005cu2062' ( i
p9626
aVwhere the hyperparameters are updated as u'\u005cu039a' n = u'\u005cu039a' 0 + n , u'\u005cu039d' n = u'\u005cu039d' 0 + n , and
p9627
aVCombining this likelihood term with the prior, the probability of customer i following j is
p9628
aVWe interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990
p9629
aVWe do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood
p9630
aVScores for the fine-grained tags will be lower for these reasons, but we argue below that they are still informative
p9631
aVSince Wikipedia and MTE are from different domains their lexicons do not fully overlap; we take the intersection of these two sets for training and evaluation
p9632
aVThough coarse-grained tags have their place [ 17 ] , in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets, which typically distinguish between verb tenses, noun number and gender, and adjectival scale (comparative, superlative, etc.), so we feel that the evaluation against fine-grained tagset is more relevant here
p9633
aVFor better comparison with previous work, we also evaluate against the coarse-grained tags; however, these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also have Polyglot embeddings
p9634
aVV-m can be somewhat sensitive to the number of clusters [ 19 ] but much less so than m-1 [ 7 ]
p9635
aVWith different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters cannot be mapped to gold clusters or vice versa
p9636
aVHowever, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes
p9637
aVHere we report also type-based measures because these can reveal differences in model behavior even when token-based measures are similar
p9638
aVWe set the prior scale matrix u'\u005cu039b' 0 by using the average covariance from a K-means run with K = 200
p9639
aVWhen setting the average covariance as the expected value of the IW distribution the suitable scale matrix can be computed as u'\u005cu039b' 0 = E u'\u005cu2062' [ X ] u'\u005cu2062' ( u'\u005cu039d' 0 - d - 1 ) , where u'\u005cu039d' 0 is the prior degrees of freedom (which we set to d + 10) and d is the data dimensionality (64 for the Polyglot embeddings
p9640
aVDue to the nonsequentiality this equivalence does not hold, but we do expect to see similar results to the IGMM
p9641
aVFor each evaluation setting we provide two sets of scores u'\u005cu2014' first are the 1-1 and V-m scores for the given model, second are the comparable scores for K-means run with the same number of clusters as induced by the non-parametric model
p9642
aVThese results show that all non-parametric models perform better than K-means, which is a strong baseline in this task [ 8 ]
p9643
aVNon-parametric models are able to produce cluster of different sizes when the evidence indicates so, and this is clearly the case here
p9644
aVThe difference could be due to the non-sequentiality, or becuase the samplers are different u'\u005cu2014' IGMM enabling resampling only one item at a time, ddCRP performing blocked sampling
p9645
aVExponentiating the prior reduces the number of induced clusters and improves results, as it can change the cluster assignment for some words where the likelihood strongly prefers one cluster but the prior clearly indicates another
p9646
aVFuture work may exploit this advantage more thoroughly for example, by using features that incorporate prior knowledge of the language u'\u005cu2019' s morphological structure
p9647
asg88
(lp9648
sg90
(lp9649
sg92
(lp9650
VThis paper demonstrates that morphology and distributional features can be combined in a flexible, joint probabilistic model, using the distance-dependent Chinese Restaurant Process.
p9651
aVA key advantage of this framework is the ability to include arbitrary features in the prior distribution.
p9652
aVFuture work may exploit this advantage more thoroughly for example, by using features that incorporate prior knowledge of the language s morphological structure.
p9653
aVAnother important goal is the evaluation of this method on languages beyond English.
p9654
aVAcknowledgments.
p9655
aVKS was supported by the Tiger University program of the Estonian Information Technology Foundation for Education.
p9656
aVJE was supported by a visiting fellowship from the Scottish Informatics Computer Science Alliance.
p9657
aVWe thank the reviewers for their helpful feedback.
p9658
ag106
asg107
S'P14-2044'
p9659
sg109
(lp9660
VWe present a new approach to inducing the syntactic categories of words, combining their distributional and morphological properties in a joint nonparametric Bayesian model based on the distance-dependent Chinese Restaurant Process.
p9661
aVThe prior distribution over word clusterings uses a log-linear model of morphological similarity; the likelihood function is the probability of generating vector word embeddings.
p9662
aVThe weights of the morphology model are learned jointly while inducing part-of-speech clusters, encouraging them to cohere with the distributional features.
p9663
aVThe resulting algorithm outperforms competitive alternatives on English POS induction.
p9664
ag106
asba(icmyPackage
FText
p9665
(dp9666
g3
(lp9667
VA common task in qualitative data analysis is to characterize the usage of a linguistic entity by issuing queries over syntactic relations between words
p9668
aVUser interface research suggests that it is easier to recognize a pattern than to compose it from scratch; therefore, interfaces for non-experts should show previews of syntactic relations
p9669
aVIn other fields, grammatical queries can be used to develop patterns for recognizing entities in text, such as medical terms [ 6 , 13 ] , and products and organizations [ 3 ] , and for coding qualitative data such as survey results
p9670
aVIn another [ 5 ] , humanities scholars and social scientists are frequently skeptical of digital tools, because they are often difficult to use
p9671
aVFor instance, the Linguist u'\u005cu2019' s Search Engine [ 17 ] uses a query-by-example strategy in which a user types in an initial sentence in English, and the system produces a graphical view of a parse tree as output, which the user can alter
p9672
aVThe user can either click on the tree or modify the LISP expression to generalize the query
p9673
aVAccording to Shneiderman and Plaisant [ 18 ] , query-by-example has largely fallen out of favor as a user interface design approach
p9674
aVA downside of QBE is that the user must manipulate an example to arrive at the desired generalization
p9675
aVA list of selectable options is shown under the search bar, filtered to be relevant as the searcher types
p9676
aVSearchers can recognize and select the option that matches their information need, without having to generate the query themselves
p9677
aVHowever, we know of no prior work on how to display grammatical relations so that they can be easily recognized
p9678
aVOne current presentation (not used with auto-suggest) is to name the relation and show blanks where the words that satisfy it would appear as in X is the subject of Y [ 14 ] ; we used this as the baseline presentation in our experiments because it employs the relation definitions found in the Stanford Dependency Parser u'\u005cu2019' s manual [ 4 ]
p9679
aVWe chose Amazon u'\u005cu2019' s Mechanical Turk (MTurk) crowdsourcing platform as a source of study participants
p9680
aVThe wide range of backgrounds provided by MTurk is desirable because our goal is to find a representation that is understandable to most people, not just linguistic experts or programmers
p9681
aVAdditionally, one word was chosen as a focus word that was present in all the sentences, to make the relationship more recognizable ( u'\u005cu201c' life u'\u005cu201d' in Figure 4
p9682
aVAhab, ___ the sentences each contained u'\u005cu2018' Ahab u'\u005cu2019' , highlighted in yellow, as the subject of different verbs highlighted in pink
p9683
aVParticipants were paid 50c (U.S.) for completing the study, with an additional 50c bonus if they correctly identified 10 or more of the 12 relationships
p9684
aVClausal relations operate over longer distances in sentences, and so it is to be expected that showing longer stretches of context would perform better in these cases; that is indeed what the results showed
p9685
aVAmong these relations, adverb modifiers stood out (Figure 5 ), because evidence suggested that words (63% success) made the relation more recognizable than phrases (47% success, p=0.056, W=574.0) u'\u005cu2013' but the difference was only almost significant, due to the smaller sample size (only 96 participants encountered this relation
p9686
aVThis may be because the words are the most salient piece of information in an adverbial relation u'\u005cu2013' adverbs usually end in u'\u005cu2018' ly u'\u005cu2019' u'\u005cu2013' and in the phrases condition the additional information distracts from recognition of this pattern
p9687
aVA list of phrases is the most recognizable presentation for clausal relationships (34% better than the baseline), and is as good as a list of words for the other types of relations, except adverb modifiers
p9688
aVFor adverb modifiers, the list of words is the most recognizable presentation
p9689
aVThis is likely because Enlglish adverbs usually end in u'\u005cu2018' -ly u'\u005cu2019' are therefore a distinctive set of words
p9690
aVThe list of candidates can be ordered by frequency of occurrence in the collection, or by an interestingness measure given the search word
p9691
aVAs the user becomes more familiar with a given relation, it may be expedient to shorten the cues shown, and then re-introduce them if a relation has not been selected after some period of time has elapsed
p9692
aVIf phrases are used, there is a tradeoff between recognizability and the space required to display the examples of usage
p9693
aVHowever, it is important to keep in mind that because the suggestions are populated with items from the collection itself, they are informative
p9694
aVThe best strategy, phrases , had an overall success rate of only 55%, although the intended user base may have more familiarity with grammatical relations than the participants did, and therefore may perform better in practice
p9695
aVFurthermore, the current study did not test three-word relationships or more complex combinations of structures, and those may require improvements to the design
p9696
asg88
(lp9697
sg90
(lp9698
sg92
(lp9699
VThe results imply that user interfaces for syntactic search should show candidate relationships augmented with a list of phrases in which they occur.
p9700
aVA list of phrases is the most recognizable presentation for clausal relationships (34% better than the baseline), and is as good as a list of words for the other types of relations, except adverb modifiers.
p9701
aVFor adverb modifiers, the list of words is the most recognizable presentation.
p9702
aVThis is likely because Enlglish adverbs usually end in -ly are therefore a distinctive set of words.
p9703
aVThe list of candidates can be ordered by frequency of occurrence in the collection, or by an interestingness measure given the search word.
p9704
aVAs the user becomes more familiar with a given relation, it may be expedient to shorten the cues shown, and then re-introduce them if a relation has not been selected after some period of time has elapsed.
p9705
aVIf phrases are used, there is a tradeoff between recognizability and the space required to display the examples of usage.
p9706
aVHowever, it is important to keep in mind that because the suggestions are populated with items from the collection itself, they are informative.
p9707
aVThe best strategy, phrases , had an overall success rate of only 55%, although the intended user base may have more familiarity with grammatical relations than the participants did, and therefore may perform better in practice.
p9708
aVNonetheless, there is room for improvement in scores, and it may be that additional visual cues, such as some kind of bracketing, will improve results.
p9709
aVFurthermore, the current study did not test three-word relationships or more complex combinations of structures, and those may require improvements to the design.
p9710
ag106
asg107
S'P14-2045'
p9711
sg109
(lp9712
VA common task in qualitative data analysis is to characterize the usage of a linguistic entity by issuing queries over syntactic relations between words.
p9713
aVPrevious interfaces for searching over syntactic structures require programming-style queries.
p9714
aVUser interface research suggests that it is easier to recognize a pattern than to compose it from scratch; therefore, interfaces for non-experts should show previews of syntactic relations.
p9715
aVWhat these previews should look like is an open question that we explored with a 400-participant Mechanical Turk experiment.
p9716
aVWe found that syntactic relations are recognized with 34% higher accuracy when contextual examples are shown than a baseline of naming the relations alone.
p9717
aVThis suggests that user interfaces should display contextual examples of syntactic relations to help users choose between different relations.
p9718
ag106
asba(icmyPackage
FText
p9719
(dp9720
g3
(lp9721
VWe develop a system that lets people overcome language barriers by letting them speak a language they do not know
p9722
aVSince the speaker may not be able to pronounce the foreign-language orthography, phrasebooks additionally provide phonetic spellings that approximate the sounds of the foreign phrase
p9723
aVIf the Franglish is well designed, an English speaker can pronounce it and be understood by a French listener
p9724
aVIn this paper, we lift this restriction by designing and evaluating a software program with the following
p9725
aVThe main challenge is that different languages have different orthographies, different phoneme inventories, and different phonotactic constraints, so mismatches are inevitable
p9726
aVWe seek to imitate phonetic transformations found in phrasebooks, so phrasebooks themselves are a good source of training data
p9727
aVFirst, because Chinglish and Chinese are written with the same characters, they render the same inventory of 416 distinct syllables
p9728
aVSyllables u'\u005cu2018' u'\u005cu2018' si u'\u005cu2019' u'\u005cu2019' and u'\u005cu2018' u'\u005cu2018' te u'\u005cu2019' u'\u005cu2019' are very popular, because while consonant clusters like English u'\u005cu2018' u'\u005cu2018' st u'\u005cu2019' u'\u005cu2019' are impossible to reproduce exactly, the particular vowels in u'\u005cu2018' u'\u005cu2018' si u'\u005cu2019' u'\u005cu2019' and u'\u005cu2018' u'\u005cu2018' te u'\u005cu2019' u'\u005cu2019' are fortunately very weak
p9729
aVIt is reasonable for u'\u005cu2018' u'\u005cu2018' can I u'\u005cu2019' u'\u005cu2019' to be rendered as u'\u005cu2018' u'\u005cu2018' kan nai u'\u005cu2019' u'\u005cu2019' , with u'\u005cu2018' u'\u005cu2018' nai u'\u005cu2019' u'\u005cu2019' spanning both English words, but this is rare
p9730
aVFSTs A, C, and D are unweighted, and remain so throughout this paper
p9731
aVWe must now estimate the values of FST B parameters, such as P( s i
p9732
aVTo do this, we first take our phrasebook triples and construct sample string pairs Epron, Pinyin-split by pronouncing the phrasebook English with FST A, and by pronouncing the phrasebook Chinglish with FSTs D and C
p9733
aVFor example, when we decode English u'\u005cu2018' u'\u005cu2018' grandmother u'\u005cu2019' u'\u005cu2019' , we get
p9734
aVwhere as the reference Pinyin-split sequence is
p9735
aVFollowing phrase-based methods in statistical machine translation [ 4 ] and machine transliteration [ 1 ] , we model substitution of longer sequences
p9736
aVWe create English, Pinyin training pairs from our phrasebook simply by pronouncing the Chinglish with FST D
p9737
aVNotice that this model makes alignment errors due to sparser data (e.g.,, the word u'\u005cu2018' u'\u005cu2018' tips u'\u005cu2019' u'\u005cu2019' and u'\u005cu2018' u'\u005cu2018' ti pu si u'\u005cu2019' u'\u005cu2019' only appear once each in the training data
p9738
aVThe word-based model can only decode 29 of the 65 test utterances, because wFST E fails if an utterance contains a new English word type, previously unseen in training
p9739
aVHere, we start with reference English and measure the accuracy of Pinyin syllable production, since the choice of Chinglish character does not affect the Chinglish pronunciation
p9740
aVAs Table 6 shows, the ratio of unseen English word tokens is small, thus large portion of tokens are transformed using word-based method
p9741
aVSpeech recognition is more fragile than human transcription, so edit distances are greater
p9742
aVOur work aims to help people speak foreign languages they don u'\u005cu2019' t know, by providing native phonetic spellings that approximate the sounds of foreign phrases
p9743
aVWe improve the model by adding phrases, word boundary constraints, and improved alignment
p9744
asg88
(lp9745
sg90
(lp9746
sg92
(lp9747
VOur work aims to help people speak foreign languages they don t know, by providing native phonetic spellings that approximate the sounds of foreign phrases.
p9748
aVWe use a cascade of finite-state transducers to accomplish the task.
p9749
aVWe improve the model by adding phrases, word boundary constraints, and improved alignment.
p9750
aVIn the future, we plan to cover more language pairs and directions.
p9751
aVEach target language raises interesting new challenges that come from its natural constraints on allowed phonemes, syllables, words, and orthography.
p9752
ag106
asg107
S'P14-2046'
p9753
sg109
(lp9754
VWe develop a system that lets people overcome language barriers by letting them speak a language they do not know.
p9755
aVOur system accepts text entered by a user, translates the text, then converts the translation into a phonetic spelling in the user s own orthography.
p9756
aVWe trained the system on phonetic spellings in travel phrasebooks.
p9757
ag106
asba(icmyPackage
FText
p9758
(dp9759
g3
(lp9760
VOur analysis is based on manual evaluations of translations of news from Chinese and Arabic to English
p9761
aVAlso related to lower translation quality is the need to employ multiple explicit discourse connectives ( because, but , etc.), as well as the presence of ambiguous discourse connectives in the English translation
p9762
aVDiscourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to interpret multi-sentential text while statistical MT systems are trained to translate a single sentence in one language into a single sentence in another
p9763
aVSimilarly discourse connectives like because, but, since and while often relate information expressed in simple sentential clauses
p9764
aVBy comparing MT output with post-edited references, HTER provides more reliable estimates of translation quality than using translated references, especially at the segment level
p9765
aVFor discourse factors that are significant at the 95% confidence level or higher according to the ANOVA analysis, we provide detailed breakdown of the system HTER for each value of the discourse factor
p9766
aVIn this paper we do not compare the performance of individual systems, but instead seek to understand if a discourse phenomena is problematic across systems
p9767
aV2 2 For the readers with keen interest in system comparison, we note that according to ANOVA none of the differences in system performance on this data is statistically significant
p9768
aVIt stands to reason that long sentences will be harder to process automatically and this reasoning has motivated the first approaches to text simplification [ 3 ]
p9769
aVSo before turning to the analysis of discourse phenomena, we examine the correlation between translation quality and sentence length
p9770
aVA strong correlation between the two would call for revival of interest in text simplification where syntactically complex sentences are transformed into several shorter sentences as a preprocessing step
p9771
aVWe find however that no strong relationship exists between the two, as shown by the correlation coefficients between HTER values and the number of words in each segment in Table 1
p9772
aVNext we examine if sentence u'\u005cu2013' discourse divergence between languages and the presence of (ambiguous) discourse connectives would be more indicative of the expected translation quality
p9773
aVChinese is known for sentences of this kind; for example, the usage of punctuation is very different in Chinese in the sense that a comma can sometimes function as a full stop in English, motivating a series of disambiguation tasks [ 7 , 20 , 19 ]
p9774
aVSpecial handling of long Chinese sentences were also shown to improve machine translation [ 8 , 21 ]
p9775
aVTo investigate the prevalence of sentences in the source language (Chinese and Arabic in our case) that do not confirm to the notion of sentence in the target language (English for the purposes of this study), we separate the translation segments in the source language into two classes a source sentence is considered 1-1 if the reference translation consists of exactly one sentence, and 1-many if the reference contains more than one sentence
p9776
aVWe conducted ANOVA on HTER, separately for each language, with type of segment (1-1 or 1-many) as the independent variable and systems treated as subjects
p9777
aVOnce 1-many segments are identified, source-side text simplification techniques may be developed [ 17 ] to improve translation quality
p9778
aVFor example, while can signal either comparison or temporal relations and since can signal either contingency or temporal
p9779
aVThis analysis gives lower bound on the translation quality degradation associated with discourse phenomena as it does not capture problems arising from connective ambiguity on the source side
p9780
aVWe base our classification of discourse connectives into ambiguous or not according to the distribution of their senses in the PDTB
p9781
aVWe call a connective ambiguous if its most frequent sense among comparison, contingency, expansion, temporal accounts for less than 80% of occurrence of that connective in the PDTB
p9782
aV4 4 The ambiguous connectives are as, as if, as long as, as though, finally, if and when, in the end, in turn, lest, meanwhile, much as, neither u'\u005cu2026' nor, now that, rather, since, ultimately, when, when and if, while
p9783
aVIn the ANOVA tests for each language, we compared the quality of segments which contained an ambiguous connective in the reference with those that do not, with systems treated as subjects
p9784
aV[ ref ] Still some others can receive further professional game training in universities and later ( Temporal ) be employed as technical consultants by large game manufacturers, etc
p9785
aV[ sys ] Some people may go to the university games professional education, which is appointed as the big game manufacturers such as technical advisers
p9786
aV[ edited ] Some people may go to university to receive professional game education, and later ( Temporal ) be appointed by the big game manufacturers as technical advisers
p9787
aVThe system fails to translate the discourse connective {CJK*} UTF8gbsn u'\u005cu201c' èå u'\u005cu201d' (later), leading to a probable misinterpretation between receiving education and being appointed as technical advisors
p9788
aVDue to the lack of reliable tools and resources, we approximate mismatches between discourse expressions in the source and MT output using discourse-related edits
p9789
aVObviously, the mismatch in implicit/explicit expression of discourse relation is related to the first problem we studied, i.e.,, if the source segment is translated into one or multiple sentences in English, since discourse relations between adjacent sentences are more often implicit (than intra-sentence ones
p9790
aVFor this reason we performed a Wilcoxon rank sum test for the translation quality of segments with discourse mismatch conditioned on whether the segment was 1-1 or 1-many
p9791
asg88
(lp9792
sg90
(lp9793
sg92
(lp9794
VWe showed that translation from Chinese to English is made more difficult by various discourse events such as the use of discourse connectives, the ambiguity of the connectives and the type of relations they signal.
p9795
aVNone of these discourse factors has a significant impact on translation quality from Arabic to English.
p9796
aVTranslation quality from both languages is adversely affected by translations of discourse relations expressed implicitly in one language but explicitly in the other or by paired connectives.
p9797
aVOur experiments indicate that discourse usage may affect machine translation between some language pairs but not others, and for particular relations such as contingency.
p9798
aVFinally, we established the need to identify sentences in the source language that would be translated into multiple sentences in English.
p9799
aVEspecially in translating from Chinese to English, there is a large number of such sentences which are currently translated much worse than other sentences.
p9800
ag106
asg107
S'P14-2047'
p9801
sg109
(lp9802
VWe present a study of aspects of discourse structure specifically discourse devices used to organize information in a sentence that significantly impact the quality of machine translation.
p9803
aVOur analysis is based on manual evaluations of translations of news from Chinese and Arabic to English.
p9804
aVWe find that there is a particularly strong mismatch in the notion of what constitutes a sentence in Chinese and English, which occurs often and is associated with significant degradation in translation quality.
p9805
aVAlso related to lower translation quality is the need to employ multiple explicit discourse connectives ( because, but , etc.), as well as the presence of ambiguous discourse connectives in the English translation.
p9806
aVFurthermore, the mismatches between discourse expressions across languages significantly impact translation quality.
p9807
ag106
asba(icmyPackage
FText
p9808
(dp9809
g3
(lp9810
VWe show further that the accuracy with which a learned classifier can detect text as machine translated is strongly correlated with the translation quality of the machine translation system that generated it
p9811
aVFinally, we offer a generic machine translation quality estimation technique based on this approach, which does not require reference sentences
p9812
aVProminent among these are how to evaluate the quality of such a system efficiently and how to detect the output of such systems (for example, to avoid using it circularly as input for refining MT systems
p9813
aVIn this paper, we will answer both these questions
p9814
aVSpecifically, given a corpus consisting of both machine-translated English text (English being the target language) and native English text (not necessarily the reference translation of the machine-translated text), we measure the accuracy of the system in classifying the sentences in the corpus as machine-translated or not
p9815
aVThis accuracy will be shown to decrease as the quality of the underlying MT system increases
p9816
aVIn fact, the correlation is strong enough that we propose that this accuracy measure itself can be used as a measure of MT system quality, obviating the need for a reference corpus, as for example is necessary for BLEU []
p9817
aVIn the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection techniques as a machine translation quality estimation method
p9818
aVIn the final section we offer conclusions and suggestions for future work
p9819
aVUsing automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals
p9820
aVSeveral works [] used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, using features like word and POS n-grams, proportion of grammatical words in the text, nouns, finite verbs, auxiliary verbs, adjectives, adverbs, numerals, pronouns, prepositions, determiners, conjunctions etc
p9821
aVRegarding the detection of machine translated text, Carter and Inkpen [] translated the Hansards of the 36th Parliament of Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features
p9822
aVArase and Zhou [] trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system
p9823
aVWhile Arase and Zhou [] considered MT detection at sentence level, as we do in this paper, they did not study the correlation between the translation quality of the machine translated text and the ability to detect it
p9824
aVDue to the sparseness of the data at the sentence level, we use common content-independent linguistic features for the classification task
p9825
aVWe consider only those entries that appear at least ten times in the entire corpus, in order to reduce sparsity in the data
p9826
aVAs our learning algorithm we use SVM with sequential minimal optimization (SMO), taken from the WEKA machine learning toolkit []
p9827
aVAfter translating the sentences, we take 20,000 sentences from each engine output and conduct the detection experiment by labeling those sentences as MT sentences, and another 20,000 sentences, which are the human reference translations, labeled as reference sentences
p9828
aVUsing simple linear regression, we also obtain an R 2 value (coefficient of determination) over the measurements of detection accuracy and BLEU score, for each of three feature set combinations (function words, POS tags and mixed) and the two data combinations (MT vs reference and MT vs non reference sentences
p9829
aVIt can also be seen that, as might be expected, it is easier to distinguish machine-translated sentences from a non-reference set than from the reference set
p9830
aVIn Figure 1 , we show the relationship of the observed detection accuracy for each system with the BLEU score of that system
p9831
aVAs is evident, regardless of the feature set or non-MT sentences used, the correlation between detection accuracy and BLEU score is very high, as we can also see from the R 2 values in Table 1
p9832
aVIn order to do so, we use the Moses statistical machine translation toolkit []
p9833
aVFor purposes of classification, we use the same content independent features as in the previous experiment, based on function words and POS tags, again with SMO-based SVM as the classifier
p9834
aVFor data, we use 20,000 random, non reference sentences from the Hansard corpus, against 20,000 sentences from one MT system per experiment, again resulting in 40,000 sentence instances per experiment
p9835
aV[color=black,mark=none,style=dashed] table[ y=create col/linear regression=y=Y] X Y 29.18 72.33 28.54 73.10 27.76 73.90 24.34 74.12 23.83 73.59 22.46 74.78 20.72 75.98 ;
p9836
aV[color=black,mark=none,style=dashed] table[ y=create col/linear regression=y=Y] X Y 0.638 58.58 0.604 58.61 0.591 57.63 0.573 58.78 0.562 59.38 0.541 58.63 0.512 59.86 0.486 58.53 0.439 60.46 0.429 61.65 0.420 62.66 0.389 60.83 0.322 64.10 ;
p9837
aV[title= R 2 = 0.829 , width=7.5cm,height=7.5cm, xlabel= human evaluation score,ylabel=detection accuracy (%)] \u005caddplot [color=black,mark=*] coordinates (0.638,60.92) (0.604,61.54) (0.591,62.41) (0.573,62.87) (0.562,62.46) (0.541,62.97) (0.512,63.81) (0.486,62.87) (0.439,64.09) (0.429,64.72) (0.420,66.81) (0.389,65.24) (0.322,65.87) ; \u005caddplot [color=black,mark=none,style=dashed] table[ y=create col/linear regression=y=Y] X Y 0.638 60.92 0.604 61.54 0.591 62.41 0.573 62.87 0.562 62.46 0.541 62.97 0.512 63.81 0.486 62.87 0.439 64.09 0.429 64.72 0.420 66.81 0.389 65.24 0.322 65.87 ;
p9838
aVAs can be seen in the above experiments, there is a strong correlation between the BLEU score and the MT detection accuracy of our method
p9839
aVWe conduct the same classification experiment as above, with features based on function words and POS tags, and SMO-based SVM as the classifier
p9840
aVWe first use 3000 reference sentences from the WMT13 u'\u005cu2019' English reference translations, against the matching 3000 output sentences from one MT system at a time, resulting in 6000 sentence instances per experiment
p9841
aVAs can be seen in Figure 3 , the detection accuracy is strongly correlated with the evaluations scores, yielding R 2 = 0.774
p9842
aVIn the second experiment, we use 3000 random, non reference sentences from the newstest 2011-2012 corpora published in WMT12 u'\u005cu2019' [] against 3000 output sentences from one MT system at a time, again resulting in 6000 sentence instances per experiment
p9843
aVWhile applying the same classification method as with the reference sentences, the detection accuracy rises, while the correlation with the translation quality yields R 2 = 0.556 , as can be seen in Figure 4
p9844
aVTo verify this intuition, we create parse trees using the Berkeley parser [] and extract the one-level CFG rules as features
p9845
aVAgain, we represent each sentence as a boolean vector, in which each entry represents the presence or absence of the CFG rule in the parse-tree of the sentence
p9846
aVUsing these features alone, without the FW and POS tag based features presented above, we obtain an R 2 = 0.829 with a proportion of identically ordered pairs at 0.923 (72 of 78), as shown in Figure 5
p9847
aVThis suggests that our method might be used as an unsupervised quality estimation method when no reference sentences are available, such as for resource-poor source languages
p9848
aVFurther work might include applying our methods to other language pairs and domains, acquiring word-level quality estimation or integrating our method in a machine translation system
p9849
aVFurthermore, additional features and feature selection techniques can be applied, both for improving detection accuracy and for strengthening the correlation with human quality estimation
p9850
asg88
(lp9851
sg90
(lp9852
sg92
(lp9853
g1538
asg107
S'P14-2048'
p9854
sg109
(lp9855
VWe show that it is possible to automatically detect machine translated text at sentence level from monolingual corpora, using text classification methods.
p9856
aVWe show further that the accuracy with which a learned classifier can detect text as machine translated is strongly correlated with the translation quality of the machine translation system that generated it.
p9857
aVFinally, we offer a generic machine translation quality estimation technique based on this approach, which does not require reference sentences.
p9858
ag106
asba(icmyPackage
FText
p9859
(dp9860
g3
(lp9861
VCiting a number of empirical studies, Tversky [ 19 ] calls symmetry directly into question, and proposes two general models that abandon symmetry
p9862
aVTversky shows that many similarity judgment tasks have an inherent asymmetry; but he also argues, following Rosch [ 17 ] , that certain kinds of stimuli are more naturally used as foci or standards than others
p9863
aVGoldstone [ 8 ] summarizes the results succinctly u'\u005cu201c' Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa; for example, North Korea is judged to be more like China than China is [like] North Korea u'\u005cu201d' Thus, one source of asymmetry is the comparison of sparse and dense representations
p9864
aVSuppose we want to measure the semantic similarity of boat , rank 682 among the nouns in the BNC corpus studied below, which has 1057 nonzero dependency features based on 50 million words of data, with dinghy , rank 6200, which has only 113 nonzero features
p9865
aVIf in Tversky/Rosch terms, the more frequent word is also a more likely focus, then this is exactly the kind of situation in which asymmetric similarity judgments will arise
p9866
aVFirst, since Tversky has primarily additive f in mind, we can reformulate f u'\u005cu2062' ( A u'\u005cu2229' B ) as follows
p9867
aVNext, since we are interested in generalizing from sets of features, to real-valued vectors of features, w 1 , w 2 , we define
p9868
aVIf the operation is min and w 1 u'\u005cu2062' [ f ] and w 2 u'\u005cu2062' [ f ] both contain the feature weights for f , then
p9869
aVso with si set to min , Equation ( 3 ) includes Equation ( 2 ) as a special case
p9870
aVIn this generalized form, then, ( 1 ) becomes
p9871
aVThus, if u'\u005cu0391' + u'\u005cu0392' = 1 , Tversky u'\u005cu2019' s ratio model becomes simply
p9872
aVWhen u'\u005cu0391' 0.5 , sim u'\u005cu2062' ( w 1 , w u'\u005cu2062' 2 ) is biased in favor of w 1 as the referent; When u'\u005cu0391' 0.5 , sim u'\u005cu2062' ( w 1 , w u'\u005cu2062' 2 ) is biased in favor of w 2
p9873
aVThe function dice prod is not well known in the word similarity literature, but in the data mining literature it is often just called Dice coefficient, because it generalized the set comparison function of Dice [ 5 ]
p9874
aVObserve that cosine is a special case of dice prod dice u'\u005cu2020' was introduced in Curran [ 3 ] and was the most successful function in his evaluation
p9875
aVSince lin was introduced in Lin [ 13 ] ; several different functions have born that name
p9876
aVThis yields the three similarity functions cited above
p9877
aVThus, all three of these functions are special cases of symmetric ratio models
p9878
aVBelow, we investigate asymmetric versions of all three, which we write as T u'\u005cu0391' , si u'\u005cu2062' ( w 1 , w 2 ) , defined as
p9879
aVHere, T u'\u005cu0391' , si u'\u005cu2062' ( w h , w l ) is as defined in ( 9 ), and the u'\u005cu0391' -weighted word is always the less frequent word
p9880
aVFor example, consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat if u'\u005cu0391' is high, we give more weight to the proportion of dinghy u'\u005cu2019' s features that are shared than we give to the proportion of boat u'\u005cu2019' s features that are shared
p9881
aVThe nearest neighbor problem is a rather a natural ground in which to try out ideas on asymmetry, since the nearest neighbor relation is itself not symmetrical
p9882
aVThe value .97 was chosen for u'\u005cu0391' because it produced the best u'\u005cu0391' -system on the MC/RG corpus
p9883
aV4 4 The approximation is based on the formula for computing Spearman u'\u005cu2019' s R with no ties
p9884
aVIf n is the number of items, then the improvement on that item is
p9885
aVChoosing u'\u005cu0391' = .9 , weights recall toward the rarer word
p9886
aVIt is natural to use the sparser representation as the focus in the comparison
p9887
aVFigure 2 gives the results of our nearest neighbor study on the BNC for the case of dice prod
p9888
aVThe graphs for the other two u'\u005cu0391' -skewed systems are nearly identical, and are not shown due to space limitations
p9889
aVAll the systems show degraded nearest neighbor quality as target words grow rare, but at lower ranks, the u'\u005cu0391' = .04 nearest neighbor system fares considerably better than the symmetric u'\u005cu0391' = .50 system; the line across the bottom tracks the score of a system with randomly generated nearest neighbors
p9890
aVThe symmetric dice prod system is as an excellent nearest neighbor system at high ranks but drops below the u'\u005cu0391' = .04 system at around rank 3500
p9891
aVTo reflect natural judgments of similarity for comparisons of representations of differing sparseness, u'\u005cu0391' should be tipped toward the sparser representation
p9892
aVThus, u'\u005cu0391' = .80 works best for high rank target words, because most nearest neighbor candidates are less frequent, and u'\u005cu0391' = .8 tips the balance toward the nontarget words
p9893
aVOn the other hand, when the target word is a low ranking word, a high u'\u005cu0391' weight means it never receives the highest weight, and this is disastrous, since most good candidates are higher ranking
p9894
aV[ 9 ] , which also proposes an asymmetric similarity framework based on Tversky u'\u005cu2019' s insights
p9895
aVMotivated by the problem of measuring how well the distribution of one word w 1 captures the distribution of another w 2 , Weeds and Weir [ 21 ] also explore asymmetric models, expressing similarity calculations as weighted combinations of several variants of what they call precision and recall
p9896
aVSome of their models are also Tverskyan ratio models
p9897
aVIf the si is min , then the two terms in the denominator are the inverses of what W W call difference-weighted precision and recall
p9898
aVSo for T min , ( 9 ) can be rewritten
p9899
aVW W u'\u005cu2019' s additive precision/recall models appear not to be Tversky models, since they compute separate sums for precision and recall from the f u'\u005cu2208' w 1 u'\u005cu2229' w 2 , one using w 1 u'\u005cu2062' [ f ] , and one using w 2 u'\u005cu2062' [ f ]
p9900
aVLike Weeds and Weir, her perspective was to calculate the effectiveness of using one distribution as a proxy for the other, a fundamentally asymmetric problem
p9901
aVFor distributions q and r , Lee u'\u005cu2019' s u'\u005cu0391' -skew divergence takes the KL-divergence of a mixture of q and r from q , using the u'\u005cu0391' parameter to define the proportions in the mixture
p9902
asg88
(lp9903
sg90
(lp9904
sg92
(lp9905
VWe have shown that Tversky s asymmetric ratio models can improve performance in capturing human judgments and produce better nearest neighbors.
p9906
aVTo validate these very preliminary results, we need to explore applications compatible with asymmetry, such as the TOEFL-like synonym discovery task in Freitag et al.
p9907
aV[ 7 ] , and the PP-attachment task in Dagan et al.
p9908
aV[ 4 ].
p9909
ag106
asg107
S'P14-2049'
p9910
sg109
(lp9911
VWe show that asymmetric models based on Tversky [ 19 ] improve correlations with human similarity judgments and nearest neighbor discovery for both frequent and middle-rank words.
p9912
aVIn accord with Tversky s discovery that asymmetric similarity judgments arise when comparing sparse and rich representations, improvement on our two tasks can be traced to heavily weighting the feature bias toward the rarer word when comparing high- and mid-frequency words.
p9913
ag106
asba(icmyPackage
FText
p9914
(dp9915
g3
(lp9916
VWord representation is central to natural language processing
p9917
aVThe default approach of representing words as discrete and distinct symbols is insufficient for many tasks, and suffers from poor generalization
p9918
aVFor example, the symbolic representation of the words u'\u005cu201c' pizza u'\u005cu201d' and u'\u005cu201c' hamburger u'\u005cu201d' are completely unrelated even if we know that the word u'\u005cu201c' pizza u'\u005cu201d' is a good argument for the verb u'\u005cu201c' eat u'\u005cu201d' , we cannot infer that u'\u005cu201c' hamburger u'\u005cu201d' is also a good argument
p9919
aVWe thus seek a representation that captures semantic and syntactic similarities between words
p9920
aVA very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris [ 16 ] , stating that words in similar contexts have similar meanings
p9921
aVBased on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community
p9922
aVOn one end of the spectrum, words are grouped into clusters based on their contexts [ 5 , 32 ]
p9923
aVOn the other end, words are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see [ 30 , 3 ] for a comprehensive survey
p9924
aVMost recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling [ 4 , 10 , 23 , 20 , 21 ]
p9925
aVThese representations, referred to as u'\u005cu201c' neural embeddings u'\u005cu201d' or u'\u005cu201c' word embeddings u'\u005cu201d' , have been shown to perform well across a variety of tasks [ 29 , 9 , 26 , 2 ]
p9926
aVWord embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations
p9927
aVIn Section 5 we show that the SkipGram model does allow for some introspection by querying it for contexts that are u'\u005cu201c' activated by u'\u005cu201d' a target word
p9928
aVIn the skip-gram model, each word w u'\u005cu2208' W is associated with a vector v w u'\u005cu2208' R d and similarly each context c u'\u005cu2208' C is represented as a vector v c u'\u005cu2208' R d , where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality
p9929
aVThe entries in the vectors are latent, and treated as parameters to be learned
p9930
aVLoosely speaking, we seek parameter values (that is, vector representations for both words and contexts) such that the dot product v w u'\u005cu22c5' v c associated with u'\u005cu201c' good u'\u005cu201d' word-context pairs is maximized
p9931
aVThis can be easily achieved by setting v c = v w and v c u'\u005cu22c5' v w = K for all c , w , where K is large enough number
p9932
aVIn order to prevent the trivial solution, the objective is extended with ( w , c ) pairs for which p ( D = 1 w , c ) must be low, i.e., pairs which are not in the data, by generating the set D u'\u005cu2032' of random ( w , c ) pairs (assuming they are all incorrect), yielding the negative-sampling training objective
p9933
aVWe follow the method proposed by Mikolov et al for each ( w , c ) u'\u005cu2208' D we construct n samples ( w , c 1 ) , u'\u005cu2026' , ( w , c n ) , where n is a hyperparameter and each c j is drawn according to its unigram distribution raised to the 3 / 4 power
p9934
aVOptimizing this objective makes observed word-context pairs have similar embeddings, while scattering unobserved pairs
p9935
aVThe context vocabulary C is thus identical to the word vocabulary W
p9936
aVHowever, this restriction is not required by the model; contexts need not correspond to words, and the number of context-types can be substantially larger than the number of word-types
p9937
aVIn this paper we experiment with dependency-based syntactic contexts
p9938
aVSyntactic contexts capture different information than bag-of-word contexts, as we demonstrate using the sentence u'\u005cu201c' Australian scientist discovers star with telescope u'\u005cu201d'
p9939
aVThe software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words
p9940
aVMoreover, the contexts are unmarked, resulting in discovers being a context of both stars and scientist , which may result in stars and scientists ending up as neighbours in the embedded space
p9941
aVA window size of 5 is commonly used to capture broad topical content, whereas smaller windows contain more focused information about the target word
p9942
aVAn alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in
p9943
aVThey capture relations to words that are far apart and thus u'\u005cu201c' out-of-reach u'\u005cu201d' with small window bag-of-words (e.g., the instrument of discover is telescope/prep_with ), and also filter out u'\u005cu201c' coincidental u'\u005cu201d' contexts which are within the window but not directly related to the target word (e.g., Australian is not used as the context for discovers
p9944
aVIn addition, the contexts are typed, indicating, for example, that stars are objects of discovery and scientist s are subjects
p9945
aVWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p9946
aVThis observation holds for Turing 3 3 Deps generated a list of scientists whose name ends with u'\u005cu201c' ing u'\u005cu201d'
p9947
aVTurney [ 31 ] described this distinction as domain similarity versus functional similarity
p9948
aVThe Florida example presents an ontological difference; bag-of-words contexts generate meronyms (counties or cities within Florida), while dependency-based contexts provide cohyponyms (other US states
p9949
aVThe next two examples demonstrate that similarities induced from Deps share a syntactic function (adjectives and gerunds), while similarities based on BoW are more diverse
p9950
aVSince word2vec removes the subsampled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality
p9951
aVThe pairs are ranked according to cosine similarities between the embedded words
p9952
aVWhen reversing the task such that the goal is to rank the related terms above the similar ones, the results are reversed, as expected (not shown
p9953
aV5 5 Additional experiments (not presented in this paper) reinforce our conclusion
p9954
aVIn particular, we found that Deps perform dramatically worse than BoW contexts on analogy tasks as in [ 22 , 17 ]
p9955
aVNeural word embeddings are often considered opaque and uninterpretable, unlike sparse vector space representations in which each dimension corresponds to a particular known context, or LDA models where dimensions correspond to latent topics
p9956
aVIf we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word
p9957
aVBy doing so, we can see what the model learned to be a good discriminative context for the word
p9958
aVAdditionally, the collapsed preposition relation is very useful (e.g., for capturing the school aspect of hogwarts
p9959
aVIn the future, we hope that insights from such model introspection will allow us to develop better contexts, by focusing on conjunctions and prepositions for example, or by trying to figure out why the subject and object relations are absent and finding ways of increasing their contributions
p9960
asg88
(lp9961
sg90
(lp9962
sg92
(lp9963
VWe presented a generalization of the SkipGram embedding model in which the linear bag-of-words contexts are replaced with arbitrary ones, and experimented with dependency-based contexts, showing that they produce markedly different kinds of similarities.
p9964
aVThese results are expected, and follow similar findings in the distributional semantics literature.
p9965
aVWe also demonstrated how the resulting embedding model can be queried for the discriminative contexts for a given word, and observed that the learning procedure seems to favor relatively local syntactic contexts, as well as conjunctions and objects of preposition.
p9966
aVWe hope these insights will facilitate further research into improved context modeling and better, possibly task-specific, embedded representations.
p9967
aVOur software, allowing for experimentation with arbitrary contexts, together with the embeddings described in this paper, are available for download at the authors websites.
p9968
ag106
asg107
S'P14-2050'
p9969
sg109
(lp9970
VWhile continuous word embeddings are gaining popularity, current models are based solely on linear contexts.
p9971
aVIn this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts.
p9972
aVIn particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings.
p9973
aVThe dependency-based embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.
p9974
ag106
asba(icmyPackage
FText
p9975
(dp9976
g3
(lp9977
VBy providing an empirical measure of semantic similarity between words derived from lexical co-occurrences, distributional semantics not only reliably captures how the verbs in the distribution of a construction are related, but also enables the use of visualization techniques and statistical modeling to analyze the semantic development of a construction over time and identify the semantic determinants of syntactic productivity in naturally occurring data
p9978
aVShe typed her way to a promotion
p9979
aVAs reported by Israel, examples like (1), in which the main verb describes the physical means whereby motion towards a goal is enabled, are attested as early as the 16 th century, but it was not until the 19 th century that examples like (2) started to appear, in which the action depicted by the verb provides a more indirect (and abstract) way of attaining the agent u'\u005cu2019' s goal
p9980
aVAlong these lines, Suttle and Goldberg [ Suttle and Goldberg2011 , 1254] posit a criterion of coverage, defined as u'\u005cu201c' the degree to which attested instances u'\u005cu2018' cover u'\u005cu2019' the category determined jointly by attested instances together with the target coinage u'\u005cu201d'
p9981
aVThe construction generally conveys an intensifying function (very broadly defined
p9982
aVThus, scare/surprise the hell out of means u'\u005cu201c' scare/surprise very much u'\u005cu201d' , and kick the hell out of means u'\u005cu201c' kick very hard u'\u005cu201d'
p9983
aVSince the corpus size varies slightly in each decade, the token frequencies are normalized per million words
p9984
aVThe construction is first attested in the corpus in the 1930s
p9985
aVSince then, it has been steadily increasing in token frequency (to the exception of a sudden decrease in the 1990s
p9986
aVTo answer these questions, I will analyze the distribution of the construction from a semantic point of view by using a measure of semantic similarity derived from distributional information
p9987
aVOne benefit of the distributional semantics approach is that it allows semantic similarity between words to be quantified by measuring the similarity in their distribution
p9988
aVA wide range of distributional information can be employed in vector-based models; the present study uses the u'\u005cu2018' bag of words u'\u005cu2019' approach, which is based on the frequency of co-occurrence of words within a given context window
p9989
aVAccording to Sahlgren [ Sahlgren2008 ] , this kind of model captures to what extent words can be substituted for each other, which is a good measure of semantic similarity between verbs
p9990
aVAs it turns out, even this relatively coarse model captures semantic distinctions in the distribution of the hell -construction that make intuitive sense
p9991
aVThis is, however, not as problematic as it might sound, since the meaning of the verbs under consideration are not likely to have changed considerably within the time frame of this study
p9992
aVOnly the nouns, verbs, adjectives, and adverbs listed among the 5,000 most frequent words in the corpus were considered (to the exclusion of be , have , and do ), thus ignoring function words (articles, prepositions, conjunctions, etc.) and all words that did not make the top 5,000
p9993
aVThe co-occurrence matrix was transformed by applying a Point-wise Mutual Information weighting scheme, using the DISSECT toolkit [ Dinu et al.2013 ] , to turn the raw frequencies into weights that reflect how distinctive a collocate is for a given target word with respect to the other target words under consideration
p9994
aVThe resulting matrix, which contains the distributional information (in 4,683 columns) for 92 verbs occurring in the hell -construction, constitutes the semantic space under consideration in this case study
p9995
aVFor convenience and ease of visualization, the verbs are color-coded according to four broad semantic groupings that were identified inductively by means of hierarchical clustering (using Ward u'\u005cu2019' s criterion
p9996
aV3 3 Another benefit of combining clustering and MDS stems from the fact that the latter often distorts the data when fitting the objects into two dimensions, in that some objects might have to be slightly misplaced if not all distance relations can be simultaneously complied with
p9997
aVSince cluster analysis operates with all 4,683 dimensions of the distributional space, it is more reliable than MDS, although it lacks the visual appeal of the latter
p9998
aVBy comparing the plots in Figure 2 , we can follow the semantic development of the hell -construction
p9999
aVThese two classes also correspond to the regions of the semantic domain that attract the most new members, and they constantly do so in all periods
p10000
aVOutside of these two clusters, the semantic space is much more sparsely populated
p10001
aVOutside of the two identified domains of predilection, other classes never become important, assumedly because they do not receive a u'\u005cu201c' critical mass u'\u005cu201d' of items, and therefore attract new members more slowly
p10002
aVOn the reasonable assumption that the semantic contribution of the construction did not change, and therefore that all verbs ever attested in it are equally plausible from a semantic point of view, the fact that some verbs joined the distribution later than others is in want of an explanation
p10003
aVIf the semantic space around the novel item is dense, i.e.,, if there is a high number of similar items, the coinage will be very likely
p10004
aVThe latter value decreases with space density (i.e.,, if there are many close neighbors), and is therefore technically a measure of sparsity; since cosine distances are between 0 and 1, subtracting the mean distance from one returns a measure of density within the same boundaries
p10005
aVThis measure of density was used as a factor in logistic regression to predict the first occurrence of a verb in the construction, coded as the binary variable Occurrence , set to 1 for the first period in which the verb is attested in the construction, and to 0 for all preceding periods (later periods were discarded
p10006
aVFor all values of N, we find a positive effect of Density , i.e.,, there is a positive relation between the measure of density and the probability of first occurrence of a verb in the construction
p10007
aVHowever, the effect is only significant for N u'\u005cu2265' 7; hence, the hypothesis that space density increases the odds of a coinage occurs in the construction is supported for measures of density based on these values of N
p10008
aVThis is arguably because a higher N helps to better discriminate between dense clusters where all items are close together from looser ones that consist of a few u'\u005cu2018' core u'\u005cu2019' items surrounded by more distant neighbors
p10009
aVThis result illustrates the role of type frequency in syntactic productivity a measure of density that is supported by a higher number of types makes better prediction than a measure supported by fewer types
p10010
aVThis means that productivity not only hinges on how the existing semantic space relates to the novel item, it also occurs more reliably when this relation is attested by more items
p10011
aVThese finding support the view that semantic density and type frequency, while they both positively influence syntactic productivity, do so in different ways density defines the necessary conditions for a new coinage to occur, while type frequency increases the confidence that this coinage is indeed possible
p10012
aVUsing multidimensional scaling and logistic regression, it was shown that the occurrence of new items throughout the history of the construction can be predicted by the density of the semantic space in the neighborhood of these items in prior usage
p10013
asg88
(lp10014
sg90
(lp10015
sg92
(lp10016
VThis paper reports the first attempt at using a distributional measure of semantic similarity derived from a vector-space model for the study of syntactic productivity in diachrony.
p10017
aVOn the basis of a case study of the construction V the hell out of NP from 1930 to 2009, the advantages of this approach were demonstrated.
p10018
aVNot only does distributional semantics provide an empirically-based measure of semantic similarity that appropriately captures semantic distinctions, it also enables the use of methods for which quantification is necessary, such as data visualization and statistical analysis.
p10019
aVUsing multidimensional scaling and logistic regression, it was shown that the occurrence of new items throughout the history of the construction can be predicted by the density of the semantic space in the neighborhood of these items in prior usage.
p10020
aVIn conclusion, this work opens new perspectives for the study of syntactic productivity in line with the growing synergy between computational linguistics and other fields.
p10021
ag106
asg107
S'P14-2051'
p10022
sg109
(lp10023
VThis paper describes an application of distributional semantics to the study of syntactic productivity in diachrony, i.e.,, the property of grammatical constructions to attract new lexical items over time.
p10024
aVBy providing an empirical measure of semantic similarity between words derived from lexical co-occurrences, distributional semantics not only reliably captures how the verbs in the distribution of a construction are related, but also enables the use of visualization techniques and statistical modeling to analyze the semantic development of a construction over time and identify the semantic determinants of syntactic productivity in naturally occurring data.
p10025
ag106
asba(icmyPackage
FText
p10026
(dp10027
g3
(lp10028
VWe used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words
p10029
aVWe formulated a summarization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document
p10030
aVThe results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts
p10031
aVExtractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g.,, sentences, clauses, and words) and select their subset as a summary
p10032
aVFormulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization [ 16 , 8 , 20 ]
p10033
aVWe can only extract important content by trimming redundant parts from sentences
p10034
aVThey formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees
p10035
aVFirst, we represent a document as a nested tree , which is composed of two types of tree structures a document tree and a sentence tree
p10036
aVThe document tree is a tree that has sentences as nodes and head modifier relationships between sentences obtained by RST as edges
p10037
aVThe sentence tree is a tree that has words as nodes and head modifier relationships between words obtained by the dependency parser as edges
p10038
aVWe can build the nested tree by regarding each node of the document tree as a sentence tree
p10039
aVFinally, we formulate the problem of single document summarization as that of combinatorial optimization, which is based on the trimming of the nested tree
p10040
aVOur method jointly utilizes relations between sentences and relations between words, and extracts a rooted document subtree from a document tree whose nodes are arbitrary subtrees of the sentence tree
p10041
aVWe converted the rhetorical relations between EDUs to the relations between sentences to build the nested tree structure
p10042
aVWe could thus take into account both relations between sentences and relations between words
p10043
aVHowever, it is not trivial to apply their method to text summarization because no compression ratio is given to sentences
p10044
aVAlthough their method generated a well-organized summary, no optimality of information coverage was guaranteed and their method could not accept large texts because of the high computational cost
p10045
aVIn addition, their method required large sets of data to calculate the accurate probability
p10046
aVEach root EDU in a sentence has the parent EDU in another sentence
p10047
aVHence, we can determine the parent-child relations between sentences
p10048
aVAs a result, we obtain a tree that represents the parent-child relations of sentences, and we can use it as a document tree
p10049
aVOur method generates a summary by trimming a nested tree
p10050
aVOur model is shown in Figure 3
p10051
aVLet us denote by w i u'\u005cu2062' j the term weight of word i u'\u005cu2062' j (word j in sentence i x i is a variable that is one if sentence i is selected as part of a summary, and z i u'\u005cu2062' j is a variable that is one if word i u'\u005cu2062' j is selected as part of a summary
p10052
aVAccording to the objective function, the score for the resulting summary is the sum of the term weights w i u'\u005cu2062' j that are included in the summary
p10053
aVWe denote by r i u'\u005cu2062' j the variable that is one if word i u'\u005cu2062' j is selected as a root of an extracting sentence subtree
p10054
aVConstraint ( 1 ) guarantees that the summary length will be less than or equal to limit L
p10055
aVConstraints ( 2 ) and ( 3 ) are tree constraints for a document tree and sentence trees r i u'\u005cu2062' j in Constraint ( 3 ) allows the system to extract non-rooted sentence subtrees, as we previously mentioned
p10056
aVWe can set the candidate for the root node of the subtree by using constraint ( 7
p10057
aVConstraints ( 11 ) and ( 12 ) guarantee that the selected sentence subtree has at least one subject and one object if it has any
p10058
aVThis dataset was first used by Marcu et al for evaluating a text summarization system [ 15 ]
p10059
aVWe used ROUGE [ 13 ] as an evaluation criterion
p10060
aVWe compared our method ( sentence subtree ) with that of EDU selection [ 11 ]
p10061
aVRooted sentence subtree only selects rooted sentence subtrees 2 2 We achieved this by making R c u'\u005cu2062' ( i ) only return the parser u'\u005cu2019' s root node in Figure 7
p10062
aVIt simply selects full sentences from a document tree 3 3 We achieved this by setting u'\u005cu0398' to a very large number
p10063
aVWe applied a multiple test by using Holm u'\u005cu2019' s method and found that our method significantly outperformed EDU selection and sentence selection
p10064
aVThe difference between the sentence subtree and the rooted sentence subtree methods was fairly small
p10065
aVWe therefore qualitatively analyzed some actual examples that will be discussed in Section 4.2.2
p10066
aVFigure 4 has two example sentences for which both methods selected a subtree as part of a summary
p10067
aVThe { u'\u005cu22c5' } indicates the parser u'\u005cu2019' s root word
p10068
aVThe [ u'\u005cu22c5' ] indicates the word that the system selected as the root of the subtree
p10069
aVSubtree selection selected a root in both examples that differed from the parser u'\u005cu2019' s root
p10070
aVAs we can see, subtree selection only selected important subtrees that did not include the parser u'\u005cu2019' s root, e.g.,, purpose-clauses and that-clauses
p10071
aVThis capability is very effective because we have to contain important content in summaries within given length limits, especially when the compression ratio is high (i.e.,, the method has to generate much shorter summaries than the source documents
p10072
aVMany studies that have utilized RST have simply adopted EDUs as textual units [ 14 , 6 , 11 , 12 ]
p10073
aVWhile EDUs are textual units for RST, they are too fine grained as textual units for methods of extractive summarization
p10074
aVTherefore, the models have tended to select small fragments from many sentences to maximize objective functions and have led to fragmented summaries being generated
p10075
aVA fragmented summary is generated when small fragments are selected from many sentences
p10076
aVHence, the number of sentences in the source document included in the resulting summary can be an indicator to measure the fragmentation of information
p10077
aVWe counted the number of sentences in the source document that each method used to generate a summary 5 5 Note that the number for the EDU method is not equal to selected textual units because a sentence in the source document may contain multiple EDUs
p10078
aVIt indicates that EDUs are shorter than the other textual units
p10079
aVHence, the number of sentences tends to be large
p10080
aVAlthough ROUGE scores are widely used as evaluation metrics for text summarization systems, they cannot take into consideration linguistic qualities such as human readability
p10081
aVHence, we plan to conduct evaluations with people 7 7 For example, the quality question metric from the Document Understanding Conference (DUC
p10082
aVHowever, there were also rhetorical structures between EDUs inside individual sentences
p10083
aVHence, utilizing these for sentence compression has been left for future work
p10084
aVThere have been related studies on building RST parsers [ 7 , 1 ] and by using such parsers, we should be able to apply our model to other corpora or to multi-document settings
p10085
asg88
(lp10086
sg90
(lp10087
sg92
(lp10088
VWe proposed a method of summarizing a single document that included relations between sentences and relations between words.
p10089
aVWe built a nested tree and formulated the problem of summarization as that of integer linear programming.
p10090
aVOur method significantly improved the ROUGE score with significantly fewer sentences than the method of EDU selection.
p10091
aVThe results suggest that our method relaxed the fragmentation of information.
p10092
aVWe also discussed the effectiveness of sentence subtree selection that did not restrict rooted subtrees.
p10093
aVAlthough ROUGE scores are widely used as evaluation metrics for text summarization systems, they cannot take into consideration linguistic qualities such as human readability.
p10094
aVHence, we plan to conduct evaluations with people 7 7 For example, the quality question metric from the Document Understanding Conference (DUC.
p10095
aVWe only used the rhetorical structures between sentences in this study.
p10096
aVHowever, there were also rhetorical structures between EDUs inside individual sentences.
p10097
aVHence, utilizing these for sentence compression has been left for future work.
p10098
aVIn addition, we used rhetorical structures that were manually annotated.
p10099
aVThere have been related studies on building RST parsers [ 7 , 1 ] and by using such parsers, we should be able to apply our model to other corpora or to multi-document settings.
p10100
ag106
asg107
S'P14-2052'
p10101
sg109
(lp10102
VMany methods of text summarization combining sentence selection and sentence compression have recently been proposed.
p10103
aVAlthough the dependency between words has been used in most of these methods, the dependency between sentences, i.e.,, rhetorical structures, has not been exploited in such joint methods.
p10104
aVWe used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words.
p10105
aVWe formulated a summarization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document.
p10106
aVThe results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts.
p10107
ag106
asba(icmyPackage
FText
p10108
(dp10109
g3
(lp10110
VAs students read expository text, comprehension is improved by pausing to answer questions that reinforce the material
p10111
aVThe vast majority of systems generate questions by selecting one sentence at a time, extracting portions of the source sentence, then applying transformation rules or patterns in order to construct a question
p10112
aVThese approaches can potentially ask deeper questions due to their focus on semantics
p10113
aVFirst, the source text is divided into sentences which are processed by SENNA 1 1 http://ml.nec-labs.com/senna/ software, described in (Collobert et al., 2011
p10114
aVAdditionally, patterns indicate the semantic arguments that provide the answer to the question, required fields, and filter condition fields
p10115
aVAs these patterns are matched, they will be rejected as candidates for generation for a particular sentence if the required arguments are absent or if filter conditions are present
p10116
aVPatterns specify whether verbs should be included in their lexical form or as they appear in the source text
p10117
aVThe lungs take in air
p10118
aVThe most common use of the verb as it appears in the sentence is with the verb be , as in
p10119
aVThis pattern takes the copular be as it appears in the source text
p10120
aVTextbooks were chosen rather than hand-crafted source material so that a more realistic assessment of performance could be achieved
p10121
aVThe patterns are designed to match only the arguments used as part of the question or the answer, in order to prevent over generation of questions
p10122
aVThe system inserted the correct forms of release and do , and ignored the phrase As this occurs since it is not part of the semantic argument
p10123
aVQuestion 3 is from the source sentence u'\u005cu2019' s 3rd predicate-argument set because this matched the pattern requirements
p10124
aVAnnotators were given instructions to read a paragraph, then the questions based on that paragraph
p10125
aVWe compared our system to the H S and LPN W systems because they produce questions that are the most similar to ours, and for the same purpose reading comprehension reinforcement
p10126
aVThe purpose of this evaluation was to determine if any patterns consistently produce poor questions
p10127
aVWe were also interested to know if first predicates make better questions than later ones
p10128
aVThis task utilized a file (Biology the body) with 56 source sentences from which our system generated 102 questions
p10129
aVThe Heilman and Smith system, as they describe it, takes an over-generate and rank approach
p10130
aVThe file has 93 sentences and our system generated 184 questions; the LPN W system generated roughly 4 times as many questions
p10131
aVFrom each system, 100 questions were randomly selected, making sure that the LPN W questions did not include questions generated from domain-specific templates such as
p10132
aVInterestingly, our system again achieved a 44% reduction in the error rate when averaging over all metrics, just as it did in the Heilman and Smith comparison
p10133
aVNegation detection is a complicated task because negation can occur at the word, phrase or clause level, and because there are subtle shades of negation between definite positive and negative polarities (Blanco and Moldovan, 2011
p10134
aVNot having coreference resolution leads to vague questions, some of which can be filtered as discussed previously
p10135
aVSince current state-of-the-art systems do not deal well with relative and possessive pronouns, this will continue to be a limitation of natural language generation systems for the time being
p10136
aVSince our focus is on expository text, system patterns deal primarily with the present and simple past tenses
p10137
aVSome patterns look for modals and so can handle future tense
p10138
aVLight verbs pose complications in NLG because they are highly idiosyncratic and subject to syntactic variability (Sag et al., 2002
p10139
aVThe catenative construction also potentially adds complexity (Huddleston and Pullum, 2005), as shown in this example
p10140
aVAs the universe expanded, it became less dense and began to cool
p10141
aVCare must be taken not to generate questions based on one predicate in the catenative construction
p10142
aVPlant roots and bacterial decay use carbon dioxide in the process of respiration, the word use was classified as NN, leaving no predicate and no semantic role labels in this sentence
p10143
aVRoediger and Pyc (2012) advocate assisting students in building a strong knowledge base because creative discoveries are unlikely to occur when students do not have a sound set of facts and principles at their command
p10144
aVTo that end, automatic question generation systems can facilitate the learning process by alternating passages of text with questions that reinforce the material learned
p10145
asg88
(lp10146
sg90
(lp10147
sg92
(lp10148
VRoediger and Pyc (2012) advocate assisting students in building a strong knowledge base because creative discoveries are unlikely to occur when students do not have a sound set of facts and principles at their command.
p10149
aVTo that end, automatic question generation systems can facilitate the learning process by alternating passages of text with questions that reinforce the material learned.
p10150
aVWe have demonstrated a semantic approach to automatic question generation that outperforms similar systems.
p10151
aVWe evaluated our system on text extracted from open domain STEM textbooks rather than hand-crafted text, showing the robustness of our approach.
p10152
aVOur system achieved a 44% reduction in the error rate relative to both the Heilman and Smith, and the Lindberg et al system on the average over all metrics.
p10153
aVThe results shows are statistically significant (p 0.001.
p10154
aVOur question generator can be used for self-study or tutoring, or by teachers to generate questions for classroom discussion or assessment.
p10155
aVFinally, we addressed linguistic challenges to question generation.
p10156
ag106
asg107
S'P14-2053'
p10157
sg109
(lp10158
VAs students read expository text, comprehension is improved by pausing to answer questions that reinforce the material.
p10159
aVWe describe an automatic question generator that uses semantic pattern recognition to create questions of varying depth and type for self-study or tutoring.
p10160
aVThroughout, we explore how linguistic considerations inform system design.
p10161
aVIn the described system, semantic role labels of source sentences are used in a domain-independent manner to generate both questions and answers related to the source sentence.
p10162
aVEvaluation results show a 44% reduction in the error rate relative to the best prior systems, averaging over all metrics, and up to 61% reduction in the error rate on grammaticality judgments.
p10163
ag106
asba(icmyPackage
FText
p10164
(dp10165
g3
(lp10166
VIt extends Eisner u'\u005cu2019' s cubic time parsing algorithm by using virtual dependency arcs to link deleted words
p10167
aVSentence compression aims to shorten a sentence by removing uninformative words to reduce reading time
p10168
aVRecent studies used a subtree deletion model for compression [ 1 , 13 , 15 ] , which deletes a word only if its modifier in the parse tree is deleted
p10169
aVTrevor et al. proposed synchronous tree substitution grammar [ 5 ] , which allows local distortion of the tree topology and can thus naturally capture structural mismatches
p10170
aVHowever, the time complexity greatly increases since the parse tree dynamically depends on the compression
p10171
aVOur method extends Eisner u'\u005cu2019' s cubic time parsing algorithm by adding signatures to each span, which indicate the number of deleted words and the rightmost kept word within the span, resulting in O u'\u005cu2062' ( n 6 ) time complexity and O u'\u005cu2062' ( n 4 ) space complexity
p10172
aVWe further propose a faster approximate algorithm based on Lagrangian relaxation, which has T u'\u005cu2062' O u'\u005cu2062' ( n 4 ) running time and O u'\u005cu2062' ( n 3 ) space complexity ( T is the iteration number in the subgradient decent algorithm
p10173
aVWe define the sentence compression task as given a sentence composed of n words, u'\u005cud835' u'\u005cudc31' = x 1 , u'\u005cu2026' , x n , and a length L u'\u005cu2264' n , we need to remove ( n - L ) words from u'\u005cud835' u'\u005cudc31' , so that the sum of the weights of the dependency tree and word bigrams of the remaining part is maximized
p10174
aVwhere u'\u005cud835' u'\u005cudc33' is a binary vector, z i indicates x i is kept or not u'\u005cud835' u'\u005cudc32' is a square matrix denoting the projective dependency parse tree over the remaining words, y i u'\u005cu2062' j indicates if x i is the head of x j (note that each word has exactly one head w i tok is the informativeness of x i , w i u'\u005cu2062' j bgr is the score of bigram x i u'\u005cu2062' x j in an n-gram model, w dep is the score of dependency arc x i u'\u005cu2192' x j in an arc-factored dependency parsing model
p10175
aVHence, the first part of the objective function is the total score of the kept words, the second and third parts are the scores of the parse tree and bigrams of the compressed sentence, z i u'\u005cu2062' z j u'\u005cu2062' u'\u005cu220f' i k j ( 1 - z k ) = 1 indicates both x i and x j are kept, and are adjacent after compression
p10176
aVA span is a subtree over a number of consecutive words, with the leftmost or the rightmost word as its root
p10177
aVAn incomplete span denoted as I j i is a subtree inside a single arc x i u'\u005cu2192' x j , with root x i
p10178
aVThere are two rules for merging spans one merges two complete spans into an incomplete span, the other merges an incomplete span and a complete span into a large complete span
p10179
aVThe scores of unigrams w i tok can be transfered to the dependency arcs, so that we can remove all linear terms w i tok u'\u005cu2062' z i from the objective function
p10180
aVIf z j = 0 , then in both equations, all terms having z j are zero; If z j = 1 , i.e.,, x j is kept, since it has exactly one head word x k in the compressed sentence, the sum of the terms having z j is w j tok + w k u'\u005cu2062' j dep for both equations
p10181
aVTherefore, we only need to consider the scores of arcs
p10182
aVFor any compressed sentence, we could augment its dependency tree by adding a virtual arc i - 1 u'\u005cu2192' i for each deleted word x i
p10183
aVIf the first word x 1 is deleted, we connect it to the root of the parse tree x 0 , as shown in Figure 3
p10184
aVIn this way, we derive a full parse tree of the original sentence
p10185
aVWe can reversely get the the compressed parse tree by removing all virtual arcs from the full parse tree
p10186
aVWe restrict the score of all the virtual arcs to be zero, so that scores of the two parse trees are equivalent
p10187
aVThe two complete spans must be single words, as the length of the virtual arc is 1
p10188
aVThe number of the virtual arcs within C j i + 1 must be j - i - 1 , since the descendants of the modifier of a virtual arc x j must be removed
p10189
aVThe root node is allowed to have two modifiers one is the modifier in the compressed sentence, the other is the first word if it is removed
p10190
aVFor each combination, the algorithm enumerates the number of virtual arcs in the left and right spans, and the split position (e.g.,, k u'\u005cu2032' , k u'\u005cu2032' u'\u005cu2032' , r in case 2), thus it takes O u'\u005cu2062' ( n 3 ) running time
p10191
aVSuppose x j is removed, there must be a virtual arc j - 1 u'\u005cu2192' j which is a conflict with the fact that x j is the leftmost word
p10192
aVAs x j is a descendant of x i , x i must be kept u'\u005cu220e'
p10193
aVAccording to the proposition above, if the right span is right-headed, its leftmost word is kept
p10194
aVIf the right span is left-headed, there are two cases its leftmost word is kept, or no word in the span is kept
p10195
aVAccording to the proposition above, we have, for any right-headed span p = i
p10196
aVFixing u'\u005cu039b' , the optimal u'\u005cud835' u'\u005cudc33' , u'\u005cud835' u'\u005cudc32' can be found using a simpler version of the algorithm above
p10197
aVWe drop the signature of the virtual arc number from each span, and thus obtain an O u'\u005cu2062' ( n 4 ) time algorithm
p10198
aVFixing u'\u005cud835' u'\u005cudc33' , u'\u005cud835' u'\u005cudc32' , the dual variable is updated by
p10199
aVWe use the same partitions as [ 10 ] , i.e.,, 1,188 sentences for training and 441 for testing
p10200
aVOur model is discriminative u'\u005cu2013' the scores of the unigrams, bigrams and dependency arcs are the linear functions of features, that is, w i tok = u'\u005cud835' u'\u005cudc2f' T u'\u005cu2062' u'\u005cud835' u'\u005cudc1f' u'\u005cu2062' ( x i ) , where u'\u005cud835' u'\u005cudc1f' is the feature vector of x i , and u'\u005cud835' u'\u005cudc2f' is the weight vector of features
p10201
aVThe learning task is to estimate the feature weight vector based on the manually compressed sentences
p10202
aVThen we augment these parse trees by adding virtual arcs and get the full parse trees of their corresponding original sentences
p10203
aVThe first is linear chain CRFs, where the compression task is casted as a binary sequence labeling problem
p10204
aVIt usually achieves high unigram F1 score but low grammatical relation F1 score since it only considers the local interdependence between adjacent words
p10205
aVThe second is the subtree deletion model [ 1 ] which is solved by integer linear programming (ILP) 2 2 We use Gurobi as the ILP solver in the paper http://www.gurobi.com/
p10206
aVThe third one is the bigram model proposed by McDonald [ 12 ] which adopts dynamic programming for efficient inference
p10207
aVAs expected, the joint models (ours and TM13) consistently outperform the subtree deletion model, since the joint models do not suffer from the subtree restriction
p10208
aVIt is not surprising that CRFs achieve high unigram F scores but low syntactic F scores as they do not consider the fluency of the compressed sentence
p10209
aVCompared with TM13 u'\u005cu2019' s system, our model with exact decoding is not significantly faster due to the high order of the time complexity
p10210
aVIn practice it achieves nearly the same accuracy as the exact one, but is much faster
p10211
aVIn the future, we will study the non-projective cases based on the recent parsing techniques for 1-endpoint-crossing trees [ 14 ]
p10212
asg88
(lp10213
sg90
(lp10214
sg92
(lp10215
VIn this paper, we proposed two polynomial time decoding algorithms using joint inference for sentence compression.
p10216
aVThe first one is an exact dynamic programming algorithm, and requires O ( n 6 ) running time.
p10217
aVThis one does not show significant advantage in speed over ILP.
p10218
aVThe second one is an approximation of the first algorithm.
p10219
aVIt adopts Lagrangian relaxation to eliminate the compression ratio constraint, yielding lower time complexity T O ( n 4.
p10220
aVIn practice it achieves nearly the same accuracy as the exact one, but is much faster.
p10221
aV3 3 Our code is available at http://code.google.com/p/sent-compress/.
p10222
aVThe main assumption of our method is that the dependency parse tree is projective, which is not true for some other languages.
p10223
aVIn that case, our method is invalid, but [ 17 ] still works.
p10224
aVIn the future, we will study the non-projective cases based on the recent parsing techniques for 1-endpoint-crossing trees [ 14 ].
p10225
ag106
asg107
S'P14-2054'
p10226
sg109
(lp10227
VWe propose two polynomial time inference algorithms to compress sentences under bigram and dependency-factored objectives.
p10228
aVThe first algorithm is exact and requires O ( n 6 ) running time.
p10229
aVIt extends Eisner s cubic time parsing algorithm by using virtual dependency arcs to link deleted words.
p10230
aVTwo signatures are added to each span, indicating the number of deleted words and the rightmost kept word within the span.
p10231
aVThe second algorithm is a fast approximation of the first one.
p10232
aVIt relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O ( n 4 ) running time.
p10233
aVExperimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach.
p10234
ag106
asba(icmyPackage
FText
p10235
(dp10236
g3
(lp10237
VIn order to summarize a document, it is often useful to have a background set of documents from the domain to serve as a reference for determining new and important information in the input document
p10238
aVWe present a model based on Bayesian surprise which provides an intuitive way to identify surprising information from a summarization input with respect to a background corpus
p10239
aVWe develop systems for generic and update summarization based on this idea
p10240
aVOur method provides competitive content selection performance with particular advantages in the update task where systems are given a small and topical background corpus
p10241
aVIn this generic task, some of the best reported results were obtained by a system [] which computed importance scores for words in the input by examining if the word occurs with significantly higher probability in the input compared to a large background collection of news articles
p10242
aVIn this work, we present a Bayesian model for assessing the novelty of a sentence taken from a summarization input with respect to a background corpus of documents
p10243
aVOur model is based on the idea of Bayesian Surprise []
p10244
aVFor illustration, assume that a user u'\u005cu2019' s background knowledge comprises of multiple hypotheses about the current state of the world and a probability distribution over these hypotheses indicates his degree of belief in each hypothesis
p10245
aVFor example, one hypothesis may be that the political situation in Ukraine is peaceful , another where it is not
p10246
aVWe use the method to do two types of summarization tasks a) generic news summarization which uses a large random collection of news articles as the background, and b) update summarization where the background is a smaller but specific set of news documents on the same topic as the input set
p10247
aVSystems were given a list of documents ranked according to relevance to a query
p10248
aVEven for generic summarization, some of the best results were obtained by by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by
p10249
aVCentral to this approach is the use of a likelihood ratio test to compute topic words , words that have significantly higher probability in the input compared to the background corpus, and are hence descriptive of the input u'\u005cu2019' s topic
p10250
aVIn this work, we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach
p10251
aVH 2 t is a topic word, hence p ( t
p10252
aVA convenient aspect of this approach is that - 2 u'\u005cu2062' log u'\u005cu2061' u'\u005cu039b' is asymptotically u'\u005cu03a7' 2 distributed
p10253
aVSo for a resulting - 2 u'\u005cu2062' log u'\u005cu2061' u'\u005cu039b' value, we can use the u'\u005cu03a7' 2 table to find the significance level with which the null hypothesis H 1 can be rejected
p10254
aVFor example, a value of 10 corresponds to a significance level of 0.001 and is standardly used as the cutoff
p10255
aVWords with - 2 u'\u005cu2062' log u'\u005cu2061' u'\u005cu039b' 10 are considered topic words u'\u005cu2019' s system gives a weight of 1 to the topic words and scores sentences using the number of topic words normalized by sentence length
p10256
aVThe surprise S u'\u005cu2062' ( D , u'\u005cud835' u'\u005cudc07' ) created by D on hypothesis space H is defined as the difference between the prior and posterior distributions over the hypotheses, and is computed using KL divergence
p10257
aVNote that since KL-divergence is not symmetric, we could also compute KL ( P ( H ) , P ( H
p10258
aVIn some cases, surprise can be computed analytically, in particular when the prior distribution is conjugate to the form of the hypothesis, and so the posterior has the same functional form as the prior
p10259
aVP u'\u005cu2062' ( H ) gives a prior probability to each hypothesis based on the information in the background corpus
p10260
aVV are the concentration parameters of the Dirichlet distribution (and will be set using the background corpus as explained in Section 4.2
p10261
aVNow consider a new observation I (a text, sentence, or paragraph from the summarization input ) and the word counts in I given by ( c 1 , c 2 , u'\u005cu2026' , c V
p10262
aV1 1 An alternative algorithm could directly compute the surprise of a sentence by incorporating the words from the sentence into the posterior
p10263
aVHowever, we found this specific method to not work well probably because the few and unrepeated content words from a sentence did not change the posterior much
p10264
aVIn future, we plan to use latent topic models to assign a topic to a sentence so that the counts of all the sentence u'\u005cu2019' s words can be aggregated into one dimension
p10265
aVWe follow a greedy approach to optimize the summary surprise by choosing the most surprising sentence, the next most surprising and so on
p10266
aVAt the same time, we aim to avoid redundancy, i.e., selecting sentences with similar content
p10267
aVSurprise is computed based on the changes in probabilities of all of these hypotheses upon seeing the summarization input ii) The computation of topic words is local, it assumes a binomial distribution and the occurrence of a word is independent of others
p10268
aVIn practice for both tasks, a new summarization input can have words unseen in the background
p10269
aVSo new words in an input are added to the background corpus with a count of 1 and the counts of existing words in the background are incremented by 1 before computing the prior parameters
p10270
aVu'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc12' u'\u005cud835' u'\u005cudc2c' u'\u005cud835' u'\u005cudc2e' u'\u005cud835' u'\u005cudc26' , u'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc12' u'\u005cud835' u'\u005cudc1a' u'\u005cud835' u'\u005cudc2f' u'\u005cud835' u'\u005cudc20' use topic words computed as described in Section 2 and utilizing the same background corpus for the generic and update tasks as the surprise-based methods
p10271
aVFor the generic task, we use a critical value of 10 (0.001 significance level) for the u'\u005cu03a7' 2 distribution during topic word computation
p10272
aVTo reduce redundancy, once a sentence is added, the topic words contained in it are removed from the topic word list before the next sentence selection
p10273
aVRather the method creates a summary by optimizing for high similarity of the summary with the input word distribution
p10274
aVThese systems combine (i) a score based on the background ( KL back , TS or SR ) with (ii) the score based on the input only ( KL inp
p10275
aVFor example, to combine TS sum and KL inp for each sentence, we compute its scores based on the two methods
p10276
aVThen we normalize the two sets of scores for candidate sentences using z-scores and compute the best sentence as arg u'\u005cu2062' max s i ( TS sum u'\u005cu2062' ( s i ) - KL inp u'\u005cu2062' ( s i )
p10277
aVWe refer to the surprise-based summaries as u'\u005cud835' u'\u005cudc12' u'\u005cud835' u'\u005cudc11' u'\u005cud835' u'\u005cudc2c' u'\u005cud835' u'\u005cudc2e' u'\u005cud835' u'\u005cudc26' and u'\u005cud835' u'\u005cudc12' u'\u005cud835' u'\u005cudc11' u'\u005cud835' u'\u005cudc1a' u'\u005cud835' u'\u005cudc2f' u'\u005cud835' u'\u005cudc20' depending on the type of composition function (Section 4.1
p10278
aVFirst, consider generic summarization and the systems which use the background corpus only (those above the horizontal line
p10279
aVNumerically, SR sum has the highest ROUGE-1 score and TS sum tops according to ROUGE-2
p10280
aVA system should be able to use smaller amounts of background information and as new data arrives, be able to incorporate the evidence
p10281
asg88
(lp10282
sg90
(lp10283
sg92
(lp10284
VWe have introduced a Bayesian summarization method that strongly aligns with intuitions about how people use existing knowledge to identify important events or content in new observations.
p10285
aVOur method is especially valuable when a system must utilize a small background corpus.
p10286
aVWhile the update task datasets we have used were carefully selected and grouped by NIST assesors into initial and background sets, for systems on the web, there is little control over the number of background documents on a particular topic.
p10287
aVA system should be able to use smaller amounts of background information and as new data arrives, be able to incorporate the evidence.
p10288
aVOur Bayesian approach is a natural fit in such a setting.
p10289
ag106
asg107
S'P14-2055'
p10290
sg109
(lp10291
VIn order to summarize a document, it is often useful to have a background set of documents from the domain to serve as a reference for determining new and important information in the input document.
p10292
aVWe present a model based on Bayesian surprise which provides an intuitive way to identify surprising information from a summarization input with respect to a background corpus.
p10293
aVSpecifically, the method quantifies the degree to which pieces of information in the input change one s beliefs about the world represented in the background.
p10294
aVWe develop systems for generic and update summarization based on this idea.
p10295
aVOur method provides competitive content selection performance with particular advantages in the update task where systems are given a small and topical background corpus.
p10296
ag106
asba(icmyPackage
FText
p10297
(dp10298
g3
(lp10299
VIn this paper, we combine existing NLP techniques with minimal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning
p10300
aV1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so that the learner, when seeing or hearing the word, immediately recalls the keyword(s
p10301
aVTo illustrate, for teaching the Italian word cuore which means heart in English, the learner might be asked to imagine u'\u005cu201c' a lonely heart with a hard core u'\u005cu201d'
p10302
aVHowever, the preparation of the memorization tips for each new word is an activity that requires considerable time, linguistic competence and creativity
p10303
aVIn [] , we proposed to automate the keyword method by retrieving sentences from the Web
p10304
aVIn this paper, we overcome these limitations by introducing a semi-automatic system implementing the keyword method that builds upon the keyword selection mechanism of and combines it with a state-of-the-art creative sentence generation framework []
p10305
aVAccording to our scenario, the teacher relies on automatic techniques to generate relatively few, high quality mnemonics in English to teach Italian vocabulary
p10306
aVTheir results show that using KM leads to better learning of second language vocabulary for beginners
p10307
aVWhile we had discussed the potentiality of creative sentence generation as a useful teaching device, we had not validated our claim experimentally yet
p10308
aVAs a previous attempt at using NLP for education, employ a riddle generator to create a language playground for children with complex communication needs
p10309
aVWe started by compiling a collection of Italian nouns consisting of three syllables from various resources for vocabulary teaching including http://didattica.org/italiano.htm and http://ielanguages.com , and produced a list of 185 target L2 words
p10310
aVUnlike in [] , where we did not enforce any constraints for selecting the keywords, in this case we applied a more sophisticated filtering and ranking strategy
p10311
aVWe allowed the keyword generation module to consider all the entries in the CMU dictionary, and rank the keyword pairs based on the following criteria in decreasing order of precedence
p10312
aV1) Keywords with a smaller orthographic/phonetic distance are preferred; 2) Keywords consisting of a single word are preferred over two words (e.g.,, for the target word lavagna , which means blackboard , lasagna takes precedence over love and onion ); 3) Keywords that do not contain stop words are preferred (e.g.,, for the target word pettine , which means comb , the keyword pair pet and inn is ranked higher than pet and in , since in is a stop word); 4) Keyword pairs obtained with orthographic similarity are preferred over those obtained with phonetic similarity, as learners might be unfamiliar with the phonetic rules of the target language
p10313
aVWe selected up to three of the highest ranked keyword pairs for each target word, obtaining 407 keyword combinations for the initial 185 Italian words, which we used as the input for the sentence generator
p10314
aVIn this step, our goal was to generate, for each Italian word, sentences containing its L1 translation and the set of orthographically (or phonetically) similar keywords that we previously selected
p10315
aVFor each keyword combination, starting from the top-ranked ones, we generated up to 10 sentences by allowing any known part-of-speech for the keywords
p10316
aVThe system relies on two corpora of automatic parses as a repository of sentence templates and lexical statistics
p10317
aVAs for the former, we combined two resources a corpus of 16,000 proverbs [] and a collection of 5,000 image captions 2 2 http://vision.cs.uiuc.edu/pascal-sentences/ collected by
p10318
aVWe chose these two collections since they offer a combination of catchy or simple sentences that we expect to be especially suitable for second language learning
p10319
aVAs for the second corpus, we used LDC u'\u005cu2019' s English GigaWord 5th Edition 3 3 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07
p10320
aVFor comparison, when we attempted to retrieve sentences from the Web as suggested in , we could collect an output for less than 10% of the input configurations
p10321
aVAmong the sentences u'\u005cu201c' The girl received a call in the bathroom u'\u005cu201d' and u'\u005cu201c' Call the blond girl in case you need u'\u005cu201d' , the first one is preferred, since the keywords are closer to each other
p10322
aVTo illustrate, for the same keywords and the target words, we would prefer the sentence u'\u005cu201c' I called him in the morning yesterday u'\u005cu201d' over u'\u005cu201c' You talk a lot in a call u'\u005cu201d'
p10323
aVAccordingly, for each target word in random order, we sequentially scanned the outputs generated for each keyword pair
p10324
aVWe continued this process until we selected a sentence for 40 distinct target words, which we set as the target size of the experiment
p10325
aVWe had to inspect the outputs generated for 48 target words before we were able to select 40 good examples, meaning that for 17% of the target words the sentence generator could not produce a sentence of acceptable quality
p10326
aV4 4 We preferred to select the experiment subjects in person as opposed to crowdsourcing the evaluation to be able to verify the proficiency of the subjects in the two languages and to ensure the reliability of the outcome of the evaluation
p10327
aVAfter obtaining the sentences as explained in Section 3 , we shuffled and then divided the whole set including 40 target words together with their translation, the generated keywords and sentences into 2 batches (A, B) and further divided each batch into 2 groups consisting of 10 elements (A1, A2, B1 and B2
p10328
aV5 5 Otherwise, they could easily filter out the wrong answers just because they were not exposed to them recently
p10329
aVDue to the presence of the u'\u005cu201c' I already knew this word u'\u005cu201d' option in the learning-assessment questionnaire, the number of the actual answers provided by each subject can be slightly different, hence the difference between macro- and micro-average
p10330
aVThe error rate for each memorization technique t (where t = R for u'\u005cu201c' Rote memorization u'\u005cu201d' and t = K for u'\u005cu201c' keyword-aided memorization u'\u005cu201d' ) is calculated as e t = i t c t + i t , where c t and i t are the number of correct and incorrect answers provided by the subjects, respectively
p10331
aVThe absolute error rate reduction u'\u005cu0394' u'\u005cu2062' e is calculated as the absolute difference in error rate between rote and keyword-aided memorization, i.e u'\u005cu0394' e = e R - e K
p10332
aVFinally, the relative error rate reduction % e is calculated as the the ratio between the absolute error rate reduction u'\u005cu0394' u'\u005cu2062' e and the error rate of rote memorization e R , i.e
p10333
aVThe results clearly indicate that one group (A1) by chance contained easier words to memorize as shown by the low error rate (between 3% and 4%) obtained with both methods
p10334
aVSimilarly, groups A2 and B1 are of average difficulty, whereas group B2 appears to be the most difficult, with an error rate higher than 22% when using only rote memorization
p10335
aVInterestingly, there is a strong correlation (Pearson u'\u005cu2019' s r = 0.85 ) between the difficulty of the words in each group (measured as the error rate on rote memorization) and the positive contribution of the generated sentences to the learning process
p10336
aVIn fact, we can see how the relative error rate reduction % e increases from u'\u005cu223c' 17% (group A1) to almost 45% (group B2
p10337
aVBased on the results obtained by , who showed that the keyword method results in better long-term word retention than rote memorization, we would expect the error rate reduction to be even higher in a delayed post-test
p10338
aVThe significant reduction in retention error rate (between 17% and 45% on different word groups) for the words learned with the aid of the automatically generated sentences shows that they are a viable low-effort alternative to human-constructed examples for vocabulary teaching
p10339
aVAs future work, it would be interesting to involve learners in an interactive evaluation to understand the extent to which learners can benefit from ad-hoc personalization
p10340
aVFurthermore, it should be possible to use frameworks similar to the one that we presented to automate other teaching devices based on sentences conforming to specific requirements [] , such as verbal chaining and acrostic
p10341
asg88
(lp10342
sg90
(lp10343
sg92
(lp10344
VIn this paper, we have presented a semi-automatic system for the automation of the keyword method and used it to teach 40 Italian words to 20 English native speakers.
p10345
aVWe let the system select appropriate keywords and generate sentences automatically.
p10346
aVFor each Italian word, we selected the most suitable among the 10 highest ranked suggestions and used it for the evaluation.
p10347
aVThe significant reduction in retention error rate (between 17% and 45% on different word groups) for the words learned with the aid of the automatically generated sentences shows that they are a viable low-effort alternative to human-constructed examples for vocabulary teaching.
p10348
aVAs future work, it would be interesting to involve learners in an interactive evaluation to understand the extent to which learners can benefit from ad-hoc personalization.
p10349
aVFurthermore, it should be possible to use frameworks similar to the one that we presented to automate other teaching devices based on sentences conforming to specific requirements [] , such as verbal chaining and acrostic.
p10350
ag106
asg107
S'P14-2058'
p10351
sg109
(lp10352
VIn this paper, we combine existing NLP techniques with minimal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning.
p10353
aVWe present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task.
p10354
aVThe results demonstrate that NLP techniques can effectively support the development of resources for second language learning.
p10355
aV[1]itemsep=-3pt.
p10356
aV0.8em \u005calgtext *EndIf \u005calgtext *EndFor \u005calgtext *EndWhile.
p10357
ag106
asba(icmyPackage
FText
p10358
(dp10359
g3
(lp10360
VWouldn u'\u005cu2019' t it be helpful if your text editor automatically suggested papers that are relevant to your research
p10361
aVWouldn u'\u005cu2019' t it be even better if those suggestions were contextually relevant
p10362
aVExploiting the human judgements that are already implicit in available resources, we avoid purpose-specific annotation
p10363
aVWe apply this evaluation to three sets of methods for representing a document, based on a) the contents of the document, b) the surrounding contexts of citations to the document found in other documents, and c) a mixture of the two
p10364
aVWouldn u'\u005cu2019' t it be helpful if your editor automatically suggested some references that you could cite here
p10365
aVIf the system is able to take into account the context in which the citation occurs u'\u005cu2014' for example, that papers relevant to our example above are not only about text generation systems, but specifically mention applying coherence theories u'\u005cu2014' then this would be much more informative
p10366
aVSo we define a context-based citation recommendation ( cbcr ) system as one that assists the author of a draft document by suggesting other documents with content that is relevant to a particular context in the draft
p10367
aVThis can be captured as a set of relevance judgements for candidate citations over a corpus of documents, which is an arduous effort that requires considerable manual input and very careful preparation
p10368
aVCitation Resolution is a method for evaluating cbcr systems that is exclusively based on this source of human judgements
p10369
aVFirst, evaluation can be carried out through user studies, which is costly because it cannot be reused (e.g., Chandrasekaran et al
p10370
aVThis is a standard approach in IR, known as building a test collection [ 13 ] , which the author herself notes was an arduous and time-consuming task
p10371
aVThird, as we outlined above, existing citations between papers can be exploited as a source of human judgements
p10372
aVNormalized Discounted Cumulative Gain , a measure based on the rank of the original reference in the list of suggested references, its score decreasing logarithmically
p10373
aVHowever, these metrics fail to adequately recognise that the particular reference used by an author e.g., in support of an argument or as exemplification of an approach, may not be the most appropriate that could be found in the whole collection
p10374
aVWe have then chosen top-1 accuracy as our metric, where every time the original citation is first on the list of suggestions, it receives a score of 1, and 0 otherwise, and these scores are averaged over all resolved citations in the document collection
p10375
aVThis metric is intuitive in measuring the efficiency of the system at this task, as it is immediately interpretable as a percentage of success
p10376
aVWhile previous experiments in cbcr , like the ones we have just presented, have treated the task as an Information Retrieval problem, our ultimate purpose is different and travels beyond IR into Question Answering
p10377
aVWe expect this will allow us to identify claims made in a draft paper and match them with related claims made in other papers for support or contrast, and so offer answers in the form of relevant passages extracted from the suggested documents
p10378
aVIt is frequently observed that the reasons for citing a paper go beyond its contribution to the field and its relevance to the research being reported [ 7 ]
p10379
aVOne is based on the contents of the document itself, one is based on the existing contexts of citations of this paper in other documents, and the third is a mixture of the two
p10380
aVThis representation can be based on any information found in the document collection, excluding the document d itself e.g., the text of the referenced document and the text of documents that cite it
p10381
aVWe then attempt to resolve the citation by computing a score for the match between each reference representation and the citation context (b.ii
p10382
aVThat is, if any of the references in a multiple citation of n elements appears in the first n positions of the list of suggestions, it counts as a successful resolution and receives a score of 1
p10383
aVThis is an ideal corpus for these tests for a large number of reasons, but these are key for us all the papers are freely available, the ratio of collection-internal references for each paper is high (the authors measure it at 0.33) and it is a familiar domain for us
p10384
aVWe then make the document u'\u005cu2019' s collection-internal references our test collection D and use a number of methods for generating the document representation
p10385
aVWe use the well-known Vector Space Model and a standard implementation of tf-idf and cosine similarity as implemented by the scikit-learn Python framework 3 3 http://scikit-learn.org
p10386
aVAt present, we are applying no cut-off and just rank all of the document u'\u005cu2019' s collection-internal references for each citation context, aiming to rank the correct one in the first positions in the list
p10387
aVWe tested three different approaches to generating a document u'\u005cu2019' s VSM representation internal representations , which are based on the contents of the document, external representations , which are built using a document u'\u005cu2019' s incoming link citation contexts (following Ritchie ( 2009 ) and He et al
p10388
aVWe present the results of using 250 , 300 and 350 as values for k
p10389
aVThe external representations ( inlink_context ) are based on extracting the context around citation tokens to the document from other documents in the collection, excluding the set of test papers
p10390
aVThis context is extracted in the same way as the query as a window, or list of w tokens surrounding the citation left and right
p10391
aVWe present our best results, using symmetrical and asymmetrical windows of w = [ ( 5 , 5 ) , ( 10 , 10 ) , ( 10 , 5 ) , ( 20 , 20 ) , ( 30 , 30 ) ]
p10392
aV2013 ) showed that automatically generated summaries lead to similar recall and better indexing precision than full-text articles for a keyword-based indexing task
p10393
aVWhether this is because the descriptions of these papers in the contexts of incoming link citations capture the essence or key relevance of the paper, or whether this effect is due to authors reusing their work or to these descriptions originating in a seed paper and being then propagated through the literature, remain interesting research questions that we intend to tackle in future work
p10394
aVThe highest score is 0.469 , achieved by a combination of inlink_context_20 and the passage method, for a window of w = 20 , with a tie between using 250 and 350 as values for k (passage size
p10395
aVThe small difference in score between parameter values is perhaps not as relevant as the finding that, taken together, mixed methods consistently beat both external and internal methods
p10396
aVOur ultimate goal is matching claims and comparing methods, which would likely benefit from an analysis of the full contents of the document and not just previous citations of it, so in future work we also intend to use the context from the successful external results as training data for a summarisation stage
p10397
aVOur method exploits the implicit human relevance judgements found in existing scientific articles and so does not require purpose-specific human annotation
p10398
aVWe have employed Citation Resolution to test three approaches to building a document representation for a CBCR system internal (based on the contents of the document), external (based on the surrounding contexts to citations to that document) and mixed (a mixture of the two
p10399
aVOur evaluation shows that
p10400
aV1) using chunks of a document (passages) as its representation yields better results that using its full text, 2) external methods obtain higher scores than internal ones, and 3) mixed methods yield better results than either in isolation
p10401
aVOur ultimate goal is not just to suggest to the author documents that are u'\u005cu201c' relevant u'\u005cu201d' to a specific chunk of the paper (sentence, paragraph, etc.), but to do so with attention to rhetorical structure and thus to citation function
p10402
aVWe also aim to apply our evaluation to other document collections in different scientific domains in order to test to what degree these results can be generalized
p10403
asg88
(lp10404
sg90
(lp10405
sg92
(lp10406
VIn this paper we have presented Citation Resolution an evaluation method for context-based citation recommendation (CBCR) systems.
p10407
aVOur method exploits the implicit human relevance judgements found in existing scientific articles and so does not require purpose-specific human annotation.
p10408
aVWe have employed Citation Resolution to test three approaches to building a document representation for a CBCR system internal (based on the contents of the document), external (based on the surrounding contexts to citations to that document) and mixed (a mixture of the two.
p10409
aVOur evaluation shows that.
p10410
aV1) using chunks of a document (passages) as its representation yields better results that using its full text, 2) external methods obtain higher scores than internal ones, and 3) mixed methods yield better results than either in isolation.
p10411
aVWe intend to investigate more sophisticated ways of document representation and of extracting a citation s context.
p10412
aVOur ultimate goal is not just to suggest to the author documents that are relevant to a specific chunk of the paper (sentence, paragraph, etc.), but to do so with attention to rhetorical structure and thus to citation function.
p10413
aVWe also aim to apply our evaluation to other document collections in different scientific domains in order to test to what degree these results can be generalized.
p10414
ag106
asg107
S'P14-2059'
p10415
sg109
(lp10416
VWouldn t it be helpful if your text editor automatically suggested papers that are relevant to your research.
p10417
aVWouldn t it be even better if those suggestions were contextually relevant.
p10418
aVIn this paper we name a system that would accomplish this a context-based citation recommendation ( cbcr ) system.
p10419
aVWe specifically present Citation Resolution, a method for the evaluation of cbcr systems which exclusively uses readily-available scientific articles.
p10420
aVExploiting the human judgements that are already implicit in available resources, we avoid purpose-specific annotation.
p10421
aVWe apply this evaluation to three sets of methods for representing a document, based on a) the contents of the document, b) the surrounding contexts of citations to the document found in other documents, and c) a mixture of the two.
p10422
ag106
asba(icmyPackage
FText
p10423
(dp10424
g3
(lp10425
VPerhaps the most demanding such application is text-to-speech synthesis (TTS) since, while for parsing, machine translation and information retrieval it may be acceptable to leave such things as numbers and abbreviations unexpanded, for TTS all tokens need to be read , and for that it is necessary to know how to pronounce them
p10426
aVIn other applications, such as TTS or typing auto-correction, the resulting normalized string itself is directly presented to the user; hence errors in normalization can have a very high cost relative to leaving tokens unnormalized
p10427
aVIn this paper we concentrate on abbreviations, which we define as alphabetic NSWs that it would be normal to pronounce as their expansion
p10428
aVThis class of NSWs is particularly common in personal ads, product reviews, and so forth
p10429
aVIf a system is unable to reliably predict the correct reading for a string, it is better to leave the string alone and have it default to, say, a character-by-character reading, than to expand it to something wrong
p10430
aVIdeally a navigation system should read turn on 30N correctly as turn on thirty north ; but if it cannot resolve the ambiguity in 30N , it is far better to read it as thirty N than as thirty Newtons , since listeners can more easily recover from the first kind of error than the second
p10431
aVWe present methods for learning abbreviation expansion models that favor high precision (incorrect expansions 2 u'\u005cu2062' %
p10432
aVThen a small amount of annotated data is used to build models to determine whether to accept a candidate expansion of an abbreviation based on these features
p10433
aVThe data we report on are taken from Google Maps TM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich
p10434
aVExpanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style
p10435
aVOn the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviations in that case is neutral with respect to preserving the intent of the original text
p10436
aVSince our target application is text-to-speech, we define the task in terms of an existing TTS lexicon
p10437
aVIf a word is already in the lexicon, it is left unprocessed, since there is an existing pronunciation for it; if a word is out-of-vocabulary (OOV), we consider expanding it to a word in the lexicon
p10438
aV1 1 We do not deal here with phonetic spellings in abbreviations such as 4get , or cases where letters have been transposed due to typographical errors ( scv
p10439
aVFirst, we used it to bootstrap a model for assigning a probability of an abbreviation/expansion pair
p10440
aVSecond, we used it to extract contextual n-gram features for predicting possible expansions
p10441
aVOOVs labeled as abbreviations were also labeled with the correct expansion
p10442
aVWe collect potential abbreviation/full-word pairs by looking for terms that could be abbreviations of full words that occur in the same context
p10443
aVThe same contextual information of course is used later on to disambiguate which of the expansions is appropriate for the context.) To compute the initial guess as to what can be a possible abbreviation, a Thrax grammar [] is used that, among other things, specifies that the abbreviation must start with the same letter as the full word; if a vowel is deleted, all adjacent vowels should also be deleted; consonants may be deleted in a cluster, but not the last one; and a (string) suffix may be deleted
p10444
aV2 2 This Thrax grammar can be found at http://openfst.cs.nyu.edu/twiki/bin/view/Contrib/ThraxContrib We count a pair of words as u'\u005cu2018' co-occurring u'\u005cu2019' if they are observed in the same context
p10445
aVThen, for any pair of words u , v , we can assign a pair count based on the count of contexts where both occur
p10446
aVLet c u'\u005cu2062' ( u ) be defined as u'\u005cu2211' v c u'\u005cu2062' ( u , v
p10447
aVWe further filter the potential abbreviations by removing ones that have a lot of potential expansions, where we set the cutoff at 10
p10448
aVOur abbreviation model is a pair character language model (LM), also known as a joint multi-gram model [] , whereby aligned symbols are treated as a single token and a smoothed n-gram model is estimated
p10449
aVThis defines a joint distribution over input and output sequences, and can be efficiently encoded as a weighted finite-state transducer
p10450
aVThe extracted abbreviation/expansion pairs are character-aligned and a 7-gram pair character LM is built over the alignments using the OpenGrm n-gram library []
p10451
aVThe cost from this LM, normalized by the length of the expansion, serves as a score for the quality of a putative expansion for an abbreviation
p10452
aVFor a small set of frequent, conventionalized abbreviations (e.g.,, ca for California u'\u005cu2014' 63 pairs in total u'\u005cu2014' mainly state abbreviations and similar items), we assign an fixed pair LM score, since these examples are in effect irregular cases, where the regularities of the productive abbreviation process do not capture their true cost
p10453
aVFirst, we simply train a smoothed n-gram LM from the data
p10454
aVBecause of the size of the data set, this is heavily pruned using relative entropy pruning []
p10455
aVWe randomly selected 14,434 OOVs in their full context, and had them manually annotated as falling within one of 8 categories, along with the expansion if the category was u'\u005cu2018' abbreviation u'\u005cu2019'
p10456
aVThe abbreviation class is defined as cases where pronouncing as the expansion would be normal
p10457
aVOther categories included letter sequence (expansion would not be normal, e.g.,, TV ); partial letter sequence (e.g.,, PurePictureTV ); misspelling; leave as is (part of a URL or pronounced as a word, e.g.,, NATO ); foreign; don u'\u005cu2019' t know; and junk
p10458
aVIn our experiments, k = 2 since we have a trigram model, though in cases where the target word is the last word in the string, k = 1 , because there only the end-of-string symbol must be predicted in addition to the expansion
p10459
aVWe then take the Bayesian fusion of this model with the pair LM, by adding them in the log space, to get prediction from both the context and abbreviation model
p10460
aVWe then quantize these scores down into 16 bins, using the histogram in the training data to define bin thresholds so as to partition the training instances evenly
p10461
aVA binary feature is defined for each bin that is set to 1 if the current candidate u'\u005cu2019' s score is less than the threshold of that bin, otherwise 0
p10462
aVThus multiple bin features can be active for a given candidate expansion of the abbreviation
p10463
aVWe also have features that fire for each type of contextual feature (e.g.,, trigram with expansion as middle word, etc.), including u'\u005cu2018' no context u'\u005cu2019' , where none of the trigrams or bigrams from the current example that include the candidate expansion are present in our list
p10464
aVFurther, we have features for the length of the abbreviation (shorter abbreviations have more ambiguity, hence are more risky to expand); membership in the list of frequent, conventionalized abbreviations mentioned earlier; and some combinations of these, along with bias features
p10465
aVTaking this very high precision system combination of the N-gram and SVM models, we then combine with the baseline TTS system as follows
p10466
aVFirst we apply our system, and expand the item if it scores above threshold; for those items left unexpanded, we let the TTS system process it in its own way
p10467
aVOf course, at test time, we will not know whether an OOV is an abbreviation or not, so we also looked at the performance on the rest of the collected data, to see how often it erroneously suggests an expansion from that set
p10468
aVOf the 11,157 examples that were hand-labeled as non-abbreviations, our SVM u'\u005cu2229' N-gram system expanded 45 items, which is a false positive rate of 0.4% under the assumption that none of them should be expanded
p10469
aVWe also experimented with a number of alternative high precision approaches that space precludes our presenting in detail here, including pruning the number of expansion candidates based on the pair LM score; only allowing abbreviation expansion when at least one extracted n-gram context is present for that expansion in that context; and CART tree [] training with real valued scores
p10470
aVWe found that, for use in combination with the baseline TTS system, large overall reductions in FP rate were achieved by using an initial system with substantially higher TP and somewhat higher FP rates, since far fewer abbreviations were then passed along unexpanded to the baseline system, with its relatively high 3% FP rate
p10471
aVMost notably, the TTS baseline system has a much lower true positive rate; yet we find our systems achieve performance very close to that for the development set, so that our final combination with the TTS baseline was actually slighly better than the numbers on the development set
p10472
aVSince the SVM features relate to general properties of abbreviations, expansions and contexts, the classifier parameters will likely carry over to new (English) domains
p10473
asg88
(lp10474
sg90
(lp10475
sg92
(lp10476
VIn this paper we have presented methods for high precision abbreviation expansion for a TTS application.
p10477
aVThe methods are largely self-organizing, using in-domain unannotated data, and depend on only a small amount of annotated data.
p10478
aVSince the SVM features relate to general properties of abbreviations, expansions and contexts, the classifier parameters will likely carry over to new (English) domains.
p10479
aVWe demonstrate that in combination with a hand-built TTS baseline, the methods afford dramatic improvement in the TP rate (to about 74% from a starting point of about 40%) and a reduction of FP to below our goal of 2%.
p10480
ag106
asg107
S'P14-2060'
p10481
sg109
(lp10482
VIncorrect normalization of text can be particularly damaging for applications like text-to-speech synthesis (TTS) or typing auto-correction, where the resulting normalization is directly presented to the user, versus feeding downstream applications.
p10483
aVIn this paper, we focus on abbreviation expansion for TTS, which requires a do no harm , high precision approach yielding few expansion errors at the cost of leaving relatively many abbreviations unexpanded.
p10484
aVIn the context of a large-scale, real-world TTS scenario, we present methods for training classifiers to establish whether a particular expansion is apt.
p10485
aVWe achieve a large increase in correct abbreviation expansion when combined with the baseline text normalization component of the TTS system, together with a substantial reduction in incorrect expansions.
p10486
ag106
asba(icmyPackage
FText
p10487
(dp10488
g3
(lp10489
VIn this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples
p10490
aVUsing leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of 84 %
p10491
aVThis accuracy is so high that current research has shifted to related more challenging problems language variety identification [ 26 ] , native language identification [ 24 ] and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths [ 1 ]
p10492
aVThe same holds for the rich use of non-manual articulators in sentences and the limited role of facial expressions in the lexicon these too make sign languages across the world very similar in appearance, even though the meaning of specific articulations may differ [ 7 ]
p10493
aVJust as speakers have different voices unique to each individual, signers have also different signing styles that are likely unique to each individual
p10494
aVThere is evidence that normalization and whitening [ 13 ] improve performance in unsupervised feature learning [ 4 ]
p10495
aVWe therefore normalize every patch x ( i ) by subtracting the mean and dividing by the standard deviation of its elements
p10496
aVWe call both the centroids and filters as the learned features
p10497
aVGiven the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows
p10498
aVPart of the other half, involving 5 signers, is used along with the other sign language videos for learning and testing classifiers
p10499
aVFor example, if the background of the videos is different across sign languages, then classifying the sign languages could be done with perfection by using signals from the background
p10500
aVTo avoid this problem, we removed the background by using background subtraction techniques and manually selected thresholds
p10501
aVThe second reason for data preprocessing is to make the input size smaller and uniform
p10502
asg88
(lp10503
sg90
(lp10504
sg92
(lp10505
VGiven that sign languages are under-resourced, unsupervised feature learning techniques are the right tools and our results show that this is realistic for sign language identification.
p10506
aVFuture work can extend this work in two directions.
p10507
aV1) by increasing the number of sign languages and signers to check the stability of the learned feature activations and to relate these to iconicity and signer differences 2) by comparing our method with deep learning techniques.
p10508
aVIn our experiments, we used a single hidden layer of features, but it is worth researching into deeper layers to improve performance and gain more insight into the hierarchical composition of features.
p10509
aVOther questions for future work.
p10510
aVHow good are human beings at identifying sign languages.
p10511
aVCan a machine be used to evaluate the quality of sign language interpreters by comparing them to a native language model.
p10512
aVThe latter question is particularly important given what happened at the Nelson Mandela s memorial service 3 3 http://www.youtube.com/watch?v=X-DxGoIVUWo.
p10513
ag106
asg107
S'P14-2061'
p10514
sg109
(lp10515
VPrior research on language identification focused primarily on text and speech.
p10516
aVIn this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples.
p10517
aVThe method is trained on unlabelled video data (unsupervised feature learning) and using these features, it is trained to discriminate between six sign languages (supervised learning.
p10518
aVWe ran experiments on short video samples involving 30 signers (about 6 hours in total.
p10519
aVUsing leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of 84 %.
p10520
aVGiven that sign languages are under-resourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification.
p10521
ag106
asba(icmyPackage
FText
p10522
(dp10523
g3
(lp10524
VCrowdsourcing services such as Amazon u'\u005cu2019' s Mechanical Turk has since been successfully used for various annotation tasks in NLP []
p10525
aVDisagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder
p10526
aVOnly a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition []
p10527
aVHowever, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable
p10528
aVExpensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter
p10529
aVIt is therefore common to aggregate over multiple annotations for the same item to get more robust annotations
p10530
aVWe also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica []
p10531
aVIf the correct label is not among the annotations, we are unable to recover the correct answer
p10532
aVThis was the case for 1497 instances in our data (cf the token u'\u005cu201c' u'\u005cu201d' in the example
p10533
aVWe thus report on oracle score, i.e.,, the best label sequence that could possibly be found, which is correct except for the missing tokens
p10534
aVNote that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER
p10535
aVSince this is a stochastic process, we average results over 100 runs
p10536
aVWe refer to this as Majority voting (MV
p10537
aVNote that in MV we trust all annotators to the same degree
p10538
aVWe use MACE 4 4 http://www.isi.edu/publications/licensed-sw/mace/ [] as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators
p10539
aVMACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM []
p10540
aV\u005cshortcite Li:ea:12 in including Wiktionary information as type constraints into our decoding if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry
p10541
aVIf the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations
p10542
aVSince we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations
p10543
aVWe use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model
p10544
aVFor both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets
p10545
aVIf we pre-filter the data using Wiktionary, the agreement becomes 80.58%
p10546
aVMACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75
p10547
aVThe small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required
p10548
aVBoth schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e., y u'\u005cu2209' u'\u005cud835' u'\u005cudc19' i
p10549
aVAnnotators mostly decided to label these tokens as punctuation
p10550
aVThey also predominantly labeled your , my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET
p10551
aVUsually, we don u'\u005cu2019' t want to match a gold standard, but we rather want to create new annotated training data
p10552
aVNote also how the Wiktionary constraints lead to improvements in downstream performance
p10553
aVIn chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations
p10554
aVSince the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations
p10555
aVThe latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning
p10556
aVUnfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators
p10557
aVThis is highly effective, as it eliminates some sources of possible disagreement
p10558
asg88
(lp10559
sg90
(lp10560
sg92
(lp10561
VWe use crowdsourcing to collect POS annotations with minimal context (five-word windows.
p10562
aVWhile the performance of POS models learned from this data is still slightly below that of models trained on expert annotations, models learned from aggregations approach oracle performance for POS tagging.
p10563
aVIn general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method.
p10564
aVFor some downstream tasks, models using the aggregated POS tags perform even better than models using expert-annotated tags.
p10565
ag106
asg107
S'P14-2062'
p10566
sg109
(lp10567
VCrowdsourcing lets us collect multiple annotations for an item from several annotators.
p10568
aVTypically, these are annotations for non-sequential classification tasks.
p10569
aVWhile there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced.
p10570
aVThis paper shows that workers can actually annotate sequential data almost as well as experts.
p10571
aVFurther, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.
p10572
aVplus1ptminus2ptplus1ptminus2pt.
p10573
ag106
asba(icmyPackage
FText
p10574
(dp10575
g3
(lp10576
VSentiment analysis in a multilingual world remains a challenging problem, because developing language-specific sentiment lexicons is an extremely resource-intensive process
p10577
aVIn this paper, we address this lexicon gap by building high-quality sentiment lexicons for 136 major languages
p10578
aV2011 ) present a more sophisticated model by considering patterns, including negation and repetition using adjusted weights
p10579
aVLiu ( 2010 ) introduces an efficient method, at the state of the art, for doing sentiment analysis and subjectivity in English
p10580
aV2010 ) perform sentiment analysis according to cross-domain contextualization and Pak and Paroubek ( 2010 ) focus on Twitter, doing research on colloquial format of English
p10581
aVMachine learning approaches to sentiment analysis are attractive, because of the promise of reduced manual processing
p10582
aVBoiy and Moens ( 2009 ) conduct machine learning sentiment analysis using multilingual web texts
p10583
aVWe seek to identify as many semantic links across languages as possible to connect our network, and so integrated several resources
p10584
aVThough not always true, words with same spelling usually have similar meanings so this can improve the coverage of semantic links
p10585
aVLinks do not always agree in a bidirectional manner, particularly for multi-sense words, thus all links in our network are unidirectional
p10586
aVWe collected all available published sentiment lexicons from non-English languages to serve as standard for our evaluation, including Arabic, Italian, German and Chinese
p10587
aVCoupled with English sentiment lexicons provides in total seven different test cases to experiment against, specifically
p10588
aVPerformance is not good on Japanese because of mismatching between our dictionary and the test data
p10589
aVWe consider evaluating our lexicons on the consistency of Wikipedia pages about a particular individual person among various languages
p10590
aVAs our candidate entities for analysis, we use the Wikipedia pages of 2,000 most significant people as measured in the recent book Who u'\u005cu2019' s Bigger
p10591
aVThe sentiment polarity of a page is simply computed by subtracting the number of negative words from that of positive words, divided by the sum of both
p10592
aVWe believe that this ratio can be considered as quality measurement of the propagation
p10593
aVSimilar approaches can be extended to other NLP tasks using different semantic links, specific dictionary and special seed words
p10594
asg88
(lp10595
sg90
(lp10596
sg92
(lp10597
VOur knowledge graph propagation is generally effective at producing useful sentiment lexicons.
p10598
aVInterestingly, the ratio of positive sentiment words is strongly connected with number of sentiment words it is noteworthy that English has the smallest ratio of positive lexicon terms.
p10599
aVThe phenomenon possibly shows that many negative words reflecting cultural nuances do not translate wel.
p10600
aVWe believe that this ratio can be considered as quality measurement of the propagation.
p10601
aVSimilar approaches can be extended to other NLP tasks using different semantic links, specific dictionary and special seed words.
p10602
aVFuture work will revolve around learning modifiers, negation terms, and various entity/sentiment attribution.
p10603
ag106
asg107
S'P14-2063'
p10604
sg109
(lp10605
VSentiment analysis in a multilingual world remains a challenging problem, because developing language-specific sentiment lexicons is an extremely resource-intensive process.
p10606
aVSuch lexicons remain a scarce resource for most languages.
p10607
aVIn this paper, we address this lexicon gap by building high-quality sentiment lexicons for 136 major languages.
p10608
aVWe integrate a variety of linguistic resources to produce an immense knowledge graph.
p10609
aVBy appropriately propagating from seed words, we construct sentiment lexicons for each component language of our graph.
p10610
aVOur lexicons have a polarity agreement of 95.7% with published lexicons, while achieving an overall coverage of 45.2%.
p10611
aVWe demonstrate the performance of our lexicons in an extrinsic analysis of 2,000 distinct historical figures Wikipedia articles on 30 languages.
p10612
aVDespite cultural difference and the intended neutrality of Wikipedia articles, our lexicons show an average sentiment correlation of 0.28 across all language pairs.
p10613
ag106
asba(icmyPackage
FText
p10614
(dp10615
g3
(lp10616
VVerbs such as hit and like do not describe a change of state and so cannot appear in both forms
p10617
aVVerbNet (Kipper et al., 2008; based on Levin, 1993) lists over 6,000 verbs, categorized into 280 classes according to the syntactic frames they can appear in
p10618
aVIn principle, these judgments would come from naive annotators, since researchers u'\u005cu2019' intuitions about subtle judgments may be unconsciously clouded by theoretical commitments [ 4 ]
p10619
aVThe Semantic Consistency Hypothesis would be supported if, within that database, predicates with the same syntactic properties were systematically related semantically
p10620
aVCollecting data from naive subjects is even more laborious, particularly since the average Man on the Street is not necessarily equipped with metalinguistic concepts like caused change of state and propositional attitude
p10621
aVOne significant challenge for any such project is first classifying verbs according to the syntactic frames they can appear in
p10622
aVThus, at least initially, we are focusing on the 6,000+ verbs already cataloged in VerbNet
p10623
aVAs such, the VerbCorner Project is also verifying and validating the semantics currently encoded in VerbNet
p10624
aVVerbNet will be edited as necessary based on the empirical results
p10625
aVIntegration with VerbNet has additional benefits, since VerbNet itself is integrated with a variety of linguistic resources, such as PropBank and Penn TreeBank
p10626
aVWe selected semantic features of interest based on those most commonly cited in the linguistics literature, with a particular focus on those that u'\u005cu2013' according to VerbNet u'\u005cu2013' apply to many predicates
p10627
aVPrevious research has shown that humans find it easier to reason about real-world scenarios than make abstract judgments [ 3 ]
p10628
aVThus, for each feature (e.g.,, movement ), we converted the metalinguistic judgment ( u'\u005cu201c' Does this verb entail movement on the part of some entity u'\u005cu201d' ) into a real-world problem
p10629
aVPrevious work suggests that it is the semantic entailments that matter, particularly for explaining the syntactic behavior of verbs [ 10 ]
p10630
aVThe exact semantics associated with a verb may depend on its syntactic frame
p10631
aVThus Sally rolled the ball entails that somebody applied force to the ball (namely
p10632
aVSally), whereas The ball rolled does not
p10633
aVThus, we investigate the semantics of each verb in each syntactic frame available to it (as described by VerbNet
p10634
aVGiven the sheer scale of the project, data-collection is expected to take several years at least
p10635
aVThus, data-collection has been broken up into a series of phases
p10636
aVBelow, we summarize the main findings thus far
p10637
aVIn Phase 1 of the project, we focused on 11 verb classes (Table 3) comprising 641 verbs and seven different semantic entailments (Table 2
p10638
aVEach task had been iteratively piloted and redesigned until inter-annotator reliability was acceptable, as described in a previous publication
p10639
aVHowever, these pilot studies involved a small number of items which were coded by all annotators
p10640
aVBecause we recruited large numbers of annotators, most of whom annotated only a few items, typical measures of inter-annotator agreement such as Cohen u'\u005cu2019' s kappa are not easily calculated
p10641
aVSince there were typically 4 or more possible answers per item, inter-annotator agreement was well above chance
p10642
aVIn fact, annotators frequently flagged these items as ungrammatical, which is a valuable result in itself for improving VerbNet
p10643
aVWe next investigated whether our results support the Semantic Consistency Hypothesis
p10644
aVAs noted above, the question is not whether all verbs in the same syntactic class share the same semantic entailments
p10645
aVEven a single verb may have different semantic entailments when placed in different syntactic frames
p10646
aVThus, calculating consistency of a class must take differing frames into account
p10647
aVThere are many sophisticated rubrics for calculating consistency
p10648
aVThe consistency for this class/frame combination is 60%
p10649
aVThe consistency for the class as a whole is the average across frames
p10650
aVMean consistency averaged across classes is shown for each task in Table 2
p10651
aVAs expected, consistency was lowest for Evaluation , which is not expected to necessarily correlate with syntax
p10652
aVResults of Phase 1 provide support for the Semantic Consistency Hypothesis, at least as a strong bias
p10653
aVMore work will be needed to determine the strength of that bias
p10654
aVWe are currently investigating whether we can achieve better reliability with fewer responses per item by taking into account an individual annotator u'\u005cu2019' s history across items, as recent work suggests is possible [ 13 , 17 , 19 ]
p10655
aVThus, crowd-sourcing VerbNet semantic entailments appears to be both feasible and productive
p10656
asg88
(lp10657
sg90
(lp10658
sg92
(lp10659
VResults of Phase 1 provide support for the Semantic Consistency Hypothesis, at least as a strong bias.
p10660
aVMore work will be needed to determine the strength of that bias.
p10661
aVThe findings are largely consistent with VerbNet s semantics, but changes are indicated in some cases.
p10662
aVWe find that inter-annotator agreement is sufficiently high that annotation can be done effectively using the modal response with an average of 6-7 responses per item.
p10663
aVWe are currently investigating whether we can achieve better reliability with fewer responses per item by taking into account an individual annotator s history across items, as recent work suggests is possible [ 13 , 17 , 19 ].
p10664
aVThus, crowd-sourcing VerbNet semantic entailments appears to be both feasible and productive.
p10665
aVData-collection continues.
p10666
aVPhase 2, which added over 10 new verb classes, is complete.
p10667
aVPhase 3, which includes both new classes and new entailments, has been launched.
p10668
ag106
asg107
S'P14-2065'
p10669
sg109
(lp10670
VAny given verb can appear in some syntactic frames ( Sally broke the vase , The vase broke ) but not others (* Sally broke at the vase , * Sally broke the vase to John.
p10671
aVThere is now considerable evidence that the syntactic behaviors of some verbs can be predicted by their meanings, and many current theories posit that this is true for most if not all verbs.
p10672
aVIf true, this fact would have striking implications for theories and models of language acquisition, as well as numerous applications in natural language processing.
p10673
aVHowever, empirical investigations to date have focused on a small number of verbs.
p10674
aVWe report on early results from VerbCorner, a crowd-sourced project extending this work to a large, representative sample of English verbs.
p10675
ag106
asba(icmyPackage
FText
p10676
(dp10677
g3
(lp10678
VFor example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings
p10679
aVIt is thus important to understand the effects of statement strength
p10680
aVHowever, even this problem is understudied, partly due to a lack of data
p10681
aVSince strength is inherently relative, revisions of texts that make claims are a natural source of data on strength differences
p10682
aV1 1 http://en.wikipedia.org/wiki/2014_Kunming_attack In the aftermath, Chinese media accused Western media of u'\u005cu201c' soft-pedaling the attack and failing to state clearly that it was an act of terrorism u'\u005cu201d'
p10683
aV2 2 http://sinosphere.blogs.nytimes.com/2014/03/03/u-n-security-council-condemns-terrorist-attack-in-kunming/ In particular, regarding the statement by the US embassy that referred to this incident as the u'\u005cu201c' terrible and senseless act of violence in Kunming u'\u005cu201d' , a Weibo user posted u'\u005cu201c' If you say that the Kunming attack is a u'\u005cu2018' terrible and senseless act of violence u'\u005cu2019' , then the 9/11 attack can be called a u'\u005cu2018' regrettable traffic incident u'\u005cu201d' u'\u005cu2019'
p10684
aVSince the strength and scope of an argument can be a crucial factor in its success, it is important to understand the effects of statement strength in communication
p10685
aVA first step towards addressing this question is to be able to distinguish between strong and weak statements
p10686
aVAs strength is inherently relative, it is natural to look at revisions that change statement strength, which we refer to as u'\u005cu201c' strength changes u'\u005cu201d'
p10687
aVSince the arXiv started in 1991, it has become u'\u005cu201c' the standard repository for new papers in mathematics, physics, statistics, computer science, biology, and other disciplines u'\u005cu201d' []
p10688
aVMany differences between these versions constitute a source of valid and motivated strength differences, as can be seen from the sentential revisions in Table 1
p10689
aVPair 1 makes the contribution seem more impressive by replacing u'\u005cu201c' studied u'\u005cu201d' with u'\u005cu201c' proposed u'\u005cu201d'
p10690
aVThe main contribution of this work is to provide the first large-scale corpus of sentence-level revisions for studying a broad range of variations in statement strength
p10691
aVPublic datasets of science communication are available, such as the ACL Anthology, 5 5 http://aclweb.org/anthology/ collections of NIPS papers, 6 6 http://nips.djvuzone.org/txt.html and so on
p10692
aVThese datasets are useful for understanding the progress of disciplines or the evolution of topics
p10693
aVBut the lack of edit histories or revisions makes them not immediately suitable for studying strength differences
p10694
aVIn order to align the first version and the final version of the same paper, we first did macro alignment of paper sections based on section titles
p10695
aVInstead of cosine similarity, we used an idf-weighted longest-common-subsequence algorithm to define the similarity between two sentences, because changes in word ordering can also be interesting
p10696
aVSince it is likely that a new version adds or deletes a large sequence of sentences, we did not impose a skip penalty
p10697
aV8 8 We did not allow cross matching (i.e.,, i u'\u005cu2192' j - 1 , i - 1 u'\u005cu2192' j ), since we thought matching this case as ( i - 1 , i ) u'\u005cu2192' j or i u'\u005cu2192' ( j , j - 1 ) can provide context for annotation purposes
p10698
aV9 9 This differs from the number in Section 1 because articles may not have the tex source available, or the differences between versions may be in non-textual content
p10699
aVOne might initially think that typo fixes represent a large proportion of revisions, but this is not correct, as shown in Figure 4
p10700
aVDeletions represent a substantial fraction, especially in the middle section of a paper
p10701
aVBut it is clear that the majority of changes are rewrites; thus revisions on the arXiv indeed provide a great source for potential strength differences
p10702
aVThis could be because a single author enjoys greater freedom and has stronger motivation to make changes, or because multiple authors tend to submit a more polished initial version
p10703
aV10 10 These decisions were made based on the results and feedback that we got from graduate students in an initial labeling
p10704
aVTo create the pool of pairs for labeling, we randomly sampled 1000 pairs and then removed pairs that we thought were processing errors
p10705
aVIt may initially seem surprising to have annotations of technical statements not done by domain experts; we did this intentionally because it is common to communicate unfamiliar topics to the public in political and science communication (we comment on non-expert rationales later
p10706
aVThe instructions included 8 pairs as examples and 10 pairs to label as a training exercise
p10707
aVParticipants were then asked to choose labels and write mandatory comments for 50 pairs
p10708
aVAccording to the comments written by participants, we believe that they did the labeling in good faith
p10709
aVFirst, participants tend to take details as evidence even when these details are not germane to the statement
p10710
aVFor pair 1, while one turker pointed out the decline in number of experiments, most turkers simply labeled it as stronger because it was more specific u'\u005cu201c' Specific u'\u005cu201d' turned out to be a common reason used in the comments, even though we said in the instructions that only additional justification and evidence matter
p10711
aVFor instance, the majority labeled pair 2 as u'\u005cu201c' stronger u'\u005cu201d'
p10712
aVBut in S2 for that pair, the result holds for strictly fewer possible worlds
p10713
aVThe comments indicate features that humans think salient
p10714
asg88
(lp10715
sg90
(lp10716
sg92
(lp10717
g1538
asg107
S'P14-2066'
p10718
sg109
(lp10719
VThe strength with which a statement is made can have a significant impact on the audience.
p10720
aVFor example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings.
p10721
aVIt is thus important to understand the effects of statement strength.
p10722
aVA first step is to be able to distinguish between strong and weak statements.
p10723
aVHowever, even this problem is understudied, partly due to a lack of data.
p10724
aVSince strength is inherently relative, revisions of texts that make claims are a natural source of data on strength differences.
p10725
aVIn this paper, we introduce a corpus of sentence-level revisions from academic writing.
p10726
aVWe also describe insights gained from our annotation efforts for this task.
p10727
ag106
asba(icmyPackage
FText
p10728
(dp10729
g3
(lp10730
VPredicates such as thinks , claims , and admits offer a range of options for framing quoted content according to the author u'\u005cu2019' s own perceptions of its credibility
p10731
aVContemporary journalism is increasingly conducted through social media services like Twitter [ 9 , 6 ]
p10732
aVAs events unfold, journalists and political commentators use quotes u'\u005cu2014' often indirect u'\u005cu2014' to convey potentially uncertain information and claims from their sources and informants, e.g
p10733
aVDetecting and reasoning about the certainty of propositional content has been identified as a key task for information extraction, and is now supported by the FactBank corpus of annotations for newstext [ 14 ]
p10734
aVThis dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message
p10735
aVThis enables us to build a predictive model of the factuality annotations, with the goal of determining the full set of relevant factors, including the predicate, the source, the journalist, and the content of the claim itself
p10736
aVOur interest in this text is specifically in quoted content u'\u005cu2014' including u'\u005cu201c' indirect u'\u005cu201d' quotes, which may include paraphrased quotations, as in the examples in Figure 1
p10737
aVWhile labeled datasets for such quotes have been created [ 12 , 13 ] , these are not freely available at present
p10738
aVIn any case, the relevance of these datasets to Twitter text is currently unproven
p10739
aVTherefore, rather than train a supervised model to detect quotations, we apply a simple dependency-based heuristic
p10740
aVWe use a combination of regular expressions and dependency rules to capture expressions of the type u'\u005cu201c' claim , according to source u'\u005cu201d' Specifically, the PCOMP path from according is searched for the pattern according to *
p10741
aVThis heuristic pipeline may miss many relevant tweets, but since the overall volume is high, we prioritize precision
p10742
aVTo ensure quality control we required the Turkers to have at least 85% hit approval rating and to reside in the United States, because the Twitter messages in our dataset were related to American politics
p10743
aVThe ratings were based on a 5-point Likert scale ranging from u'\u005cu201c' [-2] Certainly False u'\u005cu201d' to u'\u005cu201c' [2] Certainly True u'\u005cu201d' and allowing for u'\u005cu201c' [0] Uncertain u'\u005cu201d'
p10744
aVWe also allowed for u'\u005cu201c' Not Applicable u'\u005cu201d' option to capture ratings where the Turkers did not have sufficient knowledge about the statement or if the statement was not really a claim
p10745
aVHaving obtained a corpus of factuality ratings, we now model the factors that drive these ratings
p10746
aVThese features are used as predictors in a series of linear ridge regressions, where the dependent variable is the mean certainty rating
p10747
aVWe throw out tweets that were rated as u'\u005cu201c' not applicable u'\u005cu201d' by a majority of raters, but otherwise ignore u'\u005cu201c' not applicable u'\u005cu201d' ratings of the remaining tweets
p10748
aVWe performed another set of linear regressions, again using the mean certainty rating as the dependent variable
p10749
aVIn this case, there was no training/test split, so confidence intervals on the resulting parameters are computed using the analytic closed form
p10750
aVThe cues that give the highest factuality coefficients are learn and admit , which are labeled as predicates of knowledge
p10751
aVThese cues carry a substantial amount of framing, as they purport to describe the private mental state of the source
p10752
aVThe word admit often applies to statements that are perceived as damaging to the source, such as Bill Gates admits Control-Alt-Delete was a mistake ; since there can be no self-interest behind such statements, they may be perceived as more likely to be true
p10753
aVThe words suggest , think , and believe also purport to describe the private mental state of the source, but their framing function is the opposite of the predicates of knowledge they imply that it is important to mark the claim as the source u'\u005cu2019' s belief, and not a widely-accepted fact
p10754
aVFor example, Mubarak clearly believes he has the military leadership u'\u005cu2019' s support
p10755
aVBoth according and report are often used in conjunction with impersonal and institutional sources, e.g.,, Cuccinelli trails McAuliffe by 24 points , according to a new poll
p10756
aVIn this case, the fact that the predicate indicates a report is not enough to determine the framing different sorts of reports carry radically different perceptions of factuality
p10757
aVThe search for reliable signals of information credibility in social media has led to the construction of automatic classifiers to identify credible tweets [ 2 ]
p10758
aVThis result is obtained from real tweets written by journalists; a natural counterpart study would be to experimentally manipulate this framing to see if the same perceptions apply
p10759
aVAnother future direction would be to test whether the deployment of cue words as framing devices reflects the ideology of the journalist
p10760
asg88
(lp10761
sg90
(lp10762
sg92
(lp10763
VPerceptions of the factuality of quoted content are influenced by the cue words used to introduce them, while extra-linguistic factors, such as the source and the author, did not appear to be relevant in our experiments.
p10764
aVThis result is obtained from real tweets written by journalists; a natural counterpart study would be to experimentally manipulate this framing to see if the same perceptions apply.
p10765
aVAnother future direction would be to test whether the deployment of cue words as framing devices reflects the ideology of the journalist.
p10766
aVWe are also interested to group multiple instances of the same quote [ 7 ] , and examine how its framing varies across different news outlets and over time.
p10767
aVAcknowledgments.
p10768
aVThis research was supported by DARPA-W911NF-12-1-0043 and by a Computational Journalism research award from Google.
p10769
aVWe thank the reviewers for their helpful feedback.
p10770
ag106
asg107
S'P14-2068'
p10771
sg109
(lp10772
VHow do journalists mark quoted content as certain or uncertain, and how do readers interpret these signals.
p10773
aVPredicates such as thinks , claims , and admits offer a range of options for framing quoted content according to the author s own perceptions of its credibility.
p10774
aVWe gather a new dataset of direct and indirect quotes from Twitter, and obtain annotations of the perceived certainty of the quoted statements.
p10775
aVWe then compare the ability of linguistic and extra-linguistic features to predict readers assessment of the certainty of quoted content.
p10776
aVWe see that readers are indeed influenced by such framing devices and we find no evidence that they consider other factors, such as the source, journalist, or the content itself.
p10777
aVIn addition, we examine the impact of specific framing devices on perceptions of credibility.
p10778
ag106
asba(icmyPackage
FText
p10779
(dp10780
g3
(lp10781
VDue to the popularity of opinion-rich resources (e.g.,, online review sites, forums, blogs and the microblogging websites), automatic extraction of opinions, emotions and sentiments in text is of great significance to obtain useful information for social and security studies
p10782
aVVarious opinion mining applications have been proposed by different researchers, such as question answering, opinion mining, sentiment summarization, etc
p10783
aVAs the fine-grained annotated data are expensive to get, the unsupervised approaches are preferred and more used in reality
p10784
aVUsually, a high quality emotion lexicon play a significant role when apply the unsupervised approaches for fine-grained emotion classification
p10785
aVThus far, most lexicon construction approaches focus on constructing general-purpose emotion lexicons [ 11 , 7 , 16 , 4 ]
p10786
aVHowever, since a specific word can carry various emotions in different domains, a general-purpose emotion lexicon is less accurate and less informative than a domain-specific lexicon [ 1 ]
p10787
aVLastly, previous emotion lexicons are mostly annotated based on many manually constructed resources (e.g.,, emotion lexicon, parsers, etc
p10788
aVThe proposed EaLDA model extends the standard Latent Dirichlet Allocation (LDA) [ 3 ] model by employing a small set of seeds to guide the model generating topics
p10789
aVHence, the topics consequently group semantically related words into a same emotion category
p10790
aVThe lexicon is thus able to best meet the user u'\u005cu2019' s specific needs
p10791
aVOur approach is a weakly supervised approach since only some seeds emotion sentiment words are needed to lanch the process of lexicon construction
p10792
aVIn practical applications, asking users to provide some seeds is easy as they usually have a good knowledge what are important in their domains
p10793
aVThe first kind of approaches is based on thesaurus that utilizes synonyms or glosses to d etermine the sentiment orientation of a word
p10794
aVThe second kind of approaches is based on an idea that emotion words co-occurring with each others are likely to convey the same polarity
p10795
aVOur approach relates most closely to the method proposed by Xie and Li ( 2012 ) for the construction of lexicon annotated for polarity based on LDA model
p10796
aVWe descrige with the model description, a Gibbs sampling algorithm to infer the model parameters, and finally how to generate a emotion lexicon based on the model output
p10797
aVFor each word in the document, we decide whether its topic is an emotion topic or a non-emotion topic by flipping a coin with head-tail probability ( p ( e ) , p ( n ) ) , where ( p ( e ) , p ( n ) ) u'\u005cu223c' Dir u'\u005cu2062' ( u'\u005cu0391'
p10798
aVThe emotion (or non-emotion) topic is sampled according to a multinomial distribution Mult u'\u005cu2062' ( u'\u005cu0398' ( e ) ) (or Mult u'\u005cu2062' ( u'\u005cu0398' ( n ) )
p10799
aVdraw w u'\u005cu223c' Mult u'\u005cu2062' ( u'\u005cu03a6' z ( n ) ( n ) ) , emit word w
p10800
aVAs an alternative representation, the graphical model of the the generative process is shown by Figure 1
p10801
aVAssuming hyperparameters u'\u005cu0391' , u'\u005cu0391' ( e ) , u'\u005cu0391' ( n ) , and u'\u005cu0392' ( e ) , u'\u005cu0392' ( n ) , we develop a collapsed Gibbs sampling algorithm to estimate the latent variables in the EaLDA model
p10802
aVUsing the definition of the EaLDA model and the Bayes Rule, we find that the joint density of these random variables are equal to
p10803
aVAccording to equation ( 1 ), we see that { p ( e ) , p ( n ) } , { u'\u005cu0398' i ( e ) , u'\u005cu0398' j ( n ) } , { u'\u005cu03a6' i , w ( e ) } and { u'\u005cu03a6' j , w ( n ) } are mutually independent sets of random variables
p10804
aVThen, by examining the property of Dirichlet distribution, we can compute expectations on the right hand side of equation ( 2 ) and equation ( 3 ) by
p10805
aVUsing the above equations, we can sample the topic z for each word iteratively and estimate all latent random variables
p10806
aVIf u'\u005cu03a6' i , w ( e ) is the largest, then the word w is added to the emotion dictionary for the i th emotion
p10807
aVOtherwise, 1 K u'\u005cu2062' u'\u005cu2211' i = 1 K u'\u005cu03a6' i , w ( n ) is the largest among the M + 1 values, which suggests that the word w is more probably drawn from a non-emotion topic
p10808
aVThus, the word is considered neutral and not included in the emotion dictionary
p10809
aVSince there is no metric explicitly measuring the quality of an emotion lexicon, we demonstrate the performance of our algorithm in two ways
p10810
aVFinally, Snowball stemmer 2 2 http://snowball.tartarus.org/ is applied so as to reduce the vocabulary size and settle the issue of data spareness
p10811
aVThe vector u'\u005cu0392' ( e ) is constructed from the seed dictionary using u'\u005cu0393' = ( 0.25 , 0.95 )
p10812
aVAs mentioned, we use a few domain-independent seed words as prior information for our model
p10813
aVWhat we reported here are based on our judgments what are appropriate and what are not for each emotion topic
p10814
aVThese domain-specific words are mostly not included in any other existing general-purpose emotion lexicons
p10815
aVThe experimental results show that our algorithm can successfully construct a fine-grained domain-specific emotion lexicon for this corpus that is able to understand the connotation of the words that may not be obvious without the context
p10816
aVFor each emotion category, we evaluates it as a binary classification problem
p10817
aVIn the evaluation of emotion lexicons, the binary classification is performed in a very simple way
p10818
aVThe proposed EaLDA model extends the standard LDA model by accepting a set of domain-independent emotion words as prior knowledge, and guiding to group semantically related words into the same emotion category
p10819
aVThus, it makes the emotion lexicon containing much richer and adaptive domain-specific emotion words
p10820
aVFor future works, we hope to extend the proposed EaLDA model by exploiting discourse structure knowledge, which has been shown significant in identifying the polarity of content-aware words
p10821
asg88
(lp10822
sg90
(lp10823
sg92
(lp10824
VIn this paper, we have presented a novel emotion-aware LDA model that is able to quickly build a fine-grained domain-specific emotion lexicon for languages without many manually constructed resources.
p10825
aVThe proposed EaLDA model extends the standard LDA model by accepting a set of domain-independent emotion words as prior knowledge, and guiding to group semantically related words into the same emotion category.
p10826
aVThus, it makes the emotion lexicon containing much richer and adaptive domain-specific emotion words.
p10827
aVExperimental results showed that the emotional lexicons generated by our algorithm is of high quality, and can assist emotion classification task.
p10828
aVFor future works, we hope to extend the proposed EaLDA model by exploiting discourse structure knowledge, which has been shown significant in identifying the polarity of content-aware words.
p10829
ag106
asg107
S'P14-2069'
p10830
sg109
(lp10831
VEmotion lexicons play a crucial role in sentiment analysis and opinion mining.
p10832
aVIn this paper, we propose a novel Emotion-aware LDA (EaLDA) model to build a domain-specific lexicon for predefined emotions that include anger, disgust, fear, joy, sadness, surprise.
p10833
aVThe model uses a minimal set of domain-independent seed words as prior knowledge to discover a domain-specific lexicon, learning a fine-grained emotion lexicon much richer and adaptive to a specific domain.
p10834
aVBy comprehensive experiments, we show that our model can generate a high-quality fine-grained domain-specific emotion lexicon.
p10835
aV ] MinYang §] BaolinPeng §] ZhengChen ,¶] DingjuZhu thanks.
p10836
aVDingjuZhuisthecorrespondingauthor ] Kam-PuiChow \u005caffil [ ]SchoolofComputerScience,SouthChinaNormalUniversity,Guangzhou,China \u005caffil [] dingjuzhu@gmail.com \u005caffil [ ]DepartmentofComputerScience,TheUniversityofHongKong,HongKong \u005caffil []{ myang,chow } @cs.hku.hk \u005caffil [§]DepartmentofComputerScience,BeihangUniversity,Beijing,China \u005caffil [] b.peng@cse.buaa.edu.cn,tzchen86@gmail.com \u005caffil [¶]ShenzhenInstitutesofAdvancedTechnology,ChineseAcademyofSciences,Shenzhen,China.
p10837
aVenglish.
p10838
ag106
asba(icmyPackage
FText
p10839
(dp10840
g3
(lp10841
VIn this paper, we present a novel approach for extracting u'\u005cu2013' in a totally automated way u'\u005cu2013' a high-coverage and high-precision lexicon of roughly 37 thousand terms annotated with emotion scores, called DepecheMood
p10842
aVBy providing new state-of-the-art performances in unsupervised settings for regression and classification tasks, even using a naïve approach, our experiments show the beneficial impact of harvesting social media data for affective lexicon building
p10843
aVThe simple division in u'\u005cu2018' positive u'\u005cu2019' vs u'\u005cu2018' negative u'\u005cu2019' comments may not suffice, as in these examples u'\u005cu2018' I u'\u005cu2019' m so miserable, I dropped my IPhone in the water and now it u'\u005cu2019' s not working anymore u'\u005cu2019' ( sadness ) vs u'\u005cu2018' I am very upset, my new IPhone keeps not working u'\u005cu2019' ( anger
p10844
aVWhile both texts express a negative sentiment, the latter, connected to anger, is more relevant for buzz monitoring
p10845
aVThus, emotion analysis represents a natural evolution of sentiment analysis
p10846
aVThis calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree
p10847
aVIn this respect, compositional approaches represent a new promising trend, since all other approaches, either using semantic similarity or Bag-of-Words (BOW) based machine-learning, cannot handle, for example, cases of texts with same wording but different words order u'\u005cu201c' The dangerous killer escaped one month ago, but recently he was arrested u'\u005cu201d' ( relief , happyness ) vs u'\u005cu201c' The dangerous killer was arrested one month ago, but recently he escaped u'\u005cu201d' ( fear
p10848
aVIn such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations
p10849
aVFor example Mitsubishi changed the name of one of its SUVs for the Spanish market, since the original name Pajero had a very negative prior polarity, as it means u'\u005cu2018' wanker u'\u005cu2019' in Spanish []
p10850
aVTo this end, we take advantage in an original way of massive crowd-sourced affective annotations associated with news articles, obtained by crawling the rappler.com social news network
p10851
aVResults indicate that the use of our resource, even if automatically acquired, is highly beneficial in affective text recognition
p10852
aVInterestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use
p10853
aVTo build our emotion lexicon we harvested all the news articles from rappler.com , as of June 3rd 2013 the final dataset consists of 13.5 M words over 25.3 K documents, with an average of 530 words per document
p10854
aVThis way, hundreds of thousands votes have been collected since the launch of the service
p10855
aVIn our novel approach to u'\u005cu2018' crowdsourcing u'\u005cu2019' , as compared to other NLP tasks that rely on tools like Amazon u'\u005cu2019' s Mechanical Turk [] , the subjects are aware of the u'\u005cu2018' implicit annotation task u'\u005cu2019' but they are not paid
p10856
aVThere are several possible explanations, out of the scope of the present paper, for this bias i) it is due to cultural characteristics of the audience (ii) the bias is in the dataset itself, being formed mainly by u'\u005cu2018' positive u'\u005cu2019' news; (iii) it is a psychological phenomenon due to the fact that people tend to express more positive moods on social networks []
p10857
aVAs a next step we built a word-by-emotion matrix starting from M D u'\u005cu2062' E using an approach based on compositional semantics
p10858
aVTo do so, we first lemmatized and PoS tagged all the documents (where PoS can be adj., nouns, verbs, adv.) and kept only those lemma#PoS present also in WordNet, similar to SWN-prior and WordNetAffect resources, to which we want to align
p10859
aVWe then computed the term-by-document matrices using raw frequencies, normalized frequencies, and tf-idf ( M W u'\u005cu2062' D , f , M W u'\u005cu2062' D , n u'\u005cu2062' f and M W u'\u005cu2062' D , t u'\u005cu2062' f u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f respectively), so to test which of the three weights is better
p10860
aVThis method allows us to u'\u005cu2018' merge u'\u005cu2019' words with emotions by summing the products of the weight of a word with the weight of the emotions in each document
p10861
aVFinally, we transformed M W u'\u005cu2062' E by first applying normalization column-wise (so to eliminate the over representation for happiness as discussed in Section 3 ) and then scaling the data row-wise so to sum up to one
p10862
aVAn excerpt of the final Matrix M W u'\u005cu2062' E is presented in Table 3 , and it can be interpreted as a list of words with scores that represent how much weight a given word has in the affective dimensions we consider
p10863
aVSo, for example, awe#n has a predominant weight in inspired (0.38), comical#a has a predominant weight in amused (0.51), while kill#v has a predominant weight in afraid , angry and sad (0.23, 0.21 and 0.27 respectively
p10864
aVHeadlines typically consist of a few words and are often written with the intention to u'\u005cu2018' provoke u'\u005cu2019' emotions so to attract the readers u'\u005cu2019' attention
p10865
aVThis dataset is of interest to us since the u'\u005cu2018' compositional u'\u005cu2019' problem is less prominent given the simplified syntax of news headlines, containing, for example, fewer adverbs (like negations or intensifiers) than normal sentences []
p10866
aVFinally, this dataset was meant for unsupervised approaches (just a small trial sample was provided), so to avoid simple text categorization approaches
p10867
aVAs the affective dimensions present in the test set u'\u005cu2013' based on the six basic emotions model [] u'\u005cu2013' do not exactly match with the ones provided by Rappler u'\u005cu2019' s Mood Meter, we first define a mapping between the two when possible, see Table 4
p10868
aV2013), we observe that even if the number of entries of our lexicon is far lower than SWN-prior approaches, the fact that we extracted and annotated words from documents grants a high coverage of language use
p10869
aVWe do so by using a very naïve approach, similar to u'\u005cu201c' WordNetAffect presence u'\u005cu201d' discussed in [] for each headline, we simply compute a value, for any affective dimension, by averaging the corresponding affective scores u'\u005cu2013' obtained from DepecheMood - of all lemma#PoS present in the headline
p10870
aVConsidering the naïve approach we used, we can reasonably conclude that the quality and coverage of our resource are the reason of such results, and that adopting more complex approaches (i.e., compositionality) can possibly further improve performances in text-based emotion recognition
p10871
aVAs a final test, we evaluate our resource in the classification task
p10872
aVWe presented DepecheMood , an emotion lexicon built in a novel and totally automated way by harvesting crowd-sourced affective annotation from a social news network
p10873
aVOur experimental results indicate high-coverage and high-precision of the lexicon, showing significant improvements over state-of-the-art unsupervised approaches even when using the resource with very naïve classification and regression strategies
p10874
aVOur future work will include testing Singular Value Decomposition on the word-by-document matrices, allowing to propagate emotions values for a document to similar words non present in the document itself, and the study of perceived mood effects on virality indices and readers engagement by exploiting tweets, likes, reshares and comments
p10875
asg88
(lp10876
sg90
(lp10877
sg92
(lp10878
VWe presented DepecheMood , an emotion lexicon built in a novel and totally automated way by harvesting crowd-sourced affective annotation from a social news network.
p10879
aVOur experimental results indicate high-coverage and high-precision of the lexicon, showing significant improvements over state-of-the-art unsupervised approaches even when using the resource with very naïve classification and regression strategies.
p10880
aVWe believe that the wealth of information provided by social media can be harnessed to build models and resources for emotion recognition from text, going a step beyond sentiment analysis.
p10881
aVOur future work will include testing Singular Value Decomposition on the word-by-document matrices, allowing to propagate emotions values for a document to similar words non present in the document itself, and the study of perceived mood effects on virality indices and readers engagement by exploiting tweets, likes, reshares and comments.
p10882
aV3 3 footnotetext.
p10883
aVThis work has been partially supported by the Trento RISE PerTe project.
p10884
ag106
asg107
S'P14-2070'
p10885
sg109
(lp10886
VWhile many lexica annotated with words polarity are available for sentiment analysis, very few tackle the harder task of emotion analysis and are usually quite limited in coverage.
p10887
aVIn this paper, we present a novel approach for extracting in a totally automated way a high-coverage and high-precision lexicon of roughly 37 thousand terms annotated with emotion scores, called DepecheMood.
p10888
aVOur approach exploits in an original way crowd-sourced affective annotation implicitly provided by readers of news articles from rappler.com.
p10889
aVBy providing new state-of-the-art performances in unsupervised settings for regression and classification tasks, even using a naïve approach, our experiments show the beneficial impact of harvesting social media data for affective lexicon building.
p10890
ag106
asba(icmyPackage
FText
p10891
(dp10892
g3
(lp10893
VThe topic information is generated through topic modeling based on an efficient implementation of Latent Dirichlet Allocation (LDA
p10894
aVGenerally u'\u005cu201c' offensive u'\u005cu201d' is used as a negative word (as in the first tweet), but it bears no sentiment in the second tweet when people are talking about a football game
p10895
aVThe universal model serves as a strong baseline and also provides an option for smoothing later
p10896
aVWe also compare different approaches for topic modeling, such as cross-domain topic identification by utilizing data from newswire domain
p10897
aVAn SVM model represents the examples as points in space, mapped so that the examples of the different categories are separated by a clear margin as wide as possible
p10898
aVThe option of probability estimation in LibSVM is turned on so that it can produce the probability of sentiment class c given tweet x at the classification time, i.e., P ( c x )
p10899
aVWe generate two features based on the lexicons total number of positive words or negative words found in each tweet
p10900
aVIf the last sentiment word found in the tweet is positive (or negative), this feature is set to 1 (or -1
p10901
aVIf none of the words in the tweet is sentiment word, it is set to 0 by default
p10902
aVPMI unigram lexicons in [ Mohammad et al.2013 ] two lexicons were automatically generated based on pointwise mutual information (PMI
p10903
aVWe compute 7 features based on each of the two lexicons
p10904
aVNote that for the second and third features, we ignore those with sentiment scores between -1 and 1, since we found that inclusion of those weak subjective words results in unstable performance
p10905
aVInstead, we only compute two features based on counts only total number of positive bigrams; total number of negative bigrams
p10906
aVPunctuations if there exists exclamation mark or question mark in the tweet, the feature is set to 1, otherwise set to 0
p10907
aVWe conduct pre-processing by removing stop words and some of the frequent words found in Twitter data
p10908
aVOnce we identify the topics for tweets in the training data, we can split the data into multiple subsets based on topic distributions
p10909
aVFor example, K-means clustering can be conducted based on the similarity between the topic distribution vectors or their transformed versions
p10910
aVIn this work, we assign tweet x i to cluster j if P t ( t j x i ) u'\u005cu03a4' or P t ( t j x i ) = max k P t ( t k x i
p10911
aVSimilar to the universal model, we train T topic-specific sentiment models with LibSVM
p10912
aVCluster data in D based on the topic distributions from Step 5 and train a separate sentiment model for each cluster
p10913
aVFor evaluation, we use macro averaged F score as in [ Nakov et al.2013 ] , i.e., average of the F scores computed on positive and negative classes only
p10914
aVNote that this does not make the task a binary classification problem
p10915
aVDue to the skewness in class distribution in the training set, it is observed during error analysis on the development set that subjective (positive/negative) tweets are more likely to be classified as neutral tweets
p10916
aVThe weights for positive, neutral and negative samples are set to be (1, 0.4, 1) based on the results on the development set
p10917
aVAs shown in Table 2 , weighting adds a 2% improvement
p10918
aVFor the topic-based mixture model and semi-supervised training, based on the experiments on the development set, we set the parameter u'\u005cu03a4' used in soft clustering to 0.4, the data selection parameter p to 0.96, and the interpolation parameter for smoothing u'\u005cu0398' to 0.3
p10919
aVThe first row shows the performance of the universal sentiment model as a baseline
p10920
aVThe second row shows the results from re-training the universal model by simply adding tweets selected from two iterations of semi-supervised training (about 100K
p10921
aVWith the topic information inferred from Twitter data, the F score is 2 points higher than the baseline without semi-supervised training and 1.4 higher than the baseline with semi-supervised data
p10922
aVAs shown in the third column in Table 3 , surprisingly, the model with topic information inferred from the newswire data works well on the Twitter domain
p10923
asg88
(lp10924
sg90
(lp10925
sg92
(lp10926
VIn this paper, we presented multiple approaches for advanced Twitter sentiment analysis.
p10927
aVWe established a state-of-the-art baseline that utilizes a variety of features, and built a topic-based sentiment mixture model with topic-specific Twitter data, all integrated in a semi-supervised training framework.
p10928
aVThe proposed model outperforms the top system in SemEval-2013.
p10929
aVFurther research is needed to continue to improve the accuracy in this difficult domain.
p10930
ag106
asg107
S'P14-2071'
p10931
sg109
(lp10932
VIn this paper, we present multiple approaches to improve sentiment analysis on Twitter data.
p10933
aVWe first establish a state-of-the-art baseline with a rich feature set.
p10934
aVThen we build a topic-based sentiment mixture model with topic-specific data in a semi-supervised training framework.
p10935
aVThe topic information is generated through topic modeling based on an efficient implementation of Latent Dirichlet Allocation (LDA.
p10936
aVThe proposed sentiment model outperforms the top system in the task of Sentiment Analysis in Twitter in SemEval-2013 in terms of averaged F scores * This work was done when the author was with Thomson Reuters.
p10937
ag106
asba(icmyPackage
FText
p10938
(dp10939
g3
(lp10940
V2009 ) presented a method for LDA inference based on particle filters, where a sample set of models is updated online with each new token observed from a stream
p10941
aV2009 ) rejuvenates over independent draws from the history by storing all past observations and states
p10942
aVThis algorithm thus has linear storage complexity and is not an online learning algorithm in a strict sense [ 6 ]
p10943
aVLDA [ 3 ] u'\u005cu201c' explains u'\u005cu201d' the occurrence of each word by postulating that a document was generated by repeatedly
p10944
aVThe one existing algorithm that can be directly applied under this constraint, to our knowledge, is the streaming variational Bayes framework [ 4 ] in which the posterior is recursively updated as new data arrives using a variational approximation
p10945
aVwhere I u'\u005cud835' u'\u005cudc33' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc33' u'\u005cu2032' ) is the indicator function, evaluating to 1 if u'\u005cud835' u'\u005cudc33' = u'\u005cud835' u'\u005cudc33' u'\u005cu2032' and 0 otherwise
p10946
aVNow each particle p is propagated forward by drawing a topic z i ( p ) from the conditional posterior distribution u'\u005cud835' u'\u005cudc0f' ( z i ( p ) u'\u005cu2223' u'\u005cud835' u'\u005cudc33' i - 1 ( p ) , u'\u005cud835' u'\u005cudc30' i ) and scaling the particle weight by u'\u005cud835' u'\u005cudc0f' ( w i u'\u005cu2223' u'\u005cud835' u'\u005cudc33' i - 1 ( p ) , u'\u005cud835' u'\u005cudc30' i - 1
p10947
aVDropping the superscript ( p ) for notational convenience, the conditional posterior used in the propagation step is given by
p10948
aVTo combat this inefficiency, after every state transition we estimate the effective sample size (ESS) of the particle weights as u'\u005cu2225' u'\u005cu03a9' i u'\u005cu2225' 2 - 2 [ 14 ] and resample the particles when that estimate drops below a prespecified threshold
p10949
aVIf we want to fit a model to a long non-i.i.d. stream, we require an unbiased rejuvenation sequence as well as sub-linear storage complexity
p10950
aVReservoir sampling is a widely-used family of algorithms for choosing an array ( u'\u005cu201c' reservoir u'\u005cu201d' ) of k items
p10951
aVThe most common example, presented in Vitter ( 1985 ) as Algorithm R, chooses k elements of a stream such that each possible subset of k elements is equiprobable
p10952
aVTo ensure constant space over an unbounded stream, we draw the rejuvenation sequence u'\u005cu211b' u'\u005cu2062' ( i ) uniformly from a reservoir
p10953
aVAs each token of the training data is ingested by the particle filter, we decide to insert that token into the reservoir, or not, independent of the other tokens in the current document
p10954
aVThus, at the end of step i of the particle filter, each of the i tokens seen so far in the training sequence has an equal probability of being in the reservoir, hence being selected for rejuvenation
p10955
aVWe preprocess the data by splitting each line on non-alphabet characters, converting the resulting tokens to lower-case, and filtering out any tokens that appear in a list of common English stop words
p10956
aVAfter these steps, we compute the vocabulary for each dataset as the set of all non-singleton types in the training data augmented with a special out-of-vocabulary symbol
p10957
aVDuring training we report the out-of-sample NMI, calculated by holding the word proportions u'\u005cu03a6' fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document
p10958
aVThe variance of the particle filter is often large, so for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either direction
p10959
aVThen, for each dataset, for rejuvenation disabled, rejuvenation based on a reservoir of size 1000, and rejuvenation based on the entire history (in turn), we perform 30 runs of the particle filter from that fixed initial model
p10960
aVWe observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm, as measured by NMI
p10961
aVWith this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model
p10962
aVThus we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set, setting the particle filter u'\u005cu2019' s initial model to the model out of these 20 with the highest in-sample NMI
p10963
aVWe may not always have labeled data for initialization, so we also consider a variation in which Gibbs initialization is performed 20 times on the first 80% of the initialization sample, held-out perplexity (per word) is estimated on the remaining 20%, using a first-moment particle learning approximation [ 20 ] , and the particle filter is started from the model out of these 20 with the lowest held-out perplexity
p10964
aVThe results, shown in Figure 3 , show that we can ameliorate the variance due to initialization by tuning the initial model to NMI or perplexity
p10965
aVThey later showed that rejuvenation improved performance [ 6 ] , but this impaired cognitive plausibility by necessitating storage of all previous states and observations
p10966
aVWe attempted to correct this by drawing the rejuvenation sequence from a reservoir, but our results indicate that the particle filter for LDA on our dataset is highly sensitive to initialization and not influenced by rejuvenation
p10967
aVIn the experiments of Börschinger and Johnson ( 2012 ) , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances, almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions and thus resembles a batch MCMC sampler
p10968
aVPerplexity (or likelihood) is often used to estimate model performance in LDA [ 3 , 11 , 22 , 12 ] , and does not compare the inferred model against gold-standard labels, yet it appears to be a good proxy for NMI in our experiment
p10969
aVThus, if initialization continues to be crucial to performance, at least we may have the flexibility of initializing without gold-standard labels
p10970
aVWe have proposed reservoir sampling for reducing the storage complexity of a particle filter from linear to constant
p10971
aVThis work was motivated as an expected improvement on the model of Canini et al
p10972
aVIn conclusion, it is now an open question whether u'\u005cu2014' and if so, under what assumptions u'\u005cu2014' rejuvenation benefits particle filters for LDA and similar static Bayesian models
p10973
asg88
(lp10974
sg90
(lp10975
sg92
(lp10976
VWe have proposed reservoir sampling for reducing the storage complexity of a particle filter from linear to constant.
p10977
aVThis work was motivated as an expected improvement on the model of Canini et al.
p10978
aV2009.
p10979
aVHowever, in the process of establishing an empirical baseline we discovered that rejuvenation does not play a significant role in the experiments of Canini et al.
p10980
aV2009.
p10981
aVMoreover, we found that performance of the particle filter was strongly affected by the random initialization of the model, and suggested a simple approach to reduce the variability therein without using additional data.
p10982
aVIn conclusion, it is now an open question whether and if so, under what assumptions rejuvenation benefits particle filters for LDA and similar static Bayesian models.
p10983
aVWe thank Frank Ferraro, Keith Levin, and Mark Dredze for discussions.
p10984
ag106
asg107
S'P14-2073'
p10985
sg109
(lp10986
VPrevious research has established several methods of online learning for latent Dirichlet allocation (LDA.
p10987
aVHowever, streaming learning for LDA allowing only one pass over the data and constant storage complexity is not as well explored.
p10988
aVWe use reservoir sampling to reduce the storage complexity of a previously-studied online algorithm, namely the particle filter, to constant.
p10989
aVWe then show that a simpler particle filter implementation performs just as well, and that the quality of the initialization dominates other factors of performance.
p10990
ag106
asba(icmyPackage
FText
p10991
(dp10992
g3
(lp10993
VRecent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language
p10994
aVRecent approaches to this task have been based on slot-filling [ 17 , 3 ] , combining web-scale n-grams [ 11 ] , syntactic tree substitution [ 12 ] , and description-by-retrieval [ 4 , 14 , 7 ]
p10995
aVIt is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor B u'\u005cu2062' P to penalise short translations p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text
p10996
aVSetting d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' to 0 is equivalent to bigram overlap and setting d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' to u'\u005cu221e' means tokens can be any distance apart
p10997
aV2011 ) to measure the quality of generated descriptions, using a variant they describe as rouge-1
p10998
aVWe set d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' = 4 and award partial credit for unigram only matches, otherwise known as rouge-su4
p10999
aVThe modifications available are insertion, deletion, substitute a single word, and shift a word an arbitrary distance ter is expressed as the percentage of the sentence that needs to be changed, and can be greater than 100 if the candidate is longer than the reference
p11000
aVIt is calculated by generating an alignment between the tokens in the candidate and reference sentences, with the aim of a 1:1 alignment between tokens and minimising the number of chunks ch of contiguous and identically ordered tokens in the sentence pair
p11001
aVThe alignment is based on exact token matching, followed by Wordnet synonyms, and then stemmed tokens
p11002
aVOn the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models
p11003
aVThis could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the gold-standard text, as in the Flickr8K data set
p11004
aVWe can hypothesise that in both translation and summarisation, the source text acts as a lexical and semantic framework within which the translation or summarisation process takes place
p11005
aVIn Figure 3 (a), the authors of the descriptions made different decisions on what to describe
p11006
aVThe use of u'\u005cu039a' requires the transformation of real-valued scores into categorical values, and thus loses information; we use the judgement and evaluation measure scores in their original forms
p11007
aVIt is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al u'\u005cu2019' s agreement analysis, but they also reach the conclusion that unigram bleu is not an appropriate measure of image description performance
p11008
aVThis discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz ( 2009 ) included hundreds of texts with 30 human judges
p11009
aVNevertheless, we propose that unigram bleu should no longer be used as an objective function for automatic image description because it has a weak correlation with human accuracy judgements
p11010
aVWe recommend adopting either Meteor, Smoothed bleu , or rouge-su4 because they show stronger correlations with human judgements
p11011
asg88
(lp11012
sg90
(lp11013
sg92
(lp11014
VIn this paper we performed a sentence-level correlation analysis of automatic evaluation measures against expert human judgements for the automatic image description task.
p11015
aVWe found that sentence-level unigram bleu is only weakly correlated with human judgements, even though it has extensively reported in the literature for this task.
p11016
aVMeteor was found to have the highest correlation with human judgements, but it requires Wordnet and paraphrase resources that are not available for all languages.
p11017
aVOur findings held when judgements were made on human-written or computer-generated descriptions.
p11018
aVThe variability in what and how people describe images will cause problems for all of the measures compared in this paper.
p11019
aVNevertheless, we propose that unigram bleu should no longer be used as an objective function for automatic image description because it has a weak correlation with human accuracy judgements.
p11020
aVWe recommend adopting either Meteor, Smoothed bleu , or rouge-su4 because they show stronger correlations with human judgements.
p11021
aVWe believe these suggestions are also applicable to the ranking tasks proposed in Hodosh et al.
p11022
aV2013 ) , where automatic evaluation scores could act as features to a ranking function.
p11023
ag106
asg107
S'P14-2074'
p11024
sg109
(lp11025
VImage description is a new natural language generation task, where the aim is to generate a human-like description of an image.
p11026
aVThe evaluation of computer-generated text is a notoriously difficult problem, however, the quality of image descriptions has typically been measured using unigram bleu and human judgements.
p11027
aVThe focus of this paper is to determine the correlation of automatic measures with human judgements for this task.
p11028
aVWe estimate the correlation of unigram and Smoothed bleu , ter , rouge-su4 , and Meteor against human judgements on two data sets.
p11029
aVThe main finding is that unigram bleu has a weak correlation, and Meteor has the strongest correlation with human judgements.
p11030
aVAn older woman with a small dog in the snow.
p11031
aVA woman and a cat are outside in the snow.
p11032
aVA woman in a brown vest is walking on the snow with an animal.
p11033
aVA woman with a red scarf covering her head walks with her cat on snow-covered ground.
p11034
aVHeavy set woman in snow with a cat.
p11035
ag106
asba(icmyPackage
FText
p11036
(dp11037
g3
(lp11038
VThen, the algorithm re-ranks candidate links according to mutual relations between all the named entities found in the document
p11039
aVThe evaluation is based on experiments conducted on the test corpus of the TAC-KBP 2012 entity linking task
p11040
aVDealing with ambiguity is one of the key difficulties in this task, since mentions are often highly polysemous, and potentially related to many different KB entries
p11041
aVThey are used as matching sequences to locate corresponding candidate entries in the KB, and then to disambiguate those candidates using similarity measures
p11042
aVThe NED problem is related to the Word Sense Disambiguation (WSD) problem [ 16 ] , and is often more challenging since mentions of NEs can be highly ambiguous
p11043
aVFor instance, names of places can be very common as is Paris, which refers to 26 different places in Wikipedia
p11044
aVHence, systems that attempt to address the NED problem must include disambiguation resources
p11045
aVIn more recent approaches, it is suggested that annotation processes based on similarity distance measures can be improved by making use of other annotations present in the same document
p11046
aVSuch techniques are referred to as semantic relatedness [ 19 ] , collective disambiguation [ 12 ] , or joint disambiguation [ 8 ]
p11047
aVThe idea is to evaluate in a set of candidate links which one is the most likely to be correct by taking the other links contained in the document into account
p11048
aVFor example, if a NE describes a city name like Paris , it is more probable that the correct link for this city name designates Paris (France) rather than Paris (Texas) if a neighbor entity offers candidate links semantically related to Paris (France) like the Seine river or the Champs-Elysées
p11049
aVThe ontology (like YAGO or DBPedia) provides a pre-existing set of potential relations between the entities to link (like for instance, in our previous example, Paris (France) has_river Seine ) that will be used to rank the best candidates according to their mutual presence in the document
p11050
aVA strong effort has been conducted recently by the TAC-KBP evaluation task [ 13 ] to create standardized corpus, and annotation standards based on Wikipedia for evaluation and comparison of EL systems
p11051
aVWe describe below some recent approaches proposed for solving the EL task
p11052
aVLately, [ 6 , 15 , 17 ] extended this framework by using richer features for similarity comparison
p11053
aVMore recently, several systems have been launched as web services dedicated to EL tasks
p11054
aVIt uses bags of words to disambiguate semantic entities according to a cosine similarity algorithm
p11055
aVWe propose a mutual disambiguation algorithm that improves the accuracy of entity links in a document by using successive corrections applied to an annotation object representing this document
p11056
aVThe annotation object is composed of information extracted from the document along with linguistic and semantic annotations as described hereafter
p11057
aVDocuments are processed by an annotator capable of producing POS tags for each word, as well as spans, NE surface forms, NE labels and ranked candidate Wikipedia URIs for each candidate NE
p11058
aVSince the system focuses on NEs, rows with lexical units that do not belong to a NE SF are dropped from the annotation object, and NE SF are refined as described in [ 5 ]
p11059
aVWhen NE SF are spanned over several rows, these rows are merged into a single one
p11060
aVThus, we consider an annotation object u'\u005cud835' u'\u005cudc9c' u'\u005cud835' u'\u005cudc9f' , which is an array with a row for each NE, and columns storing related knowledge
p11061
aVIf n NEs were annotated in u'\u005cud835' u'\u005cudc9f' , then u'\u005cud835' u'\u005cudc9c' u'\u005cud835' u'\u005cudc9f' has n rows
p11062
aVIf l candidate URIs are provided for each NE, then u'\u005cud835' u'\u005cudc9c' u'\u005cud835' u'\u005cudc9f' has ( l + 4 ) columns c u , u u'\u005cu2208' { 1 , l + 4 }
p11063
aVTo support the correction process based on co-reference chains, the system tries to correct NE labels for all the NEs listed in the annotation object
p11064
aVThe second NE has a longer surface form than the first one, and its associated first rank URI is the most frequent
p11065
aVHence, the co-reference correction process will assign the right URI to the first NE ( URI 1 http://en.wikipedia.org/wiki /Paris ), which was wrongly linked to the actress Paris Hilton
p11066
aVBut if the IBM mention co-occurs with a Thomas Watson, Jr mention in the document, there will probably be more links between the International Business Machine and Thomas Watson, Jr related Wikipedia pages than between the International Brotherhood of Magicians and Thomas Watson, Jr related Wikipedia pages
p11067
aVThe input of the MDP is an annotation object u'\u005cud835' u'\u005cudc9c' u'\u005cud835' u'\u005cudc9f' with n rows, obtained as explained in Section 3.1
p11068
aVFor all i u'\u005cu2208' [ [ 1 , n ] ] , k u'\u005cu2208' [ [ 1 , l ] ] , we build the set S i k , composed of the Wikipedia URIs and categories contained in the source Wikipedia document related to the URI stored in u'\u005cud835' u'\u005cudc9c' u'\u005cud835' u'\u005cudc9f' [ i ] u'\u005cu2062' [ k ] that we will refer to as URI k i to ease the reading
p11069
aVWe assumed the dsr_score was much more semantically significant than the csr_score, and translated this assumption in the weight calculation by introducing two correction parameters u'\u005cu0391' and u'\u005cu0392' used in the final scoring calculation
p11070
aVFor all i u'\u005cu2208' [ [ 1 , n ] ] , for each set of URIs { URI i k , u'\u005cu2005' k u'\u005cu2208' [ [ 1 , l ] ] } , the re-ranking process is conducted according to the following steps
p11071
aVS 1 2 is associated to the International Business Machines Corporation , and S 2 1 to the Thomas Watson, Jr page dsr_score(URI 1 1 ) sums up the number of occurrences of URI 1 1 in S j 1 for all j u'\u005cu2208' [ [ 1 , n ] ] - { 1 }
p11072
aVHence, in the current example, dsr_score(URI 1 1 ) is the number of occurrences of URI 1 1 in S 2 1 , namely the number of times the International Brotherhood of Magicians are cited in the Thomas Watson, Jr page
p11073
aVSimilarly, dsr_score(URI 2 1 ) is equal to the number of times the International Business Machines Corporation is cited in the Thomas Watson, Jr page csr_score(URI 1 1 ) sums up the number of common URIs and categories between S 1 1 and S 2 1 , i.e., the number of URIs and categories appearing in both International Brotherhood of Magicians and Thomas Watson, Jr pages csr_score(URI 2 1 ) counts the number of URIs and categories appearing in both International Business Machines Corporation and Thomas Watson, Jr pages
p11074
aVAfter calculation, we have mutual_relation_score(URI 1 1 ) mutual_relation_score(URI 2 1 ) The candidate URIs for [ IBM ] are re-ranked accordingly, and International Business Machines Corporation becomes its first rank candidate
p11075
aVGiven a query that consists of a document with a specified name mention of an entity, the task is to determine the correct node in the reference KB for the entity, adding a new node for the entity if it is not already in the reference KB
p11076
aVFor the sake of reproducibility, we applied the KBP scoring metric ( B 3 + F ) described in [ 20 ] , and we used the KBP scorer 1 1 http://www.nist.gov/tac/2013/KBP/EntityLinking/tools.html
p11077
aVThe three best results and the median from TAC-KBP 2012 systems are shown in the remaining columns for the sake of comparison
p11078
aVThe presented system provides a robust semantic disambiguation method, based on mutual relation of entities inside a document, using a standard annotation engine
p11079
aVSemLinker is fully implemented, and publicly released as an open source toolkit ( http://code.google.com/p/semlinker
p11080
aVIt has been deployed in the TAC-KBP 2013 evaluation campaign
p11081
aVThis research was supported as part of Dr Eric Charton u'\u005cu2019' s Mitacs Elevate Grant sponsored by 3CE
p11082
asg88
(lp11083
sg90
(lp11084
sg92
(lp11085
VThe presented system provides a robust semantic disambiguation method, based on mutual relation of entities inside a document, using a standard annotation engine.
p11086
aVIt uses co-reference, NE normalization methods, and Wikipedia internal links as mutual disambiguation resource to improve the annotations.
p11087
aVWe show that our proposition improves the performance of a standard annotation engine applied to the TAC-KBP evaluation framework.
p11088
aVSemLinker is fully implemented, and publicly released as an open source toolkit ( http://code.google.com/p/semlinker.
p11089
aVIt has been deployed in the TAC-KBP 2013 evaluation campaign.
p11090
aVOur future work will integrate other annotation engines in the system architecture in a collaborative approach.
p11091
ag106
asg107
S'P14-2078'
p11092
sg109
(lp11093
VThe disambiguation algorithm presented in this paper is implemented in SemLinker, an entity linking system.
p11094
aVFirst, named entities are linked to candidate Wikipedia pages by a generic annotation engine.
p11095
aVThen, the algorithm re-ranks candidate links according to mutual relations between all the named entities found in the document.
p11096
aVThe evaluation is based on experiments conducted on the test corpus of the TAC-KBP 2012 entity linking task.
p11097
ag106
asba(icmyPackage
FText
p11098
(dp11099
g3
(lp11100
VThis approach is advantageous if large amounts of in-domain sentence-parallel data are available to train SMT systems, but relevance rankings to train retrieval models are not
p11101
aVFor example, in patent prior art search, patents granted at any patent office worldwide are considered relevant if they constitute prior art with respect to the invention claimed in the query patent
p11102
aVSince patent applicants and lawyers are required to list relevant prior work explicitly in the patent application, patent citations can be used to automatically extract large amounts of relevance judgments across languages [ 12 ]
p11103
aVSince authors are encouraged to avoid orphan articles and to cite their sources, Wikipedia has a rich linking structure between related articles, which can be exploited to create relevance links between articles across languages [ 2 ]
p11104
aV2013 ) advocate the use of dense features encoding domain knowledge on inventors, assignees, location and date, together with dense similarity scores based on bag-of-word representations of patents
p11105
aVFurthermore, we show that our approach can be seen as supervised model combination that allows to combine SMT-based and ranking-based approaches for further substantial improvements
p11106
aVWe conjecture that the gains are due to orthogonal information contributed by domain-knowledge, ranking-based word associations, and translation-based information
p11107
aVThe advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations [ 27 ]
p11108
aV2012 ) brought SMT back into this paradigm by projecting terms from n -best translations from synchronous context-free grammars
p11109
aVWe will refer to DT and PSQ as SMT-based models that translate a query, and then perform monolingual retrieval using BM25
p11110
aVTranslation is agnostic of the retrieval task
p11111
aVWe present two methods for optimizing W in the following
p11112
aVMemory usage was reduced using the same hashing technique as for boosting
p11113
aVDomain knowledge features for patents were inspired by Guo and Gomes ( 2009 a feature fires if two patents share similar aspects, e.g., a common inventor
p11114
aVAs we do not have access to address data, we omit geolocation features and instead add features that evaluate similarity w.r.t patent classes extracted from IPC codes
p11115
aVThe intersection between target set T n and the source category set S reflects the category level similarity between query and document, which we calculate as a mutual containment score s n = 1 2 u'\u005cu2062'
p11116
aVOptimization for these additional models including domain knowledge features was done by overloading the vector representation of queries u'\u005cud835' u'\u005cudc2a' and documents u'\u005cud835' u'\u005cudc1d' in the VW linear learner
p11117
aVIn addition to dense domain-knowledge features, we incorporate arbitrary ranking models as dense features whose value is the score of the ranking model
p11118
aVTraining data was sampled from the dev set and processed with VW
p11119
aVEN patents are regarded as relevant with level (3) to a JP query patent, if they are in a family relationship (e.g.,, same invention), cited by the patent examiner (2), or cited by the applicant (1
p11120
aVThe information need may be paraphrased as a high-level definition of the topic
p11121
aVSince typically the first sentence of any Wikipedia article is such a well-formed definition, this allows us to extract a large set of one sentence queries from Wikipedia articles
p11122
aVSince Wikipedia articles vary greatly in length, we restricted EN documents to the first 200 words after extracting the link graph to reduce the number of features for BM and VW models
p11123
aVTo reduce the EN vocabulary to a comparable size, we applied similar preprocessing and CFH with F =30k and k =5
p11124
aVSince for Wikipedia data, the DE and EN vocabularies were both large (6.7M and 6M), we used the same filtering and preprocessing as for the patent data before applying CFH with F =40k and k =5 on both sides
p11125
aVLinLearn denotes model combination by overloading the vector representation of queries u'\u005cud835' u'\u005cudc2a' and documents u'\u005cud835' u'\u005cudc1d' in the VW linear learner by incorporating arbitrary ranking models as dense features
p11126
aVIn difference to grid search for Borda , optimal weights for the linear combination of incorporated ranking models can be learned automatically
p11127
aVWe do not report combination results including the sparse BM model since they were consistently lower than the ones with the sparse VW model
p11128
aVAs can be seen from inspecting the two blocks of results, one for patents, one for Wikipedia, we find the same system rankings on both datasets
p11129
aVIn both cases, as standalone systems, DT and PSQ are very close and far better than any ranking approach, irrespective of the objective function or the choice of sparse or dense features
p11130
aVThe best result is achieved by combining DT and PSQ with DK and VW
p11131
aVThis is due to the already high scores of the combined models, but also to the combination of yet other types of orthogonal information
p11132
aVBorda voting gives the best result under MAP which is probably due to the adjustment of the interpolation parameter for MAP on the development set
p11133
aVUnder NDCG and PRES, LinLearn achieves the best results, showing the advantage of automatically learning combination weights that leads to stable results across various metrics
p11134
aVWe conjecture that if these types of information sources are available, a supervised ranking approach will yield superior results in other retrieval scenarios as well
p11135
asg88
(lp11136
sg90
(lp11137
sg92
(lp11138
VSpecial domains such as patents or Wikipedia offer the possibility to extract cross-lingual relevance data from citation and link graphs.
p11139
aVThese data can be used to directly optimizing cross-lingual ranking models.
p11140
aVWe showed on two different large-scale ranking scenarios that a supervised combination of orthogonal information sources such as domain-knowledge, translation knowledge, and ranking-specific word associations by far outperforms a pipeline of query translation and retrieval.
p11141
aVWe conjecture that if these types of information sources are available, a supervised ranking approach will yield superior results in other retrieval scenarios as well.
p11142
ag106
asg107
S'P14-2080'
p11143
sg109
(lp11144
VWe present an approach to cross-language retrieval that combines dense knowledge-based features and sparse word translations.
p11145
aVBoth feature types are learned directly from relevance rankings of bilingual documents in a pairwise ranking framework.
p11146
aVIn large-scale experiments for patent prior art search and cross-lingual retrieval in Wikipedia, our approach yields considerable improvements over learning-to-rank with either only dense or only sparse features, and over very competitive baselines that combine state-of-the-art machine translation and retrieval.
p11147
ag106
asba(icmyPackage
FText
p11148
(dp11149
g3
(lp11150
VPartly due to this dependence, most research focuses on partial orderings of a document u'\u005cu2019' s events
p11151
aVDeeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented
p11152
aVIt provided for a common dataset of annotations between events and time expressions that allowed the community to compare approaches
p11153
aVSince its creation, other corpora and several competitions have based their tasks on the TimeBank setup
p11154
aVThe original annotators were instructed to label relations critical to the document u'\u005cu2019' s understanding
p11155
aVThe result is a sparse labeling that leaves much of the document unlabeled
p11156
aVIn contrast, we aim for a shift in the community wherein all pairs are considered candidates for temporal ordering, allowing researchers to ask questions such as how must algorithms adapt to label the complete graph of pairs, and if the more difficult and ambiguous event pairs are included, how must feature-based learners change
p11157
aVThe stated goal of TempEval-3 [ 7 ] was to focus on relation identification instead of classification, but the training and evaluation data followed the TimeBank approach where only a subset of event pairs were labeled
p11158
aVAs a result, many systems focused on classification, with the top system classifying pairs in only three syntactic constructions [ 2 ]
p11159
aVWe describe the first annotation framework that forces annotators to annotate all pairs 1 1 As discussed below, all pairs in a given window size
p11160
aVThe annotator did not look at the pair of events, so a relation may or may not exist
p11161
aV[ 5 ] annotated u'\u005cu201c' temporal dependency structures u'\u005cu201d' , though they only focused on relations between pairs of events
p11162
aVThey have the densest annotation, but u'\u005cu201c' the annotator was not required to annotate all pairs of event mentions, but as many as possible u'\u005cu201d' .This paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window
p11163
aVThus this work is the first annotation effort that can guarantee its event/time graph to be strongly connected
p11164
aVThe second facet is the focus of this paper when should an annotator label an ordering relation
p11165
aVOur proposal thus starts with documents that have been already been annotated with events, time expressions, and document creation times (DCT
p11166
aVThe following example sentence serves as our motivating example
p11167
aV{quoting} Police confirmed Friday that the body found along a highway in this municipality 15 miles south of San Juan belonged to Jorge Hernandez
p11168
aVLearning algorithms handle these unlabeled edges by making incorrect assumptions, or by ignoring large parts of the temporal graph
p11169
aVSeveral models with rich temporal reasoners have been published, but since they require more connected graphs, improvement over pairwise classifiers have been minimal [ 4 , 10 ]
p11170
aVThis paper thus proposes an annotation process that builds denser graphs with formal properties that learners can rely on, such as locally complete subgraphs
p11171
aVThe TimeBank corpus uses 14 relations based on the Allen interval relations
p11172
aVThe main reason for not using a more fine-grained set is because we annotate pairs that are far more ambiguous than those considered in previous efforts
p11173
aVWe lean toward higher annotator agreement with relations that have greater separation between their semantics 4 4 For instance, a relation like starts is a special case of includes if events are viewed as open intervals, and immediately before is a special case of before
p11174
aVThe tool is unique in that it includes a transitive reasoner that infers relations based on the annotator u'\u005cu2019' s latest annotations
p11175
aVThis prohibits the annotator from entering edges that break transitivity
p11176
aVAs a result, several properties are ensured through this process the graph (1) is a strongly connected graph, (2) is consistent with no contradictions, and (3) has all required edges labeled
p11177
aVSince the annotation tool frees the annotators from the decision of when to label an edge, the focus is now what to label each edge
p11178
aVSince the annotators are forced to label all required edges, the decision to label an edge as vague instead of a defined temporal relation is critical
p11179
aVWe adopted an 80 u'\u005cu2062' % rule that instructed annotators to choose a specific non-vague relation if they are 80% confident that it was the writer u'\u005cu2019' s intent that a reader infer that relation
p11180
aVIf a document has 2 annotators, both have to agree on the relation or it is labeled vague
p11181
aVThis agreement rule acts as a check to our 80% confidence rule, backing off to vague when decisions are uncertain (arguably, this is the definition of vague
p11182
aVThe core event was treated as having occurred, whether or not the text implied that it had occurred
p11183
aVThis event pair is ordered (expect before cut) since the expectation occurs before the cutting (in the possible world where the cutting occurs
p11184
aVOne assumes the event does occur, and all other events are ordered accordingly
p11185
aVThis event pair is assigned the relation (help is included in prevent) because the help event is not meaningful on its own
p11186
aVTime Expressions the words now and today were given u'\u005cu201c' long now u'\u005cu201d' interpretations if the words could be replaced with nowadays and not change the meaning of their sentences
p11187
aVIf nowadays is not suitable, then the now was included in the DCT
p11188
aVGeneric Events finally, generic events were not skipped
p11189
aVUsing the tool described above, we annotated 36 random documents with at least two annotators each to create a training set, development set, and test set
p11190
aVThese 36 documents led to an annotation of 4 times as many relations as the entire 183 document TimeBank
p11191
aVAll four authors annotated the same initial document, conflicts and disagreements were discussed, and guidelines were updated accordingly
p11192
aVSince there were only 2 annotators for most documents (and since we require the majority of annotators to agree), in general, the vague label was applied to the final graph if either annotator chose it
p11193
aVThis seems appropriate since a disagreement between two annotators directly implies that the relation is vague
p11194
asg88
(lp11195
sg90
(lp11196
sg92
(lp11197
VWe described our annotation framework that produces corpora with formal guarantees about the annotated graph s structure.
p11198
aVBoth the annotation tool and the new TimeBank-Dense corpus are publicly available.6 This is the first corpus with guarantees of connectedness, consistency, and a semantics for unlabeled edges.
p11199
aVWe hope to encourage a shift in the temporal ordering community to consider the entire document when making local decisions.
p11200
aVFurther work is needed to handle difficult pairs with the VAGUE relation.
p11201
aVWe look forward to evaluating new algorithms on this dense corpus.
p11202
ag106
asg107
S'P14-2082'
p11203
sg109
(lp11204
VToday s event ordering research is heavily dependent on the annotated corpora available to it.
p11205
aVThe corpora influence shared evaluations and drive algorithm development.
p11206
aVPartly due to this dependence, most research focuses on partial orderings of a document s events.
p11207
aVFor instance, the TempEval competitions and the TimeBank only annotate small portions of the event graph.
p11208
aVThey focus on the most salient events or on specific types of event pairs (e.g.,, only events in the same sentence.
p11209
aVDeeper temporal reasoners struggle with this sparsity because the entire temporal picture is not represented.
p11210
aVThis paper proposes a new annotation process with a unique mechanism to force annotators to label connected graphs.
p11211
aVIt generates 10 times more relations per document than the TimeBank and the corpus is larger than all current corpora.
p11212
aVWe hope this new annotation framework will encourage research on new models, particularly global models with deep reasoning.
p11213
aVnoitemsep,topsep=0.25em \u005cquotingsetup vskip=0.25em.
p11214
ag106
asba(icmyPackage
FText
p11215
(dp11216
g3
(lp11217
VIn NLP, we often model annotation as if it reflected a single ground truth that was guided by an underlying linguistic theory
p11218
aVIf this was true, the specific theory should be learnable from the annotated data
p11219
aVHowever, it is well known that there are linguistically hard cases [] , where no theory provides a clear answer, so annotation schemes commit to more or less arbitrary decisions
p11220
aVthat actual errors are in fact so infrequent as to be negligible, even when linguists annotate without guidelines
p11221
aVWe then collect a corpus of such disagreements and have experts mark which ones are due to actual annotation errors , and which ones reflect linguistically hard cases (Section 3
p11222
aVThe results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors
p11223
aVWe did so in order to make comparison between existing data sets possible
p11224
aVMoreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set
p11225
aVWe present disagreements as Hinton diagrams in Figure 2 a u'\u005cu2013' c
p11226
aVNote that the spoken language data does not include punctuation
p11227
aVIn particular, adpositions (ADP) are confused with particles (PRT) (as in the case of u'\u005cu201c' get out u'\u005cu201d' ); adjectives (ADJ) are confused with nouns (as in u'\u005cu201c' stone lion u'\u005cu201d' ); pronouns (PRON) are confused with determiners (DET) ( u'\u005cu201c' my house u'\u005cu201d' ); numerals are confused with adjectives, determiners, and nouns ( u'\u005cu201c' 2nd time u'\u005cu201d' ); and adjectives are confused with adverbs (ADV) ( u'\u005cu201c' see you later u'\u005cu201d'
p11228
aVIn Twitter, the X category is often confused with punctuations, e.g.,, when annotating punctuation acting as discourse continuation marker
p11229
aVOur analyses show that a) experts disagree on the known hard cases when freely annotating text, and b) that these disagreements are the same across text types
p11230
aVIf we pre-filter the data via Wiktionary and use an item-response model [] rather than majority voting, the agreement rises to 80.58%
p11231
aVOne difference is that lay people do not confuse numerals very often, probably because they rely more on orthographic cues than on distributional evidence
p11232
aVIn this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus
p11233
aVIt works by collecting u'\u005cu201c' variation n -grams u'\u005cu201d' , i.e., the longest sequence of words ( n -gram) in a corpus that has been observed with a token being tagged differently in another occurence of the same n -gram in the same corpus
p11234
aVThe algorithm starts off by looking for unigrams and expands them until no longer n -grams are found
p11235
aVIn order to do so, we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features
p11236
aVIf we balance the data set and perform 3-fold cross-validation, a L2-regularized logistic regression (L2-LR) model achieves an f 1 -score for detecting errors at 70% (cf. Table 1 ), which is above average, but not very impressive
p11237
asg88
(lp11238
sg90
(lp11239
sg92
(lp11240
VIn this paper, we show that disagreements between professional or lay annotators are systematic and consistent across domains and some of them are systematic also across languages.
p11241
aVIn addition, we present an empirical analysis of POS annotations showing that the vast majority of inter-annotator disagreements are competing, but valid, linguistic interpretations.
p11242
aVWe propose to embrace such disagreements rather than using annotation guidelines to optimize inter-annotator agreement, which would bias our models in favor of some linguistic theory.
p11243
ag106
asg107
S'P14-2083'
p11244
sg109
(lp11245
VIn linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement.
p11246
aVHowever, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them.
p11247
aVWe present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages.
p11248
aVThis points to an underlying ambiguity rather than random errors.
p11249
aVMoreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors.
p11250
aVSpecifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmotivated.
p11251
ag106
asba(icmyPackage
FText
p11252
(dp11253
g3
(lp11254
VAutomatically detecting verbal irony (roughly, sarcasm) is a challenging task because ironists say something other than u'\u005cu2013' and often opposite to u'\u005cu2013' what they actually mean
p11255
aVDiscerning ironic intent exclusively from the words and syntax comprising texts (e.g.,, tweets, forum posts) is therefore not always possible additional contextual information about the speaker and/or the topic at hand is often necessary
p11256
aVWe show that annotators frequently require context to make judgements concerning ironic intent, and that machine learning approaches tend to misclassify those same comments for which annotators required additional context
p11257
aVThis work concerns the task of detecting verbal irony online
p11258
aVBut existing work on automatic irony detection u'\u005cu2013' reviewed in Section 2 u'\u005cu2013' has not explicitly attempted to operationalize such theories, and has instead relied on features (mostly word counts) intrinsic to the texts that are to be classified as ironic
p11259
aVThese approaches have achieved some success, but necessarily face an upper-bound the exact same sentence can be both intended ironically and unironically, depending on the context (including the speaker and the topic at hand
p11260
aVThis suggests that, as humans require context to make their judgements for this task, so too do computers
p11261
aVIn these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony
p11262
aVThe most common data source used to experiment with irony detection systems has been Twitter [] , though Amazon product reviews have been used experimentally as well []
p11263
aV[] also recently introduced the Internet Argument Corpus (IAC), which includes a sarcasm label (among others
p11264
aVFor example, in the case of Amazon product reviews, knowing the kinds of books that an individual typically likes might inform our judgement someone who tends to read and review Dostoevsky is probably being ironic if she writes a glowing review of Twilight
p11265
aVOf course, many people genuinely do enjoy Twilight and so if the review is written subtly it will likely be difficult to discern the author u'\u005cu2019' s intent without this background
p11266
aVFor example, http://reddit.com/r/politics features articles (and hence comments) centered around political news
p11267
aVData collection and annotation is ongoing, so we will continue to release new (larger) versions of the corpus in the future
p11268
aV2 2 We performed naïve u'\u005cu2018' segmentation u'\u005cu2019' of comments based on punctuation
p11269
aVAverage pairwise Cohen u'\u005cu2019' s Kappa [] is 0.341, suggesting fair to moderate agreement [] , as we might expect for a subjective task like this one
p11270
aVReddit is a good corpus for the irony detection task in part because it provides a natural practical realization of the otherwise ill-defined context for comments
p11271
aVBut if we peruse the author u'\u005cu2019' s comment history, we see that he or she repeatedly derides Senator Cruz (e.g.,, writing u'\u005cu201c' Ted Cruz is no Ronald Reagan
p11272
aVFrom this contextual information, then, we can reasonably assume that the comment was intended ironically (and all three annotators did so after assessing the available contextual information
p11273
aVRecall that our annotation tool allows labelers to request additional context if they cannot make a decision based on the comment text alone (Figure 1
p11274
aVOn average, annotators requested additional context for 30% of comments (range across annotators of 12% to 56%
p11275
aVAs shown in Figure 3 , annotators are consistently more confident once they have consulted this information
p11276
aVWe then model the probability of this event as a linear function of whether or not any annotator labeled any sentence in comment i as ironic
p11277
aVTo this end, we introduce a variable u'\u005cu2133' i for each comment i such that u'\u005cu2133' i = 1 if y ^ i u'\u005cu2260' y i , i.e.,, u'\u005cu2133' i is an indicator variable that encodes whether or not the classifier misclassified comment i
p11278
aVFitting this to the data, we estimated u'\u005cu0398' ^ 2 = 0.971 with a 95 u'\u005cu2062' % CI of (0.810, 1.133); p 0.001
p11279
aVWe recorded confidence judgements and requests for contextualizing information for each comment during annotation
p11280
aVWe have shown that annotators rely on contextual cues (in addition to word and grammatical features) to discern irony and argued that this implies computers should, too
p11281
asg88
(lp11282
sg90
(lp11283
sg92
(lp11284
VWe have described a new (publicly available) corpus for the task of verbal irony detection.
p11285
aVThe data comprises comments scraped from the social news website reddit.
p11286
aVWe recorded confidence judgements and requests for contextualizing information for each comment during annotation.
p11287
aVWe analyzed this corpus to provide empirical evidence that annotators quite often require context beyond the comment under consideration to discern irony; especially for those comments ultimately deemed as being intended ironically.
p11288
aVWe demonstrated that a standard token-based machine learning approach misclassified many of the same comments for which annotators tend to request context.
p11289
aVWe have shown that annotators rely on contextual cues (in addition to word and grammatical features) to discern irony and argued that this implies computers should, too.
p11290
aVThe obvious next step is to develop new machine learning models that exploit the contextual information available in the corpus we have curated (e.g.,, previous comments by the same user, the thread topic.
p11291
ag106
asg107
S'P14-2084'
p11292
sg109
(lp11293
VAutomatically detecting verbal irony (roughly, sarcasm) is a challenging task because ironists say something other than and often opposite to what they actually mean.
p11294
aVDiscerning ironic intent exclusively from the words and syntax comprising texts (e.g.,, tweets, forum posts) is therefore not always possible additional contextual information about the speaker and/or the topic at hand is often necessary.
p11295
aVWe introduce a new corpus that provides empirical evidence for this claim.
p11296
aVWe show that annotators frequently require context to make judgements concerning ironic intent, and that machine learning approaches tend to misclassify those same comments for which annotators required additional context.
p11297
ag106
asba(icmyPackage
FText
p11298
(dp11299
g3
(lp11300
VWhile most verbs have one predominant interpretation, others are more flexible for aspectual class and can occur as either stative ( 1 ) or dynamic ( 1 ) depending on the context
p11301
aVThere are also cases that allow for both readings, such as ( 1
p11302
aVIn contrast to Siegel and McKeown ( 2000 ) , we do not conduct the task of predicting aspectual class solely at the type level, as such an approach ignores the minority class of ambiguous verbs
p11303
aVIn addition, we show that type-based features, including novel distributional features based on representative verbs, accurately predict predominant aspectual class for unseen verb types
p11304
aVOur work differs from prior work in that we treat the problem as a three-way classification task, predicting dynamic , stative or both as the aspectual class of a verb in context
p11305
aVAspectual class is well treated in the linguistic literature [ 33 , 12 , 29 , for example]
p11306
aVOur notion of the stative/dynamic distinction corresponds to Bach u'\u005cu2019' s [ 1 ] distinction between states and non-states; to states versus occurrences (events and processes) according to Mourelatos ( 1978 ) ; and to Vendler u'\u005cu2019' s [ 33 ] distinction between states and the other three classes (activities, achievements, accomplishments
p11307
aVSince then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank [ 25 ] and the TempEval challenges [ 34 , 35 , 32 ] , where top-performing systems [ 16 , 3 , 6 ] use corpus-based features, WordNet synsets, parse paths and features from typed dependencies to classify events as a joint task with determining the event u'\u005cu2019' s span
p11308
aVUsing the LCS Database [ 11 ] , we identify sets of verb types whose senses are only stative (188 verbs, e.g., belong, cost, possess ), only dynamic (3760 verbs, e.g., alter, knock, resign ), or mixed (215 verbs, e.g., fill, stand, take ), following a procedure described by Dorr and Olsen ( 1997 )
p11309
aVWe use 6161 clauses for the classification task, omitting clauses with have or be as the main verb and those where no main verb could be identified due to parsing errors ( none
p11310
aVWe observe higher agreement in the jokes and news subcorpora than for letters; texts in the letters subcorpora are largely argumentative and thus have a different rhetorical style than the more straightforward narratives and reports found in jokes
p11311
aVThe data for our experiments uses the label dynamic or stative whenever annotators agree, and both whenever they disagree or when at least one annotator marked the clause as both , assuming that both readings are possible in such cases
p11312
aVBecause we don u'\u005cu2019' t want to model the authors u'\u005cu2019' personal view of the theory, we refrain from applying an adjudication step and model the data as is
p11313
aVThe data is processed in the same way as Asp-MASC, discarding instances with parsing problems
p11314
aVThis results in 2667 instances u'\u005cu039a' is 0.6, the confusion matrix is shown in Table 3
p11315
aV3 3 We thank the authors for providing us their code
p11316
aVWe aim to leverage existing, possibly noisy sets of representative stative, dynamic or mixed verb types extracted from LCS (see section 3 ), making up for unseen verbs and noise by averaging over distributional similarities
p11317
aVUsing an existing large distributional model [ 31 ] estimated over the set of Gigaword documents marked as stories, for each verb type, we build a syntactically informed vector representing the contexts in which the verb occurs
p11318
aVTense, progressive, perfect and voice are extracted from dependency parses as described above
p11319
aVFor features encoding grammatical dependents, we focus on a subset of grammatical relations
p11320
aVThe feature value is either the WordNet lexical filename (e.g., noun.person ) of the given relation u'\u005cu2019' s argument or its POS tag, if the former is not available
p11321
aVWe also include features that indicate, if there are any, the particle of the verb and its prepositional dependents
p11322
aVFor the sentence A little girl had just finished her first week of school , the instance-based feature values would include tense past , subj noun.person , dobj noun.time or particle none
p11323
aVNo feature combination significantly 4 4 According to McNemar u'\u005cu2019' s test with Yates u'\u005cu2019' correction for continuity, p 0.01 outperforms the baseline of simply memorizing the most frequent class of a verb type in the respective training folds
p11324
aVOtherwise, the experimental setup is as in experiment 1
p11325
aVResults appear in Table 8
p11326
aVFor multi-label verbs, the feature combination Lemma+LingInd+Inst leads to significant 5 improvement of 2% gain in accuracy over the baseline; Table 9 reports detailed class statistics and reveals a gain in F-measure of 3 points over the baseline
p11327
aVTo sum up, Inst features are essential for classifying multi-label verbs, and the LingInd features provide some useful prior
p11328
aVFor verbs with ambiguous aspectual class, type-based classification is not sufficient, as this approach selects a dominant sense for any given verb and then always assigns that
p11329
aVTherefore we propose handling ambiguous verbs separately
p11330
aVAs Asp-MASC contains only few instances of each of the ambiguous verbs, we turn to the Asp-Ambig dataset
p11331
aV5 5 The third column also shows the outcome of using either only the Lemma, only LingInd or only Dist in LOO; all have almost the same outcome as using the majority class, numbers differ only after the decimal point
p11332
aVUsing the Inst features alone (not shown in Table 10 ) results in a micro-average accuracy of only 58.1% these features are only useful when combined with the feature Lemma
p11333
aVWhether or not performance is improved by adding LingInd/Dist features, with their bias towards one aspectual class, depends on the verb type
p11334
aVOur experiments show that in any setting where labeled training data is available, improvement over the most frequent class baseline can only be reached by integrating instance-based features, though type-based features (LingInd, Dist) may provide useful priors for some verbs and successfully predict predominant aspectual class for unseen verb types
p11335
asg88
(lp11336
sg90
(lp11337
sg92
(lp11338
VWe have described a new, context-aware approach to automatically predicting aspectual class, including a new set of distributional features.
p11339
aVWe have also introduced two new data sets of clauses labeled for aspectual class.
p11340
aVOur experiments show that in any setting where labeled training data is available, improvement over the most frequent class baseline can only be reached by integrating instance-based features, though type-based features (LingInd, Dist) may provide useful priors for some verbs and successfully predict predominant aspectual class for unseen verb types.
p11341
aVIn order to arrive at a globally well-performing system, we envision a multi-stage approach, treating verbs differently according to whether training data is available and whether or not the verb s aspectual class distribution is highly skewed.
p11342
aVWe thank the anonymous reviewers, Omri Abend, Mike Lewis, Manfred Pinkal, Mark Steedman, Stefan Thater and Bonnie Webber for helpful comments, and our annotators A.
p11343
aVKirkland and R.
p11344
aVKühn.
p11345
aVThis research was supported in part by the MMCI Cluster of Excellence, and the first author is supported by an IBM PhD Fellowship.
p11346
ag106
asg107
S'P14-2085'
p11347
sg109
(lp11348
VThis paper describes a new approach to predicting the aspectual class of verbs in context, i.e.,, whether a verb is used in a stative or dynamic sense.
p11349
aVWe identify two challenging cases of this problem when the verb is unseen in training data, and when the verb is ambiguous for aspectual class.
p11350
aVA semi-supervised approach using linguistically-motivated features and a novel set of distributional features based on representative verb types allows us to predict classes accurately, even for unseen verbs.
p11351
aVMany frequent verbs can be either stative or dynamic in different contexts, which has not been modeled by previous work; we use contextual features to resolve this ambiguity.
p11352
aVIn addition, we introduce two new datasets of clauses marked for aspectual class.
p11353
ag106
asba(icmyPackage
FText
p11354
(dp11355
g3
(lp11356
VKnown as the Lesk algorithm, this simple and intuitive method has since been extensively cited and extended in the word sense disambiguation (WSD) community
p11357
aVAmong the popular explanations is a key limitation of the algorithm, that u'\u005cu201c' Lesk u'\u005cu2019' s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results u'\u005cu201d' ()
p11358
aVTo address this limitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-based WSD
p11359
aVIn the extraordinarily rich literature on WSD, we focus our review on those closest to the topic of Lesk and NBM
p11360
aVTo the best of our knowledge, NBMs have been employed exclusively as classifiers in WSD u'\u005cu2014' that is, in contrast to their use as a similarity measure in this study used NB classifier resembling an information retrieval system a WSD instance is regarded as a document d , and candidate senses are scored in terms of u'\u005cu201c' relevance u'\u005cu201d' to d
p11361
aVOverlap was assessed by string matching, with the number of matching words squared so as to assign higher scores to multi-word overlaps
p11362
aVBreaking away from string matching, measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order co-occurrence information in glosses
p11363
aVMore recently, proposed a tree-matching algorithm that measured gloss-context overlap as the weighted sum of dependency-induced lexical distance constructed a sentential similarity measure () using lexical similarity measures () , and overlap was measured by the cosine of their respective sentential vectors
p11364
aVThese systems compared favourably to existing methods in WSD performance, although by using sense frequency information, they are essentially supervised methods
p11365
aVDistributional methods have been used in many WSD systems in quite different flavours than the current study proposed a Lesk variant where each gloss word is weighted by its idf score in relation to all glosses, and gloss-context association was incremented by these weights rather than binary, overlap counts used distributional thesauri as a knowledge base to increase overlaps, which were, again, assessed by string matching
p11366
aVIn the second expression, Bayes u'\u005cu2019' s rule is applied not only to take advantage of the conditional independence among e i u'\u005cu2019' s, but also to facilitate probability estimation, since p ( { e i } f j ) is easier to estimate in the context of WSD, where sample spaces of u'\u005cud835' u'\u005cudc1e' and u'\u005cud835' u'\u005cudc1f' become asymmetric (Section 3.2
p11367
aV1 1 Think of the notations u'\u005cud835' u'\u005cudc1e' and u'\u005cud835' u'\u005cudc1f' mnemonically as exemplars and features , respectively
p11368
aVWSD is thus formulated as identifying the sense s * in the sense inventory u'\u005cud835' u'\u005cudcae' of w s.t
p11369
aVIn one of their simplest forms, e i u'\u005cu2019' s correspond to co-occurring words in the instance of w , and f j u'\u005cu2019' s consist of the gloss words of sense s
p11370
aVConsequently, p ( u'\u005cud835' u'\u005cudc1f' u'\u005cud835' u'\u005cudc1e' ) is essentially measuring the association between context words of w and definition texts of s , i.e.,, the gloss-context association in the simplified Lesk algorithm
p11371
aVA major difference, however, is that instead of using hard, overlap counts between the two sets of words from the gloss and the context, this probabilistic treatment can implicitly model the distributional similarity among the elements e i and f j (and consequently between the sets u'\u005cud835' u'\u005cudc1e' and u'\u005cud835' u'\u005cudc1f' ) over a wider range of contexts
p11372
aVThe result is a u'\u005cu201c' softer u'\u005cu201d' proxy of association than the binary view of overlaps in existing Lesk variants
p11373
aVThe foregoing discussion offers a second motivation for applying Bayes u'\u005cu2019' s rule on the second expression in Equation ( 1 it is easier to estimate p ( e i f j ) than p ( f j e i ) , since the vocabulary for the lexical knowledge features ( f j ) is usually more limited than that of the contexts ( e i ) and hence estimation of the former suffices on a smaller amount of data than that of the latter
p11374
aVThe input of the proposed NBM is bags of words, and thus it is straightforward to incorporate various forms of lexical knowledge (LK) for word senses by concatenating a tokenized knowledge source to the existing knowledge representation u'\u005cud835' u'\u005cudc1f' , while the similarity measure remains unchanged
p11375
aVWordNet senses are often used in Senseval and SemEval tasks, and hence senses (or synsets, and possibly their corresponding word forms) that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies
p11376
aVAs pointed out by , however, u'\u005cu201c' not all of these relations are equally helpful u'\u005cu201d' Relation pairs involving hyponyms were shown to result in better F-measure when used in gloss overlaps
p11377
aVWe further hypothesize that, beyond sheer numbers, synonyms and hyponyms offer stronger semantic specification that helps distinguish the senses of a given ambiguous word, and thus are more effective knowledge sources for WSD
p11378
aVHyponyms, on the other hand, help specify their corresponding senses with information that is possibly missing from the often overly brief glosses the many technical terms as hyponyms in Table 1 u'\u005cu2014' though rare u'\u005cu2014' are likely to occur in the (possibly domain-specific) contexts that are highly typical of the corresponding senses
p11379
aVWe also observe that some semantically related words appear under rare senses (e.g.,, still as an alcohol-manufacturing plant, and annual as a one-year-life-cycle plant; omitted from Table 1
p11380
aVWhat type of knowledge to include is eventually a decision made by the user based on the application and LK availability
p11381
aVTo avoid underflow, Equation ( 1 ) is estimated as the following log probability
p11382
aVwhere c u'\u005cu2062' ( x ) is the count of word x , c u'\u005cu2062' ( u'\u005cu22c5' ) is the corpus size, c u'\u005cu2062' ( x , y ) is the joint count of x and y , and u'\u005cud835' u'\u005cudc2f' is the dimension of vector u'\u005cud835' u'\u005cudc2f'
p11383
aVSpecifically in WSD, a source corpus is defined as the source of the majority of the WSD instances in a given dataset, and a baseline corpus of a smaller size and less resemblance to the instances is used for all datasets
p11384
aVTraining sections are used as development data and test sections held out for final testing
p11385
aVModel performance is evaluated in terms of WSD accuracy using Equation ( 2 ) as the scoring function
p11386
aVAccuracy is defined as the number of correct responses over the number of instances
p11387
aVBecause it is a rare event for the NBM to produce identical scores, 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports
p11388
aVThe MIT-JWI library () is used for accessing WordNet
p11389
aV5 5 We also compared the two Lesk baselines (with and without usage examples) on the development data but did not observe significant differences as reported by
p11390
aVBasic pre-processing is performed on the contexts and the glosses, including lower-casing, stopword removal, lemmatization on both datasets, and tokenization on the Senseval-2 instances
p11391
aV8 8 We excluded the results of UNED () in Senseval-2 because, by using sense frequency information that is only obtainable from sense-annotated corpora, it is essentially a supervised system
p11392
aVBy using only glosses, the proposed model already shows statistically significant improvement over the basic Lesk algorithm (92.4% and 140.5% relative improvement in Senseval-2 coarse- and fine-grained tracks, respectively
p11393
aVThe comparison is unavailable in SemEval2007 since we have not found existing experiments with this exact configuration
p11394
aVSpecifically, highly similar, fine-grained sense candidates apparently share more hypernyms in the fine-grained case than in the coarse-grained case; adding to the generality of hypernyms (both semantic and distributional), we postulate that their probability in the NBM is uniformly inflated among many sense candidates, and hence they decrease in distinguishability
p11395
aVFor the fine-grained track, it achieves 2nd place after that of , which used a decision list () on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are
p11396
aVTo evaluate model response to probability estimation of different quality (Section 3.4 ), source corpora are chosen as the majority value of the doc-source attribute of instances in each dataset, namely, the British National Corpus for Senseval-2 (94%) and the Wall Street Journal for SemEval-2007 (86%
p11397
aVWe have proposed a general-purpose Naive Bayes model for measuring association between two sets of random events
p11398
aVFor future work, we plan to apply the model in other shared tasks, including open-text WSD, so as to compare with more recent Lesk variants
p11399
asg88
(lp11400
sg90
(lp11401
sg92
(lp11402
VWe have proposed a general-purpose Naive Bayes model for measuring association between two sets of random events.
p11403
aVThe model replaced string matching in the Lesk algorithm for word sense disambiguation with a probabilistic measure of gloss-context overlap.
p11404
aVThe base model on average more than doubled the accuracy of Lesk in Senseval-2 on both fine- and coarse-grained tracks.
p11405
aVWith additional lexical knowledge, the model also outperformed state of the art results with statistical significance on two coarse-grained WSD tasks.
p11406
aVFor future work, we plan to apply the model in other shared tasks, including open-text WSD, so as to compare with more recent Lesk variants.
p11407
aVWe would also like to explore how to incorporate syntactic features and employ alternative statistical methods (e.g.,, parametric models) to improve probability estimation and inference.
p11408
aVOther NLP problems involving compositionality in general might also benefit from the proposed many-to-many similarity measure.
p11409
ag106
asg107
S'P14-2087'
p11410
sg109
(lp11411
VWe replace the overlap mechanism of the Lesk algorithm with a simple, general-purpose Naive Bayes model that measures many-to-many association between two sets of random variables.
p11412
aVEven with simple probability estimates such as maximum likelihood, the model gains significant improvement over the Lesk algorithm on word sense disambiguation tasks.
p11413
aVWith additional lexical knowledge from WordNet, performance is further improved to surpass the state-of-the-art results.
p11414
ag106
asba(icmyPackage
FText
p11415
(dp11416
g3
(lp11417
VUnsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations
p11418
aVWhile a number of different approaches for domain adaptation have been proposed [ 21 , 26 ] , they tend to emphasize bag-of-words features for classification tasks such as sentiment analysis
p11419
aVConsequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing [ 25 ]
p11420
aVAs we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces
p11421
aVBy using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains
p11422
aV2012 ) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising [ 29 ]
p11423
aVWhile the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 10 5 or more features
p11424
aV2003 ) define several feature u'\u005cu201c' templates u'\u005cu201d' the current word, the previous word, the suffix of the current word, and so on
p11425
aVFor each feature template, there are thousands of binary features
p11426
aVBoth structure-aware domain adaptation algorithms perform as well as standard dropout u'\u005cu2014' and better than the well-known structural correspondence learning (SCL) algorithm [ 1 ] u'\u005cu2014' but structured dropout is more than an order-of-magnitude faster
p11427
aVAs a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts
p11428
aVThen we present three versions of marginalized denoising autoencoders (mDA) by incorporating different types of noise, including two new noising processes that are designed for structured features
p11429
aVAssume instances u'\u005cud835' u'\u005cudc31' 1 , u'\u005cu2026' , u'\u005cud835' u'\u005cudc31' n , which are drawn from both the source and target domains
p11430
aVWe will u'\u005cu201c' corrupt u'\u005cu201d' these instances by adding different types of noise, and denote the corrupted version of u'\u005cud835' u'\u005cudc31' i by u'\u005cud835' u'\u005cudc31' ~ i
p11431
aVSingle-layer denoising autoencoders reconstruct the corrupted inputs with a projection matrix u'\u005cud835' u'\u005cudc16' u'\u005cu211d' d u'\u005cu2192' u'\u005cu211d' d , which is estimated by minimizing the squared reconstruction loss
p11432
aVIf we write u'\u005cud835' u'\u005cudc17' = [ u'\u005cud835' u'\u005cudc31' 1 , u'\u005cu2026' , u'\u005cud835' u'\u005cudc31' n ] u'\u005cu2208' u'\u005cu211d' d × n , and we write its corrupted version u'\u005cud835' u'\u005cudc17' ~ , then the loss in ( 1 ) can be written as
p11433
aVIt is also possible to apply stacking, by passing this vector through another autoencoder [ 4 ]
p11434
aVIn pilot experiments, this slowed down estimation and had little effect on accuracy, so we did not include it
p11435
aVWe obtain a projection matrix u'\u005cud835' u'\u005cudc16' s for each subset by reconstructing the pivot features from the features in this subset; we can then use the sum of all reconstructions as the new features, tanh u'\u005cu2061' ( u'\u005cu2211' s = 1 S u'\u005cud835' u'\u005cudc16' s u'\u005cu2062' u'\u005cud835' u'\u005cudc17' s )
p11436
aVIf we define the scatter matrix of the uncorrupted input as u'\u005cud835' u'\u005cudc12' = u'\u005cud835' u'\u005cudc17' u'\u005cud835' u'\u005cudc17' u'\u005cu22a4' , the solutions under dropout noise are
p11437
aVwhere u'\u005cu0391' and u'\u005cu0392' index two features
p11438
aVThe form of these solutions means that computing u'\u005cud835' u'\u005cudc16' requires solving a system of equations equal to the number of features (in the naive implementation), or several smaller systems of equations (in the high-dimensional version
p11439
aVWe can exploit this structure by using an alternative dropout scheme for each token, choose exactly one feature template to keep, and zero out all other features that consider this token (transition feature templates such as u'\u005cu27e8' y t , y t - 1 u'\u005cu27e9' are not considered for dropout
p11440
aVAssuming we have K feature templates, this noise leads to very simple solutions for the marginalized matrices E u'\u005cu2062' [ u'\u005cud835' u'\u005cudc0f' ] and E u'\u005cu2062' [ u'\u005cud835' u'\u005cudc10' ]
p11441
aVFor E u'\u005cu2062' [ u'\u005cud835' u'\u005cudc0f' ] , we obtain a scaled version of the scatter matrix, because in each instance u'\u005cud835' u'\u005cudc31' ~ , there is exactly a 1 / K chance that each individual feature survives dropout
p11442
aVE u'\u005cu2062' [ u'\u005cud835' u'\u005cudc10' ] is diagonal, because for any off-diagonal entry E u'\u005cu2062' [ u'\u005cud835' u'\u005cudc10' ] u'\u005cu0391' , u'\u005cu0392' , at least one of u'\u005cu0391' and u'\u005cu0392' will drop out for every instance
p11443
aVWe can therefore view the projection matrix u'\u005cud835' u'\u005cudc16' as a row-normalized version of the scatter matrix u'\u005cud835' u'\u005cudc12'
p11444
aVSince E u'\u005cu2062' [ u'\u005cud835' u'\u005cudc10' ] is a diagonal matrix, we eliminate the cost of matrix inversion (or of solving a system of linear equations
p11445
aVThis will look very similar to structured dropout the matrix E u'\u005cu2062' [ u'\u005cud835' u'\u005cudc0f' ] is identical, and E u'\u005cu2062' [ u'\u005cud835' u'\u005cudc10' ] has off-diagonal elements which are scaled by ( 1 - p ) 2 , which goes to zero as K is large
p11446
aVHowever, by including these elements, standard dropout is considerably slower, as we show in our experiments
p11447
aVFor a feature u'\u005cu0391' belonging to a template F , with probability p we will draw a noise feature u'\u005cu0392' also belonging to F , according to some distribution q
p11448
aV1 if both features are chosen as noise features, which happens with probability p 2 u'\u005cu2062' q u'\u005cu0391' u'\u005cu2062' q u'\u005cu0392'
p11449
aVu'\u005cud835' u'\u005cudc31' i , u'\u005cu0391' or u'\u005cud835' u'\u005cudc31' i , u'\u005cu0392' if one feature is unchanged and the other one is chosen as the noise feature, which happens with probability p u'\u005cu2062' ( 1 - p ) u'\u005cu2062' q u'\u005cu0392' or p u'\u005cu2062' ( 1 - p ) u'\u005cu2062' q u'\u005cu0391'
p11450
aVWith probability ( 1 - p ) , the original features are preserved, and we add the outer-product u'\u005cud835' u'\u005cudc31' i u'\u005cu2062' u'\u005cud835' u'\u005cudc31' i u'\u005cu22a4' ; with probability p , we add the outer-product u'\u005cud835' u'\u005cudc31' i u'\u005cu2062' q u'\u005cu22a4'
p11451
aVTherefore E u'\u005cu2062' [ u'\u005cud835' u'\u005cudc0f' ] can be computed as the sum of these terms
p11452
aVWe hold out 5% of data as development data to tune parameters
p11453
aVThe two most recent domains (1800-1849 and 1750-1849) are treated as source domains, and the other domains are target domains
p11454
aVThis scenario is motivated by training a tagger on a modern newstext corpus and applying it to historical documents
p11455
aVWe use a conditional random field tagger, choosing CRFsuite because it supports arbitrary real valued features [ 20 ] , with SGD optimization
p11456
aV2006 ) , we consider pivot features that appear more than 50 times in all the domains
p11457
aVThis leads to a total of 1572 pivot features in our experiments
p11458
aVWe also compute the transfer ratio , which is defined as adaptation accuracy baseline accuracy , shown in Figure 1
p11459
aVThe generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data
p11460
aVIn structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features
p11461
aVAutoencoders apply a similar idea, but use the denoised instances as the latent representation [ 28 , 12 , 4 ]
p11462
aVWithin the context of denoising autoencoders, we have focused on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks [ 13 , 29 ]
p11463
aVOn the specific problem of sequence labeling, Xiao and Guo ( 2013 ) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model
p11464
aVMoon and Baldridge ( 2007 ) tackle the challenging problem of tagging Middle English, using techniques for projecting syntactic annotations across languages
p11465
aVWe take another step towards simplicity by showing that structured dropout can make marginalization even easier, obtaining dramatic speedups without sacrificing accuracy
p11466
asg88
(lp11467
sg90
(lp11468
sg92
(lp11469
VDenoising autoencoders provide an intuitive solution for domain adaptation transform the features into a representation that is resistant to the noise that may characterize the domain adaptation process.
p11470
aVThe original implementation of this idea produced this noise directly [ 12 ] ; later work showed that dropout noise could be analytically marginalized [ 4 ].
p11471
aVWe take another step towards simplicity by showing that structured dropout can make marginalization even easier, obtaining dramatic speedups without sacrificing accuracy.
p11472
aVWe thank the reviewers for useful feedback.
p11473
aVThis research was supported by National Science Foundation award 1349837.
p11474
ag106
asg107
S'P14-2088'
p11475
sg109
(lp11476
VUnsupervised domain adaptation often relies on transforming the instance representation.
p11477
aVHowever, most such approaches are designed for bag-of-words models, and ignore the structured features present in many problems in NLP.
p11478
aVWe propose a new technique called marginalized structured dropout , which exploits feature structure to obtain a remarkably simple and efficient feature projection.
p11479
aVApplied to the task of fine-grained part-of-speech tagging on a dataset of historical Portuguese, marginalized structured dropout yields state-of-the-art accuracy while increasing speed by more than an order-of-magnitude over previous work.
p11480
ag106
asba(icmyPackage
FText
p11481
(dp11482
g3
(lp11483
VWe propose a new training objective for learning word embeddings that incorporates prior knowledge
p11484
aVOur model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text
p11485
aVThe latter was also used in [] for training a network for predicting synset relation
p11486
aVWe present a general model for learning word embeddings that incorporates prior knowledge available for a domain
p11487
aVWe begin by reviewing the word2vec objective and then present augmentations of the objective for prior knowledge, including different training strategies
p11488
aVWord2vec [] is an algorithm for learning embeddings using a neural language model
p11489
aVTraining learns these representations for each word w t (the t th word in a corpus of size T ) so as to maximize the log likelihood of each token given its context words within a window sized c
p11490
aVThe latter worked better in our experiments so we focus on it in our presentation cbow defines p ( w t w t - c t + c ) as
p11491
aVSuppose we have a resource that indicates relations between words
p11492
aVIn the case of semantics, we could have a resource that encodes semantic similarity between words
p11493
aVBased on this resource, we learn embeddings that predict one word from another related word
p11494
aVWe define u'\u005cud835' u'\u005cudc11' as a set of relations between two words w and w u'\u005cu2032' u'\u005cud835' u'\u005cudc11' can contain typed relations (e.g.,, w is related to w u'\u005cu2032' through a specific type of semantic relation), and relations can have associated scores indicating their strength
p11495
aVFor our semantic relations e w u'\u005cu2032' and e w are symmetrical, so we use a single embedding
p11496
aVThe cbow and RCM objectives use separate data for learning
p11497
aVWhile RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus
p11498
aVBased on our initial experiments, RCM uses the output embeddings of cbow
p11499
aVFor each objective (cbow or RCM), we sample 15 words as negative samples for each training instance according to their frequencies in raw texts (i.e., training data of cbow
p11500
aVSuppose w has frequency u u'\u005cu2062' ( w ) , then the probability of sampling w is p u'\u005cu2062' ( w ) u'\u005cu221d' u u'\u005cu2062' ( w ) 3 / 4
p11501
aVWe use distributed training, where shared embeddings are updated by each thread based on training data within the thread, i.e.,, asynchronous stochastic gradient ascent
p11502
aVWe found this an effective method for balancing the two objectives
p11503
aVThe resulting trained model is then used to initialize the RCM model
p11504
aVThis enables the RCM model to benefit from the unlabeled data, but refine the embeddings constrained by the given relations
p11505
aVWhile the joint model balances between fitting the text and learning relations, modeling the text at the expense of the relations may negatively impact the final embeddings for tasks that use the embeddings outside of the context of word2vec
p11506
aVTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p11507
aVWe consider two resources for training the RCM term the Paraphrase Database (PPDB) [] and WordNet []
p11508
aVSince we use both resources for evaluation, we divide each into train, dev and test
p11509
aVNext, we removed duplicate pairs if A,B occurred in PPDB, we removed relations of B,A
p11510
aVDivision into these sets is based on an automatically derived accuracy metric
p11511
aVTraining was based on one of the other sets minus relations from S
p11512
aVHowever we did not use the training data because it is too small to affect the results
p11513
aVWe initially created WordNet training data, but found it too small to affect results
p11514
aVTherefore, we include only RCM results trained on PPDB, but show evaluations on both PPDB and WordNet
p11515
aVWe trained 200-dimensional embeddings and used output embeddings for measuring similarity
p11516
aVMeasuring perplexity means computing the exact probability of each word, which requires summation over all words in the vocabulary in the denominator of the softmax
p11517
aVTherefore, we also trained the language models with hierarchical classification [] strategy (HS
p11518
aVWhile word2vec and joint are trained as language models, RCM is not
p11519
aVIn fact, RCM does not even observe all the words that appear in the training set, so it makes little sense to use the RCM embeddings directly for language modeling
p11520
aVTherefore, in order to make fair comparison, for every set of trained embeddings, we fix them as input embedding for word2vec, then learn the remaining input embeddings (words not in the relations) and all the output embeddings using cbow
p11521
aVSince this involves running cbow on NYT data for 2 iterations (one iteration for word2vec-training/pre-training/joint-modeling and the other for tuning the language model), we use Joint-r (random initialization) for a fair comparison
p11522
aVEven when our goal is to strictly model the raw text corpus, we obtain improvements by injecting semantic information into the objective
p11523
aVWe assign a score using the dot product between the output embeddings of each word in the pair, then order all 868 pairs according to this score
p11524
aVUsing the human judgements, we compute the swapped pairs rate the ratio between the number of swapped pairs and the number of all pairs
p11525
aVWhile we see improvements from XL to XXL (5 times as many relations), we get worse results on XXXL, likely because this set contains the lowest quality relations in PPDB
p11526
aVThe baseline word2vec and the joint model have nearly the same averaged running times (2,577s and 2,644s respectively), since they have same number of threads for the CBOW objective and the joint model uses additional threads for the RCM objective
p11527
aVWe demonstrated that the Relation Constrained Model can lead to better semantic embeddings by incorporating resources like PPDB, leading to better language modeling, semantic similarity metrics, and predicting human semantic judgements
p11528
aVOur implementation is based on the word2vec package and we made it available for general use 2 2 https://github.com/Gorov/JointRCM
p11529
aVAdditionally, we see opportunities for jointly learning embeddings across many tasks with many resources, and plan to extend our model accordingly
p11530
asg88
(lp11531
sg90
(lp11532
sg92
(lp11533
VWe have presented a new learning objective for neural language models that incorporates prior knowledge contained in resources to improve learned word embeddings.
p11534
aVWe demonstrated that the Relation Constrained Model can lead to better semantic embeddings by incorporating resources like PPDB, leading to better language modeling, semantic similarity metrics, and predicting human semantic judgements.
p11535
aVOur implementation is based on the word2vec package and we made it available for general use 2 2 https://github.com/Gorov/JointRCM.
p11536
aVWe believe that our techniques have implications beyond those considered in this work.
p11537
aVWe plan to explore the embeddings suitability for other semantics tasks, including the use of resources with both typed and scored relations.
p11538
aVAdditionally, we see opportunities for jointly learning embeddings across many tasks with many resources, and plan to extend our model accordingly.
p11539
aVYu is supported by China Scholarship Council and by NSFC 61173073.
p11540
ag106
asg107
S'P14-2089'
p11541
sg109
(lp11542
VWord embeddings learned on unlabeled data are a popular tool in semantics, but may not capture the desired semantics.
p11543
aVWe propose a new learning objective that incorporates both a neural language model objective [] and prior knowledge from semantic resources to learn improved lexical semantic embeddings.
p11544
aVWe demonstrate that our embeddings improve over those learned solely on raw text in three settings language modeling, measuring semantic similarity, and predicting human judgements.
p11545
ag106
asba(icmyPackage
FText
p11546
(dp11547
g3
(lp11548
VCurrent Statistical Machine Translation (SMT) approaches model the translation problem as a process of generating a derivation of atomic translation units, assuming that every unit is drawn out of the same model
p11549
aVThe simplest of these is the phrase-based approach [ Och et al.1999 , Koehn et al.2003 ] which employs a global model to process any sub-strings of the input sentence
p11550
aVSome of them developed syntax-based models on complete syntactic trees with Treebank annotations [ Liu et al.2006 , Huang et al.2006 , Zhang et al.2008 ] , and others used source-language syntax as soft constraints [ Marton and Resnik2008 , Chiang2010 ]
p11551
aVHowever, these approaches suffer from the same problem as the phrase-based counterpart and use the single global model to handle different translation units, no matter they are from the skeleton of the input tree/sentence or other not-so-important sub-structures
p11552
aVHere we choose a simple and straightforward method a skeleton is obtained by dropping all unimportant words in the original sentence, while preserving the grammaticality
p11553
aVObviously the skeleton used in this work can be viewed as a simplified sentence
p11554
aVThus the problem is in principle the same as sentence simplification/compression
p11555
aVThe motivations of defining the problem in this way are two-fold
p11556
aVFirst, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem
p11557
aVDue to the lack of space, we do not go deep into this problem
p11558
aVThen we define skeleton-based translation as a task of searching for the best target string t ^ given the source string and its skeleton u'\u005cu03a4'
p11559
aVAs is standard in SMT, we further assume that 1) the translation process can be decomposed into a derivation of phrase-pairs (for phrase-based models) or translation rules (for syntax-based models); 2) and a linear function g u'\u005cu2062' ( u'\u005cu22c5' ) is used to assign a model score to each derivation
p11560
aVE.g., one may introduce syntactic features into g s u'\u005cu2062' k u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( d ) due to their good ability in capturing structural information; and employ a standard phrase-based model for g f u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' l u'\u005cu2062' ( d ) in which not all segments of the sentence need to respect syntactic constraints
p11561
aVA derivation d is skeleton-consistent if no phrases in d cross skeleton boundaries (e.g.,, a phrase where two of the source words are in the skeleton and one is outside
p11562
aVThen, we can simply define g s u'\u005cu2062' k u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( d ) and g f u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' l u'\u005cu2062' ( d ) as the model scores of d u'\u005cu03a4' and d
p11563
aVThis model makes the skeleton translation and full translation much simpler because they perform in the same way of string translation in phrase-based MT
p11564
aVFor our skeletal language model, we trained a 5-gram language model on the target-side of the bilingual data by generalizing non-skeleton segments to Xs
p11565
aVWe used the newswire portion of the NIST MT06 evaluation data as our development set, and used the evaluation data of MT04 and MT05 as our test sets
p11566
aVWe chose the default feature set of the NiuTrans.Phrase engine for building the baseline, including phrase translation probabilities, lexical weights, a 5-gram language model, word and phrase bonuses, a ME-based lexicalized reordering model
p11567
aVWe used the NEU Chinese sentence simplification (NEUCSS) corpus as our training data [ Zhang et al.2013 ]
p11568
aVFor comparison, we also manually annotated the MT development and test data with skeleton information according to the annotation standard provided within NEUCSS
p11569
aVHowever, using different skeleton identification results for training and inference (row 3) does not show big improvements due to the data inconsistency problem
p11570
aVFurther, we regarded skeleton-consistent derivations as an indicator feature and introduced it into the baseline system
p11571
aVSeen from row s-feat., this feature does not show promising improvements
p11572
aVThese results indicate that the real improvements are due to the skeleton-based model/features used in this work, rather than the u'\u005cu201d' well-formed u'\u005cu201d' derivations
p11573
aVIn contrast, we define sentence skeleton as the key segments of a sentence and develop a new MT approach based on this information
p11574
aVThere are some previous studies on the use of sentence skeleton or related information in MT [ Mellebeek et al.2006a , Mellebeek et al.2006b , Owczarzak et al.2006 ]
p11575
asg88
(lp11576
sg90
(lp11577
sg92
(lp11578
VWe have presented a simple but effective approach to integrating the sentence skeleton information into a phrase-based system.
p11579
aVThe experimental results show that the proposed approach achieves very promising BLEU improvements and TER reductions on the NIST evaluation data.
p11580
aVIn our future work we plan to investigate methods of integrating both syntactic models (for skeleton translation) and phrasal models (for full translation) in our system.
p11581
aVWe also plan to study sophisticated reordering models for skeleton translation, rather than reusing the baseline reordering model which is learned on the full sentences.
p11582
ag106
asg107
S'P14-2092'
p11583
sg109
(lp11584
VIn this paper we explicitly consider sentence skeleton information for Machine Translation (MT.
p11585
aVThe basic idea is that we translate the key elements of the input sentence using a skeleton translation model, and then cover the remain segments using a full translation model.
p11586
aVWe apply our approach to a state-of-the-art phrase-based system and demonstrate very promising BLEU improvements and TER reductions on the NIST Chinese-English MT evaluation data.
p11587
aVGBKsong.
p11588
aVGBKsong.
p11589
ag106
asba(icmyPackage
FText
p11590
(dp11591
g3
(lp11592
VWe propose a novel approach to cross-lingual model transfer based on feature representation projection
p11593
aVOnce this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language
p11594
aVIf parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer []
p11595
aVThe transfer itself also introduces errors due to translation shifts [] and word alignment errors, which may lead to inaccurate predictions
p11596
aVThese issues are generally handled using heuristics [] and filtering, for example based on alignment coverage []
p11597
aVThe approach proposed here, which we will refer to as feature representation projection ( FRP ), constitutes an alternative to direct model transfer and annotation projection and can be seen as a compromise between the two
p11598
aVCompared to annotation projection, our approach may be expected to be less sensitive to parallel data quality, since we do not have to commit to a particular prediction on a given instance from parallel data
p11599
aVWe also believe that FRP may profit from using other sources of information about the correspondence between source and target feature representations, such as dictionary entries, and thus have an edge over annotation projection in those cases where the amount of parallel data available is limited
p11600
aVFor the sake of simplicity we cast it as a multiclass classification problem, ignoring the interaction between different arguments in a predicate
p11601
aVOur objective is to make the feature representation sufficiently compact that the mapping between source and target feature spaces could be reliably estimated from a limited amount of parallel data, while preserving, insofar as possible, the information relevant for classification
p11602
aVEstimating the mapping directly from raw categorical features ( u'\u005cu03a9' 0 ) is both computationally expensive and likely inaccurate u'\u005cu2013' using one-hot encoding the feature vectors in our experiments would have tens of thousands of components
p11603
aVWe will refer to this representation as u'\u005cu03a9' 1
p11604
aVTo go further, one can, for example, apply dimensionality reduction techniques to obtain a more compact representation of u'\u005cu03a9' 1 by eliminating redundancy or define auxiliary tasks and produce a vector representation useful for those tasks
p11605
aVBoth baselines are using the same set of features as the proposed model, as described earlier
p11606
aVThe shared feature representation for direct transfer is derived from u'\u005cu03a9' 0 by replacing language-specific part-of-speech tags with universal ones [] and adding cross-lingual word clusters [] to word types
p11607
aVThe word types themselves are left as they are in the source language and replaced with their gloss translations in the target one []
p11608
aVIn English-Czech and Czech-English we also use the dependency relation information, since the annotations are partly compatible
p11609
aVThese classifiers are implemented using Pylearn2 [] , based on Theano []
p11610
aVIn the authors consider embeddings of up to 800 dimensions, but we would not expect to benefit as much from larger vectors since we are using a much smaller corpus to train them
p11611
aVWe did not tune the size of the word representation to our task, as this would not be appropriate in a cross-lingual transfer setup, but we observe that the classifier is relatively robust to their dimension when evaluated on source language u'\u005cu2013' in our experiments the performance of the monolingual classifier does not improve significantly if the dimension is increased past 300 and decreases only by a small margin (less than one absolute point) if it is reduced to 100
p11612
aVIt should be noted, however, that the dimension that is optimal in this sense is not necessarily the best choice for FRP , especially if the amount of available parallel data is limited
p11613
aVSince the size of the latter dataset is relatively small u'\u005cu2013' one thousand sentences u'\u005cu2013' we reserve the whole dataset for testing and only evaluate transfer from English to French, but not the other way around
p11614
aVDatasets for other languages are sufficiently large, so we take 30 thousand samples for testing and use the rest as training data
p11615
aVPlease note that we report only a single value for direct transfer, since this approach does not explicitly rely on parallel data
p11616
aVApart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to or
p11617
aVIt is also somewhat similar in spirit to , where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model
p11618
aVSince we are using a relatively small set of features to start with, this does not present much of a problem
p11619
aVFor source language, this is relatively straightforward, as the intermediate representation can be directly tuned for the problem in question using labeled training data
p11620
asg88
(lp11621
sg90
(lp11622
sg92
(lp11623
VIn this paper we propose a new method of cross-lingual model transfer, report initial evaluation results and highlight directions for its further development.
p11624
aVWe observe that the performance of this method is competitive with that of established cross-lingual transfer approaches and its application requires very little manual adjustment no heuristics or filtering and no explicit shared feature representation design.
p11625
aVIt also retains compatibility with any refinement procedures similar to projected transfer [] that may have been designed to work in conjunction with direct model transfer.
p11626
ag106
asg107
S'P14-2095'
p11627
sg109
(lp11628
VWe propose a novel approach to cross-lingual model transfer based on feature representation projection.
p11629
aVFirst, a compact feature representation relevant for the task in question is constructed for either language independently and then the mapping between the two representations is determined using parallel data.
p11630
aVThe target instance can then be mapped into the source-side feature representation using the derived mapping and handled directly by the source-side model.
p11631
aVThis approach displays competitive performance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research.
p11632
aVcompat=1.8.
p11633
ag106
asba(icmyPackage
FText
p11634
(dp11635
g3
(lp11636
VIn this paper, we propose a cross-language article linking method using a mixed-language topic model and hypernym translation features based on an SVM model to link English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China
p11637
aVSince alternative encyclopedias like Baidu Baike are larger (by article count) and growing faster than the Chinese Wikipedia, it is worth-while to investigate creating cross-language links among different online encyclopedias
p11638
aVHowever, when linking between different online encyclopedia platforms this is more difficult as many of these structural features are different or not shared
p11639
aVFor each encyclopedia K , a collection of human-written articles, can be defined as K = { a i } i = 1 n , where a i is an article in K and n is the size of K
p11640
aVSince knowledge bases (KB) may contain millions of articles, comparison between all possible pairs in two knowledge bases is time-consuming and sometimes impractical
p11641
aVTo extract possible candidates, two similarity calculation methods are carried out title matching and title similarity
p11642
aVIn our title matching method, we formulate candidate selection as an English-Chinese cross-language information retrieval (CLIR) problem [ 8 ] , in which every English articleâs title is treated as a query and all the articles in the Chinese encyclopedia are treated as the documents
p11643
aVIn query translation, we translate the title of every English article into Chinese and then use these translated titles as queries to retrieve articles from the Chinese encyclopedia
p11644
aVIn the title similarity method, every Chinese article title is represented as a vector, and each distinct character in all these titles is a dimension of all vectors
p11645
aVThe title of each English article is translated into Chinese and represented as a vector
p11646
aVThen, cosine similarity between this vector and the vector of each Chinese title is measured as title similarity
p11647
aVThe second stage of our approach is to score each viable candidate using a supervised learning method, and then sort all candidates in order of score from high to low as final output
p11648
aVEach article x i in KB K 1 can be represented by a feature vector u'\u005cud835' u'\u005cudc31' i = ( f 1 u'\u005cu2062' ( x i ) , f 2 u'\u005cu2062' ( x i ) , u'\u005cu2026' , f n u'\u005cu2062' ( x i )
p11649
aVThen, individual feature functions F k u'\u005cu2062' ( x i , y j ) are based on the feature properties of both article a i and a j
p11650
aVWe use the results of title matching and title similarity from the candidate selection stage as two features for the candidate ranking stage
p11651
aVThe similarity values generated by title matching and title similarity are used directly as real value features in the SVM model
p11652
aVIf two articles do not describe the same topic, the distribution of terms is often scattered
p11653
aV[ 6 ] Thus, the distribution of terms is good measurement of article similarity
p11654
aVBecause the number of all possible words is too large, we adopt a topic model to gather the words into some latent topics
p11655
aVLDA can be seen as a typical probabilistic approach to latent topic computation
p11656
aVEach topic is represented by a distribution of words, and each word has a probability score used to measure its contribution to the topic
p11657
aVWe can calculate the entropy of the distribution as a value for SVM
p11658
aVThe entropy is defined as follows
p11659
aVThese extracted hypernyms can be treated as article categories
p11660
aVTherefore, articles containing the same hypernym are likely to belong to the same category
p11661
aVIn this study, we only carry out title hypernym extraction on the first sentences of English articles due to the looser syntactic structure of Chinese
p11662
aVThe first sentence of this article is u'\u005cu201c' The Hunger Games is a 2008 young adult novel by American writer Suzanne Collins u'\u005cu201d' Since article titles may be named entities or compound nouns, the dependency parser may mislabel them and thus output an incorrect parse tree
p11663
aVTo avoid this problem, we first replace all instances of an article u'\u005cu2019' s title in the first sentence with pronouns
p11664
aVFor example, the previous sentence is rewritten as u'\u005cu201c' It is a 2008 young adult novel by American writer Suzanne Collins u'\u005cu201d' Then, the dependency parser generates the following parse tree
p11665
aV[ 4 ] If any pattern matches the structure of the dependency parse tree, the hypernym can be extracted
p11666
aVIn this pattern, the rightmost leaf is the hypernym target
p11667
aVThus, we can extract the hypernym u'\u005cu201c' novel u'\u005cu201d' from the previous example
p11668
aVWe regard the appearance of the English title in the first sentence of a Baidu Baike article as a binary feature
p11669
aVIf the English title appears in the first sentence, the value of this feature is 1; otherwise, the value is 0
p11670
aVSince the two encyclopedias u'\u005cu2019' article formats differ, we copy the information in each article (title, content, category, etc.) into a standardized XML structure
p11671
aVNext, we check if there is a Chinese article in Baidu Baike with exactly the same title as the one in Chinese Wikipedia
p11672
aVIf so, the corresponding English Wikipedia article and the Baidu Baike article are paired in the gold standard
p11673
aVBecause our approach uses an SVM model, the data set should be split into training and test sets
p11674
aVThe final evaluation results are calculated as the mean of the average of these 30 evaluation sets
p11675
aVTo measure the quality of cross-language entity linking, we use the following three metrics
p11676
aVIf the candidate selection method can actually select the correct Chinese candidate, the recall will be high
p11677
aVBecause title similarity of the articles is a widely used method, we choose English and Chinese title similarity as the baseline
p11678
aVThe first kind of error is due to large literal differences between the English and Chinese titles
p11679
aVThe false positive u'\u005cu201c' å°¼ç¦ç u'\u005cu201d' is a historical novel about the life of the Emperor Nero
p11680
aVBecause of the large difference in title lengths, the value of the title similarity feature between the English article u'\u005cu201c' Nero u'\u005cu201d' and the corresponding Chinese article is low
p11681
aVSuch length differences may cause the SVM model to rank the correct answer lower when the difference of other features are not so significant because the contents of the Chinese candidates are similar
p11682
aVWe propose a method based on article hypernym and topic model to link English Wikipedia articles to corresponding Chinese Baidu Baike articles
p11683
aVOur method comprises two stages candidate selection and candidate ranking
p11684
aVWe formulate candidate selection as a cross-language information retrieval task based on the title similarity between English and Chinese articles
p11685
asg88
(lp11686
sg90
(lp11687
sg92
(lp11688
VCross-language article linking is the task of creating links between online encyclopedia articles in different languages that describe the same content.
p11689
aVWe propose a method based on article hypernym and topic model to link English Wikipedia articles to corresponding Chinese Baidu Baike articles.
p11690
aVOur method comprises two stages candidate selection and candidate ranking.
p11691
aVWe formulate candidate selection as a cross-language information retrieval task based on the title similarity between English and Chinese articles.
p11692
aVIn candidate ranking, we employ several features of the articles in our SVM model.
p11693
aVTo evaluate our method, we compile a dataset from English Wikipedia and Baidu Baike, containing the 500 most popular Baidu articles.
p11694
aVEvaluation results of our method show an MRR of up to 80.95% and a recall of 87.46%.
p11695
aVThis shows that our method is effective in generating cross-language links between English Wikipedia and Baidu Baike with high accuracy.
p11696
aVOur method does not heavily depend on linguistic characteristics and can be easily extended to generate cross-language article links among different encyclopedias in other languages.
p11697
ag106
asg107
S'P14-2096'
p11698
sg109
(lp11699
VCreating cross-language article links among different online encyclopedias is now an important task in the unification of multilingual knowledge bases.
p11700
aVIn this paper, we propose a cross-language article linking method using a mixed-language topic model and hypernym translation features based on an SVM model to link English Wikipedia and Chinese Baidu Baike, the most widely used Wiki-like encyclopedia in China.
p11701
aVTo evaluate our approach, we compile a data set from the top 500 Baidu Baike articles and their corresponding English Wiki articles.
p11702
aVThe evaluation results show that our approach achieves 80.95% in MRR and 87.46% in recall.
p11703
aVOur method does not heavily depend on linguistic characteristics and can be easily extended to generate cross-language article links among different online encyclopedias in other languages.
p11704
aVUTF8nsung.
p11705
ag106
asba(icmyPackage
FText
p11706
(dp11707
g3
(lp11708
VBecause this is time consuming, tutors often just rewrite the sentences without giving any explanations [ 5 ]
p11709
aVDue to the effort involved in comparing revisions with the original texts, students often fail to learn from these revisions [ 16 ]
p11710
aVComputer aided language learning tools offer a solution for providing more detailed feedback
p11711
aVFurthermore, it is not clear if the heuristics will work as well for tutors trained to mark up revisions under different guidelines
p11712
aVWe propose to improve upon the correction detection component by training a classifier that determines which edits in a revised sentence address the same error in the original sentence
p11713
aVBecause the classifier were trained on revisions where corrections are explicitly marked by English experts, it is also possible to build systems adjusted to different annotation standards
p11714
aVComparing a student-written sentence with its revision, we observe that each correction can be decomposed into a set of more basic edits such as word insertions, word deletions and word substitutions
p11715
aVIn the example shown in Figure 1 , the correction u'\u005cu201c' to change u'\u005cu21d2' changing u'\u005cu201d' is composed of a deletion of to and a substitution from change to changing ; the correction u'\u005cu201c' moment u'\u005cu21d2' minute u'\u005cu201d' is itself a single word substitution
p11716
aVThus, we can build systems to detect corrections which operates in two steps
p11717
aVIn practice, however, this two-step approach may result in mis-detections due to ambiguities
p11718
aVBecause the Levenshtein algorithm only tries to minimize the number of edits, it does not care whether the edits make any linguistic sense
p11719
aVSince mis-detected corrections cannot be analyzed down the pipeline, the correction detection component became the bottle-neck of their overall system
p11720
aVIf the resulting basic edits do not match with those that compose the actual corrections, we attribute the error to the first step
p11721
aVOur analysis confirms that the merging step is the bottleneck in the current correction detection system u'\u005cu2013' it accounts for 75% of the mis-detections
p11722
aVTherefore, to effectively reduce the algorithm u'\u005cu2019' s mis-detection errors, we propose to build a classifier to merge with better accuracies
p11723
aVA bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases
p11724
aVRecent work therefore focuses on identifying the alignment/edits between two sentences [ 13 , 8 ]
p11725
aVFor example, in Figure 11 , when the insertion of one word is followed by the deletion of the same word, the insertion and deletion are likely addressing one single error
p11726
aVThis is because these two edits would combine together as a word-order change
p11727
aVOn the other hand, in Figure 11 , if one edit includes a substitution between words with the same POS u'\u005cu2019' s, then it is likely fixing a word choice error by itself
p11728
aVTo predict whether two basic-edits address the same writing problem more discriminatively, we train a Maximum Entropy binary classifier based on features extracted from relevant contexts for the basic edits
p11729
aVWe then create a training instance for each pair of two consecutive basic edits if two consecutive basic edits need to be merged, we will mark the outcome as True , otherwise it is False
p11730
aVWe simulate the revisions by applying corrections onto the original sentence
p11731
aVThe teachers u'\u005cu2019' annotations are treated as gold standard for the detailed corrections
p11732
aVWe considered four corpora with different ESL populations and annotation standards, including FCE corpus [ 17 ] , NUCLE corpus [ 2 ] , UIUC corpus 2 2 UIUC corpus contains annotations of essays collected from ICLE [ 6 ] and CLEC [ 7 ]
p11733
aVFirst, Table 4 shows that by incorporating correction patterns into the merging algorithm, the errors in correction detection step were reduced
p11734
aVThis led to a significant improvement on the overall system u'\u005cu2019' s F 1 -score on all corpora
p11735
aVThe models built on corpora can generally improve the correction detection accuracy 5 5 We currently do not evaluate the end-to-end system over different corpora
p11736
aVThis is because different corpora employ different error type categorization standards
p11737
aVAlso, as suggested by the experimental result, among the four corpora, FCE corpus is a comparably good resource for training correction detection models with our current feature set
p11738
aVOne reason is that FCE corpus has many more training instances, which benefits model training
p11739
asg88
(lp11740
sg90
(lp11741
sg92
(lp11742
VA revision often contains multiple corrections that address different writing mistakes.
p11743
aVWe explore building computer programs to accurately detect individual corrections in one single revision.
p11744
aVOne major challenge lies in determining whether consecutive basic-edits address the same mistake.
p11745
aVWe propose a classifier specialized in this task.
p11746
aVOur experiments suggest that.
p11747
aV1) the proposed classifier reduces correction mis-detections in previous systems by 1/3, leading to significant overall system performance.
p11748
aV2) our method is generalizable over different data collections.
p11749
ag106
asg107
S'P14-2098'
p11750
sg109
(lp11751
VThis work explores methods of automatically detecting corrections of individual mistakes in sentence revisions for ESL students.
p11752
aVWe have trained a classifier that specializes in determining whether consecutive basic-edits (word insertions, deletions, substitutions) address the same mistake.
p11753
aVExperimental result shows that the proposed system achieves an F 1 -score of 81% on correction detection and 66% for the overall system, out-performing the baseline by a large margin.
p11754
ag106
asba(icmyPackage
FText
p11755
(dp11756
g3
(lp11757
VTo support empirical study of online privacy policies, as well as tools for users with privacy concerns, we consider the problem of aligning sections of a thousand policy documents, based on the issues they address
p11758
aVPrivacy policy documents are verbose, often esoteric legal documents that many people encounter as clients of companies that provide services on the web
p11759
aVMcDonald and Cranor ( 2008 ) showed that, if users were to read the privacy policies of every website they access during the course of a year, they would end up spending a substantial amount of their time doing just that and would often still not be able to answer basic questions about what these policies really say
p11760
aVUnsurprisingly, many people do not read them [ 9 ]
p11761
aVSuch policies therefore offer an excellent opportunity for NLP tools that summarize or extract key information that (i) helps users understand the implications of agreeing to these policies and (ii) helps legal analysts understand the contents of these policies and make recommendations on how they can be improved or made more clear
p11762
aV3 3 The u'\u005cu201c' Adult u'\u005cu201d' category was excluded; the u'\u005cu201c' World u'\u005cu201d' category was excluded since it contains mainly popular websites in different languages, and we opted to focus on policies in English in this first stage of research, though mulitlingual policy analysis presents interesting challenges for future work
p11763
aVSince every site is different in its placement of the document (e.g.,, buried deep within the website, distributed across several pages, or mingled together with Terms of Service) and format (e.g.,, HTML, PDF, etc.), and since we wish to preserve as much document structure as possible (e.g.,, section labels), full automation was not a viable solution
p11764
aVWe therefore crowdsourced the privacy policy document collection using Amazon Mechanical Turk
p11765
aVWe are inspired by multiple sequence alignment methods in computational biology [ 8 ] and by Barzilay and Lee ( 2004 ) , who described a hidden Markov model (HMM) for document content where each state corresponds to a distinct topic and generates sentences relevant to that topic according to a language model
p11766
aVChoose a start state y 1 from u'\u005cud835' u'\u005cudcae' according to the start-state distribution
p11767
aVSample the t th section of the document by drawing a bag of terms, u'\u005cud835' u'\u005cudc90' t , according to the emission multinomial distribution for state y t
p11768
aV4 4 The emission distributions are not a proper language models (e.g.,, a bigram may be generated by as many as three draws from the emission distribution once for each unigram it contains and once for the bigram
p11769
aVSample the next state, y t + 1 , according to the transition distribution over u'\u005cud835' u'\u005cudcae'
p11770
aVIndeed, our model does not even generate these lengths, since doing so would force the states to u'\u005cu201c' explain u'\u005cu201d' the length of each section, not just its content
p11771
aVDeveloping a gold-standard alignment of privacy policies would either require an interface that allows each annotator to interact with the entire corpus of previously aligned documents while reading the one she is annotating, or the definition (and likely iterative refinement) of a set of categories for manually labeling policy sections
p11772
aVThese were too costly for us to consider, so we instead propose two generic methods to evaluate models for sequence alignment of a collection of documents with generally similar content
p11773
aVThough our model (particularly the restricted variants) treats the problem as one of alignment , our evaluations consider groupings of policy sections
p11774
aVIn the sequel, a grouping on a set X is defined as a collection of subsets X i u'\u005cu2286' X ; these may overlap (i.e.,, there might be x u'\u005cu2208' X i u'\u005cu2229' X j ) and need not be exhaustive (i.e.,, there might be x u'\u005cu2208' X u'\u005cu2216' u'\u005cu22c3' i X i
p11775
aVExperts were allowed to select as many sections for each question as they saw fit, since answering some questions may require synthesizing information from different sections
p11776
aVOur method allows these to overlap (63% of the sections in any A i occurred in more than one A i ), and they are not exhaustive (since many sections of the policies were not deemed to contain answers to any of the nine questions by any expert
p11777
aVTogether, these can be used as a gold standard grouping of policy sections, against which we can compare our system u'\u005cu2019' s output
p11778
aVAn example is shown in Figure 2
p11779
aVAs in § 4.1 , we calculate precision and recall on pairs
p11780
aVThis does not penalize the model for grouping together a u'\u005cu201c' no u'\u005cu201d' pair; we chose it nonetheless because it is interpretable
p11781
aVIn this method, the desired K -way clustering solution is computed by performing a sequence of bisections
p11782
aVIn the human QA evaluation, this is mostly due to recall improvements (i.e.,, more pairs of sections relevant to the same policy question were grouped together
p11783
aVWe considered the task of aligning sections of a collection of roughly similarly-structured legal documents, based on the issues they address
p11784
asg88
(lp11785
sg90
(lp11786
sg92
(lp11787
VWe considered the task of aligning sections of a collection of roughly similarly-structured legal documents, based on the issues they address.
p11788
aVWe introduced an unsupervised model for this task along with two new (and reusable) evaluations.
p11789
aVOur experiments show the approach to be more effective than clustering and topic models.
p11790
aVThe corpus and evaluation data have been made available at http://usableprivacy.org/data.
p11791
aVIn future work, policy section alignments will be used in automated analysis to extract useful information for users and privacy scholars.
p11792
ag106
asg107
S'P14-2099'
p11793
sg109
(lp11794
VTo support empirical study of online privacy policies, as well as tools for users with privacy concerns, we consider the problem of aligning sections of a thousand policy documents, based on the issues they address.
p11795
aVWe apply an unsupervised HMM; in two new (and reusable) evaluations, we find the approach more effective than clustering and topic models.
p11796
aVtopsep=0pt,itemsep=-0.4em,labelwidth=0.5em,leftmargin=0.6em \u005csetenumerate topsep=0pt,itemsep=-0.4em,labelwidth=0.5em,leftmargin=0.75em.
p11797
ag106
asba(icmyPackage
FText
p11798
(dp11799
g3
(lp11800
VThe automatic labelling of such topics derived from social media poses however new challenges since topics may characterise novel events happening in the real world
p11801
aVExisting automatic topic labelling approaches which depend on external knowledge sources become less applicable here since relevant articles/concepts of the extracted topics may not exist in external sources
p11802
aVThis task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence [ 1 , 24 , 27 ] and for characterising the semantic content of a topic through automatic labelling techniques [ 12 , 14 , 22 ]
p11803
aVThe most generic approach to automatic labelling has been to use as primitive labels the top- n words in a topic distribution learned by a topic model such as LDA [ 9 , 2 ]
p11804
aVThis task can be illustrated by considering the following topic derived from social media related to Education
p11805
aVwhere the top 10 words ranked by P ( w i t j ) for this topic are listed
p11806
aVTherefore the task is to find the top- n terms which are more representative of the given topic
p11807
aVHowever previous work has shown that top terms are not enough for interpreting the coherent meaning of a topic [ 22 ]
p11808
aVMore recent approaches have explored the use of external sources (e.g., Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical [ 14 , 21 , 22 ] or graph-based [ 12 ] algorithms applied on these sources
p11809
aV[ 15 ] proposed to label topics by selecting top- n terms to label the overall topic based on different ranking mechanisms including pointwise mutual information and conditional probabilities
p11810
aV[ 14 ] generated label candidates for a topic based on top-ranking topic terms and titles of Wikipedia articles
p11811
aVThey then built a Support Vector Regression (SVR) model for ranking the label candidates
p11812
aVIn nature micropost content is sparse and present ill-formed words
p11813
aVMoreover, the use of Twitter as the u'\u005cu201c' what u'\u005cu2019' s-happening-right now u'\u005cu201d' tool, introduces new event-dependent relations between words which might not have a counter part in existing knowledge sources (e.g., Wikipedia
p11814
aVOur original interest in labelling topics stems from work in topic model based event extraction from social media, in particular from tweets [ 32 , 6 ]
p11815
aVAs opposed to previous approaches, the research presented in this paper addresses the labelling of topics exposing event-related content that might not have a counter part on existing external sources
p11816
aVBased on the observation that a short summary of a collection of documents can serve as a label characterising the collection, we propose to generate topic label candidates based on the summarisation of a topic u'\u005cu2019' s relevant documents
p11817
aVWe propose to approach the topic labelling problem as a multi-document summarisation task
p11818
aVThe following describes our proposed framework to characterise documents relevant to a topic
p11819
aVTopics are interpreted using the top N terms ranked based on the marginal probability p ( w i t j )
p11820
aVWe propose to generate topic label candidates by summarising topic relevant documents
p11821
aVIn particular, the prominent topic of a document d can be found by
p11822
aVTherefore given a topic k , a set of C documents related to this topic can be obtained via equation 1
p11823
aVWe compare different summarisation algorithms based on their ability to provide a good label to a given topic
p11824
aVIn particular we investigate the use of lexical features by comparing three different well-known multi-document summarisation algorithms against the top- n topic terms baseline
p11825
aVIt then weights each sentence in the text (in our case a micropost) by computing the average probability of the words in the sentence
p11826
aVIt is similar to SB , however rather than computing the initial word probabilities based on word frequencies it weights terms based on TFIDF
p11827
aVIn this case the document frequency is computed as the number of times a word appears in a micropost from the collection u'\u005cud835' u'\u005cudc9e'
p11828
aVThis is a relevance based ranking algorithm [ 4 ] , which avoids redundancy in the documents used for generating a summary
p11829
aVThe relevance of a vertex (term) to the graph is computed based on global information recursively drawn from the whole graph
p11830
aVThe final score of a word is therefore not only dependent on the terms immediately connected to it but also on how these terms connect to others
p11831
aVOnce a final score is calculated for each vertex of the graph, TextRank sorts the terms in a reverse order and provided the top T vertices in the ranking
p11832
aVHowever performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task
p11833
aVThis is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts
p11834
aVSince previous research has shown that headlines are good indicators of the main focus of a text, both in structure and content, and that they can act as a human produced abstract [ 26 ] , we used headlines as the GS labels of NW
p11835
aVThe News Corpus ( NW ) was collected during the same period of time as the TW corpus
p11836
aVNW consists of a collection of news articles crawled from traditional news media (BBC, CNN, and New York Times) comprising over 77,000 articles which include supplemental metadata (e.g., headline, author, publishing date
p11837
aVThe same preprocessing steps were performed on NW
p11838
aVTherefore, following a similarity alignment approach we performed the steps oulined in Algorithm 3.2 for generating the GS topic labels of a topic in TW
p11839
aV\u005cENSURE Gold standard topic label for each of the LDA topics for TW
p11840
aV\u005cSTATE Extract the headlines of news articles from u'\u005cud835' u'\u005cudc9e' N u'\u005cu2062' W j and select the top x most frequent words as the gold standard label for topic t i in the TW set \u005cENDFOR
p11841
aVThese steps can be outlined as follows
p11842
aVThe generated label was used as the gold standard label for the corresponding Twitter topic t i in the topic pair
p11843
aVWe compared the results of the summarisation techniques with the top terms ( TT ) of a topic as our baseline
p11844
aVThese TT set corresponds to the top x terms ranked based on the probability of the word given the topic ( p ( w k ) ) from the topic model
p11845
aVFigure 1 presents the ROUGE-1 performance of the summarisation approaches as the length x of the generated topic label increases
p11846
aVWe can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases
p11847
aVIn particular, in both the Education and Law Crime categories, both SB and TFIDF outperforms TT and TR by a large margin
p11848
aVWe can also notice that the GS labels generated from Newswire media presented in Table 2 appear on their own, to be good labels for the TW topics
p11849
aVHowever as we described in the introduction we want to avoid relaying on external sources for the derivation of topic labels
p11850
aVThis is an attractive property for automatically generating topic labels for tweets where their event-related content might not have a counter part on existing external sources
p11851
aVThis experiment shows that existing summarisation techniques can be exploited to provide a better label of a topic, extending in this way a topic u'\u005cu2019' s information by providing a richer context than top-terms
p11852
asg88
(lp11853
sg90
(lp11854
sg92
(lp11855
VIn this paper we proposed a novel alternative to topic labelling which do not rely on external data sources.
p11856
aVTo the best of out knowledge no existing work has been formally studied for automatic labelling through summarisation.
p11857
aVThis experiment shows that existing summarisation techniques can be exploited to provide a better label of a topic, extending in this way a topic s information by providing a richer context than top-terms.
p11858
aVThese results show that there is room to further improve upon existing summarisation techniques to cater for generating candidate labels.
p11859
ag106
asg107
S'P14-2101'
p11860
sg109
(lp11861
VLatent topics derived by topic models such as Latent Dirichlet Allocation (LDA) are the result of hidden thematic structures which provide further insights into the data.
p11862
aVThe automatic labelling of such topics derived from social media poses however new challenges since topics may characterise novel events happening in the real world.
p11863
aVExisting automatic topic labelling approaches which depend on external knowledge sources become less applicable here since relevant articles/concepts of the extracted topics may not exist in external sources.
p11864
aVIn this paper we propose to address the problem of automatic labelling of latent topics learned from Twitter as a summarisation problem.
p11865
aVWe introduce a framework which apply summarisation algorithms to generate topic labels.
p11866
aVThese algorithms are independent of external sources and only rely on the identification of dominant terms in documents related to the latent topic.
p11867
aVWe compare the efficiency of existing state of the art summarisation algorithms.
p11868
aVOur results suggest that summarisation algorithms generate better topic labels which capture event-related context compared to the top- n terms returned by LDA.
p11869
ag106
asba(icmyPackage
FText
p11870
(dp11871
g3
(lp11872
VA common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities
p11873
aVBut interpreting such lists is not always straightforward, particularly since background knowledge may be required [ 5 ]
p11874
aVFor example, a topic which has keywords school, student, university, college, teacher, class, education, learn, high, program , could be labelled as Education and a suitable label for the topic shown above would be Global Financial Crisis
p11875
aV2009 ) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al
p11876
aVA set of candidate labels is generated from Wikipedia article titles by querying using topic terms
p11877
aV2011 ) report two versions of their approach, one unsupervised (which is used as a baseline) and another which is supervised
p11878
aVThey reported that the supervised version achieves better performance than a previously reported approach [ 17 ]
p11879
aVThe most important keywords can be used to generate keyphrases for labelling the topic or weight pre-existing candidate labels
p11880
aVThe 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles retrieved used as candidate labels
p11881
aVFurther candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles
p11882
aVWe consider any remaining words in the search result metadata as nodes, v u'\u005cu2208' V , in a graph G = ( V , E
p11883
aVEach node is connected to its neighbouring words in a context window of ± n words
p11884
aVIn addition, we weight the edges of the graph by computing the relatedness between two nodes, v i and v j , as their normalised Pointwise Mutual Information (NPMI) [ 3 ]
p11885
aVWord co-occurrences are computed using Wikipedia as a a reference corpus
p11886
aVPairs of words are connected with edges only if NPMI u'\u005cu2062' ( w i , w j ) 0.2 avoiding connections between words co-occurring by chance and hence introducing noise
p11887
aVImportant terms are identified by applying the PageRank algorithm [ 19 ] in a similar way to the approach used by Mihalcea and Tarau ( 2004 ) for document keyphrase extraction
p11888
aVHowever, this has a negative effect on performance since it favoured short labels of one or two words which were not sufficiently descriptive of the topics
p11889
aVThe results obtained by applying PageRank over the unweighted graph (2.05, 1.98, 2.04 and 1.88) are consistently better than the supervised and unsupervised methods reported by Lau et al
p11890
aVThis is expected since the weighted graph contains additional information about word relatedness
p11891
aVFor example, the word hardware is more related and, therefore, closer in the graph to the word virtualization than to the word investments
p11892
aVResults from the nDCG metric imply that our methods provide better rankings of the candidate labels in the majority of the cases
p11893
aVAn interesting finding is that, although limited in length, the textual information in the search result u'\u005cu2019' s metadata contain enough salient terms relevant to the topic to provide reliable estimates of term importance
p11894
aVConsequently, it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done
p11895
aVIn addition, performance improvement gained from using the weighted graph is modest, suggesting that the computation of association scores over a large reference corpus could be omitted if resources are limited
p11896
aVIn Figure 6 , we show the scores of Top-1 average rating obtained in the different domains by experimenting with the number of search results used to generate the text graph
p11897
aVOur approach uses results retrieved from a search engine using the topic keywords as a query
p11898
aVA graph is generated from the words contained in the search results metadata and candidate labels ranked using the PageRank algorithm
p11899
aVWe would like to thank Jey Han Lau for providing us with the labels selected by Lau et al
p11900
asg88
(lp11901
sg90
(lp11902
sg92
(lp11903
VWe described an unsupervised graph-based method to associate textual labels with automatically generated topics.
p11904
aVOur approach uses results retrieved from a search engine using the topic keywords as a query.
p11905
aVA graph is generated from the words contained in the search results metadata and candidate labels ranked using the PageRank algorithm.
p11906
aVEvaluation on a standard data set shows that our method consistently outperforms the supervised state-of-the-art method for the task.
p11907
ag106
asg107
S'P14-2103'
p11908
sg109
(lp11909
VThis paper introduces an unsupervised graph-based method that selects textual labels for automatically generated topics.
p11910
aVOur approach uses the topic keywords to query a search engine and generate a graph from the words contained in the results.
p11911
aVPageRank is then used to weigh the words in the graph and score the candidate labels.
p11912
aVThe state-of-the-art method for this task is supervised [ 14 ].
p11913
aVEvaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods.
p11914
ag106
asba(icmyPackage
FText
p11915
(dp11916
g3
(lp11917
VWe then develop a supervised SRL model by leveraging morphological features of Korean that tend to correspond with semantic roles
p11918
aVOur model also employs a variety of latent morpheme representations induced from a larger body of unannotated Korean text
p11919
aVThese elements lead to state-of-the-art performance of 81.07% labeled F1, representing the best SRL performance reported to date for an agglutinative language
p11920
aVEver since Gildea and Jurafsky [ 9 ] , SRL has become an important technology used in applications requiring semantic interpretation, ranging from information extraction [ 8 ] and question answering [ 14 ] , to practical problems including textual entailment [ 4 ] and pictorial communication systems [ 10 ]
p11921
aVSRL systems in many languages have been developed as the necessary linguistic resources become available [ 18 , 20 , 5 , 12 ]
p11922
aVSeven languages were the subject of the CoNLL-2009 shared task in syntactic and semantic parsing [ 11 ]
p11923
aVAgglutinative languages such as Japanese, Korean, and Turkish are computationally difficult due to word-form sparsity, variable word order, and the challenge of using rich morphological features
p11924
aVWe view our work as building on the efforts of the Penn Korean PropBank (PKPB
p11925
aV2 2 http://catalog.ldc.upenn.edu/LDC2006T03 Our corpus is roughly similar in size to the PKPB, and taken together, the two Korean corpora now total about half the size of the Penn English PropBank
p11926
aVOur experiments will show that these finer-grained tags are crucial for achieving high SRL accuracy
p11927
aVAlthough these are based on the general English PropBank guidelines [ 3 ] , they also differ in that we used only 4 numbered arguments from ARG0 to ARG3 instead of 5 numbered arguments
p11928
aVWe thus consider 17 semantic roles in total
p11929
aVWe have annotated semantic roles by following the PropBank annotation guideline [ 3 ] and by using frame files of the Penn Korean PropBank built by Palmer et al
p11930
aVThe PropBank and our corpus are not exactly compatible, because the former is built on constituency-based parse trees, whereas our corpus uses dependency parses
p11931
aVUsing coarser suffix tags can seriously degrade SRL performance, as we show in Section 6 , where we compare the performance of our model on both the new corpus and the older PKPB
p11932
aVKorean SRL research has been limited to domestically published Korean research on small corpora
p11933
aVTherefore, the most direct precedent to the present work is a section in Björkelund et al
p11934
aVThis result showcases the computational difficulty of dealing with morphologically rich agglutinative languages
p11935
aVAs we discuss in Section 5 , we utilize these same features, but also add a set of Korean-specific features to capture aspects of Korean morphology
p11936
aVBesides these morphological features, we also employ latent continuous and discrete morpheme representations induced from a larger body of unannotated Korean text
p11937
aVAs our experiments below show, these features improve performance by dealing with sparsity issues
p11938
aVUnlike the English models, we use individual morphemes as our unit of analysis
p11939
aVFor the semantic role task, the input is a sentence consisting of a sequence of words x = x 1 , u'\u005cu2026' , x n and the output is a sequence of corresponding semantic tags y = y 1 , u'\u005cu2026' , y n
p11940
aVThese features are categorized as either general features, Korean-specific features, or latent morpheme representation features
p11941
aVKorean-specific features are built upon the morphological analysis of the suffix agglutination of the current word x i
p11942
aVJosa is used to define nominal cases and modify other phrases, while Eomi is an ending of a verb or an adjective to define a tense, show an attitude, and connect or terminate a sentence
p11943
aVThus, the Eomi and Josa categorization plays an important role in signaling semantic roles
p11944
aVIf POS_Lv1 is noun , either a proper noun, common noun, or other kinds of nouns is the POS_Lv2
p11945
aVIt is set to 1 if any Josa exists, otherwise 0
p11946
aVAlthough both the PKPB and our corpus had improvements, the improvements were the most notable on our corpus
p11947
aVThis is because PKPB POS tags might be too coarse
p11948
asg88
(lp11949
sg90
(lp11950
sg92
(lp11951
VFor Korean SRL, we semantically annotated a corpus containing detailed morphological annotation.
p11952
aVWe then developed a supervised model which leverages Korean-specific features and a variety of latent morpheme representations to help deal with a sparsity problem.
p11953
aVOur best model achieved 81.07% in F1-score.
p11954
aVIn the future, we will continue to build our corpus and look for the way to use unsupervised learning for SRL to apply to another language which does not have available corpus.
p11955
ag106
asg107
S'P14-2104'
p11956
sg109
(lp11957
VIn this paper we introduce a semantic role labeler for Korean, an agglutinative language with rich morphology.
p11958
aVFirst, we create a novel training source by semantically annotating a Korean corpus containing fine-grained morphological and syntactic information.
p11959
aVWe then develop a supervised SRL model by leveraging morphological features of Korean that tend to correspond with semantic roles.
p11960
aVOur model also employs a variety of latent morpheme representations induced from a larger body of unannotated Korean text.
p11961
aVThese elements lead to state-of-the-art performance of 81.07% labeled F1, representing the best SRL performance reported to date for an agglutinative language.
p11962
ag106
asba(icmyPackage
FText
p11963
(dp11964
g3
(lp11965
VWe develop a semantic parsing framework based on semantic similarity for open domain question answering (QA
p11966
aVWe assumed such questions are answerable by issuing a single-relation query that consists of the relation and an argument entity, against a knowledge base (KB
p11967
aV2013 ) , we train two semantic similarity models one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation
p11968
aVThe answer to the question can thus be derived by finding the relation u'\u005cu2013' entity triple r u'\u005cu2062' ( e 1 , e 2 ) in the KB and returning the entity not mentioned in the question
p11969
aVBy using a general semantic similarity model to match patterns and relations, as well as mentions and entities, our system outperforms the existing rule learning system, Paralex [ 7 ] , with higher precision at all the recall points when answering the questions in the same test set
p11970
aVAn early example of this research is the semantic parser for answering geography-related questions, learned using inductive logic programming [ 18 ]
p11971
aVBy applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of pattern-matching rules can be learned for this restricted form of semantic parsing
p11972
aVWe adapt the work of [ 8 , 15 ] for measuring the semantic distance between a question and relational triples in the KB as the core component of our semantic parsing approach
p11973
aVIn this paper, we focus on using a knowledge base to answer single-relation questions
p11974
aVA single-relation question is defined as a question composed of an entity mention and a binary relation description, where the answer to this question would be an entity that has the relation with the given entity
p11975
aVAn example of a single-relation question is u'\u005cu201c' When were DVD players invented u'\u005cu201d' The entity is dvd-player and the relation is be-invent-in
p11976
aVThe answer can thus be described as the following lambda expression
p11977
aVIf the mapping of the relation and entity in the question can be correctly resolved, then the answer can be derived by a simple table lookup, assuming that the fact exists in the KB
p11978
aVHowever, due to the large number of paraphrases of the same question, identifying the mapping accurately remains a difficult problem
p11979
aVThe probability of the rule in ( 1 ) is 1 since we assume the input is a single-relation question
p11980
aVTo determine the probabilities of such mappings, we propose using a semantic similarity model based on convolutional neural networks, which is the technical focus in this paper
p11981
aVThe CNNSM first uses a convolutional layer to project each word within a context window to a local contextual feature vector, so that semantically similar word- n -grams are projected to vectors that are close to each other in the contextual feature space
p11982
aVFurther, since the overall meaning of a sentence is often determined by a few key words in the sentence, CNNSM uses a max pooling layer to extract the most salient local features to form a fixed-length global feature vector
p11983
aVWord hashing not only makes the learning more scalable by controlling the size of the vocabulary, but also can effectively handle the OOV issues, sometimes due to spelling mistakes
p11984
aVGiven the letter-trigram based word representation, we represent a word- n -gram by concatenating the letter-trigram vectors of each word, e.g.,, for the t -th word- n -gram at the word- n -gram layer, we have
p11985
aVConsider the t -th word- n -gram, the convolution matrix projects its letter-trigram representation vector l t to a contextual feature vector h t
p11986
aVAs shown in Figure 2 , h t is computed by
p11987
aVSince many words do not have significant influence on the semantics of the sentence, we want to retain in the global feature vector only the salient features from a few key words
p11988
aVFor this purpose, we use a max operation, also known as max pooling, to force the network to retain only the most useful local features produced by the convolutional layers
p11989
aVReferring to the max-pooling layer of Figure 2 , we have
p11990
aVOne more non-linear transformation layer is further applied on top of the global feature vector v to extract the high-level semantic representation, denoted by y
p11991
aVAs shown in Figure 2 , we have y = t u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' h u'\u005cu2062' â u'\u005cu2062'  u'\u005cu2062' ¡ u'\u005cu2062' ( W s u'\u005cu22c5' v ) , where v is the global feature vector after max pooling, W s is the semantic projection matrix, and y is the vector representation of the input query (or document) in latent semantic space
p11992
aVGiven a pattern and a relation, we compute their relevance score by measuring the cosine similarity between their semantic vectors
p11993
aVThe semantic relevance score between a pattern Q and a relation R is defined as the cosine score of their semantic vectors y Q and y R
p11994
aVWe train two CNN semantic models from sets of pattern u'\u005cu2013' relation and mention u'\u005cu2013' entity pairs, respectively
p11995
aVFollowing [ 8 ] , for every pattern, the corresponding relation is treated as a positive example and 100 randomly selected other relations are used as negative examples
p11996
aVThe posterior probability of the positive relation given the pattern is computed based on the cosine scores using softmax
p11997
aVModel training is done by maximizing the log-posteriori using stochastic gradient descent
p11998
aVTo train our two CNN semantic models, we derived two parallel corpora based on the Paralex training data
p11999
aVFor relation patterns, we first scanned the original training corpus to see if there was an exact surface form match of the entity (e.g.,, dvd-player would map to u'\u005cu201c' DVD player u'\u005cu201d' in the question
p12000
aVIf an exact match was found, then the pattern would be derived by replacing the mention in the question with the special symbol
p12001
aVThe corresponding relation of this pattern was thus the relation used in the original database query, along with the variable argument position (i.e.,, 1 or 2, indicating whether the answer entity was the first or second argument of the relation
p12002
aVData were tokenized by replacing hyphens with blank spaces
p12003
aVBecause our systems might find triples that were not returned by the Paralex systems, we labeled these new question u'\u005cu2013' triple pairs ourselves
p12004
aV4 , was used as the final score of the triple for ranking the answers
p12005
aVBy limiting the systems to output only answer triples with scores higher than a predefined threshold, we could control the trade-off between recall and precision and thus plot the precision u'\u005cu2013' recall curve
p12006
aVSince the trade-off between precision and recall can be adjusted by varying the threshold, it is more informative to compare systems on the precision u'\u005cu2013' recall curves, which are shown in Figure 3
p12007
aVAs we can observe from the figure, the precision of our CNNSM p u'\u005cu2062' m system is consistently higher than Paralex across all recall regions
p12008
aVThis is understandable since the system does not match mentions with entities of different surface forms (e.g.,, u'\u005cu201c' Robert Hooke u'\u005cu201d' to u'\u005cu201c' Hooke u'\u005cu201d'
p12009
aVTuning the thresholds using a validation set would be needed if there is a metric (e.g.,, F 1 ) that specifically needs to be optimized
p12010
aVFor instance, due to the variety of entity mentions in the real world, the parallel corpus derived from the WikiAnswers data and ReVerb KB may not contain enough data to train a robust entity linking model
p12011
asg88
(lp12012
sg90
(lp12013
sg92
(lp12014
VIn this work, we propose a semantic parsing framework for single-relation questions.
p12015
aVCompared to the existing work, our key insight is to match relation patterns and entity mentions using a semantic similarity function rather than lexical rules.
p12016
aVOur similarity model is trained using convolutional neural networks with letter-trigrams vectors.
p12017
aVThis design helps the model go beyond bag-of-words representations and handles the OOV issue.
p12018
aVOur method achieves higher precision on the QA task than the previous work, Paralex , consistently at different recall points.
p12019
aVDespite the strong empirical performance, our system has room for improvement.
p12020
aVFor instance, due to the variety of entity mentions in the real world, the parallel corpus derived from the WikiAnswers data and ReVerb KB may not contain enough data to train a robust entity linking model.
p12021
aVReplacing this component with a dedicated entity linking system could improve the performance and also reduce the number of pattern/mention candidates when processing each question.
p12022
aVIn the future, we would like to extend our method to other more structured KBs, such as Freebase, and to explore approaches to extend our system to handle multi-relation questions.
p12023
ag106
asg107
S'P14-2105'
p12024
sg109
(lp12025
VWe develop a semantic parsing framework based on semantic similarity for open domain question answering (QA.
p12026
aVWe focus on single-relation questions and decompose each question into an entity mention and a relation pattern.
p12027
aVUsing convolutional neural network models, we measure the similarity of entity mentions with entities in the knowledge base (KB) and the similarity of relation patterns and relations in the KB.
p12028
aVWe score relational triples in the KB using these measures and select the top scoring relational triple to answer the question.
p12029
aVWhen evaluated on an open-domain QA task, our method achieves higher precision across different recall points compared to the previous approach, and can improve F 1 by 7 points.
p12030
ag106
asba(icmyPackage
FText
p12031
(dp12032
g3
(lp12033
VBroadly speaking, we can classify the methods to incorporate semantic information into parsers in two systems using static lexical semantic repositories, such as WordNet or similar ontologies [] , and systems using dynamic semantic clusters automatically acquired from corpora []
p12034
aVFor the sake of comparison, we will also perform the experiments using syntactic/semantic clusters automatically acquired from corpora
p12035
aVBroadly speaking, we can classify the attempts to add external knowledge to a parser in two sets using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora
p12036
aVRecently, tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters
p12037
aVThe learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers
p12038
aVWe will experiment with the semantic representations used in and , based on WordNet 2.1
p12039
aVThere are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories
p12040
aVWe experiment with both full SSs and SFs as instances of fine-grained and coarse-grained semantic representation, respectively
p12041
aVAs an example, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of words including cutter
p12042
aVFor each semantic representation, we need to determine the semantics of each occurrence of a target word used i) gold-standard annotations from SemCor, a subset of the PTB, to give an upper bound performance of the semantic representation, ii) first sense, where all instances of a word were tagged with their most frequent sense, and iii) automatic sense ranking, predicting the most frequent sense for each word []
p12043
aVAs we will make use of the full PTB, we only have access to the first sense information
p12044
aVUsing prefixes of various lengths, it can produce clusterings of different granularities
p12045
aVIt can be seen as a representation of syntactic-semantic information acquired from corpora
p12046
aVWe firstly tested the addition of each individual semantic feature to each parser, evaluating its contribution to the parser u'\u005cu2019' s performance
p12047
aVFor the combinations, instead of feature-engineering each parser with the wide array of different possibilities for features, as in , we adopted the simpler approach of combining the outputs of the individual parsers by voting []
p12048
aVWe will use Labeled Attachment Score (LAS) as our main evaluation criteria
p12049
aVAs in previous work, we exclude punctuation marks
p12050
aVLooking at table 2 , we can say that the differences in baseline parser performance are accentuated when using the LTH treebank conversion, as ZPar clearly outperforms the other two parsers by more than 4 absolute points
p12051
aVSubsection 4.1 presented the results of the base algorithms and their extensions based on semantic features report improvements over the best single parser when combining three transition-based models and one graph-based model
p12052
aVWe used MaltBlender 5 5 http://w3.msi.vxu.se/users/jni/blend/ , a tool for merging the output of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm
p12053
aVIt is known [] that adding more parsers to an ensemble usually improves accuracy, as long as they add to the diversity (and almost regardless of their accuracy level
p12054
aVSo, for the comparison to be fair, we will compare ensembles of 3 parsers, taken from sets of 6 parsers (3 baselines + 3 SF, SS, and cluster extensions, respectively
p12055
aVCombining the 3 baselines does not give an improvement over the best baseline, as ZPar clearly outperforms the other parsers
p12056
aVOne of the obstacles of automatic parsers is the presence of incorrect POS tags due to automatic tagging
p12057
aVWe will examine the influence of each type of semantic information on sentences that contain or not POS errors, and this will clarify whether the increments obtained when using semantic information are useful for correcting the negative influence of POS errors or they are orthogonal and constitute a source of new information independent of POS tags
p12058
aVThe following three rows present the enhanced (combined) parsers that make use of semantic information
p12059
aVAs the combination of the three baseline parsers did not give any improvement over the best single parser (ZPar), we can hypothesize that the gain coming from the parser combinations comes mostly from the addition of semantic information
p12060
aVOn the other hand, the increments are more evenly distributed for SS and clusters, and this can be due to the fact that the semantic information is orthogonal to the POS, giving similar improvements for sentences that contain or not POS errors
p12061
aVThis aspect deserves further investigation, as the improvements seem to be related to both the type of semantic information and the parsing algorithm.We did an initial exploration but it did not give any clear indication of the types of improvements that could be expected using each parser and semantic data
p12062
aVCompared to [] , which used MaltParser on the LTH conversion and gold POS tags, our results can be seen as a negative outcome, as the improvements are very small and non-significant in most of the cases
p12063
aVFor parser combination, WordNet semantic file information does give a small significant increment in the more fine-grained LTH representation
p12064
asg88
(lp12065
sg90
(lp12066
sg92
(lp12067
VThis work has tried to shed light on the contribution of semantic information to dependency parsing.
p12068
aVThe experiments were thorough, testing two treebank conversions and three parsing paradigms on automatically predicted POS tags.
p12069
aVCompared to [] , which used MaltParser on the LTH conversion and gold POS tags, our results can be seen as a negative outcome, as the improvements are very small and non-significant in most of the cases.
p12070
aVFor parser combination, WordNet semantic file information does give a small significant increment in the more fine-grained LTH representation.
p12071
aVIn addition we show that the improvement of automatic clusters is also weak.
p12072
aVFor the future, we think tdifferent parsers, eitherhat a more elaborate scheme is needed for word classes, requiring to explore different levels of generalization in the WordNet (or alternative) hierarchies.
p12073
ag106
asg107
S'P14-2106'
p12074
sg109
(lp12075
VThis paper presents experiments with WordNet semantic classes to improve dependency parsing.
p12076
aVWe study the effect of semantic classes in three dependency parsers, using two types of constituency-to-dependency conversions of the English Penn Treebank.
p12077
aVOverall, we can say that the improvements are small and not significant using automatic POS tags, contrary to previously published results using gold POS tags [].
p12078
aVIn addition, we explore parser combinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion.
p12079
ag106
asba(icmyPackage
FText
p12080
(dp12081
g3
(lp12082
VSome first steps at analysis of the parsing results indicate aspects of the annotation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material
p12083
aVThe PPCMBE is a million-word treebank created for researching changes in English syntax
p12084
aVDue to the historical nature of the PPCMBE, it shares some of the characteristics of treebanks based on modern unedited text [] , such as spelling variation
p12085
aVThe more complex tag set is mainly due to the desire to tag orthographic variants consistently throughout the series of historical corpora
p12086
aVFor example u'\u005cu201c' gentlemen u'\u005cu201d' and its orthographic variant u'\u005cu201c' gen u'\u005cu2019' l u'\u005cu2019' men u'\u005cu201d' are tagged with the complex tag ADJ+NS (adjective and plural noun) on the grounds that in earlier time periods, the lexical item is spelled and tagged as two orthographic words ( u'\u005cu201c' gentle u'\u005cu201d' /ADJ and u'\u005cu201c' men u'\u005cu201d' /NS
p12087
aVWhile only 81 of the 248 tags are u'\u005cu201c' simple u'\u005cu201d' (i.e.,, not associated with lexical merging or splitting), they cover the vast majority of the words in the corpus, as summarized in Table 1
p12088
aVOf these 81 tags, some are more specialized than in the PTB, accounting for the increased number of tags compared to the PTB
p12089
aVHowever, the PPCMBE annotates both types of dependents as sisters of the noun, while the PTB adjoins both types
p12090
aVFor instance in ( 3 a) in Figure 3 , the modifier PP is a sister to the noun in the PPCMBE, while in ( 3 b), the complement PP is adjoined in the PTB
p12091
aVClausal complements and modifiers are also both treated as sisters to the noun in the PPCMBE
p12092
aVIn this case, though, the complement/modifier distinction is encoded by a function tag
p12093
aVFor example, in ( 3 a) and ( 3 b), the status of the CPs as modifier and complement is indicated by their function tags
p12094
aVThe major difference in the clausal structure as compared to the PTB is the absence of a VP level 6 6 This is due to the changing headedness of VP in the overall series of English historical corpora yielding flatter trees than in the PTB
p12095
aVWe refer to the pre-release version of the corpus described in Section 2 as the u'\u005cu201c' Release u'\u005cu201d' version, and experiment with three other corpus versions
p12096
aVSince we are concerned here with parsing just the PPCMBE, we simplified the tag set
p12097
aVThe complex tags are simplified in a fully deterministic way, based on the trees and the tags
p12098
aVThe P tag is split, so that it is either left as P, if a preposition, or changed to CONJS, if a subordinating conjunction
p12099
aVAs discussed in Section 2.2.2 , noun modifiers are sisters to the noun, instead of being adjoined, as in the PTB
p12100
aVAs a result, there are fewer NP brackets in the PPCMBE than there would be if the PTB-style were followed
p12101
aVHowever, ( 3 b) remains as it is, because the following CP in that case is a complement, as indicated by the THT function tag
p12102
aVIt is worth emphasizing that the brackets added in Sections 3.2 and 3.3 add no information, since they are added automatically
p12103
aVWe split the data into four sections, as shown in Table 2
p12104
aVThe validation section consists of the four files beginning with u'\u005cu201c' a u'\u005cu201d' or u'\u005cu201c' v u'\u005cu201d' (spanning the years 1711-1860), the development section consists of the four files beginning with u'\u005cu201c' l u'\u005cu201d' (1753-1866), the test section consists of the five files beginning with u'\u005cu201c' f u'\u005cu201d' (1749-1900), and the training section consists of the remaining 81 files (1712-1913
p12105
aVHowever, the match is close enough that we will report the parsing results for sentences of length = 40 and all sentences, as with the PTB
p12106
aVThe PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser [] and score using the standard evalb program []
p12107
aVSince the Berkeley parser is capable of doing its own POS tagging, we ran it using the gold tags or supplying its own tags
p12108
aV8 8 We modified the evalb parameter file to exclude punctuation in PPCMBE, just as for PTB
p12109
aVThe results are based on a single run for each corpus/section
p12110
aVWe are not proposing that the current version be replaced by the Reduced + NPs + VPs version, on the grounds that the latter gets the highest score
p12111
aVAs a final note, the PPCMBE consists of unedited data spanning more than 200 years, while the PTB is edited newswire, and so to some extent there would almost certainly be some difference in score
p12112
aVWe hypothesized that this is due to confusion with infinitival clauses, which can have an unary-branching IP over a VP, as in the gold tree ( 6
p12113
aVPreliminary analysis shows that the CONJP structures are also difficult for the parser
p12114
aVSince these are structures that are different than the PTB 9 9 The CONJP nonterminal in the PTB serves a different purpose than in the PPCMBE and is much more limited we were particularly interested in them
p12115
aVThese can appear as intermediate conjuncts in a string of conjuncts, with the structure (CONJP word
p12116
aVThe shared pre-modifier structure described in ( 2 a) is also difficult for the parser
p12117
aVAdjusting for two major differences that are a matter of annotation convention, we showed that the PPCMBE can be parsed at approximately the same level of accuracy as the PTB
p12118
asg88
(lp12119
sg90
(lp12120
sg92
(lp12121
VWe have presented the first results on parsing the PPCMBE and discussed some significant annotation style differences from the PTB.
p12122
aVAdjusting for two major differences that are a matter of annotation convention, we showed that the PPCMBE can be parsed at approximately the same level of accuracy as the PTB.
p12123
aVThe first steps in an investigation of the parser differences show that the parser is generating structures that violate basic well-formedness conditions of the annotation.
p12124
aVFor future work, we will carry out a more serious analysis of the parser output, trying to more properly account for the differences in bracketing structure between the PPCMBE and PTB.
p12125
aVThere is also a great deal of data annotated in the style of the PPCMBE, as indicated in footnotes 1 and 1 , and we are interested in how the parser performs on these, especially comparing the results on the modern English corpora to the older historical ones, which will have greater issues of orthographic and tokenization complications.
p12126
ag106
asg107
S'P14-2108'
p12127
sg109
(lp12128
VThis paper presents the first results on parsing the Penn Parsed Corpus of Modern British English (PPCMBE), a million-word historical treebank with an annotation style similar to that of the Penn Treebank (PTB.
p12129
aVWe describe key features of the PPCMBE annotation style that differ from the PTB, and present some experiments with tree transformations to better compare the results to the PTB.
p12130
aVFirst steps in parser analysis focus on problematic structures created by the parser.
p12131
ag106
asba(icmyPackage
FText
p12132
(dp12133
g3
(lp12134
VThis paper introduces a new technique for phrase-structure parser analysis, categorizing possible treebank structures by integrating regular expressions into derivation trees
p12135
aVPhrase-structure parsing is usually evaluated using evalb [] , which provides a score based on matching brackets
p12136
aVWhile this metric serves a valuable purpose in pushing parser research forward, it has limited utility for understanding what sorts of errors a parser is making
p12137
aVThis is the case even if the score is broken down by brackets (NP, VP, etc.), because the brackets can represent different types of structures
p12138
aVFirst, inspired by the tradition of Tree Adjoining Grammar-based research [] , we use a decomposition of the full trees into u'\u005cu201c' elementary trees u'\u005cu201d' (henceforth u'\u005cu201c' etrees u'\u005cu201d' ), with a derivation tree that records how the etrees relate to each other, as in
p12139
aVIn particular, we use the u'\u005cu201c' spinal u'\u005cu201d' structure approach of [] , where etrees are constrained to be unary-branching
p12140
aVThese are best thought of as an extension of head-finding rules, which not only find a head but simultaneously identify each parent/children relation as one of a limited number of types of structures (right-modification, etc
p12141
aVThe regexes allow us to also provide scores based on spans of different construction types
p12142
aVWe trained the parser on sections 2-21, and so (a) is u'\u005cu201c' test-on-train u'\u005cu201d'
p12143
aVDecomposing the original phrase-structure tree into the smaller components requires some method of determining the u'\u005cu201c' head u'\u005cu201d' of a nonterminal, from among its children nodes, similar to parsing work such as
p12144
aVAs described above, we are also interested in the type of linguistic construction represented by that one-level structure, each of which instantiates one of a few types - recursive coordination, simple head-and-sister, etc
p12145
aVIn contrast to the sort of head rules in [] , these refer as little as possible to specific POS tags
p12146
aVInstead of explicitly listing the POS tags of possible heads, the heads are in most cases determined by their location in the structure
p12147
aV3 3 Some among the 49 are duplicates, used for different nonterminals, as with (a) and (b) in Figure 1
p12148
aVWe derived the regexes via an iterative process of inspection of tree decomposition on dataset (a), together with taking advantage of the treebanking experience from some of the co-authors
p12149
aVIn this case, the NP on the left is identified as the head d) VP-crd is also a regex for a recursive structure, in this case for VP coordination, picking out the leftmost conjunct as the head of the structure
p12150
aVEach structure within a circle is one etree, and the derivation as a whole indicates how these etrees are combined
p12151
aV4 4 We do not have space here to discuss the data structure in complete detail, but multiple regex names at a node, such a VP-aux and VP-t at tree a3 in Figure 3 , indicate multiple VP nonterminals
p12152
aVWe show in Section 2.3 how this derivation tree representation is used to score this attachment error directly, rather than obscuring it as an NP bracket error as evalb would do
p12153
aVWe decompose both the gold and parser output trees into derivation trees with spinal etrees, and score based on the regexes projected by each word
p12154
aVThere is a match for a regex if the corresponding words in gold/parser files project to that regex, a precision error if the parser file does but the gold does not, and a recall error if the gold does but the parser file does not
p12155
aVThe word u'\u005cu201c' make u'\u005cu201d' has a match for the regexes VP-t, VP-aux, and S-vp, and so on
p12156
aVSumming such scores over the corresponding gold/parser trees gives us F-scores for each regex
p12157
aV1) For each regex match, we score whether it matches based on the span as well
p12158
aVFor example, u'\u005cu201c' make u'\u005cu201d' is a match for VP-t in Figures 3 and 3 , and is also a match for the span as well, since in both derivation trees it includes the words u'\u005cu201c' make u'\u005cu2026' Florida u'\u005cu201d'
p12159
aVHowever, the F-s score is for separate syntactic constructions (including also head identification), although we can also sum it over all the structures, as done later in Figure 6
p12160
aVThe simultaneous F-h and F-s scores let us identify constructions where the parser has the head projection correct, but gets the span wrong
p12161
aV2) Since the derivation tree is really a dependency tree with more complex nodes [] , we can also score each regex for attachment
p12162
aVIn Figure 3 , NP-t in etree #a5 is considered as having the attachment to #a3
p12163
aVFor example, while u'\u005cu201c' to u'\u005cu201d' is a match for PP-t, its attachment is not, since in Figure 3 it is a child of the u'\u005cu201c' trip u'\u005cu201d' etree (#a5) and in Figure 3 it is a child of the u'\u005cu201c' make u'\u005cu201d' etree (#b3
p12164
aVThis work is in the same basic line of research as the inter-annotator agreement analysis work in
p12165
aVWe trained the parser on sections 2-21 of OntoNotes WSJ, and parsed the three datasets with the gold tags, since at present we wish to analyze the parser performance in isolation from Part-of-Speech tagging errors
p12166
aVTo facilitate comparison of our analysis with evalb, we used corpora versions with the same bracket deletion (empty yields and most punctuation) as evalb
p12167
aVWe ran the gold and parsed versions through our regex decomposition and derivation tree creation
p12168
aVTable 2 lists the most frequent categories in the three datasets, with their percentage of the overall number of brackets (%gold), their score based just on the head identification (F-h), their score based on head identification and (left and right) span (F-s), and the attachment (att) and span-right (spanR) scores for those that match based on the head
p12169
aVThe two graphs in Figure 6 show the cumulative results based on F-h and F-s, respectively
p12170
aVThe benefit of the approach described here is that now we can see the contribution to the evalb score of the particular types of constructions, and within those constructions, how well the parser is doing at getting the same head projection, but failing or not on the spans
p12171
aVAs this is work-in-progress, the analysis is not yet complete
p12172
aV1) The high performance on the OntoNotes WSJ material is in large part due to the score on the non-recursive regexes of NP-t, VP-t, S-vp, and the auxiliaries (points 1, 2, 4, 6 in the graphs
p12173
aV2) We wouldn u'\u005cu2019' t expect the test-on-training evalb score to be 100%, since it has to back off from the training data, but the results for the different categories vary widely, with e.g.,, the NP-modr F-h score much lower than other frequent regexes
p12174
aVFor example, the mediocre performance of the parser on SQ-vp barely affects the score with OntoNotes, but has a larger negative effect with Answers, due to its increased frequency in the latter
p12175
aVSince this affects the F-s scores for VP-t, VP-aux, and S-vp, the negative effect is large
p12176
asg88
(lp12177
sg90
(lp12178
sg92
(lp12179
g1538
asg107
S'P14-2109'
p12180
sg109
(lp12181
VThis paper introduces a new technique for phrase-structure parser analysis, categorizing possible treebank structures by integrating regular expressions into derivation trees.
p12182
aVWe analyze the performance of the Berkeley parser on OntoNotes WSJ and the English Web Treebank.
p12183
aVThis provides some insight into the evalb scores, and the problem of domain adaptation with the web data.
p12184
aVWe also analyze a test-on-train dataset, showing a wide variance in how the parser is generalizing from different structures in the training material.
p12185
ag106
asba(icmyPackage
FText
p12186
(dp12187
g3
(lp12188
VWe present Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis
p12189
aVTopic models [] have become standard tools for analyzing document collections, and topic analyses are quite common for social media []
p12190
aVPolylingual topic models enable cross language analysis by grouping documents by topic regardless of language
p12191
aVWe learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics
p12192
aVIn the model presentation we will refer to both as u'\u005cu201c' documents u'\u005cu201d'
p12193
aVTo train a polylingual topic model on social media, we make two modifications to the model of add a token specific language variable, and a process for identifying aligned topics
p12194
aVIn this case, a polylingual topic model, which necessarily infers a topic-specific word distribution for each topic in each language, would learn two unrelated word distributions in two languages for a single topic
p12195
aVTherefore, naively using the produced topics as u'\u005cu201c' aligned u'\u005cu201d' across languages is ill-advised
p12196
aVOur solution is to automatically identify aligned polylingual topics after learning by examining a topic u'\u005cu2019' s distribution across code-switched documents
p12197
aVTo summarize, based on the model of we will learn
p12198
aVThe first two goals are achieved by incorporating new hidden variables in the traditional polylingual topic model
p12199
aVWe use a block Gibbs sampler to jointly sample topic and language variables for each token
p12200
aVAs is customary, we collapse out u'\u005cu03a6' , u'\u005cu0398' and u'\u005cu03a8'
p12201
aVWe optimize the hyperparameters u'\u005cu0391' , u'\u005cu0392' , u'\u005cu0393' and u'\u005cu0394' by interleaving sampling iterations with a Newton-Raphson update to obtain the MLE estimate for the hyperparameters
p12202
aVTaking u'\u005cu0391' as an example, one step of the Newton-Raphson update is
p12203
aVWe begin by measuring how often each topic occurs in code-switched documents
p12204
aVIf a topic never occurs in a code-switched document, then there can be no evidence to support alignment across languages
p12205
aVTopics appearing in at least one code-switched document with probability greater than a threshold p are selected as candidates for true cross-language topics
p12206
aVWe used two datasets a Sina Weibo Chinese-English corpus [] and a Spanish-English Twitter corpus
p12207
aVWe then sampled an additional 2475 code-switched messages, 4221 English and 4211 Chinese messages as test data
p12208
aVWe collected tweets from July 27, 2012 to August 12, 2012, and identified 302,775 tweets about the Olympics based on related hashtags and keywords (e.g., olympics, #london2012, etc.) We identified code-switched tweets using the Chromium Language Detector 2 2 https://code.google.com/p/chromium-compact-language-detector/
p12209
aVThis system provides the top three possible languages for a given document with confidence scores; we identify a tweet as code-switched if two predicted languages each have confidence greater than 33%
p12210
aVWhile our goal is to learn polylingual topics, we cannot compare to previous polylingual models since they require comparable data, which we lack
p12211
aVWe experimented with different numbers of topics ( u'\u005cud835' u'\u005cudcaf'
p12212
aVSince csLDA duplicates topic distributions ( u'\u005cud835' u'\u005cudcaf' × u'\u005cu2112' ) we used twice as many topics for LDA
p12213
aVWe create alignments by classifying each LDA topic by language using the KL-divergence between the topic u'\u005cu2019' s words distribution and a word distribution for the English/foreign language inferred from the monolingual documents
p12214
aVLanguage is assigned to a topic by taking the minimum KL
p12215
aVFor Weibo data, this was not effective since the vocabularies of each language are highly unbalanced
p12216
aVFor Spanish, we removed workers who disagreed with the majority more than 50% of the time (83 deletions), leaving 6.5 annotations for each alignment (85.47% inter-annotator agreement.) For Chinese, since quality of general Chinese turkers is low [] we invited specific workers and obtained 9.3 annotations per alignment (78.72% inter-annotator agreement.) For Olympics, LDA alignments matched the judgements 25% of the time, while csLDA matched 50% of the time
p12217
asg88
(lp12218
sg90
(lp12219
sg92
(lp12220
g1538
asg107
S'P14-2110'
p12221
sg109
(lp12222
VCode-switched documents are common in social media, providing evidence for polylingual topic models to infer aligned topics across languages.
p12223
aVWe present Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis.
p12224
aVWe experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators.
p12225
aVheightadjust=object,valign=t.
p12226
ag106
asba(icmyPackage
FText
p12227
(dp12228
g3
(lp12229
VWe propose a novel text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings
p12230
aVWhen run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model
p12231
aVThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p12232
aVThis is because it is not obvious what kind of data can be used to estimate the language model there is plentiful text from the source domain, but little of it is in normalized target form
p12233
aVThis advantage does not hold for text normalization
p12234
aVWe thus propose an alternative approach where normalization is modeled directly, and which enables easy incorporation of unlabeled data from the source domain
p12235
aVOur string transduction model works by learning the sequence of edits which transform the input string into the output string
p12236
aVGiven a pair of strings such a sequence of edits (known as the shortest edit script) can be found using the Diff algorithm ( 22 ; 23
p12237
aVThe training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings
p12238
aVAs a sequence labeler we use Conditional Random Fields ( 15
p12239
aVSimple Recurrent Networks (SRNs) were introduced by Elman ( 7 ) as models of temporal, or sequential, structure in data, including linguistic data ( 8
p12240
aVMore recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models ( 19 ; 21
p12241
aVAnother version of recurrent neural nets has been used to generate plausible text with a character-level language model ( 24
p12242
aVWe run the trained model on new tweets and record the activation of the hidden layer at each position as the model predicts the next character
p12243
aVThese activation vectors form our text embeddings they are discretized and used as input features to the supervised sequence labeler as described in Section 3.4
p12244
aVWe limit the size of the string alphabet by always working with UTF-8 encoded strings, and using bytes rather than characters as basic units
p12245
aVThe trained SRN language model can be used to generate random text by sampling the next byte from its predictive distribution and extending the string with the result
p12246
aVIt is hard to interpret the results from Han and Baldwin ( 12 ) , as the evaluation is carried out by assuming that the words to be normalized are known in advance
p12247
aVNevertheless, we use it here for training and evaluating our model
p12248
aVThe simplest way to normalize tweets with a string transduction model is to treat whole tweets as input sequences
p12249
aVMany other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized forms
p12250
aVConsequently, publicly available normalization datasets are annotated at word level
p12251
aVWe can emulate this setup by training the sequence labeler on words, instead of whole tweets
p12252
aVThis approach sacrifices some generality, since transformations involving multiple words cannot be learned
p12253
aVTo keep model size within manageable limits we reduced the label set for models all-words and document by replacing labels which occur less than twice in the training data with nil
p12254
aVFor oov-only we were able to use the full label set
p12255
aVAs our sequence labeling model we use the Wapiti implementation of Conditional Random Fields ( 16 ) with the L-BFGS optimizer and elastic net regularization with default settings
p12256
aVFor the n-gram+srn feature set we augment n-gram with features derived from the activations of the hidden units as the SRN is trying to predict the current character
p12257
aVFor each of the K = 10 most active units out of total J = 400 hidden units, we create features ( f u'\u005cu2062' ( 1 ) u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' f u'\u005cu2062' ( K ) ) defined as f u'\u005cu2062' ( k ) = 1 if s j u'\u005cu2062' ( k ) 0.5 and f u'\u005cu2062' ( k ) = 0 otherwise, where s j u'\u005cu2062' ( k ) returns the activation of the k th most active unit
p12258
aVAs our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
p12259
aVSince the English dataset is pre-tokenized and only covers word-to-word transformations, this choice has little importance here and character error rates show a similar pattern to word error rates
p12260
aVSRN features seem to be especially useful for learning long-range, multi-character edits, e.g., fb for facebook
p12261
aVThey use this as the error model in a noisy-channel setup combined with a unigram language model
p12262
aVIn addition to character n-gram features they use phoneme and syllable features, while we rely on the SRN embeddings to provide generalized representations of input strings
p12263
aVIn comparison to our first-order linear-chain CRF, an MT model with reordering is more flexible but for this reason needs more training data
p12264
aVIt also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for the language model trained on a balanced text corpus
p12265
aVOur approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available
p12266
aV9 ) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction ( 11 ) , to our knowledge neural text embeddings have not been previously applied to string transduction
p12267
aVWe want to push performance further by expanding the training data and incorporating existing lexical resources
p12268
asg88
(lp12269
sg90
(lp12270
sg92
(lp12271
VLearning sequences of edit operations from examples while incorporating unlabeled data via neural text embeddings constitutes a compelling approach to tweet normalization.
p12272
aVOur results are especially interesting considering that we trained on only a small annotated data set and did not use any other manually created resources such as dictionaries.
p12273
aVWe want to push performance further by expanding the training data and incorporating existing lexical resources.
p12274
aVIt will also be important to check how our method generalizes to other language and datasets (e.g., 5 ; 1 ).
p12275
aVThe general form of our model can be used in settings where normalization is not limited to word-to-word transformations.
p12276
aVWe are planning to find or create data with such characteristics and evaluate our approach under these conditions.
p12277
ag106
asg107
S'P14-2111'
p12278
sg109
(lp12279
VTweets often contain a large proportion of abbreviations, alternative spellings, novel words and other non-canonical language.
p12280
aVThese features are problematic for standard language analysis tools and it can be desirable to convert them to canonical form.
p12281
aVWe propose a novel text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings.
p12282
aVThe text embeddings are generated using an Simple Recurrent Network.
p12283
aVWe find that enriching the feature set with text embeddings substantially lowers word error rates on an English tweet normalization dataset.
p12284
aVOur model improves on state-of-the-art with little training data and without any lexical resources.
p12285
aVnolistsep.
p12286
aVmydarkbluergb0,0.08,0.45.
p12287
ag106
asba(icmyPackage
FText
p12288
(dp12289
g3
(lp12290
VAs the web has grown in popularity and scope, so has the promise of collaborative information environments for the joint creation and exchange of knowledge [ 11 , 18 ]
p12291
aVWikipedia, a wiki-based online encyclopedia, is arguably the best example its distributed editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles 1 1 http://en.wikipedia.org of surprisingly high quality [ 6 ] in English alone since its debut in 2001
p12292
aVExisting studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g., an encyclopedia article) is highly correlated with the effectiveness of the online collaboration [ 12 , 14 ] ; fruitful collaboration, in turn, inevitably requires dealing with the disputes and conflicts that arise [ 13 ]
p12293
aV2007 ) , Kraut and Resnick ( 2012 ) ), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor
p12294
aVAs a result, we propose a sentiment analysis approach for online dispute detection that identifies the sequence of sentence-level sentiments (i.e., very negative, negative, neutral, positive, very positive) expressed during the discussion and uses them as features in a classifier that predicts the dispute / non-dispute label for the discussion as a whole
p12295
aVUnfortunately, sentence-level sentiment tagging for this domain is challenging in its own right due to the less formal, often ungrammatical, language and the dynamic nature of online conversations u'\u005cu201c' Really, grow up u'\u005cu201d' (segment 3) should presumably be tagged as a negative sentence as should the sarcastic sentences u'\u005cu201c' Sounds good u'\u005cu201d' (in the same turn) and u'\u005cu201c' congrats u'\u005cu201d' and u'\u005cu201c' thank you u'\u005cu201d' (in segment 2
p12296
aVContext information is also not considered
p12297
aVAs the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) [ 16 ] for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models [ 15 ] while providing an efficient mechanism for encoding domain knowledge u'\u005cu2014' in our case, a sentiment lexicon u'\u005cu2014' through isotonic constraints on model parameters
p12298
aVIf an article is observed to have disputes on its talk page , editors can assign dispute tags to the article to flag it for attention
p12299
aVWe use the 2013-03-04 Wikipedia data dump, and extract talk pages for articles that are labeled with dispute tags by checking the revision history
p12300
aVDispute tags can also be added to talk pages themselves
p12301
aVTherefore, in addition to the tags mentioned above, we also consider the u'\u005cu201c' Request for Comment u'\u005cu201d' ( rfc ) tag on talk pages
p12302
aVAccording to Wikipedia 4 4 http://en.wikipedia.org/wiki/Wikipedia:Requests_for_comment , rfc is used to request outside opinions concerning the disputes
p12303
aVGiven that traditional Conditional Random Fields (CRFs) [ 15 ] ignore the ordinal relations among sentiment labels, we choose isotonic CRFs [ 16 ] for sentence-level sentiment analysis as they can enforce monotonicity constraints on the parameters consistent with the ordinal structure and domain knowledge (e.g., word-level sentiment conveyed via a lexicon
p12304
aVOur lexicon is built by combining MPQA [ 22 ] , General Inquirer [ 19 ] , and SentiWordNet [ 3 ] lexicons
p12305
aVWe gather connectives from the Penn Discourse TreeBank [ 17 ] and combine them with any sentiment word that precedes or follows it as new features
p12306
aVSentiment dependency relations are the dependency relations that include a sentiment word
p12307
aVIf a sentence is only selected as positive by one annotator or obtains the label via turn-level annotation, it is positive (P
p12308
aVVery negative (NN) and negative (N) are collected in the same way
p12309
aVAmong all 16,501 sentences in AAWD, 1,930 and 1,102 are labeled as NN and N
p12310
aV532 and 99 of them are PP and P
p12311
aV1) Baseline (Polarity a sentence is predicted as positive if it has more positive words than negative words, or negative if more negative words are observed
p12312
aVOtherwise, it is neutral
p12313
aV1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences [ 8 ] , and (dis)agreement detection [ 24 ] in online debates; (2) a Linear CRF for (dis)agreement identification in broadcast conversations [ 21 ]
p12314
aVWe evaluate the systems using standard F1 on classes of positive, negative, and neutral, where samples predicted as PP and P are positive alignment, and samples tagged as NN and N are negative alignment
p12315
aVArticles on specific topics, such as politics or religions, tend to arouse more disputes
p12316
aVWe thus extract the category information of the corresponding article for each talk page
p12317
aVWe choose number of turns , number of participants , average number of words in each turn as features
p12318
aVIn addition, the frequency of revisions made during the discussion has been shown to be good indicator for controversial articles [ 20 ] , that are presumably prone to have disputes
p12319
aVTherefore, we encode the number of revisions that happened during the discussion as a feature
p12320
aVWe then have features as number/portion of sentiment transitions per type
p12321
aVFeatures described above mostly depict the global sentiment flow in the discussions
p12322
aVWe further construct a local version of them, since sentiment distribution may change as discussion proceeds
p12323
aVFor example, less positive sentiment can be observed as dispute being escalated
p12324
aVWe thus split each discussion into three equal length stages, and create sentiment distribution and transition features for each stage
p12325
aVExperimental results with various combinations of features sets are displayed in Table 5
p12326
aVAs it can be seen, sentiment features obtains the best accuracy among the four types of features
p12327
aVDue to the limitation of existing general-purpose lexicons, some opinionated dialog-specific terms are hard to catch
p12328
aVSecondly, some dispute discussions are harder to detect than the others due to different dialog structures
p12329
aVWe also thank Emily Bender and Mari Ostendorf for providing the AAWD dataset
p12330
asg88
(lp12331
sg90
(lp12332
sg92
(lp12333
VWe present a sentiment analysis-based approach to online dispute detection.
p12334
aVWe create a large-scale dispute corpus from Wikipedia Talk pages to study the problem.
p12335
aVA sentiment prediction model based on isotonic CRFs is proposed to output sentiment labels at the sentence-level.
p12336
aVExperiments on our dispute corpus also demonstrate that classifiers trained with sentiment tagging features outperform others that do not.
p12337
aVAcknowledgments We heartily thank the Cornell NLP Group, the reviewers, and Yiye Ruan for helpful comments.
p12338
aVWe also thank Emily Bender and Mari Ostendorf for providing the AAWD dataset.
p12339
aVThis work was supported in part by NSF grants IIS-0968450 and IIS-1314778, and DARPA DEFT Grant FA8750-13-2-0015.
p12340
aVThe views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF, DARPA or the U.S.
p12341
aVGovernment.
p12342
ag106
asg107
S'P14-2113'
p12343
sg109
(lp12344
VWe investigate the novel task of online dispute detection and propose a sentiment analysis solution to the problem we aim to identify the sequence of sentence-level sentiments expressed during a discussion and to use them as features in a classifier that predicts the DISPUTE/NON-DISPUTE label for the discussion as a whole.
p12345
aVWe evaluate dispute detection approaches on a newly created corpus of Wikipedia Talk page disputes and find that classifiers that rely on our sentiment tagging features outperform those that do not.
p12346
aVThe best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80.
p12347
ag106
asba(icmyPackage
FText
p12348
(dp12349
g3
(lp12350
VWith the proliferation of social media sites, social streams have proven to contain the most up-to-date information on current events
p12351
aVTherefore, it is crucial to extract events from the social streams such as tweets
p12352
aVHowever, it is not straightforward to adapt the existing event extraction systems since texts in social media are fragmented and noisy
p12353
aVPrevious work in event extraction has focused largely on news articles, as the newswire texts have been the best source of information on current events [ 6 ]
p12354
aVThey lack the flexibility of porting to new domains since extraction patterns often need to be re-defined
p12355
aVSocial media messages are often short and evolve rapidly over time
p12356
aVAs such, it is not possible to know the event types a priori and hence violates the use of existing event extraction approaches
p12357
aVApproaches to event extraction from Twitter make use of a graphical model to extract canonical entertainment events from tweets by aggregating information across multiple messages [ 1 ]
p12358
aVIn [ 7 ] , social events involving two persons are extracted from multiple similar tweets using a factor graph by harvesting the redundancy in tweets
p12359
aVThis property allows us resort to statistical models that can group similar events based on the co-occurrence patterns of their event elements
p12360
aVWe can treat an event as a latent variable and model the generation of an event as a joint distribution of its individual event elements
p12361
aVWe thus propose a Latent Event Model (LEM) which can automatically detect events from social media without the use of labeled data
p12362
aVEvents extracted in our proposed framework are represented as a 4-tuple u'\u005cu27e8' y , d , l , k u'\u005cu27e9' , where y stands for a non-location named entity, d for a date, l for a location, and k for an event-related keyword
p12363
aVIt should be noted that for some events, one or more elements in their corresponding tuples might be absent as their related information is not available in tweets
p12364
aVAs illustrated in Figure 1 , our proposed framework consists of three main steps, pre-processing, event extraction based on the LEM model and post-processing
p12365
aVTo resolve the ambiguity of the time expressions, SUTime 1 1 http://nlp.stanford.edu/software/sutime.shtml [ 2 ] is employed, which takes text and a reference date as input and outputs a more accurate date which the time expression refers to
p12366
aVNamed entity recognition (NER) is a crucial step since the results would directly impact the final extracted 4-tuple u'\u005cu27e8' y , d , l , k u'\u005cu27e9'
p12367
aVIt is not easy to accurately identify named entities in the Twitter data since tweets contain a lot of misspellings and abbreviations
p12368
aVHowever, it is often observed that events mentioned in tweets are also reported in news articles in the same period [ 10 ]
p12369
aVTherefore, named entities mentioned in tweets are likely to appear in news articles as well
p12370
aVWe thus perform named entity recognition in the following way
p12371
aVNamed entities from tweets are extracted by looking up the dictionary through fuzzy matching
p12372
aVAfter the pre-processing step, non-location entities y , locations l , dates d and candidate keywords of the tweets are collected as the input to the LEM model for event extraction
p12373
aVWe propose an unsupervised latent variable model, called the Latent Event Model (LEM), to extract events from tweets
p12374
aVIn this model, we assume that each tweet message m u'\u005cu2208' { 1 u'\u005cu2062' M } is assigned to one event instance e , while e is modeled as a joint distribution over the named entities y , the date/time d when the event occurred, the location l where the event occurred and the event-related keywords k
p12375
aVOnce the class assignments for all events are known, we can easily estimate the model parameters { u'\u005cud835' u'\u005cudf45' , u'\u005cud835' u'\u005cudf3d' , u'\u005cud835' u'\u005cudf4b' , u'\u005cud835' u'\u005cudf4d' , u'\u005cud835' u'\u005cudf4e' }
p12376
aVIf P ( element) is less than 1 u'\u005cu039e' u'\u005cu2062' P u'\u005cu2062' ( S ) , where P u'\u005cu2062' ( S ) is the sum of probabilities of the other three elements and u'\u005cu039e' is a threshold value and is set to 5 empirically, the element will be removed from the extracted results
p12377
aVIn this section, we first describe the Twitter corpus used in our experiments and then present how we build a baseline based on the previously proposed TwiCal system [ 14 ] , the state-of-the-art open event extraction system on tweets
p12378
aVWe believe that in reality, events which are mentioned in very few tweets are less likely to be significant
p12379
aVTherefore, the dataset was filtered by removing the events which are mentioned in less than 10 tweets
p12380
aVThe events extracted in the baseline are represented as a 3-tuple u'\u005cu27e8' y , d , k u'\u005cu27e9' 5 5 TwiCal also groups event instances into event types such as u'\u005cu201d' Sport u'\u005cu201d' or u'\u005cu201d' Politics u'\u005cu201d' using LinkLDA which is not considered here where y stands for a non-location named entity, d for a date and k for an event phrase
p12381
aVDue to the difficulties of re-implementing the sequence labeler without knowing the actual features set and the annotated training data, we assume all the event-related phrases are identified correctly and simply use the event trigger words annotated in the FSD corpus as k to form the event 3-tuples
p12382
aVFor the 4-tuple u'\u005cu27e8' y , d , l , k u'\u005cu27e9' , the precision is calculated based on the following criteria
p12383
aVIf the extracted representation does not contain keywords, its precision is calculated by checking the criteria 1
p12384
aVIf the extracted representation contains keywords, its precision is calculated by checking both criteria 1 and 2
p12385
aVIt can be observed from Table 2 that by using NW-NER, the performance of event extraction system is improved significantly by 7.5% and 3% respectively on F-measure when evaluated on 3-tuples (without keywords) or 4-tuples (with keywords
p12386
aVThis shows that our LEM model is less sensitive to the number of events E so long as E is set to a relatively larger value
p12387
aVAfter that, the model can automatically extract events which involving a named entity at certain time, location, and with event-related keywords based on the co-occurrence patterns of the event elements
p12388
asg88
(lp12389
sg90
(lp12390
sg92
(lp12391
VIn this paper we have proposed an unsupervised Bayesian model, called the Latent Event Model (LEM), to extract the structured representation of events from social media data.
p12392
aVInstead of employing labeled corpora for training, the proposed model only requires the identification of named entities, locations and time expressions.
p12393
aVAfter that, the model can automatically extract events which involving a named entity at certain time, location, and with event-related keywords based on the co-occurrence patterns of the event elements.
p12394
aVOur proposed model has been evaluated on the FSD corpus.
p12395
aVExperimental results show our proposed framework outperforms the state-of-the-art baseline by over 7% in F-measure.
p12396
aVIn future work, we plan to investigate inferring geolocations automatically from tweets.
p12397
aVWe also intend to study a better method to infer date more accurately from tweets and explore efficient ranking strategies to rank evens extracted for a better presentation of results.
p12398
ag106
asg107
S'P14-2114'
p12399
sg109
(lp12400
VWith the proliferation of social media sites, social streams have proven to contain the most up-to-date information on current events.
p12401
aVTherefore, it is crucial to extract events from the social streams such as tweets.
p12402
aVHowever, it is not straightforward to adapt the existing event extraction systems since texts in social media are fragmented and noisy.
p12403
aVIn this paper we propose a simple and yet effective Bayesian model, called Latent Event Model (LEM), to extract structured representation of events from social media.
p12404
aVLEM is fully unsupervised and does not require annotated data for training.
p12405
aVWe evaluate LEM on a Twitter corpus.
p12406
aVExperimental results show that the proposed model achieves 83% in F-measure, and outperforms the state-of-the-art baseline by over 7%.
p12407
aV[itemize]itemsep=0cm.
p12408
ag106
asba(icmyPackage
FText
p12409
(dp12410
g3
(lp12411
VInsight into concreteness can help systems to classify adjective-noun pairs according to their semantics
p12412
aVHowever, recent work has highlighted the application of subjectivity analysis to lexical semantics, for instance to the tasks of disambiguating words according to their usage or sense [ 28 , 2 ]
p12413
aVA principled treatment of concreteness is thus likely to be important if the multi-modal approach is to prove effective on a wider range of concepts
p12414
aVIn addition, we analyse the effect of noun concreteness and adjective subjectivity on meaning combination, illustrating how the interaction of these dimensions enables the accurate classification of adjective-noun pairs according to their semantics
p12415
aVWe conclude by discussing other potential applications of concreteness and subjectivity to NLP
p12416
aVA large and growing body of empirical evidence indicates clear differences between concrete concepts, such as donut or hotdog and abstract concepts, such as guilt or obesity
p12417
aVTogether with the practical applications outlined in Section 1, these facts indicate the potential value of concreteness for models aiming to replicate human performance in language processing tasks
p12418
aVWhile automatic methods have been proposed for the quantification of lexical concreteness, they each rely on dictionaries or similar hand-coded resources [ 15 , 27 ]
p12419
aVFor example, the utterance he sits across the table is more subjective than he sits opposite Sam as its truth depends on the speaker u'\u005cu2019' s position
p12420
aVLanguage may also be subjective because it conveys evaluations or opinions [ 19 ]
p12421
aV1985 ) theorizes that subjective adjectives tend to develop derived adverbial forms, whereas more objective adjectives do not
p12422
aVWe thus define adverbiability as the frequency of derived adverbial forms relative to the frequency of their base form, e.g
p12423
aVFollowing Wiebe, we note that the existence of comparative forms for an adjective are indicative of gradability
p12424
aVWe thus define comparability as the frequency of comparative or superlative forms relative to the frequency of the base form, e.g
p12425
aVAdamson ( 2000 ) proposes that more subjective adjectives typically occur furthest from the noun in multiple-modifier strings such as ( hot crossed buns
p12426
aVWe consequently extract the LeftTendency of our adjectives, defined as the frequency of occurrence as the leftmost of two adjectives as a proportion of the overall frequency of occurrence in multiple-modifier strings
p12427
aVAnother characteristic of gradable adjectives noted by Wiebe ( 2000 ) is that they admit degree modifiers ( very/quite delicious
p12428
aVWe therefore extract the relative frequency of occurrence with one of a hand-coded list of English degree modifiers
p12429
aVBolinger ( 1967 ) proposed that subjective adjectives occur in predicative constructions ( the cake is sweet ), rather than attributive constructions ( the German capital ) more frequently than objective adjectives
p12430
aVWe therefore extract the relative frequency of occurrence in such constructions
p12431
aVMany adjectives also function as nouns ( sweet cake vs boiled sweet
p12432
aVUnlike nouns, many adjectives are inherently subjective, and the number of adjectives in texts correlates with human judgements of their subjectivity [ 12 ]
p12433
aVWe therefore extract the frequency with which concepts are tagged as adjectives relative to as nouns, on the assumption that u'\u005cu2018' pure u'\u005cu2019' adjectives are on average more subjective than nominal-style adjectives
p12434
aVSince each of the six features takes values on [ 0 , 1 ] , we define the overall subjectivity of an adjective a with feature vector u'\u005cud835' u'\u005cudc2c' a = [ s 1 a u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' s 6 a ] as
p12435
aVTo verify the quality of our subjectivity features, we measured their performance as predictors in a logistic regression classifying the 3,250 adjectives labelled as subjective or not in the Wilson et al
p12436
aV4 4 Available at http://mpqa.cs.pitt.edu/ The combination of all features produced an overall classifiction accuracy of 79%
p12437
aVThe performance of individual features as predictors in isolation is shown in Figure 1 (top
p12438
aVFor each noun n we defined its subjectivity profile as the mean of the subjectivity vectors of its modifying adjectives
p12439
aVwhere the bag A n contains an adjective a for each occurrence of the pair ( a , n ) in the BNC
p12440
aVAs hypothesized, C u'\u005cu2062' O u'\u005cu2062' N u'\u005cu2062' C u'\u005cu2062' ( n ) was a significant predictor of the magnitude of the subjectivity profile (Pearson r = - 0.421 , p 0.01
p12441
aVIn the CONC-ExpCONC space, on the diagonal, where noun-concreteness is u'\u005cu2018' as expected u'\u005cu2019' , pairings appear to combine literally
p12442
aVWe evaluate the potential of our adjective subjectivity features, together with noun concreteness, to predict adjective-noun semantics, based on two existing classification tasks
p12443
aV2011 ) developed a list of 100 common adjective-noun pairs classified as either denotative (used literally) or connotative (non-literal) by five annotators
p12444
aVUsing an identical supervised learning procedure to Turney et al logistic regression with 10-fold cross-validation), we test whether our lexical representations based on subjectivity and concreteness convey sufficient information to perform the same classification
p12445
aVBecause a red car is both a car and red, the pair is classed as intersective , whereas dark humour , which is not literally dark, is classed as subsective
p12446
aV2011 ) is perhaps to be expected, since our model exploits the wide scope of the new Brysbaert et al
p12447
aVThe modification-type task has no performance benchmark since Boleda et al
p12448
aV2012 ) do not use their data for classification
p12449
aVIn addition, we showed that both concreteness and subjectivity improve the automatic classification of adjective-noun pairs according to compositionality or modification type (Section 4
p12450
aVWe hypothesize that concreteness and subjectivity are fundamental to human language processing because language is precisely the conveyance of information about the world from one party to another
p12451
aVAs the results suggest, they may be particularly important for capturing the intricacies of semantic composition and thus extending representations beyond the lexeme
p12452
aVOf course, two dimensions alone are not sufficient to reflect all of the subtleties of adjective and noun semantics
p12453
aVFor instance, our model classifies white spirit , a transparent cleaning product, as non-literal, since the lexical concreteness score does not allow for strong noun polysemy
p12454
aVWe aim to address these limitations in future work by integrating subjectivity and concreteness with conventially acquired semantic representations, and, ultimately, models that integrate input corresponding to the perceptual modalities
p12455
asg88
(lp12456
sg90
(lp12457
sg92
(lp12458
VWe have shown that objective adjectives are most likely to modify concrete nouns, and that non-literal combinations can emerge when this principle is violated (Section 3.
p12459
aVIndeed, the occurrence of an adjective with a more abstract noun than those it typically modifies is a strikingly consistent indicator of metaphoricity (Table 2.
p12460
aVIn addition, we showed that both concreteness and subjectivity improve the automatic classification of adjective-noun pairs according to compositionality or modification type (Section 4.
p12461
aVImportantly, a classifier with both subjectivity and concreteness features outperforms concreteness-only classifiers, including those proposed in previous work.
p12462
aVThe results underline the relevance of both subjectivity and concreteness to lexical and phrasal semantics, and their application to language processing tasks.
p12463
aVWe hypothesize that concreteness and subjectivity are fundamental to human language processing because language is precisely the conveyance of information about the world from one party to another.
p12464
aVIn decoding this signal, it is clearly informative to understand to what extent the information refers directly to the world, and also to what extent it reports a fact versus an opinion.
p12465
aVWe believe these dimensions will ultimately prove essential for computational systems aiming to replicate human performance in interpreting language.
p12466
aVAs the results suggest, they may be particularly important for capturing the intricacies of semantic composition and thus extending representations beyond the lexeme.
p12467
aVOf course, two dimensions alone are not sufficient to reflect all of the subtleties of adjective and noun semantics.
p12468
aVFor instance, our model classifies white spirit , a transparent cleaning product, as non-literal, since the lexical concreteness score does not allow for strong noun polysemy.
p12469
aVFurther, it makes no allowance for wider sentential context, which can be an important clue to modification type in such cases.
p12470
aVWe aim to address these limitations in future work by integrating subjectivity and concreteness with conventially acquired semantic representations, and, ultimately, models that integrate input corresponding to the perceptual modalities.
p12471
ag106
asg107
S'P14-2118'
p12472
sg109
(lp12473
VWe quantify the lexical subjectivity of adjectives using a corpus-based method, and show for the first time that it correlates with noun concreteness in large corpora.
p12474
aVThese cognitive dimensions together influence how word meanings combine, and we exploit this fact to achieve performance improvements on the semantic classification of adjective-noun pairs.
p12475
ag106
asba(icmyPackage
FText
p12476
(dp12477
g3
(lp12478
VRecently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases []
p12479
aVIt automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus
p12480
aVThe intuition is that any sentence which mentions a pair of entities ( e 1 and e 2 ) that participate in a relation, r , is likely to express the fact r u'\u005cu2062' ( e 1 , e 2 ) and thus forms a positive training example of r
p12481
aVSophisticated multi-instance learning algorithms [] have been proposed to address the issue by loosening the distant supervision assumption
p12482
aVOn top of that, researchers further improved performance by explicitly adding preprocessing steps [] or additional layers inside the model [] to reduce the effect of training noise
p12483
aVIn this paper, we present the first effective approach, u'\u005cud835' u'\u005cudc06' u'\u005cu2062' u'\u005cud835' u'\u005cuddce' u'\u005cud835' u'\u005cuddc2' u'\u005cud835' u'\u005cuddbd' u'\u005cud835' u'\u005cuddbe' u'\u005cud835' u'\u005cuddbd' u'\u005cu2062' u'\u005cud835' u'\u005cudda3' u'\u005cud835' u'\u005cuddb2' (distant supervision), to incorporate labeled data into distant supervision for extracting relations from sentences
p12484
aVIn contrast to simply taking the union of the hand-labeled data and the corpus labeled by distant supervision as in the previous work by Zhang et al
p12485
aV[] , we generalize the labeled data through feature selection and model this additional information directly in the latent variable approaches
p12486
aVSimply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effective since hand-labeled data will be swamped by a larger amount of distantly labeled data
p12487
aVAn effective approach must recognize that the hand-labeled data is more reliable than the automatically labeled data and so must take precedence in cases of conflict
p12488
aVConflicts cannot be limited to those cases where all the features in two examples are the same; this would almost never occur, because of the dozens of features used by a typical relation extractor []
p12489
aVInstead we propose to perform feature selection to generalize human labeled data into training guidelines , and integrate them into latent variable model
p12490
aVWe experimentally tested alternative feature sets by building supervised Maximum Entropy (MaxEnt) models using the hand-labeled data (Table 3 ), and selected an effective combination of three features from the full feature set used by Surdeanu et al., []
p12491
aVWe keep only those guidelines which make the correct prediction for a u'\u005cu2062' l u'\u005cu2062' l and at least k =3 examples in the training corpus (threshold 3 was obtained by running experiments on the development dataset
p12492
aVTo do this, we extend the MIML model [] by adding a new layer as shown in Figure 1
p12493
aVGiven a bag of sentences, u'\u005cud835' u'\u005cudc31' u'\u005cud835' u'\u005cudc22' , which mention an i th entity pair ( e 1 , e 2 ), our goal is to correctly predict which relation is mentioned in each sentence, or N u'\u005cu2062' R if none of the relations under consideration are mentioned
p12494
aVWe define relabeled relations h i u'\u005cu2062' j as following
p12495
aVThus, relation r u'\u005cu2062' ( g ) is assigned to h i u'\u005cu2062' j iff there exists a unique guideline g u'\u005cu2208' u'\u005cud835' u'\u005cudc06' , such that the feature vector u'\u005cud835' u'\u005cudc31' i u'\u005cu2062' j contains all constituents of g , i.e., entity types, a dependency path and maybe a span word, if g has one
p12496
aVwhere the last equality is due to conditional independence
p12497
aV\u005cState z i u'\u005cu2062' j * = argmax z i u'\u005cu2062' j p ( z i u'\u005cu2062' j u'\u005cud835' u'\u005cudc31' u'\u005cud835' u'\u005cudc22' , u'\u005cud835' u'\u005cudc32' u'\u005cud835' u'\u005cudc22' , u'\u005cud835' u'\u005cudc30' u'\u005cud835' u'\u005cudc33' , u'\u005cud835' u'\u005cudc30' u'\u005cud835' u'\u005cudc32' ) \u005cState h i u'\u005cu2062' j * = { r u'\u005cu2062' ( g ) , if u'\u005cu2062' u'\u005cu2203' u'\u005cu2003' g u'\u005cu2208' u'\u005cud835' u'\u005cudc06'
p12498
aVThis dataset is generated by mapping Wikipedia infoboxes into a large unlabeled corpus that consists of 1.5M documents from KBP source corpus and a complete snapshot of Wikipedia
p12499
aVWe used 40 queries as development set and the rest 160 queries (3334 entity pairs that express a relation) as the test set
p12500
aVThe official KBP evaluation is performed by pooling the system responses and manually reviewing each response, producing a hand-checked assessment data
p12501
aVTraining u'\u005cud835' u'\u005cuddac' u'\u005cud835' u'\u005cudda8' u'\u005cud835' u'\u005cuddac' u'\u005cud835' u'\u005cuddab' on a simple fusion of distantly-labeled and human-labeled datasets does not improve the maximum F-score since this hand-labeled data is swamped by a much larger amount of distant-supervised data of much lower quality
p12502
aVThe difference between u'\u005cud835' u'\u005cudc06' u'\u005cu2062' u'\u005cud835' u'\u005cuddce' u'\u005cud835' u'\u005cuddc2' u'\u005cud835' u'\u005cuddbd' u'\u005cud835' u'\u005cuddbe' u'\u005cud835' u'\u005cuddbd' u'\u005cu2062' u'\u005cud835' u'\u005cudda3' u'\u005cud835' u'\u005cuddb2' and all other systems is significant with p -value less than 0.05 according to a paired t -test assuming a normal distribution
p12503
aVWe scored our model against all 41 relations and thus replicated the actual KBP evaluation
p12504
aVWe propose a strategy to generate and select guidelines so that they are more generalized forms of labeled instances
p12505
aVOur approach significantly improves performance in practice and thus opens up many opportunities for further research in RE where only a very limited amount of labeled training data is available
p12506
asg88
(lp12507
sg90
(lp12508
sg92
(lp12509
VWe show that relation extractors trained with distant supervision can benefit significantly from a small number of human labeled examples.
p12510
aVWe propose a strategy to generate and select guidelines so that they are more generalized forms of labeled instances.
p12511
aVWe show how to incorporate these guidelines into an existing state-of-art model for relation extraction.
p12512
aVOur approach significantly improves performance in practice and thus opens up many opportunities for further research in RE where only a very limited amount of labeled training data is available.
p12513
ag106
asg107
S'P14-2119'
p12514
sg109
(lp12515
VDistant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models.
p12516
aVHowever, in some cases a small amount of human labeled data is available.
p12517
aVIn this paper, we demonstrate how a state-of-the-art multi-instance multi-label model can be modified to make use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database.
p12518
aVExperiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve.
p12519
ag106
asba(icmyPackage
FText
p12520
(dp12521
g3
(lp12522
VWhile prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios
p12523
aVThe task is to recognize whether the predicate-argument relationship, as expressed in the Hypothesis, holds implicitly also in the Text
p12524
aVTo address this task, we provide a large and freely available annotated dataset, and propose methods for coping with it
p12525
aVWhile the results reported so far on that annotation task were relatively low, we suggest that the task itself may be more complicated than what is actually required in textual inference scenarios
p12526
aVOn the other hand, the results obtained for our task, which does fit textual inference scenarios, are promising, and encourage utilizing algorithms for this task in actual inference systems
p12527
aVSecond, each NI should be classified as Definite NI (DNI) , meaning that the role filler must exist in the discourse, or Indefinite NI otherwise
p12528
aVThird, the DNI fillers should be found (DNI linking
p12529
aVComparing to the implied SRL task, our task may better fit the needs of textual inference
p12530
aVFirst, some relatively complex steps of the implied SRL task are avoided in our setting, while on the other hand it covers more relevant cases
p12531
aVMore concretely, in textual inference the candidate predicate and argument are typically identified, as they are aligned by the RTE system to a predicate and an argument of the Hypothesis
p12532
aVThus, the only remaining challenge is to verify that the sought relationship is implied in the text
p12533
aVTherefore, the sub-tasks of identifying and classifying DNIs can be avoided
p12534
aVAnother case is when an implied predicate-argument relationship holds even though the corresponding role is already filled by another argument, hence not an NI
p12535
aVConsider example 1 of Table 1
p12536
aVThis section describes a semi-automatic method for extracting candidate instances of implied predicate-argument relationship from an RTE dataset
p12537
aVThen, a human reader annotates each instance as u'\u005cu201c' Yes u'\u005cu201d' u'\u005cu2013' meaning that the implied relationship indeed holds in the Text, or u'\u005cu201c' No u'\u005cu201d' otherwise
p12538
aVBy applying this method on the RTE-6 dataset [ 1 ] , we constructed a dataset of 4022 instances, where 2271 (56%) are annotated as positive instances, and 1751 as negative ones
p12539
aVThese features do not depend on manually built resources, and hence are portable to resource-poor languages
p12540
aVFor example, example 1 in Table 1 includes the explicit relationship u'\u005cu201c' derailment of train u'\u005cu201d' , which might indicate the implied relationship u'\u005cu201c' crash of train u'\u005cu201d'
p12541
aVHence p = u'\u005cu201c' crash u'\u005cu201d' , a = u'\u005cu201c' train u'\u005cu201d' and p u'\u005cu2032' = u'\u005cu201c' derailment u'\u005cu201d'
p12542
aVThe Co-occurring predicate feature estimates the probability that a document would contain a as an argument of p , given that a appears elsewhere in that document as an argument of p u'\u005cu2032' , based on explicit predicate-argument relationships in a large corpus
p12543
aVThis is exemplified in example 1 of Table 1 , where p = u'\u005cu201c' renew u'\u005cu201d' , a = u'\u005cu201c' Patriot Act u'\u005cu201d' and a u'\u005cu2032' = u'\u005cu201c' law u'\u005cu201d'
p12544
aVAccordingly, the feature quantifies the probability that a document including the relationship p - a u'\u005cu2032' would also include the relationship p - a
p12545
aVSince our task and dataset are novel, there is no direct baseline with which we can compare this result
p12546
aVAs a reference point we mention the majority class proportion, and also report a configuration in which only features adopted from prior works (G C and S F) are utilized
p12547
aVThis comparison shows that the contribution of our new features (3%) is meaningful, which is also statistically significant with p 0.01 using Bootstrap Resampling test [ 11 ]
p12548
aVThe high results show that this task is feasible, and its solutions can be adopted as a component in textual inference systems
p12549
aVWe ran three configurations for the second feature, where in the first only syntactically expressed relationships are used, in the second all the implied relationships, as detected by a human annotator, are added, and in the third only the implied relationships detected by our algorithm are added
p12550
aVWe point out that in textual inference scenarios the candidate predicate and argument are given by the Hypothesis, while the challenge is only to verify that a predicate-argument relationship between these candidates is implied from the given Text
p12551
aVAccordingly, some complex steps necessitated in the SemEval task can be avoided, while additional relevant cases are covered
p12552
aVSimilarly, candidate arguments, which match either the expected answer type or other arguments in the question are detected too
p12553
aVConsequently, our methods which exploit the availability of the candidate predicate and argument can be adapted to this scenario as well
p12554
aVSimilarly, a typical approach for Event Extraction (a sub task of Information Extraction ) is to start by applying an entity extractor, which identifies argument candidates
p12555
aVAccordingly, candidate predicate and arguments are detected in this scenario too, while the remaining challenge is to assess the likelihood that a predicate-argument relationship holds between them
p12556
aVAn additional direction for future work is to further develop new methods for our task, possibly by incorporating SRL resources and/or linguistically oriented rules, in order to improve the results we achieved so far
p12557
asg88
(lp12558
sg90
(lp12559
sg92
(lp12560
VWe formulated the task of recognizing implied predicate-argument relationships within textual inference scenarios.
p12561
aVWe compared this task to the labeling task of SemEval 2010, where no prior information about candidate arguments in the text is available.
p12562
aVWe point out that in textual inference scenarios the candidate predicate and argument are given by the Hypothesis, while the challenge is only to verify that a predicate-argument relationship between these candidates is implied from the given Text.
p12563
aVAccordingly, some complex steps necessitated in the SemEval task can be avoided, while additional relevant cases are covered.
p12564
aVMoreover, we have shown that this simpler task is more feasibly solvable, where our 15 features achieved more than 80% accuracy.
p12565
aVWhile our dataset and algorithm were presented in the context of RTE, the same challenge and methods are applicable to other textual inference tasks as well.
p12566
aVConsider, for example, the Question Answering (QA) task.
p12567
aVTypically QA systems detect a candidate predicate that matches the question s predicate.
p12568
aVSimilarly, candidate arguments, which match either the expected answer type or other arguments in the question are detected too.
p12569
aVConsequently, our methods which exploit the availability of the candidate predicate and argument can be adapted to this scenario as well.
p12570
aVSimilarly, a typical approach for Event Extraction (a sub task of Information Extraction ) is to start by applying an entity extractor, which identifies argument candidates.
p12571
aVAccordingly, candidate predicate and arguments are detected in this scenario too, while the remaining challenge is to assess the likelihood that a predicate-argument relationship holds between them.
p12572
aVFollowing this observation, we propose future work of applying our methods to other tasks.
p12573
aVAn additional direction for future work is to further develop new methods for our task, possibly by incorporating SRL resources and/or linguistically oriented rules, in order to improve the results we achieved so far.
p12574
ag106
asg107
S'P14-2120'
p12575
sg109
(lp12576
VWe investigate recognizing implied predicate-argument relationships which are not explicitly expressed in syntactic structure.
p12577
aVWhile prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios.
p12578
aVSuch scenarios provide prior information, which substantially eases the task.
p12579
aVWe provide a large and freely available evaluation dataset for our task setting, and propose methods to cope with it, while obtaining promising results in empirical evaluations.
p12580
ag106
asba(icmyPackage
FText
p12581
(dp12582
g3
(lp12583
VWhile far from perfect, this scalar measure of metaphoricity allows different thresholds for metaphoricity so that metaphor identification can be fitted for specific tasks and datasets
p12584
aVMetaphor is a cognitive phenomenon (Lakoff Johnson, 1980, 1999) which has a significant impact on human reasoning abilities (Casasanto Jasmin, 2012; Johansson Falk Gibbs, 2012) and which, as a result, commonly appears in language in the form of metaphoric expressions (e.g.,, Deignan, 2005
p12585
aVStarting with Wilks (1978), the problem of metaphor has been approached as an identification task first identify or detect metaphoric expressions and then (1) prevent them from interfering with computational treatments of literal expressions and (2) use them to gain additional insight about a text (e.g.,, Carbonell, 1980; Neuman Nave, 2009
p12586
aVThe identification or detection task has been approached as a binary classification problem for a given unit of language (e.g.,, word, phrase, sentence) decide whether it is metaphoric or non-metaphoric
p12587
aVThe most plausible interpretation of this psycholinguistic evidence is that most linguistic units fall somewhere between metaphoric and literal, so that metaphoricity is a scalar value which influences processing gradually (and is difficult to uncover because of related factors like salience; Giora, 2002
p12588
aVIn other words, it seems clear that 18.5% of the BNC is not highly metaphoric, but rather is the sort of slightly metaphoric language that speakers are not consciously aware of because it is used so frequently
p12589
aVThis paper introduces a system for producing a scalar measurement of metaphoricity which places sentences anywhere between 0 (literal) and 1 (highly metaphoric
p12590
aVThe goal is to produce a computationally derived measurement that models the gradient nature of metaphoricity, with the result that metaphors which are clearly and consciously seen as metaphors score closer to 1 and metaphors which are not realized by speakers to be metaphoric score further from 1
p12591
aV1) it adheres more closely to the current theoretical understanding of metaphor, thus being more cognitively accurate; (2) it allows applications to control the threshold of metaphoricity when identifying metaphor, thus allowing the treatment of metaphor to be optimized for a given task
p12592
aVAn experiment was conducted to set a standard for evaluating scalar measurements of metaphoricity
p12593
aVParticipants were given a sentence and asked to identify it as u'\u005cu201c' Literal u'\u005cu201d' or u'\u005cu201c' Metaphoric u'\u005cu201d' The second task tested speakers u'\u005cu2019' ability to consistently label a given sentence as u'\u005cu201c' Not Metaphoric u'\u005cu201d' , u'\u005cu201c' Slightly Metaphoric u'\u005cu201d' , and u'\u005cu201c' Very Metaphoric u'\u005cu201d' The additional label was added in order to provide participants with a middle ground between metaphoric and literal
p12594
aVThe third task tested speakers u'\u005cu2019' ability to consistently rank three sentences according to their metaphoricity
p12595
aVAll participants were asked if they had attended a primary or elementary school conducted in English in order to ensure consistent language ability
p12596
aVOnly answers valid according to these two tests are considered in the following results
p12597
aVFor the first task, the binary identification task, the metaphoricity of a sentence was computed by taking the percentage of participants who identified it as metaphoric
p12598
aVThus, if all participants agreed that a sentence was metaphoric, then it receives a 1, while if half of the participants agreed, then it receives a 0.5
p12599
aVThe idea here is that high metaphoricity is consciously available to participants, so that the more agreement there is about metaphor the more the participants are aware of the sentence u'\u005cu2019' s metaphoricity and thus the higher its metaphoricity value should be
p12600
aVThe purpose of this is not to test a three-way distinction in metaphoricity values, but rather to improve the scale by moving intermediate sentences out of the Literal or Metaphoric categories
p12601
aVThe metaphoricity values for this experiment were calculated in the same way the percentage of participants who rated a sentence as highly metaphoric
p12602
aVThus, this measurement also is based on the idea that more participants will be consciously aware of highly metaphoric sentences, with a third category available to allow an extra distinction to be made
p12603
aVThis measurement, summarized in Table 2, is more accurate at the lower end of the scale, with many sentences receiving a 0 because participants were able to choose a category other than metaphoric
p12604
aVThe sentence in (2) above, for example, received a 0; however, the sentence in (1) above received only a 0.571, which, while high given the range of values, is still far from 1
p12605
aVThus, while the measurement makes distinctions at the top of the scale, it does not approach 1
p12606
aVThe third task gathered ordering information by presenting participants with three sentences, all of which contained the same main verb
p12607
aVThe metaphoricity value was computed by taking the percentage of participants who identified a sentence as the most metaphoric of the three given sentences
p12608
aVThis measurement, shown in Table 3, has similar averages across domains, unlike the previous measurements
p12609
aVIt tends to be better than the previous measures on the upper bound, likely because of the contextual comparison it allows
p12610
aVHowever, because sentences with the same main verb were forced into a three-way ordering, participants could not, for example, label two of the sentences as equally metaphoric
p12611
aVThus, it is possible that some of this advantage on the upper bound is a result of the task itself
p12612
aVGiven these three experiments for measuring the metaphoricity of sentences, Table 4 shows the correlations between each measure using Pearson u'\u005cu2019' s R
p12613
aVWe approach the problem by starting with an existing binary identification system and converting it to a scalar system
p12614
aVThe first step is to increase the robustness of the system u'\u005cu2019' s representation of sentences by adding additional properties
p12615
aVSecond, the fact-status property allows a distinction to be made between objects which exist as such independently of humans (e.g.,, rocks and stones) and those which exist to some degree dependent on human consciousness (e.g.,, laws and ideas
p12616
aVThird, the function-status property allows a distinction to be made between objects which encode a function (e.g.,, a screwdriver is specifically an object meant to turn screws) and those which do not encode a function (e.g.,, rocks are simply objects
p12617
aVThese WordNet senses are first mapped to SynSets and then to concepts in the sumo ontology, using existing mappings (Niles Pease, 2001, 2003
p12618
aVThus, each sentence is represented by the sumo concepts which it contains and each concept is represented by its six concept properties
p12619
aVThose features which had a significant positive relationship with the experimental results, shown in Table 5, were added together to create a rough computational measure of metaphoricity and then converted so that they fall between 0 and 1
p12620
aVThe scalar system was evaluated on the VU Amsterdam Metaphor Corpus (Steen, et al., 2010) which consists of 200,000 words from the British National Corpus divided into four genres (academic, news, fiction, and spoken; performance on the spoken genre was not evaluated for this task because it consists of many short fragmentary utterances) and manually annotated for metaphor by five raters
p12621
aVPrevious evaluations using this corpus (Dunn, 2013b) concluded that prepositions annotated as metaphoric in the corpus should not be considered metaphoric for computational purposes
p12622
aVThus, metaphorically used prepositions have been untagged as metaphoric
p12623
aVSentences with an insufficiently robust conceptual representation were removed (e.g.,, fragments
p12624
aVThe evaluation dataset thus consists of 6,893 sentences, distributed as shown in Table 6
p12625
aVFor the purposes of this evaluation, the threshold for metaphor was set independently for each genre and tied to the number of sentences containing metaphorically used words, as rated by the annotators of the corpus
p12626
aVThus, for the number x of metaphors in the genre, the x sentences with the top metaphoricity values were identified as metaphoric
p12627
aVFurther, it can be applied to any English text without the need for labelled training data
p12628
aVThus, the scalar approach performs competitively on a binary task (0.608 vs
p12629
asg88
(lp12630
sg90
(lp12631
sg92
(lp12632
g1538
asg107
S'P14-2121'
p12633
sg109
(lp12634
VThis paper presents the first computationally-derived scalar measurement of metaphoricity.
p12635
aVEach input sentence is given a value between 0 and 1 which represents how metaphoric that sentence is.
p12636
aVThis measure achieves a correlation of 0.450 (Pearson s R, p 0.01) with an experimental measure of metaphoricity involving human participants.
p12637
aVWhile far from perfect, this scalar measure of metaphoricity allows different thresholds for metaphoricity so that metaphor identification can be fitted for specific tasks and datasets.
p12638
aVWhen reduced to a binary classification evaluation using the VU Amsterdam Metaphor Corpus, the system achieves an F-Measure of 0.608, slightly lower than the comparable binary classification system s 0.638 and competitive with existing approaches.
p12639
ag106
asba(icmyPackage
FText
p12640
(dp12641
g3
(lp12642
VMonolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process (DP) models or Pitman-Yor process (PYP) models have achieved high accuracy, but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus (BTEC) due to the computational complexity
p12643
aVMany languages, especially Asian languages such as Chinese, Japanese and Myanmar, have no explicit word boundaries, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT
p12644
aVUWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required
p12645
aV[] proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data
p12646
aVWe aware that variational bayes (VB) may be used for speeding up the training of DP-based or PYP-based bilingual UWS
p12647
aVThese variables are large in number and it is not clear how to apply VB to UWS, and as far the authors aware there is no previous work related to the application of VB to monolingual UWS
p12648
aVTherefore, we have not explored VB methods in this paper, but we do show that our method is superior to the existing methods
p12649
aVThe set u'\u005cu2131' is chosen to represent an unsegmented foreign language sentence (a sequence of characters), because an unsegmented sentence can be seen as the set of all possible segmentations of the sentence denoted F , i.e., F u'\u005cu2208' u'\u005cu2131'
p12650
aVUWS learns models by maximizing the likelihood of the unsegmented corpus, formulated as
p12651
aVP ( u'\u005cu2131' k k u'\u005cu2032' u'\u005cu2131' , u'\u005cu2133' ) is the marginal probability of all the possible F u'\u005cu2208' u'\u005cu2131' that contain u'\u005cu2131' k k u'\u005cu2032' as a word, which can be calculated efficiently through dynamic programming (the process is similar to the foreward-backward algorithm in training a hidden Markov model (HMM) []
p12652
aVP ( u'\u005cu2131' k k u'\u005cu2032' u'\u005cu2131' , E , u'\u005cu212c' ) is the marginal probability of all the possible F u'\u005cu2208' u'\u005cu2131' that contain u'\u005cu2131' k k u'\u005cu2032' as a word and are aligned with E , formulated as
p12653
aV7 can be rewritten (as in IBM model 2
p12654
aVIn order to maintain both speed and accuracy, the following window function is adopted
p12655
aVwhere K is the number of characters in u'\u005cu2131' , and the k -th character is the start of the word f j , since j and J are unknown during the computation of dynamic programming u'\u005cu0394' b is the window size, u'\u005cu039b' u'\u005cu03a6' is the prior probability of an empty English word, and u'\u005cu03a3' ensures all the items sum to 1
p12656
aVThe monolingual expectation is calculated according to Eq
p12657
aVFor the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it
p12658
aVThus its complexity is U 2 times the unigram model u'\u005cu2019' s complexity
p12659
aVThe data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning []
p12660
aVThis data set mainly consists of news text 3 3 It also contains a small number of web blogs
p12661
aVThe monolingual bigram model, however, was slower to converge, so we started it from the segmentations of the unigram model, and using 10 iterations
p12662
aVNote that the comparison of speed is only for reference because the times are obtained from their respective papers
p12663
aVThe proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task; thus, it was not tested
p12664
aVThe experimental results show that the proposed UWS methods are comparable to the Stanford segmenters on the OpenMT06 corpus, while achieves a 0.96 BLEU increase on the PatentMT9 corpus
p12665
aVThis is because this corpus is out-of-domain for the supervised segmenters
p12666
aVMoreover, the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus
p12667
aVThus, we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare, and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained
p12668
aVIn future research, we plan to improve the bilingual UWS through applying VB and integrating more accurate alignment models such as HMM models and IBM model 4
p12669
asg88
(lp12670
sg90
(lp12671
sg92
(lp12672
VThis paper is devoted to large-scale Chinese UWS for SMT.
p12673
aVAn efficient unified monolingual and bilingual UWS method is proposed and applied to large-scale bilingual corpora.
p12674
aVComplexity analysis shows that our method is capable of scaling to large-scale corpora.
p12675
aVThis was verified by experiments on a corpus of 1-million sentence pairs on which traditional MCMC approaches would struggle [].
p12676
aVThe proposed method does not require any annotated data, but the SMT system with it can achieve comparable performance compared to state-of-the-art supervised word segmenters trained on precious annotated data.
p12677
aVMoreover, the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus.
p12678
aVThus, we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare, and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained.
p12679
aVIn future research, we plan to improve the bilingual UWS through applying VB and integrating more accurate alignment models such as HMM models and IBM model 4.
p12680
ag106
asg107
S'P14-2122'
p12681
sg109
(lp12682
VUnsupervised word segmentation (UWS) can provide domain-adaptive segmentation for statistical machine translation (SMT) without annotated data, and bilingual UWS can even optimize segmentation for alignment.
p12683
aVMonolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process (DP) models or Pitman-Yor process (PYP) models have achieved high accuracy, but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus (BTEC) due to the computational complexity.
p12684
aVThis paper proposes an efficient unified PYP-based monolingual and bilingual UWS method.
p12685
aVExperimental results show that the proposed method is comparable to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain.
p12686
ag106
asba(icmyPackage
FText
p12687
(dp12688
g3
(lp12689
VLarge-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT [ 21 ] significantly outperforms mainstream methods that only train on small tuning sets
p12690
aVHowever, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit
p12691
aVTo address this problem, we extend \u005cnewcite yu+:2013 to syntax-based MT by generalizing their latent variable u'\u005cu201c' violation-fixing u'\u005cu201d' perceptron from graphs to hypergraphs
p12692
aVExperiments confirm that our method leads to up to +1.2 Bleu improvement over mainstream methods such as Mert and Pro
p12693
aVWhat makes large-scale MT training so hard then
p12694
aVAfter numerous attempts by various researchers [ 17 , 20 , 1 , 2 , 5 , 9 , 10 ] , the recent work of \u005cnewcite yu+:2013 finally reveals a major reason it is the vast amount of (inevitable) search errors in MT decoding that astray learning
p12695
aVHowever, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their Ch-En experiments) of the bitext in training
p12696
aVWe generalize the latent variable violation-fixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottom-up parsing as special cases (see Fig
p12697
aVExperiments show that our training algorithm outperforms mainstream tuning methods (which optimize on small devsets) by +1.2 Bleu over Mert and Pro on FBIS
p12698
aVFor clarity reasons we will describe Hiero decoding as a two-pass process, first without a language model, and then integrating the LM
p12699
aVTo incorporate the language model, each node also needs to remember its target side boundary words
p12700
aVThus a - u'\u005cu2062' LM node N [ i j ] is split into multiple + u'\u005cu2062' LM nodes of signature N [ i j ] a u'\u005cu22c6' b , where a and b are the boundary words
p12701
aVMore formally, the whole decoding process can be cast as a deductive system
p12702
aVTake the partial translation of u'\u005cu201c' held talks with Sharon u'\u005cu201d' in Figure 2 (b) for example, the deduction is
p12703
aVwhere s u'\u005cu2062' ( r 5 ) is the score of rule r 5 , and the LM combo score u'\u005cu039b' is log P lm ( talks u'\u005cu2223' held ) P lm ( with u'\u005cu2223' talks ) P lm ( Sharon u'\u005cu2223' with )
p12704
aVAs mentioned in Section 1, the key to the success of \u005cnewcite yu+:2013 is the adoption of violation-fixing perceptron of \u005cnewcite huang+:2012 which is tailored for vastly inexact search
p12705
aVOn the other hand, \u005cnewcite zhang+:2013 has generalized \u005cnewcite huang+:2012 from graphs to hypergraphs for bottom-up parsing, which resembles Hiero decoding
p12706
aVSo we just need to combine the two generalizing directions (latent variable and hypergraph, see Fig
p12707
aVWe say D u'\u005cu2208' H u'\u005cu2062' ( x ) if D is a full derivation of decoding x , and D can be derived from the hypergraph
p12708
aVWe further denote the real decoding hypergraph with beam-pruning and cube-pruning as H u'\u005cu2032' u'\u005cu2062' ( x
p12709
aVThe set of y -bad derivations is defined as
p12710
aVThe max-violation method performs the update where the model score difference between the incorrect Viterbi partial derivation and the best y -good partial derivation is maximal, by penalizing the incorrect Viterbi partial derivation and rewarding the y -good partial derivation
p12711
aVSuch derivations can be generated in way similar to \u005cnewcite yu+:2013 by using a language model tailored for forced decoding
p12712
aVIf a boundary word does not occur in the reference, its index is set to u'\u005cu221e' so that its language model score will always be - u'\u005cu221e' ; if a boundary word occurs more than once in the reference, its - u'\u005cu2062' LM node is split into multiple + u'\u005cu2062' LM nodes, one for each such index
p12713
aV2 2 Our formulation of index-based language model fixes a bug in the word-based LM of \u005cnewcite yu+:2013 when a substring appears more than once in the reference (e.g.,  u'\u005cu201c' the man u'\u005cu2026' the man u'\u005cu2026' u'\u005cu201d' ); thanks to Dan Gildea for pointing it out
p12714
aVFollowing \u005cnewcite yu+:2013, we call our max-violation method MaxForce
p12715
aVWe find that even simple Word-Edges features boost the performance significantly, and adding complex Word-Edges features from \u005cnewcite yu+:2013 brings limited improvement and slows down the decoding
p12716
aVSo in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words, and Chinese characters, and do not use word clusters nor word types
p12717
aVFor simplicity and efficiency reasons, we also exclude all non-local features
p12718
aVIWSLT04 is used as development set in MaxForce training, and as tuning set for n -best Mert , Hypergraph Mert , and Pro
p12719
aVIWSLT05 is used as test set
p12720
aVBoth IWSLT04 and IWSLT05 contain 16 references.We mainly use this corpus to investigate the properties of MaxForce
p12721
aVNIST06 newswire is used as development set for MaxForce training, and as tuning set for all other tuning methods
p12722
aVNIST08 newswire is used as test set
p12723
aVBoth NIST06 newswire and NIST08 newswire contain 4 references
p12724
aVHowever, in the following experiments, due to efficiency considerations, we use the u'\u005cu201c' tight u'\u005cu201d' rule extraction in cdec that is more strict than the standard u'\u005cu201c' loose u'\u005cu201d' rule extraction, which generates a reduced rule set and, thus, a reduced reachability
p12725
aVWe show the reachability distributions of both tight and loose rule extraction in Figure 4
p12726
aVThe max-violation method is more than 15 Bleu points better than the standard perceptron (also known as u'\u005cu201c' bold-update u'\u005cu201d' in \u005cnewcite liang+:2006) which updates at the root of the derivation tree
p12727
aV3 3 We find that while MaxForce generates translations of length ratio close to 1 during training, the length ratios on dev/test sets are significantly lower, due to OOVs
p12728
aVSo we run a binary search for the length penalty weight after each training iteration to tune the length ratio to u'\u005cu223c' 0.97 on dev set
p12729
asg88
(lp12730
sg90
(lp12731
sg92
(lp12732
VWe have presented a latent-variable violation-fixing framework for general structured prediction problems with inexact search over hypergraphs.
p12733
aVIts application on Hiero brings significant improvement in Bleu , compared to algorithms that are specially designed for MT tuning such as Mert and Pro.
p12734
ag106
asg107
S'P14-2127'
p12735
sg109
(lp12736
VLarge-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT [ 21 ] significantly outperforms mainstream methods that only train on small tuning sets.
p12737
aVHowever, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit.
p12738
aVTo address this problem, we extend \u005cnewcite yu+:2013 to syntax-based MT by generalizing their latent variable violation-fixing perceptron from graphs to hypergraphs.
p12739
aVExperiments confirm that our method leads to up to +1.2 Bleu improvement over mainstream methods such as Mert and Pro.
p12740
aV1.5em.
p12741
aV1em.
p12742
aValgorithmAlgorithm.
p12743
ag106
asba(icmyPackage
FText
p12744
(dp12745
g3
(lp12746
VPunctuation parsing errors lead to low parsing accuracy on words
p12747
aVRather than assigning lexical heads to punctuations, we treat punctuations as properties of their neighbouring words, used as features to guide the parser to build the dependency graph
p12748
aVFirst, the lexical heads of punctuations are not as well defined as those of words
p12749
aVConsequently, punctuations are not as consistently annotated in treebanks as words, making it harder to parse punctuations
p12750
aVMoreover, experimental results showed that parsing accuracy of content words drops on sentences which contain higher ratios of punctuations
p12751
aVOne reason for this result is that projective dependency parsers satisfy the u'\u005cu201c' no crossing links u'\u005cu201d' constraint, and errors in punctuations may prevent correct word-word dependencies from being created (see section 2
p12752
aVThe fact that most previous work evaluates parsing accuracies without taking punctuations into account is also largely due to this reason
p12753
aVAlthough the improvement becomes smaller as the beam width grows larger, we still achieved 93.06 u'\u005cu2062' % UAS with a beam of width 64, which is the best result for transition-based parsers reported so far
p12754
aVWe use the Wall Street Journal portion of the Penn Treebank with the standard splits sections 02-21 are used as the training set; section 22 and section 23 are used as the development and test set, respectively
p12755
aVPenn2Malt is used to convert bracketed structures into dependencies
p12756
aVWe can see that although all the parsers achieve above 90 u'\u005cu2062' % UAS on words, the UAS on punctuations are mostly below 85 u'\u005cu2062' %
p12757
aVAs for learning, we calculate the percentage of parameter updates that are caused by associating punctuations with incorrect heads during training of the easy-first parser 3 3 For the greedy easy-first parser, whether a parameter update is caused by punctuation error can be determined with no ambiguity
p12758
aVThe result is that more than 31 u'\u005cu2062' % of the parameter updates are caused due to punctuations, though punctuations account for only 11.6 u'\u005cu2062' % of the total tokens in the training set
p12759
aVThe fact that parsers achieve low accuracies on punctuations is to some degree expected, because the head of a punctuation mark is linguistically less well-defined
p12760
aVSince long sentences are inherently more difficult to parse, to make a fair comparison, we further divide the development set according to sentence lengths as shown in the first row 4 4 1694 out of 1700 sentences on the development set with length no larger than 60 tokens
p12761
aVNote that this negative effect on parsing accuracy might be overlooked since most previous work evaluates parsing accuracy without taking punctuations into account
p12762
aVBy inspecting the parser outputs, we found that error propagation caused by assigning incorrect head to punctuations is one of the main reason that leads to this result
p12763
aVTake the sentence shown in Figure 1 (a) for example, the word Mechanisms is a modifier of entitled according to the gold reference
p12764
aVHowever, if the quotation mark, u'\u005cu201c' , is incorrectly recognized as a modifier of was , due to the u'\u005cu201c' no crossing links u'\u005cu201d' constraint, the arc between Mechanisms and entitled can never be created
p12765
aVIn this experiment, we first remove all punctuations from the original data and then modify the dependency arcs accordingly in order to maintain word-word dependencies in the original data
p12766
aVThe result indicates that by removing punctuations, we lose some information that is important for dependency parsing
p12767
aVSuch properties are used as additional features to guide the parser to construct the dependency graph
p12768
aVOur method distinguishes paired punctuations from other punctuations
p12769
aVTo utilize such boundary information, we further classify paired punctuations into two categories those that serve as the beginning of the boundary, whose POS tags are either -LRB- or u'\u005cu201c' , denoted by Bpunc ; and those that serve as the end of the boundary, denoted by Epunc
p12770
aVIn such cases, we simply remove these punctuations since the existence of paired punctuations already indicates that there should be a boundary
p12771
aVIn the example, when Congrees u'\u005cu2019' s is identified as a modifier of Buccaneers, the u'\u005cu201d' flag of Buccaneers is turned off
p12772
aVHowever, we do not assign a Paired property to Buccaneers since its ( flag is still on
p12773
aVThe only special case is that if w h already contains a Bpunc property, then our method simply ignores the non-paired property since we maintain the boundary information with the highest priority
p12774
aVFor example, the distance features in line 5 of Table 3 is used to capture the pattern that if a word w with comma property is the left modifier of a noun or a verb, the distance between w and its lexical head is often larger than 1
p12775
aVOur method is able to capture that the root, w h , of the sub-tree within the paired-punctuation, such as u'\u005cu201c' Mechanisms u'\u005cu201d' in Figure 1, generally serves as a modifier of the words outside, while the baseline parser occasionally make w h as the head of the sentence
p12776
aVAs we increase the beam width, the improvement of our method over the baseline becomes smaller
p12777
aVThis is as expected, since beam search also has the effect of reducing error propagation [ 15 ] , thereby alleviating the errors caused by punctuations
p12778
aVHowever, the exact type of errors that are corrected by using non-paired punctuations is more difficult to summarize
p12779
aVIn this work, we proposed to treat punctuations as properties of context words for dependency parsing
p12780
aVExperiments with an arc-standard parser showed that our method effectively improves parsing performance and we achieved the best accuracy for single-model transition-based parser
p12781
asg88
(lp12782
sg90
(lp12783
sg92
(lp12784
VIn this work, we proposed to treat punctuations as properties of context words for dependency parsing.
p12785
aVExperiments with an arc-standard parser showed that our method effectively improves parsing performance and we achieved the best accuracy for single-model transition-based parser.
p12786
aVRegarding punctuation processing for dependency parsing, Li et al.
p12787
aV2010 ) proposed to utilize punctuations to segment sentences into small fragments and then parse the fragments separately.
p12788
aVA similar approach is proposed by Spitkovsky et al.
p12789
aV2011 ) which also designed a set of constraints on the fragments to improve unsupervised dependency parsing.
p12790
ag106
asg107
S'P14-2128'
p12791
sg109
(lp12792
VModern statistical dependency parsers assign lexical heads to punctuations as well as words.
p12793
aVPunctuation parsing errors lead to low parsing accuracy on words.
p12794
aVIn this work, we propose an alternative approach to addressing punctuation in dependency parsing.
p12795
aVRather than assigning lexical heads to punctuations, we treat punctuations as properties of their neighbouring words, used as features to guide the parser to build the dependency graph.
p12796
aVIntegrating our method with an arc-standard parser yields a 93.06 % unlabelled attachment score, which is the best accuracy by a single-model transition-based parser reported so far.
p12797
ag106
asba(icmyPackage
FText
p12798
(dp12799
g3
(lp12800
VFinite-state chunking and tagging methods are very fast for annotating non-hierarchical syntactic information, and are often applied in applications that do not require full syntactic analyses
p12801
aVThis approach improves efficiency by bounding constituent size, and allows for efficient segmentation strategies prior to parsing
p12802
aVModels to derive such non-hierarchical annotations are finite-state, so inference is very fast
p12803
aVFor example, in incremental (simultaneous) machine translation [ 13 ] , sub-sentential segments are translated independently and sequentially, hence the fully-connected syntactic structure is not generally available
p12804
aVEven so, locally-connected source language parse structures can inform both segmentation and translation of each segment in such a translation scenario
p12805
aVWe follow the XML community in naming structures of this type hedges (not to be confused with the rhetorical device of the same name), due to the fact that they are like smaller versions of trees which occur in sequences
p12806
aVSimilar constraints have been used in dependency parsing [ 6 , 5 ] , where the use of hard constraints on the distance between heads and dependents is known as vine parsing
p12807
aVIt is also reminiscent of so-called Semi-Markov models [ 12 ] , which allow finite-state models to reason about segments rather than just tags by imposing segment length limits
p12808
aVIn such a way, hedges are sequentially connected to the top-most non-terminal in the tree, as demonstrated in Figure 1
p12809
aVAfter applying such a transform to a treebank, we can induce grammars and modify parsing to search as needed to recover just these constituents
p12810
aVIn this paper, we propose several methods to parse hedge constituents and examine their accuracy/efficiency tradeoffs
p12811
aVFor example, the span of the non-root S , SBAR , ADJP , and VP nodes in Figure 1 (a) have spans between 10 and 13, hence are removed in the tree in Figure 1 (b
p12812
aVIf we apply this transform to an entire treebank, we can use the transformed trees to induce a PCFG for parsing
p12813
aVFigure 2 plots the percentage of constituents from the original WSJ Penn treebank (sections 2-21) retained in the transformed version, as we vary the maximum span length parameter L
p12814
aVMost experiments in this paper will focus on L = 7 , which is short enough to provide a large speedup yet still cover a large fraction of constituents
p12815
aVAs stated earlier, our brute-force baseline approach is to parse the sentence using a full context-free grammar (CFG) and then hedge-transform the result
p12816
aVThis method should yield a ceiling on hedge-parsing accuracy, as it has access to rich contextual information (as compared to grammars trained on transformed trees
p12817
aVSince we limit the span of non-terminal labels, we can constrain the search performed by the parser, greatly reduce the CYK processing time
p12818
aVFurther, if the binarization systematically groups the leftmost or the rightmost children under these new non-terminals (the most common strategy), then constituents with span greater than L will either begin at the first word (leftmost grouping) or end at the last word (rightmost), further constraining the number of cells in the chart requiring work
p12819
aVNote also that these latter cells (spanning L words) may be less expensive, as the set of possible non-terminals is reduced to only those introduced by binarization
p12820
aVIt is possible to parse with a standardly induced PCFG using this sort of hedge constrained parsing that only considers a subset of the chart cells, and speedups are achieved, however this is clearly non-optimal, since the model is ill-suited to combining hedges into flat structures at the root of the tree
p12821
aVSpace constraints preclude inclusion of trials with this method, but the net result is a severe degradation in accuracy (tens of points of F-measure) versus standard parsing
p12822
aVThus, we train a grammar in a matched condition, which we call it a hedgebank grammar
p12823
aVA unique property of hedge constituents compared to constituents in the original parse trees is that they are sequentially connected to the top-most node
p12824
aVThis property enables us to chunk the sentence into segments that correspond to complete hedges, and parse the segments independently (and simultaneously) instead of parsing the entire sentence
p12825
aVWe treat this as a binary classification task which decides if a word can begin a new hedge
p12826
aVWe use hedge segmentation as a finite-state pre-processing step for hedge context-free parsing
p12827
aVOur task is to learn which words can begin ( B ) a hedge constituent
p12828
aVGiven a set of labeled pairs ( S , H ) where S is a sentence of n words w 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' w n and H is its hedge parse tree, word w b belongs to B if there is a hedge constituent spanning w b u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' w e for some e u'\u005cu2265' b and w b belongs to B ¯ otherwise
p12829
aVWe restrict the types to the most important types u'\u005cu2013' following the 11 chunk types annotated in the CoNLL-2000 chunking task [ 11 ] u'\u005cu2013' by replacing all other types with a new type OUT
p12830
aVThus, u'\u005cu201c' Analysts u'\u005cu201d' is labeled B G ; u'\u005cu201c' much u'\u005cu201d' , B u'\u005cud835' u'\u005cudc41' u'\u005cud835' u'\u005cudc43' ; u'\u005cu201c' will u'\u005cu201d' , B u'\u005cud835' u'\u005cudc49' u'\u005cud835' u'\u005cudc43' and so on
p12831
aVSegmentation accuracy is reported as an F1-score of unlabeled segment bracketing
p12832
aVWe ran timing tests on an Intel 2.66GHz processor with 3MB of cache and 2GB of memory
p12833
aVNote that segmentation time is negligible compared to the parsing time, hence is omitted in reported time
p12834
aVEfficiency results are reported as number of words parsed per second (w/s
p12835
aVThe final two rows show performance with automatic segmentation, using a model that includes either unlabeled or labeled segmentation tags, as described in the last section
p12836
aVSegmentation accuracy is better for the model with labels, although overall that accuracy is rather low
p12837
aVThe results show the same patterns as on the development set
p12838
aVFinally, Figure 3 shows the speed of inference, labeled precision and labeled recall of annotating hedge constituents on the test set as a function of the maximum span parameter L , versus the baseline parser
p12839
aVKeep in mind that the number of reference constituents increases as L increases, hence both precision and recall can decrease as the parameter grows
p12840
aVWe proposed a novel partial parsing approach for applications that require a fast syntactic analysis of the input beyond shallow bracketing
p12841
asg88
(lp12842
sg90
(lp12843
sg92
(lp12844
VWe proposed a novel partial parsing approach for applications that require a fast syntactic analysis of the input beyond shallow bracketing.
p12845
aVThe span-limit parameter allows tuning the annotation of internal structure as appropriate for the application domain, trading off annotation complexity against inference time.
p12846
aVThese properties make hedge parsing potentially very useful for incremental text or speech processing, such as streaming text analysis or simultaneous translation.
p12847
aVOne interesting characteristic of these annotations is that they allow for string segmentation prior to inference, provided that the segment boundaries do not cross any hedge boundaries.
p12848
aVWe found that baseline segmentation models did provide a significant speedup in parsing, but that cascading errors remain a problem.
p12849
aVThere are many directions of future work to pursue here.
p12850
aVFirst, the current results are all for exhaustive CYK parsing, and we plan to perform a detailed investigation of the performance of hedgebank parsing with prioritization and pruning methods of the sort available in BUBS [ 2 ].
p12851
aVFurther, this sort of annotation seems well suited to incremental parsing with beam search, which has been shown to achieve high accuracies even for fully connected parsing [ 14 ].
p12852
aVImprovements to the transform (e.g.,, grouping items not in hedges under non-terminals) and to the segmentation model (e.g.,, increasing precision at the expense of recall) could improve accuracy without greatly reducing efficiency.
p12853
aVFinally, we intend to perform an extrinsic evaluation of this parsing in an on-line task such as simultaneous translation.
p12854
ag106
asg107
S'P14-2129'
p12855
sg109
(lp12856
VFinite-state chunking and tagging methods are very fast for annotating non-hierarchical syntactic information, and are often applied in applications that do not require full syntactic analyses.
p12857
aVScenarios such as incremental machine translation may benefit from some degree of hierarchical syntactic analysis without requiring fully connected parses.
p12858
aVWe introduce hedge parsing as an approach to recovering constituents of length up to some maximum span L.
p12859
aVThis approach improves efficiency by bounding constituent size, and allows for efficient segmentation strategies prior to parsing.
p12860
aVUnlike shallow parsing methods, hedge parsing yields internal hierarchical structure of phrases within its span bound.
p12861
aVWe present the approach and some initial experiments on different inference strategies.
p12862
ag106
asba(icmyPackage
FText
p12863
(dp12864
g3
(lp12865
VWe isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon
p12866
aVThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p12867
aVWord embeddings u'\u005cu2014' representations of lexical items as points in a real vector space u'\u005cu2014' have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ]
p12868
aVWhile word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity [ 3 , 10 ]
p12869
aVSemi-supervised and unsupervised models for a variety of core NLP tasks, including named-entity recognition [ 5 ] , part-of-speech tagging [ 13 ] , and chunking [ 15 ] have been shown to benefit from the inclusion of word embeddings as features
p12870
aVIn the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally [ 7 , 1 ]
p12871
aVIt could be that the distinctions between lexical items that embeddings capture are already modeled by parsers in other ways and therefore provide no further benefit
p12872
aVIn this paper, we investigate this question empirically, by isolating three potential mechanisms for improvement from pre-trained word embeddings
p12873
aVBut we don u'\u005cu2019' t know how prevalent or important such u'\u005cu201c' syntactic axes u'\u005cu201d' are in practice
p12874
aVThus we have two questions
p12875
aVWord embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations
p12876
aVWord embeddings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words
p12877
aVA parser which exploited this effect could use this to acquire a robust model of name behavior by sharing statistics from all first names together, preventing low counts from producing noisy models of names
p12878
aVOur first task is thus to design a set of orthogonal experiments which make it possible to test each of the three hypotheses in isolation
p12879
aVIf we want to encourage similarly-embedded words to exhibit similar behavior in the generative model, we need to ensure that the are preferentially mapped onto the same latent preterminal tag
p12880
aVEach u'\u005cu0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u005cu2014' during each M step of the training procedure, u'\u005cu0391' w , t is set to the expected number of times the word w appears under the refined tag t
p12881
aVIntuitively, as u'\u005cu0392' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where u'\u005cu0392' = 0 , Equation 1 is a uniform distribution over the entire vocabulary
p12882
aVAs u'\u005cu0392' grows large words become more independent (and in the limit where u'\u005cu0392' = u'\u005cu221e' , each summand in Equation 1 is zero except where w u'\u005cu2032' = w , and we recover the original direct-lookup model
p12883
aVThis causes parsing to become unacceptably slow, so an approximation is necessary
p12884
aVEmpirically, taking k = 20 gives adequate performance, and increasing it does not seem to alter the behavior of the parser
p12885
aVAs in the OOV model, we also need to worry about how to handle words for which we have no vector representation
p12886
aVIn these cases, we simply treat the words as if their vectors were so far away from everything else they had no influence, and report their weights as p ( w t ) = u'\u005cu0391' w
p12887
aVThis ensures that our model continues to include the original Berkeley parser model as a limiting case
p12888
aVTo evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding
p12889
aVThe extensions we propose are certainly not the only way to target the hypotheses described above, but they have the advantage of being minimal and straightforwardly interpretable, and each can be reasonably expected to improve parser performance if its corresponding hypothesis is true
p12890
aVWe use the Maryland implementation of the Berkeley parser as our baseline for the kernel-smoothed lexicon, and the Maryland featured parser as our baseline for the embedding-featured lexicon
p12891
aVPer-corpus-size settings of the parameter u'\u005cu0392' are set by searching over several possible settings on the development set
p12892
aVWe begin by investigating the OOV model
p12893
aVAs can be seen, this model alone achieves small gains over the baseline for a 300-word training corpus, but these gains become statistically insignificant with more training data
p12894
aVWe began by searching over exponentially-spaced values of u'\u005cu0392' to determine an optimal setting for each training set size; as expected, for small settings of u'\u005cu0392' (corresponding to aggressive smoothing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance
p12895
aVThe first block in Table 1 shows the best settings of u'\u005cu0392' for each corpus size; as can be seen, this also gives a small improvement on the 300-sentence training corpus, but no discernible once the system has access to a few thousand labeled sentences
p12896
aVA baseline featured model ( u'\u005cu201c' ident u'\u005cu201d' ) contains only indicator features on word identity (and performs considerably worse than its generative counterpart on small data sets
p12897
aVAs described above, the full featured model adds indicator features on the bucketed value of each dimension of the word embedding
p12898
aVHere, the trend observed in the other two models is even more prominent u'\u005cu2014' embedding features lead to improvements over the featured baseline, but in no case outperform the standard baseline with a generative lexicon
p12899
aVWe take the best-performing combination of all of these models (based on development experiments, a combination of the lexical pooling model with u'\u005cu0392' = 0.3 , and OOV, both using c w word embeddings), and evaluate this on the WSJ test set (Table 2
p12900
aVApparent gains from the OOV and lexicon pooling models remain so small as to be statistically indistinguishable
p12901
aVEvaluation of these modified parsers revealed modest gains on extremely small training sets, which quickly vanish as training set size increases
p12902
aVThus, at least restricted to phenomena which can be explained by the experiments described here, our results are consistent with two claims
p12903
aVHowever, the failure to uncover gains when searching across a variety of possible mechanisms for improvement, training procedures for embeddings, hyperparameter settings, tasks, and resource scenarios suggests that these gains (if they do exist) are extremely sensitive to these training conditions, and not nearly as accessible as they seem to be in dependency parsers
p12904
aVIndeed, our results suggest a hypothesis that word embeddings are useful for dependency parsing (and perhaps other tasks) because they provide a level of syntactic abstraction which is explicitly annotated in constituency parses
p12905
asg88
(lp12906
sg90
(lp12907
sg92
(lp12908
VWith the goal of exploring how much useful syntactic information is provided by unsupervised word embeddings, we have presented three variations on a state-of-the-art parsing model, with extensions to the out-of-vocabulary model, lexicon, and feature set.
p12909
aVEvaluation of these modified parsers revealed modest gains on extremely small training sets, which quickly vanish as training set size increases.
p12910
aVThus, at least restricted to phenomena which can be explained by the experiments described here, our results are consistent with two claims.
p12911
aV1) unsupervised word embeddings do contain some syntactically useful information, but (2) this information is redundant with what the model is able to determine for itself from only a small amount of labeled training data.
p12912
aVIt is important to emphasize that these results do not argue against the use of continuous representations in a parser s state space, nor argue more generally that constituency parsers cannot possibly benefit from word embeddings.
p12913
aVHowever, the failure to uncover gains when searching across a variety of possible mechanisms for improvement, training procedures for embeddings, hyperparameter settings, tasks, and resource scenarios suggests that these gains (if they do exist) are extremely sensitive to these training conditions, and not nearly as accessible as they seem to be in dependency parsers.
p12914
aVIndeed, our results suggest a hypothesis that word embeddings are useful for dependency parsing (and perhaps other tasks) because they provide a level of syntactic abstraction which is explicitly annotated in constituency parses.
p12915
aVWe leave explicit investigation of this hypothesis for future work.
p12916
ag106
asg107
S'P14-2133'
p12917
sg109
(lp12918
VDo continuous word embeddings encode any useful information for constituency parsing.
p12919
aVWe isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon.
p12920
aVWe test each of these hypotheses with a targeted change to a state-of-the-art baseline.
p12921
aVDespite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data.
p12922
aVOur results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.
p12923
ag106
asba(icmyPackage
FText
p12924
(dp12925
g3
(lp12926
VWe introduce a model for incorporating contextual information (such as geography) in learning vector-space representations of situated language
p12927
aVThe vast textual resources used in NLP u'\u005cu2013' newswire, web text, parliamentary proceedings u'\u005cu2013' can encourage a view of language as a disembodied phenomenon
p12928
aVThe rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time
p12929
aVOne desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences u'\u005cu2013' while wicked is used throughout the United States to mean bad or evil ( u'\u005cu201c' he is a wicked man u'\u005cu201d' ), in New England it is used as an adverbial intensifier ( u'\u005cu201c' my boy u'\u005cu2019' s wicked smart u'\u005cu201d'
p12930
aVIn bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations [] , this work generally exploits information about the object being described (e.g.,, strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker u'\u005cu2019' s perspective
p12931
aVThe model we introduce is grounded in the distributional hypothesis [] , that two words are similar by appearing in the same kinds of contexts (where u'\u005cu201c' context u'\u005cu201d' itself can be variously defined as the bag or sequence of tokens around a target word, either by linear distance or dependency path
p12932
aVWe can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts
p12933
aVHere, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs
p12934
aVGiven the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks [] , we use a simple, but state-of-the-art neural architecture [] to learn low-dimensional real-valued representations of words
p12935
aVThe final prediction over the output vocabulary is then found by passing this resulting vector through the softmax function u'\u005cud835' u'\u005cudc90' = softmax u'\u005cu2062' ( X u'\u005cu2062' u'\u005cud835' u'\u005cudc89' ) , giving a vector in the
p12936
aVA joint model has three a priori advantages over independent models i) sharing data across variable values encourages representations across those values to be similar; e.g.,, while city may be closer to Boston in Massachusetts and Chicago in Illinois, in both places it still generally connotes a municipality ; (ii) such sharing can mitigate data sparseness for less-witnessed areas; and (iii) with a joint model, all representations are guaranteed to be in the same vector space and can therefore be compared to each other; with individual models (each with different initializations), word vectors across different states may not be directly compared
p12937
aVIn all experiments, the contextual variable is the observed US state (including DC), so that u'\u005cud835' u'\u005cudc9e'
p12938
aVTable 2 likewise presents the terms with the highest cosine similarity to city in both California and New York; while the terms most evoked by city in California include regional locations like Chinatown, Los Angeles u'\u005cu2019' South Bay and San Francisco u'\u005cu2019' s East Bay, in New York the most similar terms include hamptons , upstate and borough (New York City u'\u005cu2019' s term of administrative division
p12939
aVAs a quantitative measure of our model u'\u005cu2019' s performance, we consider the task of judging semantic similarity among words whose meanings are likely to evoke strong geographical correlations
p12940
aVIn the absence of a sizable number of linguistically interesting terms (like wicked ) that are known to be geographically variable, we consider the proxy of estimating the named entities evoked by specific terms in different geographical regions
p12941
aVAs noted above, geographic terms like city provide one such example in Massachusetts we expect the term city to be more strongly connected to grounded named entities like Boston than to other US cities
p12942
aVNote that this is not the same as simply asking which sports team is most frequently (or most characteristically) mentioned in a given area; by measuring the distance to a target word ( football ), we are attempting to estimate the varying strengths of association between concepts in different regions
p12943
aVWe introduced a model for leveraging situational information in learning vector-space representations of words that are sensitive to the speaker u'\u005cu2019' s social context
p12944
aVBy allowing all words in different regions (or more generally, with different metadata factors) to exist in the same vector space, we are able compare different points in that space u'\u005cu2013' for example, to ask what terms used in Chicago are most similar to hot dog in New York, or what word groups shift together in the same region in comparison to the background (indicating the shift of an entire semantic field
p12945
asg88
(lp12946
sg90
(lp12947
sg92
(lp12948
VWe introduced a model for leveraging situational information in learning vector-space representations of words that are sensitive to the speaker s social context.
p12949
aVWhile our results use geographical information in learning low-dimensional representations, other contextual variables are straightforward to include as well; incorporating effects for time such as time of day, month of year and absolute year may be a powerful tool for revealing periodic and historical influences on lexical semantics.
p12950
aVOur approach explores the degree to which geography, and other contextual factors, influence word meaning in addition to frequency of usage.
p12951
aVBy allowing all words in different regions (or more generally, with different metadata factors) to exist in the same vector space, we are able compare different points in that space for example, to ask what terms used in Chicago are most similar to hot dog in New York, or what word groups shift together in the same region in comparison to the background (indicating the shift of an entire semantic field.
p12952
aVAll datasets and software to support these geographically-informed representations can be found at http://www.ark.cs.cmu.edu/geoSGLM.
p12953
ag106
asg107
S'P14-2134'
p12954
sg109
(lp12955
VWe introduce a model for incorporating contextual information (such as geography) in learning vector-space representations of situated language.
p12956
aVIn contrast to approaches to multimodal representation learning that have used properties of the object being described (such as its color), our model includes information about the subject (i.e.,, the speaker), allowing us to learn the contours of a word s meaning that are shaped by the context in which it is uttered.
p12957
aVIn a quantitative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation.
p12958
aVnoitemsep,topsep=10pt,parsep=0pt,partopsep=0pt \u005csetenumerate noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt.
p12959
ag106
asba(icmyPackage
FText
p12960
(dp12961
g3
(lp12962
VSince perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types
p12963
aVThe potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts
p12964
aVIn light of these considerations, we propose a novel algorithm for approximating conceptual concreteness
p12965
aVMulti-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in capturing the semantic similarity of concepts
p12966
aVThey are also more scalable since high-quality tagged images are freely available in several web-scale image datasets
p12967
aVWe use Google Images as our image source, and extract the first n image results for each concept word
p12968
aVBy exploiting this connection, the method approximates the concreteness of concepts, and provides a basis to filter the corresponding perceptual information
p12969
aVFormally, we propose a measure, image dispersion d of a concept word w , defined as the average pairwise cosine distance between all the image representations { w 1 u'\u005cu2192' u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' w n u'\u005cu2192' } in the set of images for that concept
p12970
aVWe use an average pairwise distance-based metric because this emphasizes the total variation more than e.g., the mean distance from the centroid
p12971
aVPrevious NLP-related work uses SIFT [ 11 , 6 ] or SURF [ 22 ] descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors
p12972
aVWe apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly well-suited for object categorization, a key component of image similarity and thus dispersion [ 5 ]
p12973
aVPHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches [ 5 ]
p12974
aVThis model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping [ 19 ]
p12975
aVWe evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity u'\u005cu2013' a standard measure for evaluating the quality of representations (see e.g., Agirre et al
p12976
aVWordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g., [ 15 , 6 ]
p12977
aVAs a complementary gold-standard, we use the University of South Florida Norms (USF) [ 20 ]
p12978
aVThe USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators
p12979
aVWe apply image dispersion-based filtering as follows if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included
p12980
aVFor both datasets, we set the threshold as the median image dispersion, although performance could in principle be improved by adjusting this parameter
p12981
aVWe compare dispersion filtered representations with linguistic, perceptual and standard multi-modal representations (concatenated linguistic and perceptual representations
p12982
aVSimilarity between concept pairs is calculated using cosine similarity
p12983
aVAs Figure 3 shows, dispersion-filtered multi-modal representations significantly outperform standard multi-modal representations on both evaluation datasets
p12984
aVBased on the correlation comparison method of Steiger ( 1980 ) , both represent significant improvements (WordSim353, t = 2.42 , p 0.05 ; USF, t = 1.86 , p 0.1
p12985
aVThe filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts
p12986
aVSince research has demonstrated the applicability of concreteness to a range of other NLP tasks [ 28 , 16 ] , it is important to examine the connection between image dispersion and concreteness in more detail
p12987
aVTo evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A u'\u005cu222a' C introduced in Section 2
p12988
aVAccording to the Dual Coding Theory, however, concrete concepts are precisely those with a salient perceptual representation
p12989
aVAs illustrated in Figure 4 , our binary classification conforms to this characterization
p12990
aVThe importance of the visual modality is significantly greater when evaluating on pairs for which both concepts are classified as concrete than on pairs of two abstract concepts
p12991
aVImage dispersion is also an effective predictor of concreteness on samples for which the abstract/concrete distinction is less clear
p12992
aVOn this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying the very abstract or very concrete concepts
p12993
aVAs Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract
p12994
aVKwong ( 2008 ) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept
p12995
aV2011 ) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology [ 10 ]
p12996
aVThe Turney et al algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space
p12997
aVIt is therefore more scalable, and instantly applicable to a wide range of languages
p12998
aVFinally, we explored whether image dispersion can be applied to specific NLP tasks as an effective proxy for concreteness
p12999
aV2011 ) showed that concreteness is applicable to the classification of adjective-noun modification as either literal or non-literal
p13000
aVBy applying a logistic regression with noun concreteness as the predictor variable, Turney et al achieved a classification accuracy of 79% on this task
p13001
aVWe presented a novel method, image dispersion-based filtering, that improves multi-modal representations by approximating conceptual concreteness from images and filtering model input
p13002
aVTo our knowledge, our algorithm constitutes the first unsupervised method for quantifying conceptual concreteness as applied to NLP, although it does, of course, rely on the Google Images retrieval algorithm
p13003
aVMoreover, we presented a method to classify adjective-noun pairs according to modification type that exploits the link between image dispersion and concreteness
p13004
asg88
(lp13005
sg90
(lp13006
sg92
(lp13007
VWe presented a novel method, image dispersion-based filtering, that improves multi-modal representations by approximating conceptual concreteness from images and filtering model input.
p13008
aVThe results clearly show that including more perceptual input in multi-modal models is not always better.
p13009
aVMotivated by this fact, our approach provides an intuitive and straightforward metric to determine whether or not to include such information.
p13010
aVIn addition to improving multi-modal representations, we have shown the applicability of the image dispersion metric to several other tasks.
p13011
aVTo our knowledge, our algorithm constitutes the first unsupervised method for quantifying conceptual concreteness as applied to NLP, although it does, of course, rely on the Google Images retrieval algorithm.
p13012
aVMoreover, we presented a method to classify adjective-noun pairs according to modification type that exploits the link between image dispersion and concreteness.
p13013
aVIt is striking that this apparently linguistic problem can be addressed solely using the raw data encoded in images.
p13014
aVIn future work, we will investigate the precise quantity of perceptual information to be included for best performance, as well as the optimal filtering threshold.
p13015
aVIn addition, we will explore whether the application of image data, and the interaction between images and language, can yield improvements on other tasks in semantic processing and representation.
p13016
ag106
asg107
S'P14-2135'
p13017
sg109
(lp13018
VModels that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition.
p13019
aVHowever, experiments suggest that while the inclusion of perceptual input improves representations of certain concepts, it degrades the representations of others.
p13020
aVWe propose an unsupervised method to determine whether to include perceptual input for a concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings.
p13021
aVThe method relies solely on image data, and can be applied to a variety of other NLP tasks.
p13022
ag106
asba(icmyPackage
FText
p13023
(dp13024
g3
(lp13025
VIn this paper, we explore latent features of temporality for understanding relation equivalence, and empirically show that the explicit and latent features complement each other
p13026
aVHowever, not all NE translations are based on transliterations, but they might be based on semantics (e.g.,, u'\u005cu2018' WTO u'\u005cu2019' u'\u005cu2192' u'\u005cu2018' {CJK*} UTF8zhfsä¸è´¸ç»ç» u'\u005cu2019' [ShiMaoZuZhi]), or even arbitrary (e.g.,, u'\u005cu2018' Jackie Chan u'\u005cu2019' u'\u005cu2192' u'\u005cu2018' {CJK*} UTF8zhfsæé¾ u'\u005cu2019' [ChengLong]
p13027
aVTo address this challenge, current state-of-the-art approaches build an entity graph for each language corpus, and align the two graphs by propagating the seed translation similarities (Figure 1 ) [ 7 , 17 ]
p13028
aVFor example, arbitrary translation pair such as (Jackie Chan, {CJK*} UTF8zhfsæé¾) can be obtained, if he is connected to his film u'\u005cu2018' Drunken Master u'\u005cu2019' ( {CJK*} UTF8zhfséæ³) in both graphs
p13029
aVA key contribution of this paper is using relation temporality for determining relation equivalence
p13030
aVOut of 158 randomly chosen correct relation translation pairs we labeled, 64% has only one co-occurring entity pair, which makes EF not very effective to identify these relation translations
p13031
aVTherefore, we leverage relation temporality , which is both orthogonal and complementary to existing efforts leveraging entity temporality [ 8 , 6 , 16 ]
p13032
aVBased on these challenges, we identify three new features for LF
p13033
aVIn our experiments, we use a non-selective (hence not requiring relation translations) propagation approach [ 17 ] with [ 10 ] for a base translation matrix
p13034
aVIn clear contrast, by discovering novel latent features based on temporal properties, we can increase the accuracy of both entity and relation translations
p13035
aVIn this section, we briefly illustrate a baseline method EF [ 11 ]
p13036
aVAs we mentioned in the introduction, traditional approaches leverage common co-occurring entity-pairs
p13037
aVThis observation also holds in the bilingual environment by exploiting seed entity translations
p13038
aVSpecifically, we quantify this similarity based on the number of such common entity pairs that we denote as
p13039
aVOur baseline implementation uses the one by [ 11 ] , and we refer the reader to the paper for formal definitions and processing steps we omitted due to the page limit
p13040
aVUnfortunately, this approach suffers from sparsity of the common entity pairs due to the incomparability of the corpora and those entities that cannot be translated by T N
p13041
aVTherefore, we leverage corpus latent features as an additional signal to overcome this problem
p13042
aVTo address the first challenge, we use a finer-granularity unit for observing the temporality
p13043
aVBut in this section, we only use d [ e , r , * ] for readability
p13044
aVAs shown in Figure 5 , d [ e , r , * ] is more distinctive and hence a key clue to find semantically equivalent relations
p13045
aV[C2] Considering entity-relation coupling distribution d [ e , r , * ] alone is not sufficient due to the domination of individual temporality
p13046
aVIf an entity has a peak at some period (Figure 8 ), most relations that are coupled with the entity would have a peak at the very same period (Figure 8
p13047
aV[C3] Lastly, we have to eliminate false positives in relation temporality
p13048
aVFor example, we can obtain u'\u005cu201c' Russia deployed an aircraft carrier u'\u005cu201d' , but not u'\u005cu201c' Russia deployed at ( {CJK*} UTF8zhfså¨ u'\u005cu2026' é¨ç½²) an aircraft carrier u'\u005cu201d'
p13049
aVThus, we cannot acquire any common entity pair like (Russia, aircraft carrier) for deploy and {CJK*} UTF8zhfså¨ u'\u005cu2026' é¨ç½² (deploy at
p13050
aVThe temporal similarity of the couplings, where J u'\u005cu2062' S u'\u005cu2062' D u'\u005cu2062' ( P , Q ) is the Jensen-Shannon divergence of two distributions P and Q , defined as J S D ( P , Q ) = 1 2 D ( P
p13051
aVAdditionally, we use the following features to consider absolute frequencies f u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q u'\u005cu2062' ( u'\u005cu22c5' ) of textual elements as well because 1) we are more confident with more evidence and 2) in the comparable corpora, the equivalent elements are likely to show similar frequencies
p13052
aVThat is, for ( r E , r C ) , we redefine the score as T R L u'\u005cu2062' F u'\u005cu2062' ( r E , r C ) / u'\u005cu2211' i u'\u005cu2208' [ 1 , k ] T R L u'\u005cu2062' F u'\u005cu2062' ( r E , r C r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' k i ) where r C r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' k i is the i -th rank Chinese relation for r E by Equation 2
p13053
aVThat is, we always {CJK*} UTF8zhfsè®¨è®º (discuss) before we {CJK*} UTF8zhfsæ¹å (ratify) something and hence the temporal behavior of {CJK*} UTF8zhfsè®¨è®º (discuss) is also very similar to that of ratify
p13054
aVOn the other hand, it can be correctly translated using EF
p13055
aVThus, we produce the hybrid relation translation, and we empirically set u'\u005cu039b' = 0.4
p13056
aVThe number of English articles is 100,746, and that of Chinese articles is 88,031
p13057
aVAs we can see from the difference in the numbers of the documents, the news corpora are not direct translations, but they have asymmetry of entities and relations
p13058
aVTo measure the effectiveness, we use a set of gold standard entity translation pairs which consist of 221 person entities and 52 organization entities
p13059
aVWe measure precision, recall, and F1-score based on the returned translation pairs for each English entity as it is done in [ 11 ]
p13060
aVWe compare our hybrid approach, which is denoted by LF+EF with EF [ 11 ] , a combined approach PH+SM of phonetic similarity and letter-wise semantic translation for better accuracy for organizations [ 10 ] , and the seed translations Seed that we adopt [ 17 ] with PH+SM as a base translation matrix
p13061
aV3 3 Our results leveraging relational temporality outperforms the reported results using entity temporality on the same data set
p13062
aVOut of 3342 pairs, 399 are labeled as correct
p13063
aVTable 3 shows the comparisons of LF, EF and their hybrid LF+EF
p13064
aVWe can clearly see that LF shows higher recall than EF while EF shows higher precision
p13065
aVAs we emphasized in Section 3.3 , we can see their complementary property
p13066
aVTheir hybrid LF+EF has both high precision and recall, thus has the highest F1-score
p13067
aVNote that the absolute numbers (due to the harsh evaluation criteria) may look low
p13068
aVIn addition, the lower ranked but correct relation translations also affect entity translation
p13069
aVTherefore, even lower-performing EF boosted the entity translations, and in effect, our approach could achieve higher F1-score in the entity translation task
p13070
asg88
(lp13071
sg90
(lp13072
sg92
(lp13073
VThis paper studied temporality features for relation equivalence.
p13074
aVWith the proposed features, we devised a hybrid approach combining corpus latent and explicit features with complementary strength.
p13075
aVWe empirically showed the effectiveness of our hybrid approach on relation translation, and it, in turn, improved entity translation.
p13076
ag106
asg107
S'P14-2137'
p13077
sg109
(lp13078
VThis paper demonstrates the importance of relation equivalence for entity translation pair discovery.
p13079
aVExisting approach of understanding relation equivalence has focused on using explicit features of co-occurring entities.
p13080
aVIn this paper, we explore latent features of temporality for understanding relation equivalence, and empirically show that the explicit and latent features complement each other.
p13081
aVOur proposed hybrid approach of using both explicit and latent features improves relation translation by 0.16 F1-score, and in turn improves entity translation by 0.02.
p13082
ag106
asba(icmyPackage
FText
p13083
(dp13084
g3
(lp13085
VThe relative frequencies of character bigrams appear to contain much information for predicting the first language (L1) of the writer of a text in another language (L2
p13086
aVIn particular, word n -gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy [ 13 ]
p13087
aVThere is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguistic perspective, we are more interested in what characteristics of L1 show up in L2 texts
p13088
aVThe authors propose the following hypothesis to explain this finding u'\u005cu201c' the choice of words [emphasis added] people make when writing in a second language is strongly influenced by the phonology of their native language u'\u005cu201d'
p13089
aVAs the orthography of alphabetic languages is at least partially representative of the underlying phonology, character bigrams may capture these phonological preferences
p13090
aVWe conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1
p13091
aVThe classifier computes a weight for each feature coupled with each L1 language by attempting to maximize the overall accuracy on the training set
p13092
aVFor example, if we train the classifier using words as features, with values representing their frequency relative to the length of the document, the features corresponding to the word China might receive the following weights
p13093
aVThese weights indicate that the word provides strong positive evidence for Chinese as L1, as opposed to the other four languages
p13094
aVWe propose to quantify the importance of each word by converting its SVM feature weights into a single score using the following formula
p13095
aVWe use the Euclidean norm rather than the sum of raw weights because we are interested in the discriminative power of the words
p13096
aVWe normalize the word scores by dividing them by the score of the 200th word
p13097
aVConsequently, only the top 200 words have scores greater than or equal to 1.0
p13098
aVFor example, the bigram an occurs in words like Japan , German , and Italian , but also by itself as a determiner, as an adjectival suffix, and as part of the conjunction and
p13099
aVTherefore, we calculate the importance score for each character bigram by multiplying the scores of each word in which the bigram occurs
p13100
aVWe follow the setup of Tsur and Rappoport ( 2007 ) by extracting two sets, denoted I1 and I2 (Table 1 ), from the International Corpus of Learner English (ICLE), Version 2 [ 10 ]
p13101
aVWe replicate the experiments of Tsur and Rappoport ( 2007 ) by limiting the features to the 200 most frequent character bigrams
p13102
aVHowever, we limit the set of features to the 200 most frequent bigrams for the sake of consistency with previous work
p13103
aVWe use these feature vectors as input to the SVM-Multiclass classifier [ 14 ]
p13104
aVThe results are shown in the Baseline column of Table 2
p13105
aVUsing Algorithm 2 , we identify the 100 most discriminative words, and remove them from the training data
p13106
aVWe see a statistically significant drop in the accuracy of the classifier with respect to the baseline in all sets except T3
p13107
aVThe words that are identified as the most discriminative include function words, punctuation, very common content words, and the toponymic terms
p13108
aVUsing Algorithm 2 , we identify the top 20 character bigrams, and replace them with randomly selected bigrams
p13109
aVThe results of this experiment are reported in the Indicative Bigrams column of Table 2
p13110
aVThe remaining bigrams indicate function words, toponymic terms like Germany , and frequent content words like take and new
p13111
aVHowever, the fact that the two bigrams are also on the list for the I2 set, which does not include these languages, suggests that their importance is mostly due to the function words
p13112
aVIf the hypothesis of Tsur and Rappoport ( 2007 ) was true, this should not be the case, as the phonology of L1 would influence the choice of words across the lexicon
p13113
aVIf the hypothesis was true, this should not be the case, as the diverse L1 phonologies would induce different sets of bigrams
p13114
aVOur explanation concurs with the findings of Daland ( 2013 ) that unigram frequency differences in certain types of phonological segments between child-directed and adult-directed speech are due to a small number of word types, such as you , what , and want , rather than to any general phonological preferences
p13115
aVWe conclude by noting that our experimental results do not imply that the phonology of L1 has absolutely no influence on L2 writing
p13116
aVWe thank the participants and the organizers of the shared task on NLI at the BEA8 workshop for sharing their reflections on the task
p13117
aVWe also thank an anonymous reviewer for pointing out the study of Daland ( 2013 )
p13118
asg88
(lp13119
sg90
(lp13120
sg92
(lp13121
VWe have provided experimental evidence against the hypothesis that the phonology of L1 strongly affects the choice of words in L2.
p13122
aVWe showed that a small set of high-frequency function words have disproportionate influence on the accuracy of a bigram-based NLI classifier, and that the majority of the indicative bigrams appear to be independent of L1.
p13123
aVThis suggests an alternative explanation of the effectiveness of a bigram-based classifier in identifying the native language of a writer that the character bigrams simply mirror differences in the word usage rather than the phonology of L1.
p13124
aVOur explanation concurs with the findings of Daland ( 2013 ) that unigram frequency differences in certain types of phonological segments between child-directed and adult-directed speech are due to a small number of word types, such as you , what , and want , rather than to any general phonological preferences.
p13125
aVHe argues that the relative frequency of sounds in speech is driven by the relative frequency of words.
p13126
aVIn a similar vein, Koppel et al.
p13127
aV2005 ) see the usefulness of character n -grams as simply an artifact of variable usage of particular words, which in turn might be the result of different thematic preferences, or as a reflection of the L1 orthography.
p13128
aVWe conclude by noting that our experimental results do not imply that the phonology of L1 has absolutely no influence on L2 writing.
p13129
aVRather, they show that the evidence from the Native Language Identification task has so far been inconclusive in this regard.
p13130
ag106
asg107
S'P14-2138'
p13131
sg109
(lp13132
VThe relative frequencies of character bigrams appear to contain much information for predicting the first language (L1) of the writer of a text in another language (L2.
p13133
aVTsur and Rappoport ( 2007 ) interpret this fact as evidence that word choice is dictated by the phonology of L1.
p13134
aVIn order to test their hypothesis, we design an algorithm to identify the most discriminative words and the corresponding character bigrams, and perform two experiments to quantify their impact on the L1 identification task.
p13135
aVThe results strongly suggest an alternative explanation of the effectiveness of character bigrams in identifying the native language of a writer.
p13136
ag106
asba.